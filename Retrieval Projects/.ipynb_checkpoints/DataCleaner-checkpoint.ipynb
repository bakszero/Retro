{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First , we import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import copy\n",
    "\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "path = 'WinnersInterviewBlogPosts.csv'\n",
    "data = pd.read_csv(path, header=None, names=['Title', 'Link', 'Publication_Date', 'Content'])\n",
    "data = data.drop(data.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Publication_Date</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer scientist Jure Zbontar on winning the...</td>\n",
       "      <td>http://blog.kaggle.com/2010/06/09/computer-sci...</td>\n",
       "      <td>2010-06-09 18:22:29</td>\n",
       "      <td>My approach was actually quite simple. The onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How I won the  Predict HIV Progression data mi...</td>\n",
       "      <td>http://blog.kaggle.com/2010/08/09/how-i-won-th...</td>\n",
       "      <td>2010-08-09 12:35:46</td>\n",
       "      <td>&lt;strong&gt;Initial Strategy&lt;/strong&gt;\\r\\n\\r\\nThe g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How I did it: Lee Baker on winning Tourism For...</td>\n",
       "      <td>http://blog.kaggle.com/2010/09/27/how-i-did-it...</td>\n",
       "      <td>2010-09-27 18:30:25</td>\n",
       "      <td>&lt;strong&gt;About me:&lt;/strong&gt;\\r\\nI’m an embedded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How I did it: The top three from the 2010 INFO...</td>\n",
       "      <td>http://blog.kaggle.com/2010/10/11/how-i-did-it...</td>\n",
       "      <td>2010-10-11 13:31:35</td>\n",
       "      <td>The 2010 INFORMS Data Mining Contest has just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How I did it: Jeremy Howard on finishing second</td>\n",
       "      <td>http://blog.kaggle.com/2010/11/19/how-i-did-it...</td>\n",
       "      <td>2010-11-19 17:39:47</td>\n",
       "      <td>Wow, this is a surprise! I looked at this comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "1  Computer scientist Jure Zbontar on winning the...   \n",
       "2  How I won the  Predict HIV Progression data mi...   \n",
       "3  How I did it: Lee Baker on winning Tourism For...   \n",
       "4  How I did it: The top three from the 2010 INFO...   \n",
       "5    How I did it: Jeremy Howard on finishing second   \n",
       "\n",
       "                                                Link     Publication_Date  \\\n",
       "1  http://blog.kaggle.com/2010/06/09/computer-sci...  2010-06-09 18:22:29   \n",
       "2  http://blog.kaggle.com/2010/08/09/how-i-won-th...  2010-08-09 12:35:46   \n",
       "3  http://blog.kaggle.com/2010/09/27/how-i-did-it...  2010-09-27 18:30:25   \n",
       "4  http://blog.kaggle.com/2010/10/11/how-i-did-it...  2010-10-11 13:31:35   \n",
       "5  http://blog.kaggle.com/2010/11/19/how-i-did-it...  2010-11-19 17:39:47   \n",
       "\n",
       "                                             Content  \n",
       "1  My approach was actually quite simple. The onl...  \n",
       "2  <strong>Initial Strategy</strong>\\r\\n\\r\\nThe g...  \n",
       "3  <strong>About me:</strong>\\r\\nI’m an embedded ...  \n",
       "4  The 2010 INFORMS Data Mining Contest has just ...  \n",
       "5  Wow, this is a surprise! I looked at this comp...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My approach was actually quite simple. The onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;strong&gt;Initial Strategy&lt;/strong&gt;\\r\\n\\r\\nThe g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;strong&gt;About me:&lt;/strong&gt;\\r\\nI’m an embedded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 2010 INFORMS Data Mining Contest has just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow, this is a surprise! I looked at this comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content\n",
       "1  My approach was actually quite simple. The onl...\n",
       "2  <strong>Initial Strategy</strong>\\r\\n\\r\\nThe g...\n",
       "3  <strong>About me:</strong>\\r\\nI’m an embedded ...\n",
       "4  The 2010 INFORMS Data Mining Contest has just ...\n",
       "5  Wow, this is a surprise! I looked at this comp..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = data.iloc[:, 3:4]\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list=[]\n",
    "str =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a normalised python list from numpy array list, and convert the result into a string for processing via BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My approach was actually quite simple. The only attributes I used where the approximate betting odds and the information on past voting. I sought patterns in the voting behaviour of all countries and combined that knowledge with this year's betting odds. I used cross-validation to select my model and to avoid overfitting it.<!--more-->\r\n",
      "\r\n",
      "<strong>Predicting the finalists </strong>\r\n",
      "\r\n",
      "I trusted the bookmakers on this one and just took the top ten countries from each semi-final group. I got the betting odds from Betfair.\r\n",
      "\r\n",
      "<strong>Learning the voting patterns </strong>\r\n",
      "\r\n",
      "A simple approach worked well enough here. The idea was to calculate, for each country, the average points awarded to each other country. Coming from Slovenia which was once part of Yugoslavia, together with Croatia, Serbia, Bosnia and Herzegovina, Macedonia and Montenegro, it is perhaps not surprising that our voting patterns are rather interesting:\r\n",
      "<pre><code>  AVG' COUNTRY\r\n",
      "10.38  Serbia\r\n",
      " 8.53  Croatia\r\n",
      " 8.00  Bosnia and Herzegovina\r\n",
      " 5.91  Macedonia\r\n",
      " 3.21  Norway\r\n",
      " 3.17  Russia\r\n",
      " 3.07  Greece\r\n",
      "...\r\n",
      " 0.18  Portugal\r\n",
      " 0.17  Belarus\r\n",
      "</code></pre>\r\n",
      "It is painfully obvious that Slovenia is not judging the quality of the artist alone and it is well known that other countries follow similar patterns. It would, therefore, seem like a good idea to use this knowledge in predicting this year's voting.<!--more-->\r\n",
      "\r\n",
      "The estimated average points awarded are not very stable, especially for newer countries. To remedy this, instead of using:\r\n",
      "<pre><code>avg := sum(x) / |x|\r\n",
      "</code></pre>\r\n",
      "I used\r\n",
      "<pre><code>avg' := (sum(x) + 1) / (|x| + 1)\r\n",
      "</code></pre>\r\n",
      "The new estimate got better results on the cross-validation tests.\r\n",
      "\r\n",
      "<strong>Betting Odds</strong>\r\n",
      "\r\n",
      "Using just the voting patterns of countries to predict this year's results was not enough. I had to, somehow, incorporate the approximate betting odds as well. Many approaches could have worked well here. In the end I opted for the one that gave the best cross-validation results.\r\n",
      "\r\n",
      "I had to convert the approximate betting odds into something comparable with the average points awarded. I used:\r\n",
      "<pre><code>odds'(ctr) := 1 / log(odds(ctr)) * a + b\r\n",
      "</code></pre>\r\n",
      "The coefficients a and b were chosen experimentally, as the ones that gave the best cross-validation score.\r\n",
      "\r\n",
      "A small example will elucidate how I calculated the converted betting odds.\r\n",
      "<pre><code>odds'(Croatia) = 1 / log(odds(Croatia)) * 4.4 + 0.8 =\r\n",
      "               = 1 / log(48) * 4.4 + 0.8\r\n",
      "               = 1.94\r\n",
      "</code></pre>\r\n",
      "The converted betting odds for the top and bottom countries were:\r\n",
      "<pre><code>ODDS' COUNTRY\r\n",
      "5.23  Azerbaijan\r\n",
      "3.21  Germany\r\n",
      "2.54  Armenia\r\n",
      "...\r\n",
      "1.94  Croatia\r\n",
      "...\r\n",
      "1.45  Slovenia\r\n",
      "1.44  Bulgaria\r\n",
      "1.44  Macedonia\r\n",
      "1.44  Switzerland</code></pre>\r\n",
      "<strong>Combining the voting patterns with the betting odds</strong>\r\n",
      "\r\n",
      "It was now time to bring everything together. This was simply a matter of summing the average points awarded with the converted betting odds.\r\n",
      "\r\n",
      "This was how I predicted Slovenia's votes for this year:\r\n",
      "<pre><code>COUNTRY                  AVG'  ODDS'    SUM POINTS\r\n",
      "Serbia                 10.38 + 1.84 = 12.21     12\r\n",
      "Croatia                 8.53 + 1.94 = 10.47     10\r\n",
      "Bosnia and Herzegovina  8.00 + 1.49 =  9.49      8\r\n",
      "Macedonia               5.91 + 1.44 =  7.35      7\r\n",
      "Azerbaijan              1.80 + 5.23 =  7.03      6\r\n",
      "Norway                  3.21 + 2.01 =  5.22      5\r\n",
      "Greece                  3.07 + 1.96 =  5.03      4\r\n",
      "Sweden                  2.85 + 2.18 =  5.03      3\r\n",
      "Russia                  3.17 + 1.62 =  4.79      2\r\n",
      "Germany                 1.50 + 3.21 =  4.71      1\r\n",
      "Denmark                 2.42 + 2.25 =  4.66      0\r\n",
      "...\r\n",
      "</code></pre>\r\n",
      "We saw earlier that Slovenia's votes have little to do with song quality, as we usually award the top points to Balkan countries, no matter how bad they sing. The added betting odds should not influence the prediction of such countries considerably. On the other hand, if we take a country that is perhaps a bit more fair, like Israel, we see that the final predictions are affected to a greater extent:\r\n",
      "<pre><code>COUNTRY                  AVG'   ODDS'    SUM POINTS\r\n",
      "Armenia                 7.50 +  2.54 = 10.04     12\r\n",
      "Azerbaijan              3.75 +  5.23 =  8.98     10\r\n",
      "Russia                  7.23 +  1.62 =  8.85      8\r\n",
      "Ukraine                 6.30 +  1.54 =  7.84      7\r\n",
      "Romania                 6.07 +  1.61 =  7.68      6\r\n",
      "Greece                  4.08 +  1.96 =  6.04      5\r\n",
      "Georgia                 4.25 +  1.77 =  6.02      4\r\n",
      "Iceland                 3.83 +  1.72 =  5.55      3\r\n",
      "Serbia                  3.71 +  1.84 =  5.55      2\r\n",
      "Denmark                 3.25 +  2.25 =  5.50      1\r\n",
      "Sweden                  3.27 +  2.18 =  5.45      0\r\n",
      "...\r\n",
      "</code></pre>\r\n",
      "<h2><span style=\"font-size: 13px;\">Cross validation</span></h2>\r\n",
      "<h2><span style=\"font-weight: normal; font-size: 13px;\">The most important component of my solution was cross-validation and was probably the reason why I won the competition in the first place. It enabled me to try many different models and, between them, choose the one that was most likely to give the best results.</span></h2>\r\n",
      "The dataset was split into partitions, one for each Eurovision event. I then proceeded to build the model on all but one partition and calculated the error of that model on the partition that was left out. The procedure was repeated so that each time a different partition was left out. This gave me a fair estimate of how the model performs on unseen data.\r\n",
      "\r\n",
      "The cross-validation procedure in pseudocode:\r\n",
      "<pre><code>function crossValidation(dataset, buildModel):\r\n",
      "  error = 0\r\n",
      "  for year in eurovisionEvents:\r\n",
      "    learnData = {example | example in dataset and example.year != year}\r\n",
      "    testData  = {example | example in dataset and example.year == year}\r\n",
      "    model = buildModel(learnData)\r\n",
      "    error += testModel(model, testData)\r\n",
      "  return error</code></pre>\r\n",
      "<pre><span style=\"font-family: monospace, Monaco, 'Courier New', Courier, monospace;\"><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px; white-space: normal; font-size: 13px;\"><strong>Conclusion </strong></span></span></pre>\r\n",
      "<pre><span style=\"font-family: monospace, Monaco, 'Courier New', Courier, monospace;\"><strong><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-weight: normal; line-height: 19px; white-space: normal; font-size: 13px;\">I am well aware that certain parts of my approach are not very strong. I had to do my best with the time that was available. I have many ideas for next year, which I will, for the moment at least, keep to myself :)</span></strong></span></pre>\r\n",
      "I really enjoy competing in events like this and hope there will be more to come in the future.\n"
     ]
    }
   ],
   "source": [
    "for i in range(content.shape[0]):\n",
    "    posts = content.values[i] #Get all posts.\n",
    "    \n",
    "    for post in posts:\n",
    "        list.append(post)\n",
    "    \n",
    "#print(list[0])\n",
    "\n",
    "for i, post in enumerate(list):\n",
    "    str.append(''.join(list[i]))\n",
    "    \n",
    "print (str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, BeautifulSoup to remove html tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleantext=[]\n",
    "for i, post in enumerate(str):\n",
    "    cleantext.append(BeautifulSoup(str[i], \"lxml\").get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, just check if html tags have been removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2010 INFORMS Data Mining Contest has just finished. The competition attracted entries from 147 teams with participants from 27 countries. The winner was Cole Harris, followed by Christopher Hefele and Nan Zhou. Here is some background on the winners and the techniques they applied.\\r\\n\\r\\nCole Harris\\nAbout Cole:\\r\\n\\r\\n\"Since 2002 I have been VP Discovery and cofounder of Exagen Diagnostics. We mine genomic/medical data to identify genetic features that are diagnostic of disease, predictive of drug response, etc. and then develop medical tests from the results. Prior to this (2000-2002), at Quasar/Magnaflux, I developed pattern recognition algorithms for identifying defective metal parts from acoustic spectral data.\\xa0 From 1990-1999 I worked for Veritas Geophysical, most of that time developing algorithms for imaging seismic data. Prior to this I was in grad school (physics): MA 1990 Johns Hopkins University.\"\\r\\n\\r\\n\\nCole\\'s Method:\\r\\n\\r\\n\"As far as techniques, my submissions were mostly based on:\\r\\n\\r\\n1. pre-processing - I did many things, but none seemed to make a large difference in my early results on test data, so in the end, other than exclude the non-price data, I didn\\'t filter the data. I did append the data with data advanced 5 min, 60 min, and 65 min. So for each stock there were 16 features (open,hi,lo,last)X(0min,5min,60min,65min)\\r\\n\\r\\n2. feature selection - forward stepwise selection of stocks, reverse stepwise selection of particular features for a given stock, evaluated using logistic regression. This resulted in 5-6 features selected from 2 stocks.\\r\\n\\r\\nmodels: logistic regression and neural networks (not sure which won).\"\\r\\n\\r\\n\\nChristopher Hefele\\nAbout Christopher: \\r\\n\\r\\nChristopher is a Systems Engineer at AT&T. He was a member of The Ensemble, the team which finished second in the $1m Netflix Prize.\\r\\n\\r\\nChristopher\\'s Method:\\r\\n\\r\\n\"I was using a simple logistic regression on Variable 74 for most of the  contest. During the last few days, when every last bit counted, I  switched to a SVM & added more variables (i.e. Variables 167 &  55, chosen by forward stepwise logistic regression).\\r\\n\\r\\nIn the end, to me, this contest really was a  good lesson about the power of proper variable selection &  preprocessing.\"\\r\\n\\r\\nNan Zhou\\nAbout Nan:\\r\\n\\r\\nNan is currently completing his PhD in statistics at the University of Pittsburgh. His PhD research involves the estimation and prediction of  integrated volatility and model  calibration for financial stochastic processes. Prior to this he was a graduate student at Carnege Mellon University (focusing on statistical machine learning).\\r\\n\\r\\nNan\\'s Method:\\r\\n\\r\\n\"Among lots of other models (Support Vector Machine, Random Forest, Neutral Network, Gradient Boosting, AdaBoost, and etc.) I finally used ‘Two-Stages’ L1-penalized Logistic Regression, and tune the penalty parameter by 5-folds Cross Validation.\"\\r\\n\\r\\nYou can hear more from the winners (and others) on the competition\\'s forum.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleantext[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move onto some more pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = (set(stopwords.words('english')))\n",
    "\n",
    "#Union operation on sets\n",
    "stop_words = stop_words | {\",\",\".\",\";\",\"(\",\")\",\"'\",\"?\",\"...\",'-','=',':=','+','/',':', '%', ']','[', '+='}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, newsentence_array is the list being passed down for further POS tagging and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS Tagging\n",
    "tagged_docs=[]\n",
    "\n",
    "for doc in cleantext:\n",
    "    tagged_docs.append(nltk.pos_tag(word_tokenize(doc)))\n",
    "   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('About', 'IN'), ('me', 'PRP'), (':', ':'), ('I', 'PRP'), ('’', 'VBP'), ('m', 'PDT'), ('an', 'DT'), ('embedded', 'JJ'), ('systems', 'NNS'), ('engineer', 'NN'), (',', ','), ('currently', 'RB'), ('working', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('small', 'JJ'), ('engineering', 'NN'), ('company', 'NN'), ('in', 'IN'), ('Las', 'NNP'), ('Cruces', 'NNP'), (',', ','), ('New', 'NNP'), ('Mexico', 'NNP'), ('.', '.'), ('I', 'PRP'), ('graduated', 'VBD'), ('from', 'IN'), ('New', 'NNP'), ('Mexico', 'NNP'), ('Tech', 'NNP'), ('in', 'IN'), ('2007', 'CD'), (',', ','), ('with', 'IN'), ('degrees', 'NNS'), ('in', 'IN'), ('Electrical', 'NNP'), ('Engineering', 'NNP'), ('and', 'CC'), ('Computer', 'NNP'), ('Science', 'NNP'), ('.', '.'), ('Like', 'IN'), ('many', 'JJ'), ('people', 'NNS'), (',', ','), ('I', 'PRP'), ('first', 'RB'), ('became', 'VBD'), ('interested', 'JJ'), ('in', 'IN'), ('algorithm', 'JJ'), ('competitions', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('Netflix', 'NNP'), ('Prize', 'NNP'), ('a', 'DT'), ('few', 'JJ'), ('years', 'NNS'), ('ago', 'RB'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('quite', 'RB'), ('excited', 'JJ'), ('to', 'TO'), ('find', 'VB'), ('the', 'DT'), ('Kaggle', 'NNP'), ('site', 'NN'), ('a', 'DT'), ('few', 'JJ'), ('months', 'NNS'), ('ago', 'RB'), (',', ','), ('as', 'IN'), ('I', 'PRP'), ('enjoy', 'VBP'), ('participating', 'VBG'), ('in', 'IN'), ('these', 'DT'), ('types', 'NNS'), ('of', 'IN'), ('competitions', 'NNS'), ('.', '.'), ('Explanation', 'NN'), ('of', 'IN'), ('Technique', 'NN'), (':', ':'), ('Though', 'IN'), ('I', 'PRP'), ('tried', 'VBD'), ('several', 'JJ'), ('different', 'JJ'), ('methods', 'NNS'), (',', ','), ('I', 'PRP'), ('used', 'VBD'), ('a', 'DT'), ('weighted', 'JJ'), ('combination', 'NN'), ('of', 'IN'), ('three', 'CD'), ('predictors', 'NNS'), ('to', 'TO'), ('come', 'VB'), ('up', 'RP'), ('with', 'IN'), ('the', 'DT'), ('final', 'JJ'), ('forecast', 'NN'), ('.', '.'), ('#', '#'), ('1', 'CD'), (':', ':'), ('After', 'IN'), ('reviewing', 'VBG'), ('Athanasopoulos', 'NNP'), ('et', 'CC'), ('al', 'NN'), (',', ','), ('it', 'PRP'), ('became', 'VBD'), ('obvious', 'JJ'), ('that', 'IN'), ('the', 'DT'), ('naive', 'JJ'), ('predictor', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('good', 'JJ'), ('algorithm', 'NN'), ('with', 'IN'), ('which', 'WDT'), ('to', 'TO'), ('start', 'VB'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('both', 'DT'), ('easy', 'JJ'), ('to', 'TO'), ('implement', 'VB'), ('and', 'CC'), ('performed', 'VBN'), ('well', 'RB'), ('when', 'WRB'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('other', 'JJ'), ('algorithms', 'NN'), ('in', 'IN'), ('the', 'DT'), ('paper', 'NN'), ('.', '.'), ('After', 'IN'), ('graphing', 'VBG'), ('a', 'DT'), ('few', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), (',', ','), ('it', 'PRP'), ('became', 'VBD'), ('apparent', 'JJ'), ('that', 'IN'), ('many', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('series', 'NN'), ('increase', 'NN'), ('with', 'IN'), ('time', 'NN'), ('.', '.'), ('Indeed', 'RB'), (',', ','), ('the', 'DT'), ('second', 'JJ'), ('sentence', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Athanasopoulos', 'NNP'), ('paper', 'NN'), ('states', 'NNS'), ('that', 'IN'), ('globally', 'RB'), ('tourism', 'NN'), ('has', 'VBZ'), ('grown', 'VBN'), ('“', 'NNS'), ('at', 'IN'), ('a', 'DT'), ('rate', 'NN'), ('of', 'IN'), ('6', 'CD'), ('%', 'NN'), ('annually.', 'JJ'), ('”', 'NN'), ('In', 'IN'), ('order', 'NN'), ('to', 'TO'), ('take', 'VB'), ('advantage', 'NN'), ('of', 'IN'), ('this', 'DT'), ('knowledge', 'NN'), (',', ','), ('I', 'PRP'), ('multiplied', 'VBD'), ('the', 'DT'), ('(', '('), ('Naive', 'JJ'), ('algorithm', 'NN'), ('’', 'NNP'), ('s', 'NN'), (')', ')'), ('predicted', 'VBD'), ('value', 'NN'), ('by', 'IN'), ('a', 'DT'), ('factor', 'NN'), ('to', 'TO'), ('take', 'VB'), ('this', 'DT'), ('growth', 'NN'), ('into', 'IN'), ('account', 'NN'), ('.', '.'), ('With', 'IN'), ('some', 'DT'), ('testing', 'NN'), (',', ','), ('I', 'PRP'), ('determined', 'VBD'), ('a', 'DT'), ('5.5', 'CD'), ('%', 'NN'), ('growth', 'NN'), ('factor', 'NN'), ('to', 'TO'), ('yield', 'VB'), ('the', 'DT'), ('lowest', 'JJS'), ('MASE', 'NNP'), ('.', '.'), ('prediction1', 'NN'), ('=', 'JJ'), ('last_value', 'JJ'), ('*', 'NNP'), ('(', '('), ('1.055', 'CD'), ('**', 'NNP'), ('number_of_years_in_the_future', 'NN'), (')', ')'), ('#', '#'), ('2', 'CD'), (':', ':'), ('I', 'PRP'), ('examined', 'VBD'), ('fitting', 'VBG'), ('a', 'DT'), ('polynomial', 'JJ'), ('line', 'NN'), ('to', 'TO'), ('the', 'DT'), ('data', 'NNS'), ('and', 'CC'), ('using', 'VBG'), ('the', 'DT'), ('line', 'NN'), ('to', 'TO'), ('predict', 'VB'), ('future', 'JJ'), ('values', 'NNS'), ('.', '.'), ('I', 'PRP'), ('tried', 'VBD'), ('using', 'VBG'), ('first', 'RB'), ('through', 'IN'), ('fifth', 'JJ'), ('order', 'NN'), ('polynomials', 'NNS'), ('to', 'TO'), ('find', 'VB'), ('that', 'IN'), ('the', 'DT'), ('lowest', 'JJS'), ('MASE', 'NN'), ('was', 'VBD'), ('obtained', 'VBN'), ('using', 'VBG'), ('a', 'DT'), ('first', 'JJ'), ('order', 'NN'), ('polynomial', 'NN'), ('(', '('), ('simple', 'JJ'), ('regression', 'NN'), ('line', 'NN'), (')', ')'), ('.', '.'), ('This', 'DT'), ('best', 'JJS'), ('fit', 'JJ'), ('line', 'NN'), ('was', 'VBD'), ('used', 'VBN'), ('to', 'TO'), ('predict', 'VB'), ('future', 'JJ'), ('values', 'NNS'), ('.', '.'), ('I', 'PRP'), ('also', 'RB'), ('kept', 'VBD'), ('the', 'DT'), ('r**2', 'JJ'), ('value', 'NN'), ('of', 'IN'), ('the', 'DT'), ('fit', 'NN'), ('for', 'IN'), ('use', 'NN'), ('in', 'IN'), ('blending', 'VBG'), ('the', 'DT'), ('results', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('predictor', 'NN'), ('.', '.'), ('#', '#'), ('3', 'CD'), (':', ':'), ('In', 'IN'), ('thinking', 'VBG'), ('about', 'IN'), ('these', 'DT'), ('two', 'CD'), ('predictors', 'NNS'), (',', ','), ('I', 'PRP'), ('recognized', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('naive', 'JJ'), ('predictor', 'NN'), (',', ','), ('though', 'IN'), ('accurate', 'JJ'), (',', ','), ('throws', 'VBZ'), ('away', 'RB'), ('most', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('provided', 'VBN'), ('data', 'NN'), (',', ','), ('and', 'CC'), ('only', 'RB'), ('uses', 'VBZ'), ('a', 'DT'), ('single', 'JJ'), ('element', 'NN'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), ('.', '.'), ('The', 'DT'), ('polynomial', 'JJ'), ('line', 'NN'), ('predictor', 'NN'), ('uses', 'VBZ'), ('all', 'DT'), ('of', 'IN'), ('the', 'DT'), ('data', 'NNS'), (',', ','), ('weighted', 'VBD'), ('equally', 'RB'), (',', ','), ('though', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('recent', 'JJ'), ('data', 'NN'), ('is', 'VBZ'), ('probably', 'RB'), ('of', 'IN'), ('more', 'JJR'), ('value', 'NN'), ('in', 'IN'), ('indicating', 'VBG'), ('future', 'JJ'), ('performance', 'NN'), ('than', 'IN'), ('the', 'DT'), ('earlier', 'JJR'), ('data', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), ('.', '.'), ('I', 'PRP'), ('examined', 'VBD'), ('and', 'CC'), ('eventually', 'RB'), ('used', 'VBD'), ('an', 'DT'), ('exponentially-weighted', 'JJ'), ('least', 'NN'), ('squares', 'NNS'), ('regression', 'VBP'), ('to', 'TO'), ('fit', 'VB'), ('a', 'DT'), ('line', 'NN'), ('to', 'TO'), ('the', 'DT'), ('data', 'NN'), ('.', '.'), ('This', 'DT'), ('algorithm', 'NN'), ('gave', 'VBD'), ('more', 'RBR'), ('accurate', 'JJ'), ('predictions', 'NNS'), ('for', 'IN'), ('many', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), ('by', 'IN'), ('itself', 'PRP'), (',', ','), ('and', 'CC'), ('also', 'RB'), ('lowered', 'VBD'), ('the', 'DT'), ('MASE', 'NNP'), ('when', 'WRB'), ('used', 'VBN'), ('in', 'IN'), ('combination', 'NN'), ('with', 'IN'), ('the', 'DT'), ('two', 'CD'), ('above', 'IN'), ('predictors', 'NNS'), ('.', '.'), ('The', 'DT'), ('r**2', 'JJ'), ('value', 'NN'), ('of', 'IN'), ('this', 'DT'), ('fit', 'NN'), ('was', 'VBD'), ('also', 'RB'), ('used', 'VBN'), ('for', 'IN'), ('blending', 'VBG'), ('the', 'DT'), ('predictors', 'NNS'), ('.', '.'), ('Blending', 'VBG'), ('stage', 'NN'), (':', ':'), ('I', 'PRP'), ('started', 'VBD'), ('with', 'IN'), ('a', 'DT'), ('basic', 'JJ'), ('weighted', 'JJ'), ('blend', 'NN'), ('of', 'IN'), ('predictors', 'NNS'), ('.', '.'), ('I', 'PRP'), ('used', 'VBD'), ('a', 'DT'), ('constant', 'JJ'), ('weight', 'NN'), ('factor', 'NN'), ('for', 'IN'), ('the', 'DT'), ('modified', 'JJ'), ('naive', 'JJ'), ('predictor', 'NN'), (',', ','), ('while', 'IN'), ('blending', 'VBG'), ('weights', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('unweighted', 'JJ'), ('and', 'CC'), ('weighted', 'JJ'), ('regression', 'NN'), ('lines', 'NNS'), ('depending', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('r**2', 'NN'), ('values', 'NNS'), ('found', 'VBN'), ('in', 'IN'), ('fitting', 'VBG'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), ('.', '.'), ('I', 'PRP'), ('selected', 'VBD'), ('the', 'DT'), ('logistic', 'JJ'), ('function', 'NN'), ('as', 'IN'), ('a', 'DT'), ('way', 'NN'), ('of', 'IN'), ('gradually', 'RB'), ('increasing', 'VBG'), ('the', 'DT'), ('weight', 'NN'), ('with', 'IN'), ('increasing', 'VBG'), ('r**2', 'JJ'), ('value', 'NN'), (':', ':'), ('weight', 'NN'), ('=', 'VBZ'), ('a', 'DT'), ('*', 'JJ'), ('logistic', 'JJ'), ('(', '('), ('b', 'JJ'), ('*', 'NN'), ('(', '('), ('r**2', 'JJ'), ('-', ':'), ('c', 'NN'), (')', ')'), (')', ')'), ('Values', 'VBZ'), ('for', 'IN'), ('a', 'DT'), (',', ','), ('b', 'NN'), (',', ','), ('and', 'CC'), ('c', 'NNS'), ('were', 'VBD'), ('determined', 'VBN'), ('by', 'IN'), ('trial', 'NN'), ('and', 'CC'), ('error', 'NN'), ('.', '.'), ('I', 'PRP'), ('also', 'RB'), ('examined', 'VBD'), ('using', 'VBG'), ('some', 'DT'), ('numeric', 'JJ'), ('optimization', 'NN'), ('functions', 'NNS'), ('included', 'VBD'), ('in', 'IN'), ('Python', 'NNP'), ('to', 'TO'), ('minimize', 'VB'), ('a', 'DT'), ('training', 'NN'), ('set', 'VBN'), ('MASE', 'NNP'), ('.', '.'), ('While', 'IN'), ('this', 'DT'), ('succeeded', 'VBN'), ('in', 'IN'), ('lower', 'JJR'), ('the', 'DT'), ('training', 'NN'), ('set', 'VBN'), ('MASE', 'NNP'), (',', ','), ('I', 'PRP'), ('discarded', 'VBD'), ('this', 'DT'), ('method', 'NN'), ('when', 'WRB'), ('I', 'PRP'), ('received', 'VBD'), ('a', 'DT'), ('lower', 'JJR'), ('leaderboard', 'NN'), ('MASE', 'NNP'), ('(', '('), ('possibly', 'RB'), ('from', 'IN'), ('overfitting', 'VBG'), (')', ')'), ('.', '.'), ('Testing', 'VBG'), ('/', 'JJ'), ('developing', 'VBG'), ('algorithms', 'NN'), (':', ':'), ('When', 'WRB'), ('testing', 'VBG'), ('algorithms', 'RB'), (',', ','), ('I', 'PRP'), ('used', 'VBD'), ('the', 'DT'), ('last', 'JJ'), ('four', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('training', 'NN'), ('dataset', 'NN'), ('(', '('), ('after', 'IN'), ('removing', 'VBG'), ('them', 'PRP'), ('from', 'IN'), ('the', 'DT'), ('data', 'NN'), ('I', 'PRP'), ('was', 'VBD'), ('using', 'VBG'), ('for', 'IN'), ('training', 'VBG'), (')', ')'), ('to', 'TO'), ('test', 'VB'), ('against', 'IN'), ('.', '.'), ('While', 'IN'), ('this', 'DT'), ('worked', 'VBN'), ('well', 'RB'), ('in', 'IN'), ('the', 'DT'), ('initial', 'JJ'), ('stages', 'NNS'), (',', ','), ('I', 'PRP'), ('found', 'VBD'), ('that', 'IN'), ('once', 'RB'), ('my', 'PRP$'), ('leaderboard', 'JJ'), ('MASE', 'NNP'), ('got', 'VBD'), ('below', 'IN'), ('about', 'IN'), ('2.05', 'CD'), (',', ','), ('this', 'DT'), ('‘', 'JJ'), ('training', 'NN'), ('MASE', 'NNP'), ('’', 'NNP'), ('became', 'VBD'), ('a', 'DT'), ('much', 'RB'), ('less', 'RBR'), ('reliable', 'JJ'), ('indicator', 'NN'), ('of', 'IN'), ('whether', 'IN'), ('the', 'DT'), ('leaderboard', 'JJ'), ('MASE', 'NNP'), ('would', 'MD'), ('increase', 'VB'), ('or', 'CC'), ('decrease', 'VB'), ('with', 'IN'), ('a', 'DT'), ('change', 'NN'), ('.', '.'), ('So', 'RB'), (',', ','), ('during', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('few', 'JJ'), ('weeks', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('contest', 'NN'), (',', ','), ('I', 'PRP'), ('primarily', 'RB'), ('made', 'VBD'), ('small', 'JJ'), ('tweaks', 'NNS'), (',', ','), ('and', 'CC'), ('tested', 'VBD'), ('their', 'PRP$'), ('value', 'NN'), ('by', 'IN'), ('submitting', 'VBG'), ('a', 'DT'), ('new', 'JJ'), ('prediction', 'NN'), ('to', 'TO'), ('Kaggle', 'NNP'), ('rather', 'RB'), ('than', 'IN'), ('comparing', 'VBG'), ('my', 'PRP$'), ('MASE', 'NNP'), ('results', 'NNS'), ('.', '.'), ('I', 'PRP'), ('believe', 'VBP'), ('that', 'IN'), ('this', 'DT'), ('indicates', 'VBZ'), ('a', 'DT'), ('significant', 'JJ'), ('difference', 'NN'), ('in', 'IN'), ('nature', 'NN'), ('of', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('4', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('training', 'NN'), ('set', 'VBN'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('4', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('test', 'NN'), ('set', 'NN'), ('.', '.'), ('If', 'IN'), ('the', 'DT'), ('test', 'NN'), ('set', 'NN'), ('includes', 'VBZ'), ('data', 'NNS'), ('from', 'IN'), ('2008-2009', 'CD'), (',', ','), ('I', 'PRP'), ('’', 'VBP'), ('m', 'JJ'), ('speculating', 'NN'), ('that', 'WDT'), ('depressed', 'VBD'), ('tourism', 'NN'), ('numbers', 'NNS'), ('as', 'IN'), ('a', 'DT'), ('result', 'NN'), ('of', 'IN'), ('the', 'DT'), ('global', 'JJ'), ('economic', 'JJ'), ('recession', 'NN'), ('could', 'MD'), ('have', 'VB'), ('caused', 'VBN'), ('a', 'DT'), ('significant', 'JJ'), ('difference', 'NN'), ('in', 'IN'), ('the', 'DT'), ('trends', 'NNS'), ('.', '.'), ('Possible', 'JJ'), ('improvements', 'NNS'), (':', ':'), ('While', 'IN'), ('the', 'DT'), ('above', 'JJ'), ('method', 'NN'), ('seemed', 'VBD'), ('to', 'TO'), ('work', 'VB'), ('fairly', 'RB'), ('well', 'RB'), ('at', 'IN'), ('predicting', 'VBG'), ('tourism', 'NN'), ('numbers', 'NNS'), (',', ','), ('there', 'EX'), ('are', 'VBP'), ('several', 'JJ'), ('steps', 'NNS'), ('that', 'WDT'), ('could', 'MD'), ('have', 'VB'), ('likely', 'RB'), ('improved', 'VBN'), ('the', 'DT'), ('score', 'NN'), ('.', '.'), ('I', 'PRP'), ('only', 'RB'), ('implemented', 'VBD'), ('the', 'DT'), ('Naive', 'NNP'), ('method', 'NN'), ('implemented', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('Athanasopoulos', 'NNP'), ('paper', 'NN'), (';', ':'), ('I', 'PRP'), ('do', 'VBP'), ('think', 'VB'), ('that', 'IN'), ('including', 'VBG'), ('a', 'DT'), ('couple', 'NN'), ('of', 'IN'), ('other', 'JJ'), ('algorithms', 'JJ'), ('’', 'NN'), ('output', 'NN'), ('into', 'IN'), ('the', 'DT'), ('final', 'JJ'), ('blend', 'NN'), ('could', 'MD'), ('have', 'VB'), ('further', 'RBR'), ('increased', 'VBN'), ('the', 'DT'), ('score', 'NN'), ('.', '.'), ('If', 'IN'), ('I', 'PRP'), ('had', 'VBD'), ('a', 'DT'), ('few', 'JJ'), ('more', 'JJR'), ('days', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('on', 'IN'), ('a', 'DT'), ('solution', 'NN'), (',', ','), ('I', 'PRP'), ('would', 'MD'), ('have', 'VB'), ('tried', 'VBN'), ('to', 'TO'), ('implement', 'VB'), ('the', 'DT'), ('Theta', 'NNP'), ('and', 'CC'), ('ARIMA', 'NNP'), ('methods', 'NNS'), ('described', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('paper', 'NN'), ('and', 'CC'), ('looked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('effects', 'NNS'), ('of', 'IN'), ('including', 'VBG'), ('them', 'PRP'), ('in', 'IN'), ('the', 'DT'), ('blend', 'NN'), ('.', '.'), ('I', 'PRP'), ('think', 'VBP'), ('an', 'DT'), ('investigation', 'NN'), ('into', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('come', 'VB'), ('up', 'RP'), ('with', 'IN'), ('a', 'DT'), ('blending', 'NN'), ('method', 'NN'), ('that', 'WDT'), ('doesn', 'VBZ'), ('’', 'NNP'), ('t', 'NN'), ('use', 'NN'), ('as', 'IN'), ('much', 'JJ'), ('manual', 'JJ'), ('tweaking', 'NN'), ('would', 'MD'), ('also', 'RB'), ('be', 'VB'), ('of', 'IN'), ('benefit', 'NN'), ('.', '.'), ('I', 'PRP'), ('enjoyed', 'VBD'), ('participating', 'VBG'), ('in', 'IN'), ('part', 'NN'), ('one', 'CD'), (',', ','), ('and', 'CC'), ('look', 'VB'), ('forward', 'RB'), ('to', 'TO'), ('part', 'NN'), ('two', 'CD'), ('of', 'IN'), ('the', 'DT'), ('contest', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print (tagged_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let us develop a chunker which will help us in identifying certain sets of entities from doc to doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = \"CCCC: {<JJ.*>*<NN.*>+}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(tagged_docs):\n",
    "    if (i==2):\n",
    "        print (type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(result)\n",
    "\n",
    "chunk_docs= []\n",
    "temp_chunk=[]\n",
    "#for i, doc in enumerate(tagged_docs):\n",
    "#result = chunk_parser.parse(tagged_docs[1])\n",
    "for i,  doc in enumerate(tagged_docs):\n",
    "    result = chunk_parser.parse(doc)\n",
    "    \n",
    "    for j, subtree in enumerate(result.subtrees(filter=lambda t: t.label() == 'CCCC')):\n",
    "        temp_chunk.append(\" \".join([a for (a,b) in subtree.leaves()]))\n",
    "    \n",
    "       \n",
    "    chunk_docs.append(temp_chunk)\n",
    "    temp_chunk=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "underscore_chunk_docs=copy.deepcopy(chunk_docs)\n",
    "\n",
    "for k in range(len(chunk_docs)):\n",
    "    for i, item in enumerate(chunk_docs[k]):\n",
    "        item = item.strip()\n",
    "        #print(item)\n",
    "        underscore_chunk_docs[k][i] = \"-\".join( item.split() )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "print (len(underscore_chunk_docs[100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_cleantext = copy.deepcopy(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'surprise', 'competition', 'first-time', 'days', 'target', 'result', 'clever-techniques', 'others', 'anything', 'Anyhoo', 'bit', 'things', 'many-ways', 'process', 'result', 'lessons', 'others', 'future-competitions', 'weeks', 'long-way', 'best-bet', 'work', 'use-stuff', 'Microsoft', 'TrueSkill-algorithm', 'C', 'implementation', 'language', 'practice', 'XBox-live', 'various-papers', 'import', 'data', 'excellent-FileHelpers-library', 'minutes', 'Step', 'try', 'algorithm', 'Jeff-Moser', 'brilliant-exposition', 'TrueSkill', 'full-source-code', 'few-hours', 're--reading', 'point', 'gist', 'start', 'interesting-Turing-Lecture', 'Chris-Bishop', 'book', 'pattern-recognition', 'influential-books', 'years', 'modern-Bayesian-Graphical-Model-approach', 'briefly-touches', 'TrueSkill-application', 'Step', 'way', 'progress', 'leaderboard-results', 'submissions', 'day', 'competition', 'validation-set', 'modelling', 'final-submissions', 'months', 'data', 'point', 'conclusions', 'months', 'validation-set', 'things', 'leaderboard', 'day', 'submission', 'data', 'Moser', 'class', 'default-settings', 'algorithm', 'few-times', 'previous-scores', 'starting-points', 'results', 'Result', 'place', 'long-way', 'predictions', 'scores', 'scores', '[', 'black-]', '[-s1', 's2-]', 's1+100', '/', 's1+s2', 'top', 'little-advantage', '%-score', 'average', 'next-few-days', 'backwards', 'graphs', 'score-difference', '%', 'logistic-function', 'parameters', 'simple-hill-climb-algorithm', 'score', 'individual-player-scores', 'score', 'wasted-effort', 'pictures', 'algorithms', 'graph', 'win-%', 'white-score', 'separate-lines', 'quartile', 'black-score', 'logistic-function', 'interactions', 'So', 'days', 'much-improvement', 'minor-tweaks', 'Trueskill-params', 'improvement', 'day', 'whole-different-approach', 'days', 'Trueskill', 'individual-match', 'scores', 'later-results', 're-score', 'earlier-matches', 'course', 'first-person', 'problem', 'TrueSkill-Through-Time', 'paper', 'MS-Research', 'Applied-Games-Group', 'Bayesian-inference', 'theoretically-optimal-set', 'scores', 'mean', 'standard-deviation', 'player', 'code', 'old-version', 'F', 'current-version', 'while', 'F', 'Project-Euler-problems', 'Don-Syme', 'Real-Work', 'few-hours', 'changes', 'class', 'C', 'F', 'console-app', 'formula', 'predictions', 'scores', 'cumulative-gaussian', 'TrueSkill-Through-Time-paper', 'score', 'paper', 'annual-results', 'valuable-information', 'monthly-results', 'new-set', 'parameters', 'different-situation', 'simple-trial', 'error', 'params', 'hill-climbing', 'optimum-values', 'score', 'something', 'Chessmetrics-writeup', 'forum', 'average-score', 'players', 'person', 'weighted-average', 'player', 'actual-score', 'average', 'opponents', 'hill-climb-algorithm', 'weighting', 'standard-deviation', 'weighting', 'output', 'Trueskill/Time', 'position', 'someone', 'days', 'backwards', 'ensemble-approach', 'weighted-average', 'TrueSkill', 'TrueSkill/Time', 'ELO', 'TrueSkill/Time', 'approaches', 'ensemble-approaches', 'approaches', 'parameters', 'rating-algorithm', 'gaussian', 'probabilities', 'result', 'draw-probabilities', 'win-probabilities', 'problem', 'results', 'validation-set', 'final-leaderboard', 'resampling', 'validation', 'different-samples', 'different-results', 'validation-set', 'impact', 'change', 'solid-theoretical-basis', 'meaningless-increases', 'thoughtless-parameter-optimisation', 'Nov', 'improvement', 'gaussian-predictor-function', 'standard-deviation', 'linear-function', 'overall-match-level-[-i.e', 's1+s2', '/2-]', 'graphs', 'stronger-black-player', 'draw', 'skill', 'standard-deviation', 'linear-function', 'skill', 'Result', 'Nov', 'days', 'measure', 'things', 'submissions', 'week', 'much-valuable-time', 'following', 'biggest-impact', 'validation-set', 'first-few-months', 'training-data', 'months', 'validation-set', 'months', 'full-set-*', 'constant', 'calculation', 'gaussian', 'standard-deviation', 'predictions', 'predictions', '*', 'little-trick', 'anything', 'chess', 'knockout-comps', 'people', 'count', 'player', 'matches', 'test', 'predictor', 'huge-difference', 'results', 'little-bit-counts', 'submissions', 'Remove', 'months', 'Include-count', 'matches', 'prediction', 'stdev-formula', '%', 'My', 'final-submission', 'players', 'matches', 'weight', 'count', 'matches', 'main-lesson', 'process', 'fundamentals', 'minor-details', 'solution', 'PC', 'while', 'reading', 'basics', 'paper', 'little-breakthoughs', 'lot', 'validation-sets', 'leaderboard', 'fundamental-piece', 'solution', 'little-parameter-adjustments', 'improvements', 'factors', 'important-predictor', 'small-improvements', 'validation-set', 'things', 'big-difference', 'competition', 'important-things', 'little-improvements', 'Please', 'questions', 'TrueSkill', 'great-way', 'Chess-leaderboards', 'future', 'competition', 'historical-ratings', 'Chris-Bishop', 'talk', 'rating-people', 'true-skill', 'matches', 'excellent-results', 'little-pic', 'submission-history', '[-gallery-link=', 'file', 'points', 'public-leaderboard-score', 'final-score', 'submission', 'r^2-=', 'Kaggle', 'pool', 'other-submissions', 'leaderboard', 'close-relationship', 'short-time', 'submissions', 'solid-theoretical-basis', 'parameter', 'parameter', 'above-picture', 'changes', 'good-idea', 'leaderboard', 'similar-level', 'relationship', 'validation', 'results', 'leaderboard-results', 'lines', 'points', 'progress', 'scores', 'time', 'first-few-days', 'standard-TrueSkill', 'TrueSkill-Through-Time', 'results', 'sudden-jump', 'time', 'backwards', 'TrueSkill-Through-Time', 'few-days', 'last-few-days', 'thoughtful-approach', 'lack', 'time', 'takeaway', 'chart', 'few-days', 'failure', 'reason', 'improvements', 'new-insights', 'competitions', 'certain-amount', 'resilience', 'things', 'leaderboard', 'skill-learning', 'chance', 'improvements', 'takeaway', 'lots', 'different-things', 'competition', 'results', 'big-improvements', 'substantial-pieces', 'algorithm', 'details']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, doc in enumerate(ch_cleantext):\n",
    "    for j in range(len(underscore_chunk_docs[i])):\n",
    "        #print (underscore_chunk_docs[i][j])\n",
    "        ch_cleantext[i] = ch_cleantext[i].replace(chunk_docs[i][j],underscore_chunk_docs[i][j])\n",
    "    \n",
    "\n",
    "print (underscore_chunk_docs\n",
    "       [4])\n",
    "\n",
    "#nltk.pos_tag(word_tokenize(ch_cleantext[2]))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
