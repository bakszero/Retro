{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First , we import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import copy\n",
    "\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "path = 'WinnersInterviewBlogPosts.csv'\n",
    "data = pd.read_csv(path, header=None, names=['Title', 'Link', 'Publication_Date', 'Content'])\n",
    "data = data.drop(data.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Publication_Date</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer scientist Jure Zbontar on winning the...</td>\n",
       "      <td>http://blog.kaggle.com/2010/06/09/computer-sci...</td>\n",
       "      <td>2010-06-09 18:22:29</td>\n",
       "      <td>My approach was actually quite simple. The onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How I won the  Predict HIV Progression data mi...</td>\n",
       "      <td>http://blog.kaggle.com/2010/08/09/how-i-won-th...</td>\n",
       "      <td>2010-08-09 12:35:46</td>\n",
       "      <td>&lt;strong&gt;Initial Strategy&lt;/strong&gt;\\r\\n\\r\\nThe g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How I did it: Lee Baker on winning Tourism For...</td>\n",
       "      <td>http://blog.kaggle.com/2010/09/27/how-i-did-it...</td>\n",
       "      <td>2010-09-27 18:30:25</td>\n",
       "      <td>&lt;strong&gt;About me:&lt;/strong&gt;\\r\\nI’m an embedded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How I did it: The top three from the 2010 INFO...</td>\n",
       "      <td>http://blog.kaggle.com/2010/10/11/how-i-did-it...</td>\n",
       "      <td>2010-10-11 13:31:35</td>\n",
       "      <td>The 2010 INFORMS Data Mining Contest has just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How I did it: Jeremy Howard on finishing second</td>\n",
       "      <td>http://blog.kaggle.com/2010/11/19/how-i-did-it...</td>\n",
       "      <td>2010-11-19 17:39:47</td>\n",
       "      <td>Wow, this is a surprise! I looked at this comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "1  Computer scientist Jure Zbontar on winning the...   \n",
       "2  How I won the  Predict HIV Progression data mi...   \n",
       "3  How I did it: Lee Baker on winning Tourism For...   \n",
       "4  How I did it: The top three from the 2010 INFO...   \n",
       "5    How I did it: Jeremy Howard on finishing second   \n",
       "\n",
       "                                                Link     Publication_Date  \\\n",
       "1  http://blog.kaggle.com/2010/06/09/computer-sci...  2010-06-09 18:22:29   \n",
       "2  http://blog.kaggle.com/2010/08/09/how-i-won-th...  2010-08-09 12:35:46   \n",
       "3  http://blog.kaggle.com/2010/09/27/how-i-did-it...  2010-09-27 18:30:25   \n",
       "4  http://blog.kaggle.com/2010/10/11/how-i-did-it...  2010-10-11 13:31:35   \n",
       "5  http://blog.kaggle.com/2010/11/19/how-i-did-it...  2010-11-19 17:39:47   \n",
       "\n",
       "                                             Content  \n",
       "1  My approach was actually quite simple. The onl...  \n",
       "2  <strong>Initial Strategy</strong>\\r\\n\\r\\nThe g...  \n",
       "3  <strong>About me:</strong>\\r\\nI’m an embedded ...  \n",
       "4  The 2010 INFORMS Data Mining Contest has just ...  \n",
       "5  Wow, this is a surprise! I looked at this comp...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My approach was actually quite simple. The onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;strong&gt;Initial Strategy&lt;/strong&gt;\\r\\n\\r\\nThe g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;strong&gt;About me:&lt;/strong&gt;\\r\\nI’m an embedded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 2010 INFORMS Data Mining Contest has just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow, this is a surprise! I looked at this comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content\n",
       "1  My approach was actually quite simple. The onl...\n",
       "2  <strong>Initial Strategy</strong>\\r\\n\\r\\nThe g...\n",
       "3  <strong>About me:</strong>\\r\\nI’m an embedded ...\n",
       "4  The 2010 INFORMS Data Mining Contest has just ...\n",
       "5  Wow, this is a surprise! I looked at this comp..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = data.iloc[:, 3:4]\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list=[]\n",
    "str =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we create a normalised python list from numpy array list, and convert the result into a string for processing via BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My approach was actually quite simple. The only attributes I used where the approximate betting odds and the information on past voting. I sought patterns in the voting behaviour of all countries and combined that knowledge with this year's betting odds. I used cross-validation to select my model and to avoid overfitting it.<!--more-->\r\n",
      "\r\n",
      "<strong>Predicting the finalists </strong>\r\n",
      "\r\n",
      "I trusted the bookmakers on this one and just took the top ten countries from each semi-final group. I got the betting odds from Betfair.\r\n",
      "\r\n",
      "<strong>Learning the voting patterns </strong>\r\n",
      "\r\n",
      "A simple approach worked well enough here. The idea was to calculate, for each country, the average points awarded to each other country. Coming from Slovenia which was once part of Yugoslavia, together with Croatia, Serbia, Bosnia and Herzegovina, Macedonia and Montenegro, it is perhaps not surprising that our voting patterns are rather interesting:\r\n",
      "<pre><code>  AVG' COUNTRY\r\n",
      "10.38  Serbia\r\n",
      " 8.53  Croatia\r\n",
      " 8.00  Bosnia and Herzegovina\r\n",
      " 5.91  Macedonia\r\n",
      " 3.21  Norway\r\n",
      " 3.17  Russia\r\n",
      " 3.07  Greece\r\n",
      "...\r\n",
      " 0.18  Portugal\r\n",
      " 0.17  Belarus\r\n",
      "</code></pre>\r\n",
      "It is painfully obvious that Slovenia is not judging the quality of the artist alone and it is well known that other countries follow similar patterns. It would, therefore, seem like a good idea to use this knowledge in predicting this year's voting.<!--more-->\r\n",
      "\r\n",
      "The estimated average points awarded are not very stable, especially for newer countries. To remedy this, instead of using:\r\n",
      "<pre><code>avg := sum(x) / |x|\r\n",
      "</code></pre>\r\n",
      "I used\r\n",
      "<pre><code>avg' := (sum(x) + 1) / (|x| + 1)\r\n",
      "</code></pre>\r\n",
      "The new estimate got better results on the cross-validation tests.\r\n",
      "\r\n",
      "<strong>Betting Odds</strong>\r\n",
      "\r\n",
      "Using just the voting patterns of countries to predict this year's results was not enough. I had to, somehow, incorporate the approximate betting odds as well. Many approaches could have worked well here. In the end I opted for the one that gave the best cross-validation results.\r\n",
      "\r\n",
      "I had to convert the approximate betting odds into something comparable with the average points awarded. I used:\r\n",
      "<pre><code>odds'(ctr) := 1 / log(odds(ctr)) * a + b\r\n",
      "</code></pre>\r\n",
      "The coefficients a and b were chosen experimentally, as the ones that gave the best cross-validation score.\r\n",
      "\r\n",
      "A small example will elucidate how I calculated the converted betting odds.\r\n",
      "<pre><code>odds'(Croatia) = 1 / log(odds(Croatia)) * 4.4 + 0.8 =\r\n",
      "               = 1 / log(48) * 4.4 + 0.8\r\n",
      "               = 1.94\r\n",
      "</code></pre>\r\n",
      "The converted betting odds for the top and bottom countries were:\r\n",
      "<pre><code>ODDS' COUNTRY\r\n",
      "5.23  Azerbaijan\r\n",
      "3.21  Germany\r\n",
      "2.54  Armenia\r\n",
      "...\r\n",
      "1.94  Croatia\r\n",
      "...\r\n",
      "1.45  Slovenia\r\n",
      "1.44  Bulgaria\r\n",
      "1.44  Macedonia\r\n",
      "1.44  Switzerland</code></pre>\r\n",
      "<strong>Combining the voting patterns with the betting odds</strong>\r\n",
      "\r\n",
      "It was now time to bring everything together. This was simply a matter of summing the average points awarded with the converted betting odds.\r\n",
      "\r\n",
      "This was how I predicted Slovenia's votes for this year:\r\n",
      "<pre><code>COUNTRY                  AVG'  ODDS'    SUM POINTS\r\n",
      "Serbia                 10.38 + 1.84 = 12.21     12\r\n",
      "Croatia                 8.53 + 1.94 = 10.47     10\r\n",
      "Bosnia and Herzegovina  8.00 + 1.49 =  9.49      8\r\n",
      "Macedonia               5.91 + 1.44 =  7.35      7\r\n",
      "Azerbaijan              1.80 + 5.23 =  7.03      6\r\n",
      "Norway                  3.21 + 2.01 =  5.22      5\r\n",
      "Greece                  3.07 + 1.96 =  5.03      4\r\n",
      "Sweden                  2.85 + 2.18 =  5.03      3\r\n",
      "Russia                  3.17 + 1.62 =  4.79      2\r\n",
      "Germany                 1.50 + 3.21 =  4.71      1\r\n",
      "Denmark                 2.42 + 2.25 =  4.66      0\r\n",
      "...\r\n",
      "</code></pre>\r\n",
      "We saw earlier that Slovenia's votes have little to do with song quality, as we usually award the top points to Balkan countries, no matter how bad they sing. The added betting odds should not influence the prediction of such countries considerably. On the other hand, if we take a country that is perhaps a bit more fair, like Israel, we see that the final predictions are affected to a greater extent:\r\n",
      "<pre><code>COUNTRY                  AVG'   ODDS'    SUM POINTS\r\n",
      "Armenia                 7.50 +  2.54 = 10.04     12\r\n",
      "Azerbaijan              3.75 +  5.23 =  8.98     10\r\n",
      "Russia                  7.23 +  1.62 =  8.85      8\r\n",
      "Ukraine                 6.30 +  1.54 =  7.84      7\r\n",
      "Romania                 6.07 +  1.61 =  7.68      6\r\n",
      "Greece                  4.08 +  1.96 =  6.04      5\r\n",
      "Georgia                 4.25 +  1.77 =  6.02      4\r\n",
      "Iceland                 3.83 +  1.72 =  5.55      3\r\n",
      "Serbia                  3.71 +  1.84 =  5.55      2\r\n",
      "Denmark                 3.25 +  2.25 =  5.50      1\r\n",
      "Sweden                  3.27 +  2.18 =  5.45      0\r\n",
      "...\r\n",
      "</code></pre>\r\n",
      "<h2><span style=\"font-size: 13px;\">Cross validation</span></h2>\r\n",
      "<h2><span style=\"font-weight: normal; font-size: 13px;\">The most important component of my solution was cross-validation and was probably the reason why I won the competition in the first place. It enabled me to try many different models and, between them, choose the one that was most likely to give the best results.</span></h2>\r\n",
      "The dataset was split into partitions, one for each Eurovision event. I then proceeded to build the model on all but one partition and calculated the error of that model on the partition that was left out. The procedure was repeated so that each time a different partition was left out. This gave me a fair estimate of how the model performs on unseen data.\r\n",
      "\r\n",
      "The cross-validation procedure in pseudocode:\r\n",
      "<pre><code>function crossValidation(dataset, buildModel):\r\n",
      "  error = 0\r\n",
      "  for year in eurovisionEvents:\r\n",
      "    learnData = {example | example in dataset and example.year != year}\r\n",
      "    testData  = {example | example in dataset and example.year == year}\r\n",
      "    model = buildModel(learnData)\r\n",
      "    error += testModel(model, testData)\r\n",
      "  return error</code></pre>\r\n",
      "<pre><span style=\"font-family: monospace, Monaco, 'Courier New', Courier, monospace;\"><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px; white-space: normal; font-size: 13px;\"><strong>Conclusion </strong></span></span></pre>\r\n",
      "<pre><span style=\"font-family: monospace, Monaco, 'Courier New', Courier, monospace;\"><strong><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-weight: normal; line-height: 19px; white-space: normal; font-size: 13px;\">I am well aware that certain parts of my approach are not very strong. I had to do my best with the time that was available. I have many ideas for next year, which I will, for the moment at least, keep to myself :)</span></strong></span></pre>\r\n",
      "I really enjoy competing in events like this and hope there will be more to come in the future.\n"
     ]
    }
   ],
   "source": [
    "for i in range(content.shape[0]):\n",
    "    posts = content.values[i] #Get all posts.\n",
    "    \n",
    "    for post in posts:\n",
    "        list.append(post)\n",
    "    \n",
    "#print(list[0])\n",
    "\n",
    "for i, post in enumerate(list):\n",
    "    str.append(''.join(list[i]))\n",
    "    \n",
    "print (str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, BeautifulSoup to remove html tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleantext=[]\n",
    "for i, post in enumerate(str):\n",
    "    cleantext.append(BeautifulSoup(str[i], \"lxml\").get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, just check if html tags have been removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2010 INFORMS Data Mining Contest has just finished. The competition attracted entries from 147 teams with participants from 27 countries. The winner was Cole Harris, followed by Christopher Hefele and Nan Zhou. Here is some background on the winners and the techniques they applied.\\r\\n\\r\\nCole Harris\\nAbout Cole:\\r\\n\\r\\n\"Since 2002 I have been VP Discovery and cofounder of Exagen Diagnostics. We mine genomic/medical data to identify genetic features that are diagnostic of disease, predictive of drug response, etc. and then develop medical tests from the results. Prior to this (2000-2002), at Quasar/Magnaflux, I developed pattern recognition algorithms for identifying defective metal parts from acoustic spectral data.\\xa0 From 1990-1999 I worked for Veritas Geophysical, most of that time developing algorithms for imaging seismic data. Prior to this I was in grad school (physics): MA 1990 Johns Hopkins University.\"\\r\\n\\r\\n\\nCole\\'s Method:\\r\\n\\r\\n\"As far as techniques, my submissions were mostly based on:\\r\\n\\r\\n1. pre-processing - I did many things, but none seemed to make a large difference in my early results on test data, so in the end, other than exclude the non-price data, I didn\\'t filter the data. I did append the data with data advanced 5 min, 60 min, and 65 min. So for each stock there were 16 features (open,hi,lo,last)X(0min,5min,60min,65min)\\r\\n\\r\\n2. feature selection - forward stepwise selection of stocks, reverse stepwise selection of particular features for a given stock, evaluated using logistic regression. This resulted in 5-6 features selected from 2 stocks.\\r\\n\\r\\nmodels: logistic regression and neural networks (not sure which won).\"\\r\\n\\r\\n\\nChristopher Hefele\\nAbout Christopher: \\r\\n\\r\\nChristopher is a Systems Engineer at AT&T. He was a member of The Ensemble, the team which finished second in the $1m Netflix Prize.\\r\\n\\r\\nChristopher\\'s Method:\\r\\n\\r\\n\"I was using a simple logistic regression on Variable 74 for most of the  contest. During the last few days, when every last bit counted, I  switched to a SVM & added more variables (i.e. Variables 167 &  55, chosen by forward stepwise logistic regression).\\r\\n\\r\\nIn the end, to me, this contest really was a  good lesson about the power of proper variable selection &  preprocessing.\"\\r\\n\\r\\nNan Zhou\\nAbout Nan:\\r\\n\\r\\nNan is currently completing his PhD in statistics at the University of Pittsburgh. His PhD research involves the estimation and prediction of  integrated volatility and model  calibration for financial stochastic processes. Prior to this he was a graduate student at Carnege Mellon University (focusing on statistical machine learning).\\r\\n\\r\\nNan\\'s Method:\\r\\n\\r\\n\"Among lots of other models (Support Vector Machine, Random Forest, Neutral Network, Gradient Boosting, AdaBoost, and etc.) I finally used ‘Two-Stages’ L1-penalized Logistic Regression, and tune the penalty parameter by 5-folds Cross Validation.\"\\r\\n\\r\\nYou can hear more from the winners (and others) on the competition\\'s forum.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleantext[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move onto some more pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = (set(stopwords.words('english')))\n",
    "\n",
    "#Union operation on sets\n",
    "stop_words = stop_words | {\",\",\".\",\";\",\"(\",\")\",\"'\",\"?\",\"...\",'-','=',':=','+','/',':', '%', ']','[', '+='}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, newsentence_array is the list being passed down for further POS tagging and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#POS Tagging\n",
    "tagged_docs=[]\n",
    "\n",
    "for doc in cleantext:\n",
    "    tagged_docs.append(nltk.pos_tag(word_tokenize(doc)))\n",
    "   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('About', 'IN'), ('me', 'PRP'), (':', ':'), ('I', 'PRP'), ('’', 'VBP'), ('m', 'PDT'), ('an', 'DT'), ('embedded', 'JJ'), ('systems', 'NNS'), ('engineer', 'NN'), (',', ','), ('currently', 'RB'), ('working', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('small', 'JJ'), ('engineering', 'NN'), ('company', 'NN'), ('in', 'IN'), ('Las', 'NNP'), ('Cruces', 'NNP'), (',', ','), ('New', 'NNP'), ('Mexico', 'NNP'), ('.', '.'), ('I', 'PRP'), ('graduated', 'VBD'), ('from', 'IN'), ('New', 'NNP'), ('Mexico', 'NNP'), ('Tech', 'NNP'), ('in', 'IN'), ('2007', 'CD'), (',', ','), ('with', 'IN'), ('degrees', 'NNS'), ('in', 'IN'), ('Electrical', 'NNP'), ('Engineering', 'NNP'), ('and', 'CC'), ('Computer', 'NNP'), ('Science', 'NNP'), ('.', '.'), ('Like', 'IN'), ('many', 'JJ'), ('people', 'NNS'), (',', ','), ('I', 'PRP'), ('first', 'RB'), ('became', 'VBD'), ('interested', 'JJ'), ('in', 'IN'), ('algorithm', 'JJ'), ('competitions', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('Netflix', 'NNP'), ('Prize', 'NNP'), ('a', 'DT'), ('few', 'JJ'), ('years', 'NNS'), ('ago', 'RB'), ('.', '.'), ('I', 'PRP'), ('was', 'VBD'), ('quite', 'RB'), ('excited', 'JJ'), ('to', 'TO'), ('find', 'VB'), ('the', 'DT'), ('Kaggle', 'NNP'), ('site', 'NN'), ('a', 'DT'), ('few', 'JJ'), ('months', 'NNS'), ('ago', 'RB'), (',', ','), ('as', 'IN'), ('I', 'PRP'), ('enjoy', 'VBP'), ('participating', 'VBG'), ('in', 'IN'), ('these', 'DT'), ('types', 'NNS'), ('of', 'IN'), ('competitions', 'NNS'), ('.', '.'), ('Explanation', 'NN'), ('of', 'IN'), ('Technique', 'NN'), (':', ':'), ('Though', 'IN'), ('I', 'PRP'), ('tried', 'VBD'), ('several', 'JJ'), ('different', 'JJ'), ('methods', 'NNS'), (',', ','), ('I', 'PRP'), ('used', 'VBD'), ('a', 'DT'), ('weighted', 'JJ'), ('combination', 'NN'), ('of', 'IN'), ('three', 'CD'), ('predictors', 'NNS'), ('to', 'TO'), ('come', 'VB'), ('up', 'RP'), ('with', 'IN'), ('the', 'DT'), ('final', 'JJ'), ('forecast', 'NN'), ('.', '.'), ('#', '#'), ('1', 'CD'), (':', ':'), ('After', 'IN'), ('reviewing', 'VBG'), ('Athanasopoulos', 'NNP'), ('et', 'CC'), ('al', 'NN'), (',', ','), ('it', 'PRP'), ('became', 'VBD'), ('obvious', 'JJ'), ('that', 'IN'), ('the', 'DT'), ('naive', 'JJ'), ('predictor', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('good', 'JJ'), ('algorithm', 'NN'), ('with', 'IN'), ('which', 'WDT'), ('to', 'TO'), ('start', 'VB'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('both', 'DT'), ('easy', 'JJ'), ('to', 'TO'), ('implement', 'VB'), ('and', 'CC'), ('performed', 'VBN'), ('well', 'RB'), ('when', 'WRB'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('other', 'JJ'), ('algorithms', 'NN'), ('in', 'IN'), ('the', 'DT'), ('paper', 'NN'), ('.', '.'), ('After', 'IN'), ('graphing', 'VBG'), ('a', 'DT'), ('few', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), (',', ','), ('it', 'PRP'), ('became', 'VBD'), ('apparent', 'JJ'), ('that', 'IN'), ('many', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('series', 'NN'), ('increase', 'NN'), ('with', 'IN'), ('time', 'NN'), ('.', '.'), ('Indeed', 'RB'), (',', ','), ('the', 'DT'), ('second', 'JJ'), ('sentence', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Athanasopoulos', 'NNP'), ('paper', 'NN'), ('states', 'NNS'), ('that', 'IN'), ('globally', 'RB'), ('tourism', 'NN'), ('has', 'VBZ'), ('grown', 'VBN'), ('“', 'NNS'), ('at', 'IN'), ('a', 'DT'), ('rate', 'NN'), ('of', 'IN'), ('6', 'CD'), ('%', 'NN'), ('annually.', 'JJ'), ('”', 'NN'), ('In', 'IN'), ('order', 'NN'), ('to', 'TO'), ('take', 'VB'), ('advantage', 'NN'), ('of', 'IN'), ('this', 'DT'), ('knowledge', 'NN'), (',', ','), ('I', 'PRP'), ('multiplied', 'VBD'), ('the', 'DT'), ('(', '('), ('Naive', 'JJ'), ('algorithm', 'NN'), ('’', 'NNP'), ('s', 'NN'), (')', ')'), ('predicted', 'VBD'), ('value', 'NN'), ('by', 'IN'), ('a', 'DT'), ('factor', 'NN'), ('to', 'TO'), ('take', 'VB'), ('this', 'DT'), ('growth', 'NN'), ('into', 'IN'), ('account', 'NN'), ('.', '.'), ('With', 'IN'), ('some', 'DT'), ('testing', 'NN'), (',', ','), ('I', 'PRP'), ('determined', 'VBD'), ('a', 'DT'), ('5.5', 'CD'), ('%', 'NN'), ('growth', 'NN'), ('factor', 'NN'), ('to', 'TO'), ('yield', 'VB'), ('the', 'DT'), ('lowest', 'JJS'), ('MASE', 'NNP'), ('.', '.'), ('prediction1', 'NN'), ('=', 'JJ'), ('last_value', 'JJ'), ('*', 'NNP'), ('(', '('), ('1.055', 'CD'), ('**', 'NNP'), ('number_of_years_in_the_future', 'NN'), (')', ')'), ('#', '#'), ('2', 'CD'), (':', ':'), ('I', 'PRP'), ('examined', 'VBD'), ('fitting', 'VBG'), ('a', 'DT'), ('polynomial', 'JJ'), ('line', 'NN'), ('to', 'TO'), ('the', 'DT'), ('data', 'NNS'), ('and', 'CC'), ('using', 'VBG'), ('the', 'DT'), ('line', 'NN'), ('to', 'TO'), ('predict', 'VB'), ('future', 'JJ'), ('values', 'NNS'), ('.', '.'), ('I', 'PRP'), ('tried', 'VBD'), ('using', 'VBG'), ('first', 'RB'), ('through', 'IN'), ('fifth', 'JJ'), ('order', 'NN'), ('polynomials', 'NNS'), ('to', 'TO'), ('find', 'VB'), ('that', 'IN'), ('the', 'DT'), ('lowest', 'JJS'), ('MASE', 'NN'), ('was', 'VBD'), ('obtained', 'VBN'), ('using', 'VBG'), ('a', 'DT'), ('first', 'JJ'), ('order', 'NN'), ('polynomial', 'NN'), ('(', '('), ('simple', 'JJ'), ('regression', 'NN'), ('line', 'NN'), (')', ')'), ('.', '.'), ('This', 'DT'), ('best', 'JJS'), ('fit', 'JJ'), ('line', 'NN'), ('was', 'VBD'), ('used', 'VBN'), ('to', 'TO'), ('predict', 'VB'), ('future', 'JJ'), ('values', 'NNS'), ('.', '.'), ('I', 'PRP'), ('also', 'RB'), ('kept', 'VBD'), ('the', 'DT'), ('r**2', 'JJ'), ('value', 'NN'), ('of', 'IN'), ('the', 'DT'), ('fit', 'NN'), ('for', 'IN'), ('use', 'NN'), ('in', 'IN'), ('blending', 'VBG'), ('the', 'DT'), ('results', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('predictor', 'NN'), ('.', '.'), ('#', '#'), ('3', 'CD'), (':', ':'), ('In', 'IN'), ('thinking', 'VBG'), ('about', 'IN'), ('these', 'DT'), ('two', 'CD'), ('predictors', 'NNS'), (',', ','), ('I', 'PRP'), ('recognized', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('naive', 'JJ'), ('predictor', 'NN'), (',', ','), ('though', 'IN'), ('accurate', 'JJ'), (',', ','), ('throws', 'VBZ'), ('away', 'RB'), ('most', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('provided', 'VBN'), ('data', 'NN'), (',', ','), ('and', 'CC'), ('only', 'RB'), ('uses', 'VBZ'), ('a', 'DT'), ('single', 'JJ'), ('element', 'NN'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), ('.', '.'), ('The', 'DT'), ('polynomial', 'JJ'), ('line', 'NN'), ('predictor', 'NN'), ('uses', 'VBZ'), ('all', 'DT'), ('of', 'IN'), ('the', 'DT'), ('data', 'NNS'), (',', ','), ('weighted', 'VBD'), ('equally', 'RB'), (',', ','), ('though', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('recent', 'JJ'), ('data', 'NN'), ('is', 'VBZ'), ('probably', 'RB'), ('of', 'IN'), ('more', 'JJR'), ('value', 'NN'), ('in', 'IN'), ('indicating', 'VBG'), ('future', 'JJ'), ('performance', 'NN'), ('than', 'IN'), ('the', 'DT'), ('earlier', 'JJR'), ('data', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), ('.', '.'), ('I', 'PRP'), ('examined', 'VBD'), ('and', 'CC'), ('eventually', 'RB'), ('used', 'VBD'), ('an', 'DT'), ('exponentially-weighted', 'JJ'), ('least', 'NN'), ('squares', 'NNS'), ('regression', 'VBP'), ('to', 'TO'), ('fit', 'VB'), ('a', 'DT'), ('line', 'NN'), ('to', 'TO'), ('the', 'DT'), ('data', 'NN'), ('.', '.'), ('This', 'DT'), ('algorithm', 'NN'), ('gave', 'VBD'), ('more', 'RBR'), ('accurate', 'JJ'), ('predictions', 'NNS'), ('for', 'IN'), ('many', 'JJ'), ('of', 'IN'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), ('by', 'IN'), ('itself', 'PRP'), (',', ','), ('and', 'CC'), ('also', 'RB'), ('lowered', 'VBD'), ('the', 'DT'), ('MASE', 'NNP'), ('when', 'WRB'), ('used', 'VBN'), ('in', 'IN'), ('combination', 'NN'), ('with', 'IN'), ('the', 'DT'), ('two', 'CD'), ('above', 'IN'), ('predictors', 'NNS'), ('.', '.'), ('The', 'DT'), ('r**2', 'JJ'), ('value', 'NN'), ('of', 'IN'), ('this', 'DT'), ('fit', 'NN'), ('was', 'VBD'), ('also', 'RB'), ('used', 'VBN'), ('for', 'IN'), ('blending', 'VBG'), ('the', 'DT'), ('predictors', 'NNS'), ('.', '.'), ('Blending', 'VBG'), ('stage', 'NN'), (':', ':'), ('I', 'PRP'), ('started', 'VBD'), ('with', 'IN'), ('a', 'DT'), ('basic', 'JJ'), ('weighted', 'JJ'), ('blend', 'NN'), ('of', 'IN'), ('predictors', 'NNS'), ('.', '.'), ('I', 'PRP'), ('used', 'VBD'), ('a', 'DT'), ('constant', 'JJ'), ('weight', 'NN'), ('factor', 'NN'), ('for', 'IN'), ('the', 'DT'), ('modified', 'JJ'), ('naive', 'JJ'), ('predictor', 'NN'), (',', ','), ('while', 'IN'), ('blending', 'VBG'), ('weights', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('unweighted', 'JJ'), ('and', 'CC'), ('weighted', 'JJ'), ('regression', 'NN'), ('lines', 'NNS'), ('depending', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('r**2', 'NN'), ('values', 'NNS'), ('found', 'VBN'), ('in', 'IN'), ('fitting', 'VBG'), ('the', 'DT'), ('time', 'NN'), ('series', 'NN'), ('.', '.'), ('I', 'PRP'), ('selected', 'VBD'), ('the', 'DT'), ('logistic', 'JJ'), ('function', 'NN'), ('as', 'IN'), ('a', 'DT'), ('way', 'NN'), ('of', 'IN'), ('gradually', 'RB'), ('increasing', 'VBG'), ('the', 'DT'), ('weight', 'NN'), ('with', 'IN'), ('increasing', 'VBG'), ('r**2', 'JJ'), ('value', 'NN'), (':', ':'), ('weight', 'NN'), ('=', 'VBZ'), ('a', 'DT'), ('*', 'JJ'), ('logistic', 'JJ'), ('(', '('), ('b', 'JJ'), ('*', 'NN'), ('(', '('), ('r**2', 'JJ'), ('-', ':'), ('c', 'NN'), (')', ')'), (')', ')'), ('Values', 'VBZ'), ('for', 'IN'), ('a', 'DT'), (',', ','), ('b', 'NN'), (',', ','), ('and', 'CC'), ('c', 'NNS'), ('were', 'VBD'), ('determined', 'VBN'), ('by', 'IN'), ('trial', 'NN'), ('and', 'CC'), ('error', 'NN'), ('.', '.'), ('I', 'PRP'), ('also', 'RB'), ('examined', 'VBD'), ('using', 'VBG'), ('some', 'DT'), ('numeric', 'JJ'), ('optimization', 'NN'), ('functions', 'NNS'), ('included', 'VBD'), ('in', 'IN'), ('Python', 'NNP'), ('to', 'TO'), ('minimize', 'VB'), ('a', 'DT'), ('training', 'NN'), ('set', 'VBN'), ('MASE', 'NNP'), ('.', '.'), ('While', 'IN'), ('this', 'DT'), ('succeeded', 'VBN'), ('in', 'IN'), ('lower', 'JJR'), ('the', 'DT'), ('training', 'NN'), ('set', 'VBN'), ('MASE', 'NNP'), (',', ','), ('I', 'PRP'), ('discarded', 'VBD'), ('this', 'DT'), ('method', 'NN'), ('when', 'WRB'), ('I', 'PRP'), ('received', 'VBD'), ('a', 'DT'), ('lower', 'JJR'), ('leaderboard', 'NN'), ('MASE', 'NNP'), ('(', '('), ('possibly', 'RB'), ('from', 'IN'), ('overfitting', 'VBG'), (')', ')'), ('.', '.'), ('Testing', 'VBG'), ('/', 'JJ'), ('developing', 'VBG'), ('algorithms', 'NN'), (':', ':'), ('When', 'WRB'), ('testing', 'VBG'), ('algorithms', 'RB'), (',', ','), ('I', 'PRP'), ('used', 'VBD'), ('the', 'DT'), ('last', 'JJ'), ('four', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('training', 'NN'), ('dataset', 'NN'), ('(', '('), ('after', 'IN'), ('removing', 'VBG'), ('them', 'PRP'), ('from', 'IN'), ('the', 'DT'), ('data', 'NN'), ('I', 'PRP'), ('was', 'VBD'), ('using', 'VBG'), ('for', 'IN'), ('training', 'VBG'), (')', ')'), ('to', 'TO'), ('test', 'VB'), ('against', 'IN'), ('.', '.'), ('While', 'IN'), ('this', 'DT'), ('worked', 'VBN'), ('well', 'RB'), ('in', 'IN'), ('the', 'DT'), ('initial', 'JJ'), ('stages', 'NNS'), (',', ','), ('I', 'PRP'), ('found', 'VBD'), ('that', 'IN'), ('once', 'RB'), ('my', 'PRP$'), ('leaderboard', 'JJ'), ('MASE', 'NNP'), ('got', 'VBD'), ('below', 'IN'), ('about', 'IN'), ('2.05', 'CD'), (',', ','), ('this', 'DT'), ('‘', 'JJ'), ('training', 'NN'), ('MASE', 'NNP'), ('’', 'NNP'), ('became', 'VBD'), ('a', 'DT'), ('much', 'RB'), ('less', 'RBR'), ('reliable', 'JJ'), ('indicator', 'NN'), ('of', 'IN'), ('whether', 'IN'), ('the', 'DT'), ('leaderboard', 'JJ'), ('MASE', 'NNP'), ('would', 'MD'), ('increase', 'VB'), ('or', 'CC'), ('decrease', 'VB'), ('with', 'IN'), ('a', 'DT'), ('change', 'NN'), ('.', '.'), ('So', 'RB'), (',', ','), ('during', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('few', 'JJ'), ('weeks', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('contest', 'NN'), (',', ','), ('I', 'PRP'), ('primarily', 'RB'), ('made', 'VBD'), ('small', 'JJ'), ('tweaks', 'NNS'), (',', ','), ('and', 'CC'), ('tested', 'VBD'), ('their', 'PRP$'), ('value', 'NN'), ('by', 'IN'), ('submitting', 'VBG'), ('a', 'DT'), ('new', 'JJ'), ('prediction', 'NN'), ('to', 'TO'), ('Kaggle', 'NNP'), ('rather', 'RB'), ('than', 'IN'), ('comparing', 'VBG'), ('my', 'PRP$'), ('MASE', 'NNP'), ('results', 'NNS'), ('.', '.'), ('I', 'PRP'), ('believe', 'VBP'), ('that', 'IN'), ('this', 'DT'), ('indicates', 'VBZ'), ('a', 'DT'), ('significant', 'JJ'), ('difference', 'NN'), ('in', 'IN'), ('nature', 'NN'), ('of', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('4', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('training', 'NN'), ('set', 'VBN'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('4', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('test', 'NN'), ('set', 'NN'), ('.', '.'), ('If', 'IN'), ('the', 'DT'), ('test', 'NN'), ('set', 'NN'), ('includes', 'VBZ'), ('data', 'NNS'), ('from', 'IN'), ('2008-2009', 'CD'), (',', ','), ('I', 'PRP'), ('’', 'VBP'), ('m', 'JJ'), ('speculating', 'NN'), ('that', 'WDT'), ('depressed', 'VBD'), ('tourism', 'NN'), ('numbers', 'NNS'), ('as', 'IN'), ('a', 'DT'), ('result', 'NN'), ('of', 'IN'), ('the', 'DT'), ('global', 'JJ'), ('economic', 'JJ'), ('recession', 'NN'), ('could', 'MD'), ('have', 'VB'), ('caused', 'VBN'), ('a', 'DT'), ('significant', 'JJ'), ('difference', 'NN'), ('in', 'IN'), ('the', 'DT'), ('trends', 'NNS'), ('.', '.'), ('Possible', 'JJ'), ('improvements', 'NNS'), (':', ':'), ('While', 'IN'), ('the', 'DT'), ('above', 'JJ'), ('method', 'NN'), ('seemed', 'VBD'), ('to', 'TO'), ('work', 'VB'), ('fairly', 'RB'), ('well', 'RB'), ('at', 'IN'), ('predicting', 'VBG'), ('tourism', 'NN'), ('numbers', 'NNS'), (',', ','), ('there', 'EX'), ('are', 'VBP'), ('several', 'JJ'), ('steps', 'NNS'), ('that', 'WDT'), ('could', 'MD'), ('have', 'VB'), ('likely', 'RB'), ('improved', 'VBN'), ('the', 'DT'), ('score', 'NN'), ('.', '.'), ('I', 'PRP'), ('only', 'RB'), ('implemented', 'VBD'), ('the', 'DT'), ('Naive', 'NNP'), ('method', 'NN'), ('implemented', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('Athanasopoulos', 'NNP'), ('paper', 'NN'), (';', ':'), ('I', 'PRP'), ('do', 'VBP'), ('think', 'VB'), ('that', 'IN'), ('including', 'VBG'), ('a', 'DT'), ('couple', 'NN'), ('of', 'IN'), ('other', 'JJ'), ('algorithms', 'JJ'), ('’', 'NN'), ('output', 'NN'), ('into', 'IN'), ('the', 'DT'), ('final', 'JJ'), ('blend', 'NN'), ('could', 'MD'), ('have', 'VB'), ('further', 'RBR'), ('increased', 'VBN'), ('the', 'DT'), ('score', 'NN'), ('.', '.'), ('If', 'IN'), ('I', 'PRP'), ('had', 'VBD'), ('a', 'DT'), ('few', 'JJ'), ('more', 'JJR'), ('days', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('on', 'IN'), ('a', 'DT'), ('solution', 'NN'), (',', ','), ('I', 'PRP'), ('would', 'MD'), ('have', 'VB'), ('tried', 'VBN'), ('to', 'TO'), ('implement', 'VB'), ('the', 'DT'), ('Theta', 'NNP'), ('and', 'CC'), ('ARIMA', 'NNP'), ('methods', 'NNS'), ('described', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('paper', 'NN'), ('and', 'CC'), ('looked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('effects', 'NNS'), ('of', 'IN'), ('including', 'VBG'), ('them', 'PRP'), ('in', 'IN'), ('the', 'DT'), ('blend', 'NN'), ('.', '.'), ('I', 'PRP'), ('think', 'VBP'), ('an', 'DT'), ('investigation', 'NN'), ('into', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('come', 'VB'), ('up', 'RP'), ('with', 'IN'), ('a', 'DT'), ('blending', 'NN'), ('method', 'NN'), ('that', 'WDT'), ('doesn', 'VBZ'), ('’', 'NNP'), ('t', 'NN'), ('use', 'NN'), ('as', 'IN'), ('much', 'JJ'), ('manual', 'JJ'), ('tweaking', 'NN'), ('would', 'MD'), ('also', 'RB'), ('be', 'VB'), ('of', 'IN'), ('benefit', 'NN'), ('.', '.'), ('I', 'PRP'), ('enjoyed', 'VBD'), ('participating', 'VBG'), ('in', 'IN'), ('part', 'NN'), ('one', 'CD'), (',', ','), ('and', 'CC'), ('look', 'VB'), ('forward', 'RB'), ('to', 'TO'), ('part', 'NN'), ('two', 'CD'), ('of', 'IN'), ('the', 'DT'), ('contest', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print (tagged_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us develop a chunker which will help us in identifying certain sets of entities from doc to doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = \"NPC: {<JJ.*>*<NN.*>+}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(tagged_docs):\n",
    "    if (i==2):\n",
    "        print (type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(result)\n",
    "\n",
    "chunk_docs= []\n",
    "temp_chunk=[]\n",
    "#for i, doc in enumerate(tagged_docs):\n",
    "#result = chunk_parser.parse(tagged_docs[1])\n",
    "for i,  doc in enumerate(tagged_docs):\n",
    "    result = chunk_parser.parse(doc)\n",
    "    \n",
    "    for j, subtree in enumerate(result.subtrees(filter=lambda t: t.label() == 'CCCC')):\n",
    "        temp_chunk.append(\" \".join([a for (a,b) in subtree.leaves()]))\n",
    "    \n",
    "       \n",
    "    chunk_docs.append(temp_chunk)\n",
    "    temp_chunk=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "underscore_chunk_docs=copy.deepcopy(chunk_docs)\n",
    "\n",
    "for k in range(len(chunk_docs)):\n",
    "    for i, item in enumerate(chunk_docs[k]):\n",
    "        item = item.strip()\n",
    "        #print(item)\n",
    "        underscore_chunk_docs[k][i] = \"-\".join( item.split() )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print (len(underscore_chunk_docs[100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_cleantext = copy.deepcopy(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following Q&A is with Jeremy Howard who together with teammate Lee Baker won Kaggle's Tourism Forecasting Competition. (This was a two-part competition - Lee previously described his work on Part I of the competition in a separate blog post).\r\n",
      " \n",
      "- where you're from, what you have studied\r\n",
      "\r\n",
      "I live in Melbourne, Australia, recently voted the world's 3rd most livable city... Perhaps this is inspiring a bit of data mining success around here (it's also the home of the tourism2 winner, and the Kaggle CEO)! I studied philosophy at the University of Melbourne.\r\n",
      "\r\n",
      "- what you do\r\n",
      "\r\n",
      "After university I worked in management consulting (McKinsey & Co, AT Kearney), and then went on to found 2 businesses (FastMail.FM, an email provider, and Optimal Decisions Group, an insurance pricing optimisation specialist). Having sold both in the last couple of years, I now have the free time to follow my interests.\r\n",
      "\r\n",
      "- core technical approach\r\n",
      "\r\n",
      "My goal was to try to stay in line with the approach taken in the paper being submitted by the contest organisers - I wanted to find a general, automated algorithm for forecasting, which I could apply to all time series without any parameter tuning or manual involvement. I had hoped therefore to only do a single submission to the leaderboard. However, an early data problem in the posted data (later rectified by the organisers) unfortunately meant this wasn't possible. After the fixed data was posted, I only did 3 further submissions.\r\n",
      "\r\n",
      "I realised that a fundamental issue was that the final results were calculated using a novel algorithm called \"MASE\", which is a ratio. The denominator of the ratio could in some cases be extremely small - this occured in series which had close to constant additive seasonality, no growth, and no noise. I found that the contribution of these series to the overall result was so high that in practice the algorithm should be tuned to favor these, even at the expense of other series (which had a relatively high denominator, and thus contributed much less to the overall result).\r\n",
      "\r\n",
      "To do this, I only used linear growth (as opposed to exponential) and additive seasonality (as opposed to multiplicative) for all series, since any series which had exponential growth and/or multiplicative seasonality would have very small weights in the overall metric. Later, I experimented with allowing some series to use exponential growth and multiplicative seasonality, if the statistical evidence for those series was particularly strong, and confirmed that the impact was negative, as expected.\r\n",
      "\r\n",
      "- methodology that proved most effective\r\n",
      "\r\n",
      "I first created an algorithm to automatically remove outliers. Outliers can occur in, for example, a tourism time series if an area has a once-off event (positive outlier), or temporary closing of a major attraction (negative outlier), which will not impact future results. I used a customised local smoother, and used the residuals to determine seasonality. I ran this twice to create a double-smoothed time series, which I then compared to the original data, and removed data points outside 3 standard deviations of residuals.\r\n",
      "\r\n",
      "I then fitted a weighted regression (weighted the most recent observations the most heavily) combined with weighted additive seasonality (again weighting the most recent observations the most heavily) on all but the last 2 years of each series. A simple optimiser found the optimal weighting of each in order to predict the final 2 years. This weighted model was then applied to the full data set to create predictions. The intercept of the weighted regression was adjusted such that the residual on the final observation was always zero - this was important for ensuring that the series with a low denominator in the MASE metric were forecast as accurately as possible.\r\n",
      "\r\n",
      "I've since realised I had some bugs in my code (e.g. failing to truncate series to be positive, and some bugs with going from a validation set to the final predictions). It would be interesting to see how much better the predictions would be if these bugs were fixed.\r\n",
      "\r\n",
      "The whole algorithm takes only 10 seconds to complete for the entire set of time series. Since the algorithm is fast, accurate, and automated, I think it is a good system for automated time series prediction. I plan to test it in the future on other data sets (e.g. the \"M3\" time series prediction competition data) to confirm that it can be effectively applied to other types of data.\r\n",
      "\r\n",
      "- what first attracted you to the competition? \r\n",
      "\r\n",
      "The tourism forecasting competition was my first data mining contest - I entered it in order to try to update and strengthen my analysis skills, and to learn something new (having never done time series forecasting before).\r\n",
      "\r\n",
      "- did you do much background reading or research? \r\n",
      "\r\n",
      "Yes, I read most of the recent papers and online tutorials by one of the conference organisers, Rob J Hyndman. I found that they were a great way for a time-series newbie like myself to get up to speed with the topic.\r\n",
      "\r\n",
      "- what tools and programming language did you use?\r\n",
      "\r\n",
      "I used C#, in Visual Studio 2010.\r\n",
      "\r\n",
      "- how much time did you spend on the competition?\r\n",
      "\r\n",
      "I spent longer than I expected because the initial data problems left me stumped and confused! Once they were fixed, I had submitted my result within a couple of hours. I estimate I spent a couple of weeks on the problem, including reading and research.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, doc in enumerate(ch_cleantext):\n",
    "    for j in range(len(underscore_chunk_docs[i])):\n",
    "        #print (underscore_chunk_docs[i][j])\n",
    "        ch_cleantext[i] = ch_cleantext[i].replace(chunk_docs[i][j],underscore_chunk_docs[i][j])\n",
    "    \n",
    "\n",
    "print (ch_cleantext[8])\n",
    "\n",
    "#nltk.pos_tag(word_tokenize(ch_cleantext[2]))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we move onto plotting the networkx graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "match : 0.24088550781659265\n",
      "variables : 0.2339334386953435\n",
      "using : 0.19746554506680664\n",
      "set : 0.18526401690175182\n",
      "test-data : 0.16899215641508344\n",
      "group : 0.16372617822532445\n",
      "selected : 0.15964117272336786\n",
      "features : 0.147886789233308\n",
      "predictions : 0.1478129164324859\n",
      "five : 0.12891254218564033\n",
      "training : 0.1287607667492887\n",
      "rfe-function : 0.12324398375432322\n",
      "order : 0.1104534882755333\n",
      "best : 0.09185143125809032\n",
      "“yellow” : 0.08965760247537904\n",
      "data : 0.08930379909795577\n",
      "rt215 : 0.08547386760249594\n",
      "“r” : 0.08274820961692521\n",
      "allowed : 0.07918085308523551\n",
      "see : 0.07449967347520005\n",
      "would : 0.06587915797743156\n",
      "120 : 0.0638945124242033\n",
      "make : 0.06327215702571419\n",
      "closely : 0.06273472324031992\n",
      "machine-learning. : 0.061721368494276854\n",
      "important. : 0.05797362125146227\n",
      "package : 0.057946536940086536\n",
      "important : 0.05618671127350694\n",
      "contest). : 0.05422753430721558\n",
      "enjoyed : 0.05422753430721558\n",
      "[rows : 0.054131642193679864\n",
      "training-data : 0.049480759040140605\n",
      "predict : 0.047597909303468036\n",
      "identified : 0.04723897305712904\n",
      "obtained : 0.04667046101309049\n",
      "contest : 0.04667046101309049\n",
      "via : 0.0451504738630047\n",
      "unaware : 0.0431185897795649\n",
      "r-'caret' : 0.041687605758702305\n",
      "separate-groups : 0.041269731764893916\n",
      "caret-package : 0.04082715896532061\n",
      "patient-response. : 0.040357031544661984\n",
      "(which : 0.0390501486119016\n",
      "immensely : 0.0390501486119016\n",
      "vl.t0, : 0.03876813687096372\n",
      "previous : 0.03846364252715041\n",
      "randomforest : 0.03821133843425629\n",
      "rt184. : 0.03814341342798941\n",
      "last-line-shows : 0.03656623990763622\n",
      "package. : 0.035877712020758866\n",
      "accurately : 0.03583653544050055\n",
      "function : 0.035402833111996455\n",
      "different-model : 0.03511289937252753\n",
      "cd4.t0, : 0.03491235474967216\n",
      "adjusting : 0.03392312583879646\n",
      "[patients : 0.03354887328990914\n",
      "attended : 0.03312247769656229\n",
      "step. : 0.033053085209934004\n",
      "0.7383 : 0.03255860408203417\n",
      "known : 0.03189885958810261\n",
      "try : 0.031803387243531434\n",
      "private-scores : 0.03136659710364889\n",
      "look : 0.03136659710364889\n",
      "time : 0.030703013092502757\n",
      "particularly : 0.03022939090217595\n",
      "0.1153 : 0.02909952530517888\n",
      "‘caret’ : 0.0286526440387188\n",
      "shown : 0.027670756083296423\n",
      "group, : 0.02759798548311168\n",
      "find : 0.02733915799273417\n",
      "overall-group-response : 0.026408211949380565\n",
      "353:903] : 0.026023419549315953\n",
      "“’r’ : 0.02599068739744132\n",
      "–-rt184 : 0.02557016374366026\n",
      "needed : 0.025381881393548994\n",
      "more-features : 0.025289156938749037\n",
      "rt184 : 0.025231876725900628\n",
      "0.3493 : 0.024969911234377182\n",
      "last-submission: : 0.024515919692356808\n",
      "contain : 0.02434178629397752\n",
      "interesting-– : 0.02374324177231595\n",
      "balanced : 0.023705334765854603\n",
      "public : 0.02361980648833238\n",
      "forward : 0.02361980648833238\n",
      "made : 0.02316448491747296\n",
      "test-dataset. : 0.02236695696428194\n",
      "training-dataset: : 0.022138957397921547\n",
      "further-imbalances : 0.021822477834145444\n",
      "max-kuhn : 0.021522434717274417\n",
      "0.04698 : 0.02151083245752189\n",
      "matchcontrols : 0.020457324142583107\n",
      "recommend : 0.020427026062816225\n",
      "became : 0.020083071593695774\n",
      "could : 0.01991294196995203\n",
      "refinements. : 0.01990044862246537\n",
      "user : 0.01908926813119565\n",
      "estimating : 0.018893298059964737\n",
      "best-matches : 0.018819796286121456\n",
      "looked : 0.018802661438072174\n",
      "three : 0.018801061814721468\n",
      "listed. : 0.01872070799693641\n",
      "average-response : 0.01854539513902063\n",
      "mentioned : 0.017643183878243637\n",
      "felt : 0.017566925440194264\n",
      "0.05648 : 0.01738121838672019\n",
      "highly : 0.01716376494525802\n",
      "many-functions : 0.016995486200807756\n",
      "pick : 0.016980065263123508\n",
      "actual-output : 0.016950941509848726\n",
      "600 : 0.01684389928318974\n",
      "346 : 0.016811062627795696\n",
      "models : 0.016740445624907778\n",
      "0.1121 : 0.016727674610429313\n",
      "certain-areas : 0.01588935264234069\n",
      "read. : 0.015814983954331334\n",
      "shows : 0.015809776765952063\n",
      "free-time : 0.015809776765952063\n",
      "probably : 0.015788759800806844\n",
      "impressive : 0.01569459124951442\n",
      "omitted] : 0.015612428318745873\n",
      "recursive-feature-elimination : 0.015578055654654017\n",
      "optimum, : 0.01557714719353876\n",
      "balance : 0.015383176289550783\n",
      "partitioned. : 0.015350766247180587\n",
      "population : 0.014926574879619835\n",
      "non--responders. : 0.014768966462193547\n",
      "presentation : 0.014618832196142002\n",
      "segmenting : 0.01458188410379645\n",
      "amino : 0.01442783915986344\n",
      "narrowed : 0.01437613115188528\n",
      "obviously : 0.014240957560536395\n",
      "wanted : 0.013960660256429563\n",
      "0.3225 : 0.013922139609864898\n",
      "much : 0.013660174118341447\n",
      "didn’t : 0.013456579326433247\n",
      "machine-learning : 0.013059477885886541\n",
      "used : 0.01293187040198992\n",
      "conference : 0.012790201359732757\n",
      "pain : 0.012629640169480781\n",
      "sure : 0.012609752474104586\n",
      "quickly : 0.012400896004576513\n",
      "predicted). : 0.012101631267823638\n",
      "correctly, : 0.011825713020932141\n",
      "resolved : 0.011778283690634288\n",
      "rows : 0.011778283690634288\n",
      "simply : 0.01146966681020941\n",
      "5 : 0.011424297541732591\n",
      "(out : 0.01140902958632042\n",
      "trained : 0.011380931301250027\n",
      "judged : 0.011262470902482283\n",
      "(the : 0.011227162401038323\n",
      "32.9% : 0.011178039166086976\n",
      "possible-– : 0.010934951871206848\n",
      "“red” : 0.010498689250349278\n",
      "120): : 0.010433867498466249\n",
      "several-papers : 0.010180969244714264\n",
      "0.1393 : 0.009792525539063197\n",
      "‘randomforest’ : 0.009717742785471868\n",
      "each. : 0.009596534496932903\n",
      "part : 0.009561752988047805\n",
      "second : 0.009449917638591506\n",
      "– : 0.00909710866787323\n",
      "overall : 0.009072327470164688\n",
      "0.04884 : 0.009028313325410672\n",
      "saw : 0.009023488702392139\n",
      "contained : 0.008755981365543114\n",
      "represented. : 0.008748866965998437\n",
      "accuracy : 0.008710986238969542\n",
      "dataset : 0.008673243533801302\n",
      "papers : 0.008563566344347984\n",
      "originally : 0.008540557757973799\n",
      "packages : 0.008482472227492143\n",
      "able : 0.008476938805624067\n",
      "important, : 0.008429245979046774\n",
      "randomly : 0.008405004321338982\n",
      "2010” : 0.007960841550293267\n",
      "various-reasons. : 0.007947800634197899\n",
      "validation-enhancements : 0.007947047787685237\n",
      "graph : 0.007936507936507936\n",
      "work : 0.007936507936507934\n",
      "went : 0.007830356578222262\n",
      "down. : 0.007673839788239331\n",
      "confident : 0.007601240289534731\n",
      "except : 0.00750941810987343\n",
      "794 : 0.007388435675288268\n",
      "began : 0.007205979900914447\n",
      "separate : 0.007161828874976285\n",
      "kappasd : 0.007035551419918714\n",
      "90 : 0.00679727321730738\n",
      "acid : 0.006681048544546931\n",
      "overall-dataset). : 0.006507831109424736\n",
      "test-set. : 0.006490741493587252\n",
      "late-july : 0.006426486043731347\n",
      "found : 0.0064202123227924904\n",
      "0.7276 : 0.006333446762207909\n",
      "majority : 0.006211284868086234\n",
      "research : 0.006153165117308543\n",
      "successful. : 0.005871349134012765\n",
      "(close : 0.005855414321549778\n",
      "graph, : 0.00563303829050367\n",
      "results : 0.00547682035777274\n",
      "third : 0.00507690829024542\n",
      "similar : 0.005031762594369308\n",
      "predicted-group : 0.00500063239107064\n",
      "top : 0.004840730073133782\n",
      "function-countless-times, : 0.004522951278785084\n",
      "mentioned-rt-codon : 0.004311828021750237\n",
      "few-more-iterations : 0.00407655093910074\n",
      "match-controls : 0.004041505933936211\n",
      "32.6% : 0.004015683298551824\n",
      "excluding : 0.003968253968253968\n",
      "230 : 0.003968253968253968\n",
      "ran : 0.003934300590532804\n",
      "tried : 0.0038391407913320272\n",
      "appear : 0.0038307089103901855\n",
      "didn't : 0.0038296549252724555\n",
      "close : 0.0036788098422513194\n",
      "e1071-package : 0.003626073821482663\n",
      "better : 0.0035284535829973114\n",
      "read : 0.003280001686376188\n",
      "matching-controls : 0.002904255991905395\n",
      "initial-attempts : 0.00260348310528542\n",
      "two : 0.0025321992453466555\n",
      "* : 0.0022986913520019188\n",
      "separately : 0.002248037329141485\n",
      "206 : 0.002237083412382216\n",
      "entire-dataset. : 0.0020057336790404523\n",
      "reflect : 0.001979233481794665\n",
      "separately. : 0.001829266456471919\n",
      "kappa : 0.0017822386443141515\n",
      "responders : 0.0017206307046944492\n",
      "%-responders) : 0.0017104672767734805\n",
      "qiyqepfknlk, : 0.0017065524749076252\n",
      "amount : 0.001651820533436924\n",
      "smaller : 0.001593098505449103\n",
      "0.3148 : 0.0015186693615834163\n",
      "topic, : 0.0012595122156875142\n",
      "treat : 0.0011619775279422405\n",
      "represented : 0.0010400072674783354\n",
      "accuracysd : 0.0009445212347887361\n",
      "tuning : 0.0007852189127089524\n",
      "alone : 0.0007114399544678429\n",
      "150 : 0.0005691519635742742\n",
      "rt184, : 0.0005412213579544257\n",
      "0.7233 : 0.0003952444191488016\n",
      "(32.6 : 0.00019551423933894047\n",
      "first : 6.323910706380825e-05\n",
      "mean-response. : 3.161955353190413e-05\n",
      "initial-strategy : 0.0\n",
      "designated : 0.0\n",
      "feature-selection. : 0.0\n",
      "anyone : 0.0\n",
      "chess-contest. : 0.0\n"
     ]
    }
   ],
   "source": [
    "xword_graph = nx.Graph()\n",
    "\n",
    "#Change the no. to access different documents\n",
    "old_flat_list = ch_cleantext[1].split()\n",
    "\n",
    "#Remove stopwords from the graphs.\n",
    "\n",
    "flat_list = [w.lower() for w in old_flat_list if w.lower() not in stop_words]\n",
    "\n",
    "print (type(xword_graph.nodes()))\n",
    "#set sliding window size\n",
    "#sliding_size = 2\n",
    "#print ((flat_list))\n",
    "#Iterate and form the word graph, increase weight when more than 1 similar directedness is encountered\n",
    "for i in range(0,len(flat_list)-1):\n",
    "    \n",
    "    if flat_list[i] in xword_graph.nodes():\n",
    "        \n",
    "        if flat_list[i+1] in xword_graph.neighbors(flat_list[i]):\n",
    "            #print (word_graph.vertList[flat_list[i]].connectedto)\n",
    "            xword_graph[flat_list[i]][flat_list[i+1]]['weight'] +=1\n",
    "        else:\n",
    "            xword_graph.add_edge(flat_list[i], flat_list[i+1], weight =1)\n",
    "\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        xword_graph.add_edge(flat_list[i], flat_list[i+1], weight = 1)\n",
    "\n",
    "\n",
    "#nx.draw_networkx(xword_graph,node_color='#A0CBE2',edge_color='#BB0000',width=2,edge_cmap=plt.cm.Blues,with_labels=True)\n",
    "#plt.xlim(-4.0, 4.0)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "for u, v , a in xword_graph.edges(data=True):\n",
    "    #print( u, v,a )\n",
    "    pass\n",
    "\n",
    "#Compute closeness centrality\n",
    "cc_dict = nx.closeness_centrality(xword_graph, u=None, distance =None, normalized=True)\n",
    "sorted_cc_dict =  OrderedDict(sorted(cc_dict.items(), key=itemgetter(1), reverse=True))\n",
    "\n",
    "\n",
    "for x, y in sorted_cc_dict.items():\n",
    "    #print (\"{0} : {1}\".format(x, y))\n",
    "    pass\n",
    "\n",
    "#Compute betweenness centrality\n",
    "bc_dict= nx.betweenness_centrality(xword_graph, k=None, normalized=True, weight=None, endpoints=False, seed=None)\n",
    "sorted_bc_dict =  OrderedDict(sorted(bc_dict.items(), key=itemgetter(1), reverse=True))\n",
    "\n",
    "for x, y in sorted_bc_dict.items():\n",
    "    \n",
    "    print (\"{0} : {1}\".format(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
