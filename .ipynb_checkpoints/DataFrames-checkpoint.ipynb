{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "path = 'WinnersInterviewBlogPosts.csv'\n",
    "data = pd.read_csv(path, header=None, names=['Title', 'Link', 'Publication_Date', 'Content'])\n",
    "data = data.drop(data.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Publication_Date</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer scientist Jure Zbontar on winning the...</td>\n",
       "      <td>http://blog.kaggle.com/2010/06/09/computer-sci...</td>\n",
       "      <td>2010-06-09 18:22:29</td>\n",
       "      <td>My approach was actually quite simple. The onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How I won the  Predict HIV Progression data mi...</td>\n",
       "      <td>http://blog.kaggle.com/2010/08/09/how-i-won-th...</td>\n",
       "      <td>2010-08-09 12:35:46</td>\n",
       "      <td>&lt;strong&gt;Initial Strategy&lt;/strong&gt;\\r\\n\\r\\nThe g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How I did it: Lee Baker on winning Tourism For...</td>\n",
       "      <td>http://blog.kaggle.com/2010/09/27/how-i-did-it...</td>\n",
       "      <td>2010-09-27 18:30:25</td>\n",
       "      <td>&lt;strong&gt;About me:&lt;/strong&gt;\\r\\nI’m an embedded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How I did it: The top three from the 2010 INFO...</td>\n",
       "      <td>http://blog.kaggle.com/2010/10/11/how-i-did-it...</td>\n",
       "      <td>2010-10-11 13:31:35</td>\n",
       "      <td>The 2010 INFORMS Data Mining Contest has just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How I did it: Jeremy Howard on finishing second</td>\n",
       "      <td>http://blog.kaggle.com/2010/11/19/how-i-did-it...</td>\n",
       "      <td>2010-11-19 17:39:47</td>\n",
       "      <td>Wow, this is a surprise! I looked at this comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "1  Computer scientist Jure Zbontar on winning the...   \n",
       "2  How I won the  Predict HIV Progression data mi...   \n",
       "3  How I did it: Lee Baker on winning Tourism For...   \n",
       "4  How I did it: The top three from the 2010 INFO...   \n",
       "5    How I did it: Jeremy Howard on finishing second   \n",
       "\n",
       "                                                Link     Publication_Date  \\\n",
       "1  http://blog.kaggle.com/2010/06/09/computer-sci...  2010-06-09 18:22:29   \n",
       "2  http://blog.kaggle.com/2010/08/09/how-i-won-th...  2010-08-09 12:35:46   \n",
       "3  http://blog.kaggle.com/2010/09/27/how-i-did-it...  2010-09-27 18:30:25   \n",
       "4  http://blog.kaggle.com/2010/10/11/how-i-did-it...  2010-10-11 13:31:35   \n",
       "5  http://blog.kaggle.com/2010/11/19/how-i-did-it...  2010-11-19 17:39:47   \n",
       "\n",
       "                                             Content  \n",
       "1  My approach was actually quite simple. The onl...  \n",
       "2  <strong>Initial Strategy</strong>\\r\\n\\r\\nThe g...  \n",
       "3  <strong>About me:</strong>\\r\\nI’m an embedded ...  \n",
       "4  The 2010 INFORMS Data Mining Contest has just ...  \n",
       "5  Wow, this is a surprise! I looked at this comp...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = data.iloc[:, 3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list=[]\n",
    "str_representation =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a normalised python list from numpy array list, and convert the result into a string for processing via BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My approach was actually quite simple. The only attributes I used where the approximate betting odds and the information on past voting. I sought patterns in the voting behaviour of all countries and combined that knowledge with this year's betting odds. I used cross-validation to select my model and to avoid overfitting it.<!--more-->\r\n",
      "\r\n",
      "<strong>Predicting the finalists </strong>\r\n",
      "\r\n",
      "I trusted the bookmakers on this one and just took the top ten countries from each semi-final group. I got the betting odds from Betfair.\r\n",
      "\r\n",
      "<strong>Learning the voting patterns </strong>\r\n",
      "\r\n",
      "A simple approach worked well enough here. The idea was to calculate, for each country, the average points awarded to each other country. Coming from Slovenia which was once part of Yugoslavia, together with Croatia, Serbia, Bosnia and Herzegovina, Macedonia and Montenegro, it is perhaps not surprising that our voting patterns are rather interesting:\r\n",
      "<pre><code>  AVG' COUNTRY\r\n",
      "10.38  Serbia\r\n",
      " 8.53  Croatia\r\n",
      " 8.00  Bosnia and Herzegovina\r\n",
      " 5.91  Macedonia\r\n",
      " 3.21  Norway\r\n",
      " 3.17  Russia\r\n",
      " 3.07  Greece\r\n",
      "...\r\n",
      " 0.18  Portugal\r\n",
      " 0.17  Belarus\r\n",
      "</code></pre>\r\n",
      "It is painfully obvious that Slovenia is not judging the quality of the artist alone and it is well known that other countries follow similar patterns. It would, therefore, seem like a good idea to use this knowledge in predicting this year's voting.<!--more-->\r\n",
      "\r\n",
      "The estimated average points awarded are not very stable, especially for newer countries. To remedy this, instead of using:\r\n",
      "<pre><code>avg := sum(x) / |x|\r\n",
      "</code></pre>\r\n",
      "I used\r\n",
      "<pre><code>avg' := (sum(x) + 1) / (|x| + 1)\r\n",
      "</code></pre>\r\n",
      "The new estimate got better results on the cross-validation tests.\r\n",
      "\r\n",
      "<strong>Betting Odds</strong>\r\n",
      "\r\n",
      "Using just the voting patterns of countries to predict this year's results was not enough. I had to, somehow, incorporate the approximate betting odds as well. Many approaches could have worked well here. In the end I opted for the one that gave the best cross-validation results.\r\n",
      "\r\n",
      "I had to convert the approximate betting odds into something comparable with the average points awarded. I used:\r\n",
      "<pre><code>odds'(ctr) := 1 / log(odds(ctr)) * a + b\r\n",
      "</code></pre>\r\n",
      "The coefficients a and b were chosen experimentally, as the ones that gave the best cross-validation score.\r\n",
      "\r\n",
      "A small example will elucidate how I calculated the converted betting odds.\r\n",
      "<pre><code>odds'(Croatia) = 1 / log(odds(Croatia)) * 4.4 + 0.8 =\r\n",
      "               = 1 / log(48) * 4.4 + 0.8\r\n",
      "               = 1.94\r\n",
      "</code></pre>\r\n",
      "The converted betting odds for the top and bottom countries were:\r\n",
      "<pre><code>ODDS' COUNTRY\r\n",
      "5.23  Azerbaijan\r\n",
      "3.21  Germany\r\n",
      "2.54  Armenia\r\n",
      "...\r\n",
      "1.94  Croatia\r\n",
      "...\r\n",
      "1.45  Slovenia\r\n",
      "1.44  Bulgaria\r\n",
      "1.44  Macedonia\r\n",
      "1.44  Switzerland</code></pre>\r\n",
      "<strong>Combining the voting patterns with the betting odds</strong>\r\n",
      "\r\n",
      "It was now time to bring everything together. This was simply a matter of summing the average points awarded with the converted betting odds.\r\n",
      "\r\n",
      "This was how I predicted Slovenia's votes for this year:\r\n",
      "<pre><code>COUNTRY                  AVG'  ODDS'    SUM POINTS\r\n",
      "Serbia                 10.38 + 1.84 = 12.21     12\r\n",
      "Croatia                 8.53 + 1.94 = 10.47     10\r\n",
      "Bosnia and Herzegovina  8.00 + 1.49 =  9.49      8\r\n",
      "Macedonia               5.91 + 1.44 =  7.35      7\r\n",
      "Azerbaijan              1.80 + 5.23 =  7.03      6\r\n",
      "Norway                  3.21 + 2.01 =  5.22      5\r\n",
      "Greece                  3.07 + 1.96 =  5.03      4\r\n",
      "Sweden                  2.85 + 2.18 =  5.03      3\r\n",
      "Russia                  3.17 + 1.62 =  4.79      2\r\n",
      "Germany                 1.50 + 3.21 =  4.71      1\r\n",
      "Denmark                 2.42 + 2.25 =  4.66      0\r\n",
      "...\r\n",
      "</code></pre>\r\n",
      "We saw earlier that Slovenia's votes have little to do with song quality, as we usually award the top points to Balkan countries, no matter how bad they sing. The added betting odds should not influence the prediction of such countries considerably. On the other hand, if we take a country that is perhaps a bit more fair, like Israel, we see that the final predictions are affected to a greater extent:\r\n",
      "<pre><code>COUNTRY                  AVG'   ODDS'    SUM POINTS\r\n",
      "Armenia                 7.50 +  2.54 = 10.04     12\r\n",
      "Azerbaijan              3.75 +  5.23 =  8.98     10\r\n",
      "Russia                  7.23 +  1.62 =  8.85      8\r\n",
      "Ukraine                 6.30 +  1.54 =  7.84      7\r\n",
      "Romania                 6.07 +  1.61 =  7.68      6\r\n",
      "Greece                  4.08 +  1.96 =  6.04      5\r\n",
      "Georgia                 4.25 +  1.77 =  6.02      4\r\n",
      "Iceland                 3.83 +  1.72 =  5.55      3\r\n",
      "Serbia                  3.71 +  1.84 =  5.55      2\r\n",
      "Denmark                 3.25 +  2.25 =  5.50      1\r\n",
      "Sweden                  3.27 +  2.18 =  5.45      0\r\n",
      "...\r\n",
      "</code></pre>\r\n",
      "<h2><span style=\"font-size: 13px;\">Cross validation</span></h2>\r\n",
      "<h2><span style=\"font-weight: normal; font-size: 13px;\">The most important component of my solution was cross-validation and was probably the reason why I won the competition in the first place. It enabled me to try many different models and, between them, choose the one that was most likely to give the best results.</span></h2>\r\n",
      "The dataset was split into partitions, one for each Eurovision event. I then proceeded to build the model on all but one partition and calculated the error of that model on the partition that was left out. The procedure was repeated so that each time a different partition was left out. This gave me a fair estimate of how the model performs on unseen data.\r\n",
      "\r\n",
      "The cross-validation procedure in pseudocode:\r\n",
      "<pre><code>function crossValidation(dataset, buildModel):\r\n",
      "  error = 0\r\n",
      "  for year in eurovisionEvents:\r\n",
      "    learnData = {example | example in dataset and example.year != year}\r\n",
      "    testData  = {example | example in dataset and example.year == year}\r\n",
      "    model = buildModel(learnData)\r\n",
      "    error += testModel(model, testData)\r\n",
      "  return error</code></pre>\r\n",
      "<pre><span style=\"font-family: monospace, Monaco, 'Courier New', Courier, monospace;\"><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px; white-space: normal; font-size: 13px;\"><strong>Conclusion </strong></span></span></pre>\r\n",
      "<pre><span style=\"font-family: monospace, Monaco, 'Courier New', Courier, monospace;\"><strong><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-weight: normal; line-height: 19px; white-space: normal; font-size: 13px;\">I am well aware that certain parts of my approach are not very strong. I had to do my best with the time that was available. I have many ideas for next year, which I will, for the moment at least, keep to myself :)</span></strong></span></pre>\r\n",
      "I really enjoy competing in events like this and hope there will be more to come in the future.\n"
     ]
    }
   ],
   "source": [
    "for i in range(content.shape[0]):\n",
    "    posts = content.values[i]\n",
    "    \n",
    "for post in posts:\n",
    "    list.append(post)\n",
    "    \n",
    "#print(list[0])\n",
    "\n",
    "for i, post in enumerate(list):\n",
    "    str.append(''.join(list[i]))\n",
    "    \n",
    "print (str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, BeautifulSoup to remove html tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantext=[]\n",
    "for i, post in enumerate(str):\n",
    "    cleantext.append(BeautifulSoup(str[i], \"lxml\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, just check if html tags have been removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Initial Strategy\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n\\nTo make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.\\nTo make my training set closely match the test data in order to have a population similar to the test set.\\n\\r\\nI   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n\\nMatching Controls\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n\\nRecursive Feature Elimination via R 'caret' package\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R 'caret' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the Chess contest.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleantext[1].get_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
