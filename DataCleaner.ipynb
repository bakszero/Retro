{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "path = 'WinnersInterviewBlogPosts.csv'\n",
    "data = pd.read_csv(path, header=None, names=['Title', 'Link', 'Publication_Date', 'Content'])\n",
    "data = data.drop(data.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Publication_Date</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer scientist Jure Zbontar on winning the...</td>\n",
       "      <td>http://blog.kaggle.com/2010/06/09/computer-sci...</td>\n",
       "      <td>2010-06-09 18:22:29</td>\n",
       "      <td>My approach was actually quite simple. The onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How I won the  Predict HIV Progression data mi...</td>\n",
       "      <td>http://blog.kaggle.com/2010/08/09/how-i-won-th...</td>\n",
       "      <td>2010-08-09 12:35:46</td>\n",
       "      <td>&lt;strong&gt;Initial Strategy&lt;/strong&gt;\\r\\n\\r\\nThe g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How I did it: Lee Baker on winning Tourism For...</td>\n",
       "      <td>http://blog.kaggle.com/2010/09/27/how-i-did-it...</td>\n",
       "      <td>2010-09-27 18:30:25</td>\n",
       "      <td>&lt;strong&gt;About me:&lt;/strong&gt;\\r\\nI’m an embedded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How I did it: The top three from the 2010 INFO...</td>\n",
       "      <td>http://blog.kaggle.com/2010/10/11/how-i-did-it...</td>\n",
       "      <td>2010-10-11 13:31:35</td>\n",
       "      <td>The 2010 INFORMS Data Mining Contest has just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How I did it: Jeremy Howard on finishing second</td>\n",
       "      <td>http://blog.kaggle.com/2010/11/19/how-i-did-it...</td>\n",
       "      <td>2010-11-19 17:39:47</td>\n",
       "      <td>Wow, this is a surprise! I looked at this comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "1  Computer scientist Jure Zbontar on winning the...   \n",
       "2  How I won the  Predict HIV Progression data mi...   \n",
       "3  How I did it: Lee Baker on winning Tourism For...   \n",
       "4  How I did it: The top three from the 2010 INFO...   \n",
       "5    How I did it: Jeremy Howard on finishing second   \n",
       "\n",
       "                                                Link     Publication_Date  \\\n",
       "1  http://blog.kaggle.com/2010/06/09/computer-sci...  2010-06-09 18:22:29   \n",
       "2  http://blog.kaggle.com/2010/08/09/how-i-won-th...  2010-08-09 12:35:46   \n",
       "3  http://blog.kaggle.com/2010/09/27/how-i-did-it...  2010-09-27 18:30:25   \n",
       "4  http://blog.kaggle.com/2010/10/11/how-i-did-it...  2010-10-11 13:31:35   \n",
       "5  http://blog.kaggle.com/2010/11/19/how-i-did-it...  2010-11-19 17:39:47   \n",
       "\n",
       "                                             Content  \n",
       "1  My approach was actually quite simple. The onl...  \n",
       "2  <strong>Initial Strategy</strong>\\r\\n\\r\\nThe g...  \n",
       "3  <strong>About me:</strong>\\r\\nI’m an embedded ...  \n",
       "4  The 2010 INFORMS Data Mining Contest has just ...  \n",
       "5  Wow, this is a surprise! I looked at this comp...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My approach was actually quite simple. The onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;strong&gt;Initial Strategy&lt;/strong&gt;\\r\\n\\r\\nThe g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;strong&gt;About me:&lt;/strong&gt;\\r\\nI’m an embedded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 2010 INFORMS Data Mining Contest has just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow, this is a surprise! I looked at this comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content\n",
       "1  My approach was actually quite simple. The onl...\n",
       "2  <strong>Initial Strategy</strong>\\r\\n\\r\\nThe g...\n",
       "3  <strong>About me:</strong>\\r\\nI’m an embedded ...\n",
       "4  The 2010 INFORMS Data Mining Contest has just ...\n",
       "5  Wow, this is a surprise! I looked at this comp..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = data.iloc[:, 3:4]\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list=[]\n",
    "str =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a normalised python list from numpy array list, and convert the result into a string for processing via BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My approach was actually quite simple. The only attributes I used where the approximate betting odds and the information on past voting. I sought patterns in the voting behaviour of all countries and combined that knowledge with this year's betting odds. I used cross-validation to select my model and to avoid overfitting it.<!--more-->\r\n",
      "\r\n",
      "<strong>Predicting the finalists </strong>\r\n",
      "\r\n",
      "I trusted the bookmakers on this one and just took the top ten countries from each semi-final group. I got the betting odds from Betfair.\r\n",
      "\r\n",
      "<strong>Learning the voting patterns </strong>\r\n",
      "\r\n",
      "A simple approach worked well enough here. The idea was to calculate, for each country, the average points awarded to each other country. Coming from Slovenia which was once part of Yugoslavia, together with Croatia, Serbia, Bosnia and Herzegovina, Macedonia and Montenegro, it is perhaps not surprising that our voting patterns are rather interesting:\r\n",
      "<pre><code>  AVG' COUNTRY\r\n",
      "10.38  Serbia\r\n",
      " 8.53  Croatia\r\n",
      " 8.00  Bosnia and Herzegovina\r\n",
      " 5.91  Macedonia\r\n",
      " 3.21  Norway\r\n",
      " 3.17  Russia\r\n",
      " 3.07  Greece\r\n",
      "...\r\n",
      " 0.18  Portugal\r\n",
      " 0.17  Belarus\r\n",
      "</code></pre>\r\n",
      "It is painfully obvious that Slovenia is not judging the quality of the artist alone and it is well known that other countries follow similar patterns. It would, therefore, seem like a good idea to use this knowledge in predicting this year's voting.<!--more-->\r\n",
      "\r\n",
      "The estimated average points awarded are not very stable, especially for newer countries. To remedy this, instead of using:\r\n",
      "<pre><code>avg := sum(x) / |x|\r\n",
      "</code></pre>\r\n",
      "I used\r\n",
      "<pre><code>avg' := (sum(x) + 1) / (|x| + 1)\r\n",
      "</code></pre>\r\n",
      "The new estimate got better results on the cross-validation tests.\r\n",
      "\r\n",
      "<strong>Betting Odds</strong>\r\n",
      "\r\n",
      "Using just the voting patterns of countries to predict this year's results was not enough. I had to, somehow, incorporate the approximate betting odds as well. Many approaches could have worked well here. In the end I opted for the one that gave the best cross-validation results.\r\n",
      "\r\n",
      "I had to convert the approximate betting odds into something comparable with the average points awarded. I used:\r\n",
      "<pre><code>odds'(ctr) := 1 / log(odds(ctr)) * a + b\r\n",
      "</code></pre>\r\n",
      "The coefficients a and b were chosen experimentally, as the ones that gave the best cross-validation score.\r\n",
      "\r\n",
      "A small example will elucidate how I calculated the converted betting odds.\r\n",
      "<pre><code>odds'(Croatia) = 1 / log(odds(Croatia)) * 4.4 + 0.8 =\r\n",
      "               = 1 / log(48) * 4.4 + 0.8\r\n",
      "               = 1.94\r\n",
      "</code></pre>\r\n",
      "The converted betting odds for the top and bottom countries were:\r\n",
      "<pre><code>ODDS' COUNTRY\r\n",
      "5.23  Azerbaijan\r\n",
      "3.21  Germany\r\n",
      "2.54  Armenia\r\n",
      "...\r\n",
      "1.94  Croatia\r\n",
      "...\r\n",
      "1.45  Slovenia\r\n",
      "1.44  Bulgaria\r\n",
      "1.44  Macedonia\r\n",
      "1.44  Switzerland</code></pre>\r\n",
      "<strong>Combining the voting patterns with the betting odds</strong>\r\n",
      "\r\n",
      "It was now time to bring everything together. This was simply a matter of summing the average points awarded with the converted betting odds.\r\n",
      "\r\n",
      "This was how I predicted Slovenia's votes for this year:\r\n",
      "<pre><code>COUNTRY                  AVG'  ODDS'    SUM POINTS\r\n",
      "Serbia                 10.38 + 1.84 = 12.21     12\r\n",
      "Croatia                 8.53 + 1.94 = 10.47     10\r\n",
      "Bosnia and Herzegovina  8.00 + 1.49 =  9.49      8\r\n",
      "Macedonia               5.91 + 1.44 =  7.35      7\r\n",
      "Azerbaijan              1.80 + 5.23 =  7.03      6\r\n",
      "Norway                  3.21 + 2.01 =  5.22      5\r\n",
      "Greece                  3.07 + 1.96 =  5.03      4\r\n",
      "Sweden                  2.85 + 2.18 =  5.03      3\r\n",
      "Russia                  3.17 + 1.62 =  4.79      2\r\n",
      "Germany                 1.50 + 3.21 =  4.71      1\r\n",
      "Denmark                 2.42 + 2.25 =  4.66      0\r\n",
      "...\r\n",
      "</code></pre>\r\n",
      "We saw earlier that Slovenia's votes have little to do with song quality, as we usually award the top points to Balkan countries, no matter how bad they sing. The added betting odds should not influence the prediction of such countries considerably. On the other hand, if we take a country that is perhaps a bit more fair, like Israel, we see that the final predictions are affected to a greater extent:\r\n",
      "<pre><code>COUNTRY                  AVG'   ODDS'    SUM POINTS\r\n",
      "Armenia                 7.50 +  2.54 = 10.04     12\r\n",
      "Azerbaijan              3.75 +  5.23 =  8.98     10\r\n",
      "Russia                  7.23 +  1.62 =  8.85      8\r\n",
      "Ukraine                 6.30 +  1.54 =  7.84      7\r\n",
      "Romania                 6.07 +  1.61 =  7.68      6\r\n",
      "Greece                  4.08 +  1.96 =  6.04      5\r\n",
      "Georgia                 4.25 +  1.77 =  6.02      4\r\n",
      "Iceland                 3.83 +  1.72 =  5.55      3\r\n",
      "Serbia                  3.71 +  1.84 =  5.55      2\r\n",
      "Denmark                 3.25 +  2.25 =  5.50      1\r\n",
      "Sweden                  3.27 +  2.18 =  5.45      0\r\n",
      "...\r\n",
      "</code></pre>\r\n",
      "<h2><span style=\"font-size: 13px;\">Cross validation</span></h2>\r\n",
      "<h2><span style=\"font-weight: normal; font-size: 13px;\">The most important component of my solution was cross-validation and was probably the reason why I won the competition in the first place. It enabled me to try many different models and, between them, choose the one that was most likely to give the best results.</span></h2>\r\n",
      "The dataset was split into partitions, one for each Eurovision event. I then proceeded to build the model on all but one partition and calculated the error of that model on the partition that was left out. The procedure was repeated so that each time a different partition was left out. This gave me a fair estimate of how the model performs on unseen data.\r\n",
      "\r\n",
      "The cross-validation procedure in pseudocode:\r\n",
      "<pre><code>function crossValidation(dataset, buildModel):\r\n",
      "  error = 0\r\n",
      "  for year in eurovisionEvents:\r\n",
      "    learnData = {example | example in dataset and example.year != year}\r\n",
      "    testData  = {example | example in dataset and example.year == year}\r\n",
      "    model = buildModel(learnData)\r\n",
      "    error += testModel(model, testData)\r\n",
      "  return error</code></pre>\r\n",
      "<pre><span style=\"font-family: monospace, Monaco, 'Courier New', Courier, monospace;\"><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; line-height: 19px; white-space: normal; font-size: 13px;\"><strong>Conclusion </strong></span></span></pre>\r\n",
      "<pre><span style=\"font-family: monospace, Monaco, 'Courier New', Courier, monospace;\"><strong><span style=\"font-family: Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif; font-weight: normal; line-height: 19px; white-space: normal; font-size: 13px;\">I am well aware that certain parts of my approach are not very strong. I had to do my best with the time that was available. I have many ideas for next year, which I will, for the moment at least, keep to myself :)</span></strong></span></pre>\r\n",
      "I really enjoy competing in events like this and hope there will be more to come in the future.\n"
     ]
    }
   ],
   "source": [
    "for i in range(content.shape[0]):\n",
    "    posts = content.values[i]\n",
    "    for post in posts:\n",
    "        list.append(post)\n",
    "    \n",
    "#print(list[0])\n",
    "\n",
    "for i, post in enumerate(list):\n",
    "    str.append(''.join(list[i]))\n",
    "    \n",
    "print (str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, BeautifulSoup to remove html tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleantext=[]\n",
    "for i, post in enumerate(str):\n",
    "    cleantext.append(BeautifulSoup(str[i], \"lxml\").get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, just check if html tags have been removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My approach was actually quite simple. The only attributes I used where the approximate betting odds and the information on past voting. I sought patterns in the voting behaviour of all countries and combined that knowledge with this year's betting odds. I used cross-validation to select my model and to avoid overfitting it.\\nPredicting the finalists \\r\\n\\r\\nI\\xa0trusted the bookmakers on this one and just took the top ten countries from each semi-final group. I got the betting odds from Betfair.\\r\\n\\r\\nLearning the voting patterns \\r\\n\\r\\nA simple approach worked well enough here. The idea was to calculate, for each country, the average points awarded to each other country. Coming from Slovenia which was once part of Yugoslavia, together with Croatia, Serbia, Bosnia and Herzegovina, Macedonia and Montenegro, it is perhaps not surprising that our voting patterns are rather interesting:\\r\\n  AVG' COUNTRY\\r\\n10.38  Serbia\\r\\n 8.53  Croatia\\r\\n 8.00  Bosnia and Herzegovina\\r\\n 5.91  Macedonia\\r\\n 3.21  Norway\\r\\n 3.17  Russia\\r\\n 3.07  Greece\\r\\n...\\r\\n 0.18  Portugal\\r\\n 0.17  Belarus\\r\\n\\r\\nIt is painfully obvious that Slovenia is not judging the quality of the artist alone and it is well known that other countries follow similar patterns. It would, therefore, seem like a good idea to use this knowledge in predicting this year's voting.\\r\\n\\r\\nThe estimated average points awarded are not very stable, especially for newer countries. To remedy this, instead of using:\\r\\navg := sum(x) / |x|\\r\\n\\r\\nI used\\r\\navg' := (sum(x) + 1) / (|x| + 1)\\r\\n\\r\\nThe new estimate got better results on the cross-validation tests.\\r\\n\\r\\nBetting Odds\\r\\n\\r\\nUsing just the voting patterns of countries to predict this year's results was not enough. I had to, somehow, incorporate the approximate betting odds as well. Many approaches could have worked well here. In the end I opted for the one that gave the best cross-validation results.\\r\\n\\r\\nI had to convert the approximate betting odds into something comparable with the average points awarded. I used:\\r\\nodds'(ctr) := 1 / log(odds(ctr)) * a + b\\r\\n\\r\\nThe coefficients a and b were chosen experimentally, as the ones that gave the best cross-validation score.\\r\\n\\r\\nA small example will elucidate how I calculated the converted betting odds.\\r\\nodds'(Croatia) = 1 / log(odds(Croatia)) * 4.4 + 0.8 =\\r\\n               = 1 / log(48) * 4.4 + 0.8\\r\\n               = 1.94\\r\\n\\r\\nThe converted betting odds for the top and bottom countries were:\\r\\nODDS' COUNTRY\\r\\n5.23  Azerbaijan\\r\\n3.21  Germany\\r\\n2.54  Armenia\\r\\n...\\r\\n1.94  Croatia\\r\\n...\\r\\n1.45  Slovenia\\r\\n1.44  Bulgaria\\r\\n1.44  Macedonia\\r\\n1.44  Switzerland\\nCombining the voting patterns with the betting odds\\r\\n\\r\\nIt was now time to bring everything together. This was simply a matter of summing the average points awarded with the converted betting odds.\\r\\n\\r\\nThis was how I predicted Slovenia's votes for this year:\\r\\nCOUNTRY                  AVG'  ODDS'    SUM POINTS\\r\\nSerbia                 10.38 + 1.84 = 12.21     12\\r\\nCroatia                 8.53 + 1.94 = 10.47     10\\r\\nBosnia and Herzegovina  8.00 + 1.49 =  9.49      8\\r\\nMacedonia               5.91 + 1.44 =  7.35      7\\r\\nAzerbaijan              1.80 + 5.23 =  7.03      6\\r\\nNorway                  3.21 + 2.01 =  5.22      5\\r\\nGreece                  3.07 + 1.96 =  5.03      4\\r\\nSweden                  2.85 + 2.18 =  5.03      3\\r\\nRussia                  3.17 + 1.62 =  4.79      2\\r\\nGermany                 1.50 + 3.21 =  4.71      1\\r\\nDenmark                 2.42 + 2.25 =  4.66      0\\r\\n...\\r\\n\\r\\nWe saw earlier that Slovenia's votes have little to do with song quality, as we usually award the top points to Balkan countries, no matter how bad they sing. The added betting odds should not influence the prediction of such countries considerably. On the other hand, if we take a country that is perhaps a bit more fair, like Israel, we see that the final predictions are affected to a greater extent:\\r\\nCOUNTRY                  AVG'   ODDS'    SUM POINTS\\r\\nArmenia                 7.50 +  2.54 = 10.04     12\\r\\nAzerbaijan              3.75 +  5.23 =  8.98     10\\r\\nRussia                  7.23 +  1.62 =  8.85      8\\r\\nUkraine                 6.30 +  1.54 =  7.84      7\\r\\nRomania                 6.07 +  1.61 =  7.68      6\\r\\nGreece                  4.08 +  1.96 =  6.04      5\\r\\nGeorgia                 4.25 +  1.77 =  6.02      4\\r\\nIceland                 3.83 +  1.72 =  5.55      3\\r\\nSerbia                  3.71 +  1.84 =  5.55      2\\r\\nDenmark                 3.25 +  2.25 =  5.50      1\\r\\nSweden                  3.27 +  2.18 =  5.45      0\\r\\n...\\r\\n\\nCross validation\\nThe most important component of my solution was cross-validation and was probably the reason why I won the competition in the first place. It enabled me to try many different models and, between them, choose the one that was most likely to give the best results.\\r\\nThe dataset was split into partitions, one for each Eurovision event. I then proceeded to build the model on all but one partition and calculated the error of that model on the partition that was left out. The procedure was repeated so that each time a different partition was left out. This gave me a fair estimate of how the model performs on unseen data.\\r\\n\\r\\nThe cross-validation procedure in pseudocode:\\r\\nfunction crossValidation(dataset, buildModel):\\r\\n  error = 0\\r\\n  for year in eurovisionEvents:\\r\\n    learnData = {example | example in dataset and example.year != year}\\r\\n    testData  = {example | example in dataset and example.year == year}\\r\\n    model = buildModel(learnData)\\r\\n    error += testModel(model, testData)\\r\\n  return error\\nConclusion\\xa0\\nI am well aware that certain parts of my approach are not very strong. I had to do my best with the time that was available. I have many ideas for next year, which I will, for the moment at least, keep to myself :)\\r\\nI really enjoy competing in events like this and hope there will be more to come in the future.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleantext[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move onto some more pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"My approach was actually quite simple. The only attributes I used where the approximate betting odds and the information on past voting. I sought patterns in the voting behaviour of all countries and combined that knowledge with this year's betting odds. I used cross-validation to select my model and to avoid overfitting it.\\nPredicting the finalists \\r\\n\\r\\nI\\xa0trusted the bookmakers on this one and just took the top ten countries from each semi-final group. I got the betting odds from Betfair.\\r\\n\\r\\nLearning the voting patterns \\r\\n\\r\\nA simple approach worked well enough here. The idea was to calculate, for each country, the average points awarded to each other country. Coming from Slovenia which was once part of Yugoslavia, together with Croatia, Serbia, Bosnia and Herzegovina, Macedonia and Montenegro, it is perhaps not surprising that our voting patterns are rather interesting:\\r\\n  AVG' COUNTRY\\r\\n10.38  Serbia\\r\\n 8.53  Croatia\\r\\n 8.00  Bosnia and Herzegovina\\r\\n 5.91  Macedonia\\r\\n 3.21  Norway\\r\\n 3.17  Russia\\r\\n 3.07  Greece\\r\\n...\\r\\n 0.18  Portugal\\r\\n 0.17  Belarus\\r\\n\\r\\nIt is painfully obvious that Slovenia is not judging the quality of the artist alone and it is well known that other countries follow similar patterns. It would, therefore, seem like a good idea to use this knowledge in predicting this year's voting.\\r\\n\\r\\nThe estimated average points awarded are not very stable, especially for newer countries. To remedy this, instead of using:\\r\\navg := sum(x) / |x|\\r\\n\\r\\nI used\\r\\navg' := (sum(x) + 1) / (|x| + 1)\\r\\n\\r\\nThe new estimate got better results on the cross-validation tests.\\r\\n\\r\\nBetting Odds\\r\\n\\r\\nUsing just the voting patterns of countries to predict this year's results was not enough. I had to, somehow, incorporate the approximate betting odds as well. Many approaches could have worked well here. In the end I opted for the one that gave the best cross-validation results.\\r\\n\\r\\nI had to convert the approximate betting odds into something comparable with the average points awarded. I used:\\r\\nodds'(ctr) := 1 / log(odds(ctr)) * a + b\\r\\n\\r\\nThe coefficients a and b were chosen experimentally, as the ones that gave the best cross-validation score.\\r\\n\\r\\nA small example will elucidate how I calculated the converted betting odds.\\r\\nodds'(Croatia) = 1 / log(odds(Croatia)) * 4.4 + 0.8 =\\r\\n               = 1 / log(48) * 4.4 + 0.8\\r\\n               = 1.94\\r\\n\\r\\nThe converted betting odds for the top and bottom countries were:\\r\\nODDS' COUNTRY\\r\\n5.23  Azerbaijan\\r\\n3.21  Germany\\r\\n2.54  Armenia\\r\\n...\\r\\n1.94  Croatia\\r\\n...\\r\\n1.45  Slovenia\\r\\n1.44  Bulgaria\\r\\n1.44  Macedonia\\r\\n1.44  Switzerland\\nCombining the voting patterns with the betting odds\\r\\n\\r\\nIt was now time to bring everything together. This was simply a matter of summing the average points awarded with the converted betting odds.\\r\\n\\r\\nThis was how I predicted Slovenia's votes for this year:\\r\\nCOUNTRY                  AVG'  ODDS'    SUM POINTS\\r\\nSerbia                 10.38 + 1.84 = 12.21     12\\r\\nCroatia                 8.53 + 1.94 = 10.47     10\\r\\nBosnia and Herzegovina  8.00 + 1.49 =  9.49      8\\r\\nMacedonia               5.91 + 1.44 =  7.35      7\\r\\nAzerbaijan              1.80 + 5.23 =  7.03      6\\r\\nNorway                  3.21 + 2.01 =  5.22      5\\r\\nGreece                  3.07 + 1.96 =  5.03      4\\r\\nSweden                  2.85 + 2.18 =  5.03      3\\r\\nRussia                  3.17 + 1.62 =  4.79      2\\r\\nGermany                 1.50 + 3.21 =  4.71      1\\r\\nDenmark                 2.42 + 2.25 =  4.66      0\\r\\n...\\r\\n\\r\\nWe saw earlier that Slovenia's votes have little to do with song quality, as we usually award the top points to Balkan countries, no matter how bad they sing. The added betting odds should not influence the prediction of such countries considerably. On the other hand, if we take a country that is perhaps a bit more fair, like Israel, we see that the final predictions are affected to a greater extent:\\r\\nCOUNTRY                  AVG'   ODDS'    SUM POINTS\\r\\nArmenia                 7.50 +  2.54 = 10.04     12\\r\\nAzerbaijan              3.75 +  5.23 =  8.98     10\\r\\nRussia                  7.23 +  1.62 =  8.85      8\\r\\nUkraine                 6.30 +  1.54 =  7.84      7\\r\\nRomania                 6.07 +  1.61 =  7.68      6\\r\\nGreece                  4.08 +  1.96 =  6.04      5\\r\\nGeorgia                 4.25 +  1.77 =  6.02      4\\r\\nIceland                 3.83 +  1.72 =  5.55      3\\r\\nSerbia                  3.71 +  1.84 =  5.55      2\\r\\nDenmark                 3.25 +  2.25 =  5.50      1\\r\\nSweden                  3.27 +  2.18 =  5.45      0\\r\\n...\\r\\n\\nCross validation\\nThe most important component of my solution was cross-validation and was probably the reason why I won the competition in the first place. It enabled me to try many different models and, between them, choose the one that was most likely to give the best results.\\r\\nThe dataset was split into partitions, one for each Eurovision event. I then proceeded to build the model on all but one partition and calculated the error of that model on the partition that was left out. The procedure was repeated so that each time a different partition was left out. This gave me a fair estimate of how the model performs on unseen data.\\r\\n\\r\\nThe cross-validation procedure in pseudocode:\\r\\nfunction crossValidation(dataset, buildModel):\\r\\n  error = 0\\r\\n  for year in eurovisionEvents:\\r\\n    learnData = {example | example in dataset and example.year != year}\\r\\n    testData  = {example | example in dataset and example.year == year}\\r\\n    model = buildModel(learnData)\\r\\n    error += testModel(model, testData)\\r\\n  return error\\nConclusion\\xa0\\nI am well aware that certain parts of my approach are not very strong. I had to do my best with the time that was available. I have many ideas for next year, which I will, for the moment at least, keep to myself :)\\r\\nI really enjoy competing in events like this and hope there will be more to come in the future.\", \"Initial Strategy\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n\\nTo make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.\\nTo make my training set closely match the test data in order to have a population similar to the test set.\\n\\r\\nI   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n\\nMatching Controls\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n\\nRecursive Feature Elimination via R 'caret' package\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R 'caret' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the Chess contest.\", 'About me:\\r\\nI’m an embedded systems engineer, currently working for a small engineering company in Las Cruces, New Mexico. I graduated from New Mexico Tech in 2007, with degrees in Electrical Engineering and Computer Science. Like many people, I first became interested in algorithm competitions with the Netflix Prize a few years ago. I was quite excited to find the Kaggle site a few months ago, as I enjoy participating in these types of competitions.\\r\\n\\r\\nExplanation of Technique:\\r\\nThough I tried several different methods, I used a weighted combination of three predictors to come up with the final forecast.\\r\\n\\r\\n\\r\\n\\r\\n#1: After reviewing Athanasopoulos et al, it became obvious that the naive predictor was a good algorithm with which to start. It is both easy to implement and performed well when compared to the other algorithms in the paper.\\r\\n\\r\\nAfter graphing a few of the time series, it became apparent that many of the series increase with time. Indeed, the second sentence of the Athanasopoulos paper states that globally tourism has grown “at a rate of 6% annually.” In order to take advantage of this knowledge, I multiplied the\\xa0 (Naive algorithm’s) predicted value by a factor to take this growth into account. With some testing, I determined a 5.5% growth factor to yield the lowest MASE.\\r\\n\\r\\nprediction1 = last_value * (1.055 ** number_of_years_in_the_future)\\r\\n\\r\\n#2: I examined fitting a polynomial line to the data and using the line to predict future values. I tried using first through fifth order polynomials to find that the lowest MASE was obtained using a first order polynomial (simple regression line). This best fit line was used to predict future values. I also kept the r**2 value of the fit for use in blending the results of the predictor.\\r\\n\\r\\n#3: In thinking about these two predictors, I recognized that the naive predictor, though accurate, throws away most of the provided data, and only uses a single element of the time series. The polynomial line predictor uses all of the data, weighted equally, though the most recent data is probably of more value in indicating future performance than the earlier data in the time series. I examined and eventually used an exponentially-weighted least squares regression to fit a line to the data. This algorithm gave more accurate predictions for many of the time series by itself, and also lowered the MASE when used in combination with the two above predictors. The r**2 value of this fit was also used for blending the predictors.\\nBlending stage:\\r\\nI started with a basic weighted blend of predictors. I used a constant weight factor for the modified naive predictor, while blending weights for the unweighted and weighted regression lines depending on the r**2 values found in fitting the time series. I selected the logistic function as a way of gradually increasing the weight with increasing r**2 value:\\r\\n\\r\\nweight = a * logistic( b * (r**2 - c))\\r\\n\\r\\nValues for a, b, and c were determined by trial and error. I also examined using some numeric optimization functions included in Python to minimize a training set MASE. While this succeeded in lower the training set MASE, I discarded this method when I received a lower leaderboard MASE (possibly from overfitting).\\r\\n\\r\\nTesting / developing algorithms:\\r\\nWhen testing algorithms, I used the last four years of the training dataset (after removing them from the data I was using for training) to test against. While this worked well in the initial stages, I found that once my leaderboard MASE got below about 2.05, this ‘training MASE’ became a much less reliable indicator of whether the leaderboard MASE would increase or decrease with a change. So, during the last few weeks of the contest, I primarily made small tweaks, and tested their value by submitting a new prediction to Kaggle rather than comparing my MASE results. I believe that this indicates a significant difference in nature of the last 4 years of the training set compared to the 4 years in the test set. If the test set includes data from 2008-2009, I’m speculating that depressed tourism numbers as a result of the global economic recession could have caused a significant difference in the trends.\\r\\n\\r\\nPossible improvements:\\r\\nWhile the above method seemed to work fairly well at predicting tourism numbers, there are several steps that could have likely improved the score. I only implemented the Naive method implemented in the Athanasopoulos paper; I do think that including a couple of other algorithms’ output into the final blend could have further increased the score. If I had a few more days to work on a solution, I would have tried to implement the Theta and ARIMA methods described in the paper and looked at the effects of including them in the blend.\\r\\n\\r\\nI think an investigation into how to come up with a blending method that doesn’t use as much manual tweaking would also be of benefit.\\r\\n\\r\\nI enjoyed participating in part one, and look forward to part two of the contest.', 'The 2010 INFORMS Data Mining Contest has just finished. The competition attracted entries from 147 teams with participants from 27 countries. The winner was Cole Harris, followed by Christopher Hefele and Nan Zhou. Here is some background on the winners and the techniques they applied.\\r\\n\\r\\nCole Harris\\nAbout Cole:\\r\\n\\r\\n\"Since 2002 I have been VP Discovery and cofounder of Exagen Diagnostics. We mine genomic/medical data to identify genetic features that are diagnostic of disease, predictive of drug response, etc. and then develop medical tests from the results. Prior to this (2000-2002), at Quasar/Magnaflux, I developed pattern recognition algorithms for identifying defective metal parts from acoustic spectral data.\\xa0 From 1990-1999 I worked for Veritas Geophysical, most of that time developing algorithms for imaging seismic data. Prior to this I was in grad school (physics): MA 1990 Johns Hopkins University.\"\\r\\n\\r\\n\\nCole\\'s Method:\\r\\n\\r\\n\"As far as techniques, my submissions were mostly based on:\\r\\n\\r\\n1. pre-processing - I did many things, but none seemed to make a large difference in my early results on test data, so in the end, other than exclude the non-price data, I didn\\'t filter the data. I did append the data with data advanced 5 min, 60 min, and 65 min. So for each stock there were 16 features (open,hi,lo,last)X(0min,5min,60min,65min)\\r\\n\\r\\n2. feature selection - forward stepwise selection of stocks, reverse stepwise selection of particular features for a given stock, evaluated using logistic regression. This resulted in 5-6 features selected from 2 stocks.\\r\\n\\r\\nmodels: logistic regression and neural networks (not sure which won).\"\\r\\n\\r\\n\\nChristopher Hefele\\nAbout Christopher: \\r\\n\\r\\nChristopher is a Systems Engineer at AT&T. He was a member of The Ensemble, the team which finished second in the $1m Netflix Prize.\\r\\n\\r\\nChristopher\\'s Method:\\r\\n\\r\\n\"I was using a simple logistic regression on Variable 74 for most of the  contest. During the last few days, when every last bit counted, I  switched to a SVM & added more variables (i.e. Variables 167 &  55, chosen by forward stepwise logistic regression).\\r\\n\\r\\nIn the end, to me, this contest really was a  good lesson about the power of proper variable selection &  preprocessing.\"\\r\\n\\r\\nNan Zhou\\nAbout Nan:\\r\\n\\r\\nNan is currently completing his PhD in statistics at the University of Pittsburgh. His PhD research involves the estimation and prediction of  integrated volatility and model  calibration for financial stochastic processes. Prior to this he was a graduate student at Carnege Mellon University (focusing on statistical machine learning).\\r\\n\\r\\nNan\\'s Method:\\r\\n\\r\\n\"Among lots of other models (Support Vector Machine, Random Forest, Neutral Network, Gradient Boosting, AdaBoost, and etc.) I finally used ‘Two-Stages’ L1-penalized Logistic Regression, and tune the penalty parameter by 5-folds Cross Validation.\"\\r\\n\\r\\nYou can hear more from the winners (and others) on the competition\\'s forum.', 'Wow, this is a surprise! I looked at this competition for the first time 15 days ago, and set myself the target to break into the top 100. So coming 2nd is a much better result than I had hoped for!...  I\\'m slightly embarrassed too, because all I really did was to combine the clever techniques that others had already developed - I didn\\'t really invent anything new, I\\'m afraid. Anyhoo, for those who are interested I\\'ll describe here a bit about how I went about things. I suspect in many ways the process is more interesting than the result, since the lessons I learnt will perhaps be useful to others in future competitions.\\r\\n\\r\\n\\r\\n\\r\\nI realised that, by starting when there was only 2 weeks to go, I was already a long way behind.  So my best bet was to leverage existing work as much as possible - use stuff which has already been shown to work! Also, I would have to stick to stuff I\\'m already familiar with, as much as possible. Therefore, I decided initially to look at Microsoft\\'s TrueSkill algorithm: there is already a C# implementation available (a language which I\\'m very familiar with), and it\\'s been well tested (both in practice, on XBox live, and theoretically, in various papers).\\r\\n\\r\\nSo, step one: import the data. The excellent FileHelpers library meant that this was done in 5 minutes.\\r\\n\\r\\nStep two: try to understand the algorithm. Jeff Moser has a brilliant exposition about how TrueSkill works, along with full source code, which he most generously provides. I spent a few hours reading and re- reading this, and can\\'t say I ever got to a point where I fully understood it, but at least I got enough of the gist to make a start. I also watched the very interesting Turing Lecture by Chris Bishop (who\\'s book on pattern recognition was amongst the most influential books I\\'ve read over the years), which discusses the modern Bayesian Graphical Model approach more generally, and briefly touches on the TrueSkill application.\\r\\n\\r\\nStep three: make sure I have a way to track my progress, other than through leaderboard results (since we only get 2 submissions per day). Luckily, the competition provides a validation set, so I tried to use that where possible. I only ever did my modelling (other than final submissions) using the first 95 months of data - there\\'s no point drawing conclusions based on months that overlap with the validation set!\\r\\n\\r\\nI also figured I should try to submit twice everyday, just to see how things looked on the leaderboard. My day one submission was just to throw the data at Moser\\'s class using the default settings. I noticed that if I reran the algorithm a few times, feeding in the previous scores as the starting points, I got better results. So I ran it twice, and submitted that. Result: 0.696 (1st place was about 0.640 - a long way away!) (For the predictions based on the scores, assuming the scores for [white, black] are [s1, s2], I simply used (s1+100)/(s1+s2). The 100 on the top is give white a little advantage, and was selected to get the 54% score that white gets on average).\\r\\n\\r\\nFor the next few days, I went backwards. Rather than looking at graphs of score difference vs win%, I assumed that I should switch to a logistic function, which I did, and I optimised the parameters to the using a simple hill-climb algorithm. This sent my score back to 0.724. I also tried optimising the individual player scores directly. This sent my score back to 0.701. This wasted effort reminded me that I should look at pictures before I jump into algorithms. A graph of win% against white score (with separate lines for each quartile of black score), clearly showed the a logistic function was inappropriate, and also showed that there were interactions that I needed to think about.\\r\\n\\r\\nSo, after 5 days, I still hadn\\'t made much improvement (minor tweaks to Trueskill params had got me to 0.691, barely any improvement from day 1). So I figured I needed a whole different approach. And now I only had 10 days to go...\\r\\n\\r\\nIt concerned me that Trueskill took each individual match and updated the scores after every one - it never fed the later results back to re-score the earlier matches. It turns out that (of course!) I wasn\\'t the first person to think about this problem, and that it had been thoroughly tackled in the \" TrueSkill Through Time\" paper from MS Research\\'s Applied Games Group. This uses Bayesian inference to calculate a theoretically-optimal set of scores (both mean and standard deviation, by player).\\r\\n\\r\\nUnfortunately the code was written for an old version of F#, so it no longer works with the current version. And it\\'s been a while since I\\'ve used F# (actually, all I\\'ve done with it is some Project Euler problems, back when Don Syme was first developing it; I\\'ve never actually done any Real Work with it). It took a few hours of hacking to get it to compile. I also had to make some changes to make it more convenient to use it as a class from C# (since it was originally designed to be consumed from an F# console app). I also changed my formula for calculating predictions from scores, to use a cumulative gaussian - since that is what is suggested in the TrueSkill Through Time paper. My score now jumped to 0.669.\\r\\n\\r\\nThe paper used annual results, but it seemed to me that this was throwing away valuable information. I switched to monthly results, which meant I had to find a new set of parameters appropriate for this very different situation. Through simple trial and error I found which params were the most sensitive, and then used hill-climbing to find the optimum values. This took my score to 0.663.\\r\\n\\r\\nThen I added something suggested in the Chessmetrics writeup on the forum - I calculated the average score of the players that each person played against. I then calculated a weighted average of each player\\'s actual score, and the average of their opponents. I used a hill-climb algorithm to find the weighting, and also weighted it by the standard deviation of their weighting (as output by Trueskill/Time). This got me to 0.660 - 20th position, although later someone else jumped above me to push me to 21st.\\r\\n\\r\\nThe next 5 days I went backwards again! I tried an ensemble approach (weighted average of TrueSkill, TrueSkill/Time, and ELO), which didn\\'t help - I think because TrueSkill/Time was so much better, and also because the approaches aren\\'t different enough (ensemble approaches are best when combining approaches which are very different). I tried optimising some parameters in both the rating algorithm, and in the gaussian which turns that into probabilities for each result. I also tried directly estimating and using draw probabilities separate from win probabilities.\\r\\n\\r\\nI realised that one problem was that my results on the validation set weren\\'t necessarily showing me what would happen on the final leaderboard. I tried doing some resampling of the validation set, and realised that different samples gave very different results. So, the validation set did generally effectively show the impact when I made a change which was based on a solid theoretical basis, but it was also easy to get meaningless increases by thoughtless parameter optimisation.\\r\\n\\r\\nOn Nov 15 I finally made an improvement - previously in the gaussian predictor function I had made the standard deviation a linear function of the overall match level [i.e. (s1+s2)/2]. But I realised from looking at graphs that really it\\'s that a stronger black player is better at forcing a draw - it\\'s really driven by that, not by the combined skill. So I made the standard deviation a linear function of black\\'s skill only. Result: 0.659.\\r\\n\\r\\nSo, it was now Nov 16 - two days to go, and not yet even in the top 20! I finally decided to actually carefully measure which things were most sensitive, so that I could carefully manage my last 4 submissions. If I had been this thorough a week ago, I wouldn\\'t have wasted so much valuable time! So, I discovered that the following had the biggest impact on the validation set:\\r\\n\\r\\n* - Removing the first few months from the training data; removing the first 34 months was optimal for the validation set, so I figured removing the first 40 months would be best for the full set\\r\\n* - Adjusting the constant in the calculation of the gaussian\\'s standard deviation - if too high, the predictions varied too much, if too little, the predictions were all too close to 0.5\\r\\n* - And a little trick: I don\\'t know much (anything!) about chess, but I figured that there must be some knockout comps, so people who play more perhaps are doing so because they\\'re not getting knocked out! So, I tried using the count of a player\\'s matches in the test set as a predictor! It didn\\'t make a huge difference to the results, but every little bit counts...\\r\\n\\r\\nBased on this, my next 3 submissions were:\\r\\n\\r\\n* - Remove first 40 months: 0.658\\r\\n* - Include count of matches as a prediction: 0.654\\r\\n* - Increase the constant in the stdev formula by 5%: 0.653\\r\\n\\r\\n(My final submission was a little worse - I tried removing players who hadn\\'t played at least 2 matches, and I also increased the weight of the count of matches: back to 0.654).\\r\\n\\r\\nFor me, the main lesson from this process has been that I should more often step back and think about the fundamentals. It\\'s easy to get lost in optimising the minor details, and focus on the solution you already have. But when I stepped away from the PC for a while, did some reading, and got back to basics with pen and paper, is when I had little breakthoughs.\\r\\n\\r\\nI also learnt a lot about how to use validation sets and the leaderboard. In particular, I realised that when you\\'re missing a fundamental piece of the solution, then little parameter adjustments that you think are improvements, are actually only acting as factors that happen to correlate with some other more important predictor. So when I came across small improvements in the validation set, I actually didn\\'t include them in my next submitted answer - I only included things that made a big difference. Later in the competition, when I had already included the most important things, I re-tested the little improvements I had earlier put aside.\\r\\n\\r\\nPlease let me know if you have any questions. I would say that, overall, TrueSkill would be a great way to handle Chess leaderboards in the future. Not because it did well in this competition (which is better at finding historical ratings), but because, as shown in Chris Bishop\\'s talk, it is amazingly fast at rating people\\'s \"true skill\". Just 3 matches or so is enough for it to give excellent results.\\r\\n\\r\\nHere\\'s a little pic of my submission history\\n[gallery link=\"file\"]\\r\\n\\r\\n\\r\\nThe points show the public leaderboard score, and the final score, for each submission. As you can see, they are very closely related (r^2 = 0.97). Kaggle has previously shown that a pool of other submissions from the leaderboard do not have such a close relationship - I think this is because I only had a short time, so my submissions really needed to be based on a solid theoretical basis, rather than parameter tuning. Although parameter tuning can easily overfit, you can see from the above picture that carefully targeted changes can allow you can get a good idea of how you\\'re doing from the leaderboard. (I also found a similar level of relationship between my validation set results, and the leaderboard results).\\r\\n\\r\\nThe lines connecting the points show the progress of my scores over time. You can clearly see how after the first few days, going from standard TrueSkill over to TrueSkill Through Time, the results make a sudden jump. You can see how up until that time, I was mainly going backwards!\\r\\n\\r\\nYou can also see after I switched to TrueSkill Through Time, I had a few days of going nowhere - up until the last few days, when I forced myself to take a more thoughtful approach, due to the lack of time.\\r\\n\\r\\nOne takeaway from this chart, I think, is to note that a few days of failure is no reason to give up - improvements tend to be sudden, as new insights are made, rather than gradual. So, doing well in these competitions requires a certain amount of resilience - even when things are going badly on the leaderboard, you\\'re skill learning, and still have the chance to make improvements as long as you\\'re still trying.\\r\\n\\r\\nAnother takeaway is that it\\'s better to try lots of completely different things, rather than trying to precisely tune whatever you have so far. At least in this competition, for my results, I saw big improvements from adding substantial pieces to the algorithm, and not much from fine-tuning the details.', 'About\\xa0Martin:\\r\\n\\r\\nMartin is a retired Senior Project Manager and IT Consulting Manager at Siemens\\xa0AG with a university degree in physics.\\xa0 He\\xa0now likes to\\xa0develop and improve\\xa0rating methods, especially regarding professional boxing ratings - see\\xa0http://boxrec.com - maybe I will launch a competition regarding these ratings in the future ...\\r\\n\\nMartin’s Method:\\r\\n\\r\\n“I first evaluated established ratings like Elo and Chessmetrics -\\xa0and found Chessmetrics a very promising approach, regarding the parameters:\\xa0performance, opposition quality,\\xa0activity, weighting and\\xa0self consistant rating\\xa0over a time period. By varying the parameters and algorithms and evaluating the predictions against the cross validation data, I step by step could improve my score. Finally my last submission #50 with\\xa0my best\\xa0public\\xa0score was not the submission with my best final score. My prize-winning submission turned-out to be\\xa0my submission #23 - so never give up.”\\r\\n\\r\\n', 'Having participated in the contest almost from the beginning and posting 162 submissions by the end, I have tried a large variety of different prediction approaches. The first of them were Elo-based, using ratings updated iteratively as the games were read in sequentially, later ones had Chessmetrics-style simultaneous ratings which eventually culminated in the non-rating, graph theory-based prediction system which held the top spot in the leaderboard for the past weeks yet ended up finishing somewhere in the vicinity of 15th place.\\r\\n\\r\\n\\r\\n\\r\\nIn between, though, when my progress stalled and I was slowly being pushed back through the ranks, I took a few days’ time to rethink rating systems from the very core, with the idea that a purely mathematical approach might be the ingredient I needed in order to advance again. While my submissions using this approach were only moderately successful, I do think that this is the most interesting and intuitive idea that I tried in the contest, and therefore have decided to write this blog post about it.\\r\\n\\r\\nPart I.\\u2003Ideal Ratings\\r\\n\\r\\nThe basic mathematical idea behind every rating system is the following:\\r\\n\\r\\nLet {pk} be the set of players. Consider the set of games {gk}; we wish to predict their individual outcomes (i.e. the white scores) {sk} by using only two numbers for each game gk: The rating rw =\\u2005r(w(gk)) of the white player w(gk), and the rating rb =\\u2005r(b(gk)) of the white player b(gk). We are therefore looking for a set of ratings {rk} (one rating assigned to each player) with the following property:\\r\\n\\n(1) sk =\\u2005f(rw,\\u2005rb)\\r\\n\\r\\nfor some function f, the prediction function.\\r\\n\\r\\nOf course, this condition is impossible to fulfill in most practical settings. When at least two of the games feature the same two players playing white and black and their outcomes are different, (1↑) cannot hold for both of these games simultaneously.\\r\\n\\r\\nPart II.\\u2003“Optimal Ratings”\\r\\n\\r\\nFor simplicity’s sake, let the prediction function f in (1↑) be defined as\\r\\n\\n(2) f(rw,\\u2005rb)\\u2005=\\u2005rw −\\u2005rb +\\u2005s\\r\\n\\r\\nwith s denoting the average outcome, or average white score (in the competition’s training dataset, s =\\u20050.5456…). Thus between two players of equal rating, the predicted score will precisely match the average score s.\\r\\n\\r\\nThis linear formulation is a very good approximation to the “ideal” prediction function, as shown by Jeff Sonas with his Chessmetrics system. It also has the advantage that it can easily be “inverted”, meaning that it is easy to find a function h which, when supplied a game score between two players, gives the expected rating difference that is a maximum likelihood estimator for the players’ actual rating difference. Namely,\\r\\n\\n(3) h(sk)\\u2005=\\u2005sk −\\u2005s\\r\\n\\r\\nThis function allows us to recast the condition (1↑) as follows:\\r\\n\\n(4) sk −\\u2005s =\\u2005rw −\\u2005rb\\r\\n\\r\\nthat is, the expected rating difference should match the actual one.\\r\\n\\r\\nWhile we have only been considering the individual games gk and their outcomes sk so far, it is easy to formulate (weaker) conditions which look at the average score between two players, and thus more closely approximate their “true” relative strengths:\\r\\n\\n(5) sp1,\\u2005p2 −\\u2005s =\\u2005rp1 −\\u2005rp2\\r\\n\\r\\nwhere sp1,\\u2005p2 denotes the average score p1 has achieved against p2.\\r\\n\\r\\nRecognizing that many conditions of this type interlinking the players may be difficult to fulfill, we arrive at the important idea of restating (5↑) as a minimization problem:\\r\\n(6) minrp1,\\u2005rp2∥rp1 −\\u2005rp2 −\\u2005sp1,\\u2005p2 +\\u2005s∥\\r\\nExpanding the scope to look at all player connections simultaneously, this becomes\\r\\n(7) min{rk}∑|{pk}|i,\\u2005j =\\u20051n(pi,\\u2005pj)∥rpi −\\u2005rpj −\\u2005spi,\\u2005pj +\\u2005s∥\\r\\nwith the additional idea that the higher the number of games n(pi,\\u2005pj) between two players pi and pj becomes, the more weight their individual condition (6↑) carries, which makes sense because their score average spi,\\u2005pj carries more information. In a mathematical sense, ratings generated using (7↑) can justifiably be called “optimal”. Note that for players that didn’t face each other at all (i.e. the vast majority of pairings), n(pi,\\u2005pj) will be zero, and thus the evaluation of the term inside the norm does not matter for such pairings.\\r\\n\\r\\nPart III.\\u2003Implementing “Optimal Ratings”\\r\\n\\r\\nFrom a numerical standpoint, (7↑) is an unconstrained, quadratic (if ∥⋅∥ is taken to be the Euclidean norm) optimization problem - with, in case of the competition, 8631 variables. This is large, especially if you don’t have a supercomputer available, but not intractible. Ordinary computer algebra systems, however, will likely not be up to the task, because the choice of optimization method is paramount for a sensible runtime.\\r\\n\\r\\nI used a truncated Newton method with line search and analytically computed first derivatives, implemented in Fortran 90 (Yes!) by Stephen Nash (http://jblevins.org/mirror/amiller/tn.zip), which after some modifications gave convergence times of less than 3 minutes on my notebook. The ratings were then used to generate predictions via (1↑). I still remember the excitement I felt when I uploaded my first optimization-based prediction to Kaggle; naturally, I expected it to do really well, given the hyper-advanced number crunching algorithms used.\\r\\n\\r\\nThat is, until I saw the public score. It was by far my worst one at that time.\\r\\n\\r\\nAt first I couldn’t believe it. I uploaded the same prediction again, with an identical result. Then I started looking for a bug in my code, but couldn’t find one. What was going on?\\r\\n\\r\\nPart IV.\\u2003Revisiting the Optimization Problem\\r\\n\\r\\nWhere there is no software bug, there is likely a flaw within the original line of reasoning. It took a while to find it, but in the end I figured it out.\\r\\n\\r\\nConsider the following crosstable of games played among three players, with the average score given in brackets:\\r\\n\\n\\n\\nPlayer\\n1\\n2\\n3\\n\\n\\n1\\n-\\n3 (1.0)\\n0\\n\\n\\n2\\n3 (0.0)\\n-\\n2 (1.0)\\n\\n\\n3\\n0\\n2 (0.0)\\n-\\n\\n\\n\\r\\nWe see that player 1 has been doing really well against player 2; his score is 100%. Similarly, player 2 has a 100% score against player 3, while player 1 and player 3 did not play against each other.\\r\\n\\r\\nA system based on (7↑) would make the rating difference between player 1 and player 3 unreasonably large, because there is no connection between those players at all. The optimization engine is thus free to vary the rating distance of those players, even to the point where it generates ratings that would fit those of Rybka and a monkey, respectively.\\r\\n\\r\\nThe solution is to tie the ratings of all unconnected players together, with the sensible assumption that players in the same pool are (very roughly) of the same class, and thus of similar strength. This gives rise to a new, slightly more complex optimization problem:\\r\\n(8) min{rk}∑|{pk}|i,\\u2005j =\\u20051c(pi,\\u2005pj)\\r\\nwhere\\r\\n(9) c(pi,\\u2005pj)\\u2005=\\n\\n\\n\\nn(pi,\\u2005pj)∥rpi −\\u2005rpj −\\u2005spi,\\u2005pj +\\u2005s∥\\nn(pi,\\u2005pj)\\u2005>\\u20050\\n\\n\\nα∥rpi −\\u2005rpj∥\\nn(pi,\\u2005pj)\\u2005=\\u20050\\n\\n\\n\\r\\nThe scaling factor α is necessary because without it, the ties ∥rpi −\\u2005rpj∥ bind the ratings much too tightly together: Of 86312 ≈\\u200574.5 million parts in the sum, only about 57,000 (or less than 0.1%) are connections between players (made by games played), while the overwhelming rest are conditions basically stating that all ratings should be equal, which is what they will be if α =\\u20051. However, I have found a scaling factor of α =\\u20050.0001 to work quite well in practice, and indeed, minimizing (8↑) with this factor gives results similar in quality to that of Chessmetrics, provided that some additional tricks are used (such as a precisely controlled rating pool). This is no surprise, since Chessmetrics is essentially an algorithm designed to accomplish this very optimization (with some slight differences). However, the advantage of this direct numerical approach is that deep insights into the mechanics of the rating system are easier to gain and implement, and doing so might give birth to some completely new ideas.\\r\\n\\r\\nIn the contest I soon found a different approach which didn’t require optimization and gave better results, so I abandoned this technique in favor of it. Nevertheless, optimization might be worth investigation further as a means for both prediction and rating generation, in chess and in other sports.', 'Our team, \"Old Dogs With New Tricks\", consists of me and Peter Frey, a former university professor.  We have worked together for many years on a variety of machine learning and other computer-related projects. Now that we are retired from full-time employment, we have endeavored to keep our skills sharp by participating in machine learning and data mining contests, of which the chess ratings contest was our fourth.\\r\\n\\r\\nOur approach to this contest has been to treat it primarily as a forecasting problem, not as an exercise in developing a chess ratings system.  We built forecasting models from the training data using a home-grown variant of \"Ensemble Recursive Binary Partitioning\", a method we have previously employed for other contests and\\r\\napplications.  Like various other machine-learning/forecasting methods, this one trains a model on data consisting of records (e.g. cases, instances, objects, or in this case chess games) for which the values of a set of predictor variables and an outcome variable are known, and then applies the model to estimate (or forecast) the outcome values for a separate set of test or production data records for which the predictor values but not the outcomes are known.\\r\\n\\r\\nSince each game in the chess dataset was supplied with a minimal amount of information (month, player id\\'s, and result), we synthesized a variety of predictor variables based on an analysis of the dataset. We attempted to optimize parameter settings and variable selections by observing model performance both on the leaderboard set and on various holdout sets created from the last 5, 7, or 10 months of the training data.  By the end of the competition we had done over 1100 runs building models on part of the training data and testing them on the remainder, and had created and tested various combinations of approximately 65 home-grown predictors.\\r\\n\\r\\nOf our 120 submissions, the 93rd on November 4 produced the best score on the official test data, 0.699472.  The model used for that submission employed 22 predictors:\\r\\n\\r\\n1, 2:\\tWhite and Black player skill rankings calculated by an iterative process from the results of the training games.\\r\\n\\r\\n3, 4:\\tCounts of \"quality\" games for White and Black players used in calculating vars 1 and 2.  \"quality\" games are those for which one\\'s opponent has a skill level that is not too dissimilar from one\\'s own.\\r\\n\\r\\n5, 6:\\tAverage rankings of White\\'s and Black\\'s opponents as calculated for vars 1 and 2.\\r\\n\\r\\n7, 8:\\tAverage number of games per month up to this game for White and Black.\\r\\n\\r\\n9, 10: White and Black ratings calculated from the training data according to an ELO-like algorithm that evolves ratings chronologically from a fixed starting value.\\r\\n\\r\\n11, 12: Maximum ratings (as calculated for vars 9 and 10) of opponents beaten by White and Black up to this game.\\r\\n\\t\\r\\n13, 14: Minimum ratings (as calculated for vars 9 and 10) of opponents lost to by White and Black up to this game.\\r\\n\\r\\n15, 16: Mean ratings of White\\'s and Black\\'s previous opponents.\\r\\n\\r\\n17, 18: Mean scores of White and Black against their common opponents.\\r\\n\\r\\n19, 20: Rating difference between White\\'s next and current opponents. Similarly for Black.\\r\\n\\r\\n21, 22: Rating difference between White\\'s current and previous opponents. Similarly for Black.\\r\\n\\r\\nThe computing resources employed for the contest consisted of a few workstations running the Linux operating system.  Our core general purpose data analysis and forecasting engine is written in ANSI C, but was driven by code specific to this contest written in the scripting language Lua. \\r\\n', 'The following Q&A is with Jeremy Howard who together with\\xa0teammate Lee Baker won Kaggle\\'s Tourism Forecasting Competition. (This was a two-part competition -\\xa0Lee previously described his work on Part\\xa0I of the competition in\\xa0a separate blog post).\\r\\n\\xa0\\n- where you\\'re from, what you have studied\\r\\n\\r\\nI live in Melbourne, Australia, recently voted the world\\'s 3rd most livable city... Perhaps this is inspiring a bit of data mining success around here (it\\'s also the home of the tourism2 winner, and the Kaggle CEO)! I studied philosophy at the University of Melbourne.\\r\\n\\r\\n- what you do\\r\\n\\r\\nAfter university I worked in management consulting (McKinsey & Co, AT Kearney), and then went on to found 2 businesses (FastMail.FM, an email provider, and Optimal Decisions Group, an insurance pricing optimisation specialist). Having sold both in the last couple of years, I now have the free time to follow my interests.\\r\\n\\r\\n- core technical approach\\r\\n\\r\\nMy goal was to try to stay in line with the approach taken in the paper being submitted by the contest organisers - I wanted to find a general, automated algorithm for forecasting, which I could apply to all time series without any parameter tuning or manual involvement. I had hoped therefore to only do a single submission to the leaderboard. However, an early data problem in the posted data (later rectified by the organisers) unfortunately meant this wasn\\'t possible. After the fixed data was posted, I only did 3 further submissions.\\r\\n\\r\\nI realised that a fundamental issue was that the final results were calculated using a novel algorithm called \"MASE\", which is a ratio. The denominator of the ratio could in some cases be extremely small - this occured in series which had close to constant additive seasonality, no growth, and no noise. I found that the contribution of these series to the overall result was so high that in practice the algorithm should be tuned to favor these, even at the expense of other series (which had a relatively high denominator, and thus contributed much less to the overall result).\\r\\n\\r\\nTo do this, I only used linear growth (as opposed to exponential) and additive seasonality (as opposed to multiplicative) for all series, since any series which had exponential growth and/or multiplicative seasonality would have very small weights in the overall metric. Later, I experimented with allowing some series to use exponential growth and multiplicative seasonality, if the statistical evidence for those series was particularly strong, and confirmed that the impact was negative, as expected.\\r\\n\\r\\n- methodology that proved most effective\\r\\n\\r\\nI first created an algorithm to automatically remove outliers. Outliers can occur in, for example, a tourism time series if an area has a once-off event (positive outlier), or temporary closing of a major attraction (negative outlier), which will not impact future results. I used a customised local smoother, and used the residuals to determine seasonality. I ran this twice to create a double-smoothed time series, which I then compared to the original data, and removed data points outside 3 standard deviations of residuals.\\r\\n\\r\\nI then fitted a weighted regression (weighted the most recent observations the most heavily) combined with weighted additive seasonality (again weighting the most recent observations the most heavily) on all but the last 2 years of each series. A simple optimiser found the optimal weighting of each in order to predict the final 2 years. This weighted model was then applied to the full data set to create predictions. The intercept of the weighted regression was adjusted such that the residual on the final observation was always zero - this was important for ensuring that the series with a low denominator in the MASE metric were forecast as accurately as possible.\\r\\n\\r\\nI\\'ve since realised I had some bugs in my code (e.g. failing to truncate series to be positive, and some bugs with going from a validation set to the final predictions). It would be interesting to see how much better the predictions would be if these bugs were fixed.\\r\\n\\r\\nThe whole algorithm takes only 10 seconds to complete for the entire set of time series. Since the algorithm is fast, accurate, and automated, I think it is a good system for automated time series prediction. I plan to test it in the future on other data sets (e.g. the \"M3\" time series prediction competition data) to confirm that it can be effectively applied to other types of data.\\r\\n\\r\\n- what first attracted you to the competition? \\r\\n\\r\\nThe tourism forecasting competition was my first data mining contest - I entered it in order to try to update and strengthen my analysis skills, and to learn something new (having never done time series forecasting before).\\r\\n\\r\\n- did you do much background reading or research? \\r\\n\\r\\nYes, I read most of the recent papers and online tutorials by one of the conference organisers, Rob J Hyndman. I found that they were a great way for a time-series newbie like myself to get up to speed with the topic.\\r\\n\\r\\n- what tools and programming language did you use?\\r\\n\\r\\nI used C#, in Visual Studio 2010.\\r\\n\\r\\n- how much time did you spend on the competition?\\r\\n\\r\\nI spent longer than I expected because the initial data problems left me stumped and confused! Once they were fixed, I had submitted my result within a couple of hours. I estimate I spent a couple of weeks on the problem, including reading and research.', \"My first contact with the inner workings of the Elo rating system was in the mid-90s, when I came across an article in the Europe Echecs magazine. I remember thinking that the problem of ranking chess players was much different from chess itself, so I didn’t pay much attention to it at the time.\\r\\n\\r\\n\\r\\n\\r\\nIn August 2010, when I was checking some news on slashdot.org, I was surprised to find a post on a chess ratings competition. I thought it would be interesting to give it a try, and perhaps I could spend two or three days of my holidays on it. I never thought that during the following 15 weeks(!) I would be so absorbed by the competition, with an average of one submission a day.\\r\\n\\r\\nThere were times when my laptop (a 2.4GHz dual core) would be running for days on end with both processors running two different parameter tunings. There was also a Saturday morning when I got up in the middle of the night to check how the tuning was going. At 5am I decided to do another submission; it was my 51st submission, and it was the one that made it to 4th place.\\r\\n\\r\\nThe thing that got me so enthusiastic about the competition was more than the challenge itself. It was the fact that dozens of other people were thinking about the same problem, and that every day there was some change in the leaderboard, either I was able to jump a few positions, or someone else just popped out with an amazing score! It was this continuous challenge that I found so thrilling.\\r\\n\\r\\nBack in August I started thinking about the problem with pencil and paper, while doing a few trial submissions. From the beginning I looked at ratings as an indication of the probability of one player beating another, and then devised a probabilistic approach where the rating of each player was something between 0 and 1. Later I found that this approach was very much related to the well-known Bradley-Terry model.\\r\\n\\r\\nThe details can be found in the link below, together with an implementation (source code) in Python 2.6:\\r\\n\\r\\nhttp://web.tagus.ist.utl.pt/~diogo.ferreira/chess/\\r\\n\\r\\nI’m making this available in the hope that others will be able to improve it even further (and also share the results). One thing I learned from this experience is that the spirit of competition can push things forward, and push things with such impetus that I'm convinced all of us together did more progress in a few months than one alone could\\xa0have achieved\\xa0over\\xa0several\\xa0years.\\r\\n\\r\\nDiogo R. Ferreira is professor of information systems at the Technical University of Lisbon, where he works in the area of process mining.\", 'Neeral (@beladia) and I (@jacksheep) are glad to have participated in the first Kaggle-in-Class competition for Stats-202 at Stanford and we have learnt a lot!\\xa0With one full month of hard work, excitement and learning coming to an end and coming out as the winning team, it certainly feels like icing on the cake. The fact that both of us were looking\\xa0for nothing else than winning the competition, contributed a lot to the motivation and zeal with which we kept going each and every day. Each of us may have spent about 100 hours on this competition, but it was totally worth it.\\r\\n\\r\\n\\r\\n\\r\\nEven though the professor and textbook already mentioned some points which ended up to be very useful, the experiences we gained from this practice were much more influential to us. We strongly recommend every data mining student to participate in such competitions, make your hands dirty, and eventually every minute you invest on it will reward back.\\r\\n\\r\\nANALYZE THE DATA FIRST \\r\\n\\r\\nThe most important lesson we learnt was: we should always analyze the data first.\\r\\n\\r\\nAs newbies in data mining, we had thought a cool model could give us the best result, so the main effort was to find such advanced models and keep tuning it. We used the features from the raw data, did some feature transformation to deal with missing values, and tried some supervised learning model, such as lm, randomForest and glmboost in R. It seemed working, because Random Forest could improve 18% comparing to linear regression.\\r\\n\\r\\nHowever, we found the top 3 teams were more than 30% better than our result. We felt our approach was not on the right track, because we didn’t believe any model on the same features could achieve such big improvement. So we switched the focus to the data itself and began studying its characteristics. We hoped to find clues from the data: was there any trick we didn’t figure out yet?\\r\\n\\r\\nSoon later, we found about 30% of test instances could exactly match instances in training set and got 0 error. Naturally, the next question was: for each of the rest 70% of the test instances, how to find the best training instance approximate the price for the test instance? Intuitively, a wine in the same location with most similar name and the closest year could be a good candidate. Following this idea, we thought k-NN may be a good model for this data.\\r\\n\\r\\nThe next things became relatively easy: how to define the distance between instances. We explored different distances and tried some optimizations, and finally combined the results from different k-NN modes to reduce the variance, which was inspired by the idea of ensemble methods.\\r\\n\\r\\nIn a word, we would not have thought of the k-NN model without carefully studying the data. If you only can remember one thing from this blog, we hope it is: analyze the data first.\\r\\n\\r\\nMODELING\\r\\n\\r\\nThe main model we used was k-NN (k-Nearest Neighbor, with k=1) using edit distance, tf-idf cosine similarity and lucene score on wine titles as the distance metrics. We found k-NN achieved much better performance (40+% improvement) than other supervised learning methods that we tried; and it works well even when there are many missing values. One challenge to use k-NN is that the computation cost is very high. We solved this problem by clustering the wines based on a fewer indexes, such as location (the combination of country, sub-region and appellation), cleaned name (after removing the descriptive information), short name (the first 2 words of length >=3 in cleaned name), brand name (the string in quotes); to ensure a cluster assignment for every test instance.\\r\\n\\r\\nWhile performing look-up of neighbors for each test instance, we only selected the training instances in the same cluster (with the same index). For brand name based k-NN, for each test instance, we find the corpus of wines from the training set with the exact brand name as the test instance and find the closest match in terms of clean name(edit distance), volume and price.\\r\\n\\r\\nFinally, we averaged all the predictions from the k-NN models. For test instances that had a missing prediction from a k-NN model, missing values were imputed as the average of non-missing predictions from other k-NN models for the test instance. We also used predictions from randomForest built on previous/elsewhere-year to replace the average of predictions from the k-NN models. If available, now price replaced the final prediction.\\r\\n\\r\\nWe found the prices of the same wines were proportional to its volume. Therefore, the price should be normalized by the volume. The prediction of price from k-NN for each test instance was then computed as the price of the closest training instance in the same cluster * (volume of test instance wine/volume of closest training instance wine).\\r\\n\\r\\nEach k-NN model (using different clustering or distance metrics) can generate a prediction. Finally we average all the predictions to make the final one. The missing values are imputed using the average prediction from other models with non-missing predictions for the test instance.\\r\\n\\r\\nSome improvements and tuning were made based on the ideas. For example, we applied query expansion method on the year and wine scores, we considered the difference of years and volumes when compute the distance so that the model will prefer the neighbor wines with closest year and volume, etc.\\r\\n\\r\\nFEATURES\\r\\n\\r\\nEven though about 80% of features in the data were continuous variables(score, alcohol content) and about 20% were nominal variables(location based, name), our final model, comprising of k-NN models utilized only the nominal variables as explanatory variables and a lot of credit goes to the fact that we were able to discover hidden useful information residing within the name/title of the wine. The following were the features derived from the name/title field:\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 volume\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 previous/elsewhere price\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 now price\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 pack (# of bottles)\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 brand name (string withing quotation marks)\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 clean name (name/title, after removing quoted text, year, punctuations, etc.)\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 short name (first 2 words (length>=3) of the clean name)\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 loc (contanenation of Country, SubRegion and Appellation delimited by ‘.’)\\r\\n\\r\\nA standard bottle of wine has a volume of 750mL and hence wines for which volume wasn’t explicitly mentioned in the name/title field, volume was defaulted to 750mL. Also, for wines where #bottles or pack size was available, volume was adjusted as the volume specified(or default) times the pack size.\\r\\n\\r\\nHANDLING OUTLIERS\\r\\n\\r\\nAny previous price(prevprice) in training or test data or predicted price as mapped to minprice(minimum price from training data, if predicted/previous price < minprice) or maxprice(maximum price from training data, if predicted/previous price > maxprice)\\r\\n\\r\\nMODEL SELECTION\\r\\n\\r\\nWe selected final prediction based on the following order of model precedence\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 If the test instance has now price available from the title, use it as the final prediction (MAE was 0 in ⅓ cross-validation training data used as test set during learning phase)\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 If the test wine finds a neighbor in training set with edit distance <= 1, directly use the price of this neighbor (MAE < 0.1 in ⅓ CV training data)\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 If the test wine has previous or elsewhere price in the title, we used randomForest to predict the price using the previous/elsewhere price and year as features (MAE was about 9 for ⅓ CV training data).\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 For all the rest of the test instances, use k-NN models with k=1, then build a SVM model using the predictions from the 5 best k-NN models as the features, and finally use the average of the 5 k-NN predictions and SVM prediction.\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 Fix outliers, for e.g. if predicted price > 1000, set predicted price = 999.99, which is the maximum price in the training data(based on information provided that all wines with price > 1000 were removed from training and test data)\\r\\n\\r\\nR packages used\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 randomForest\\xa0 (for random forest)\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 e1071\\xa0 (for svm)\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 cba\\xa0 (for edit distance)\\r\\n\\r\\nJava package used\\r\\n\\r\\n●\\xa0\\xa0\\xa0\\xa0\\xa0 Apache Lucene\\r\\n\\r\\nFUTURE WORK\\r\\n\\r\\nIf time permitted, we would try our hands on sub-models based on locations, ratings, etc, and locally regression models based on the distance. We also thought about tuning this problem into a search problem, and using learning to rank models (using the price difference to rank items) to find the closest instance (answer) given a test instance (query).\\r\\n\\r\\nACKNOWLEDGEMENT\\r\\n\\r\\nWe thank Nelson Ray, the TA for Stats202(Data Mining and Analysis) class at Stanford University to organize the competition and provide great support throughout. We also thank Prof. Susan Holmes for imparting both theoretical and practical knowledge during the class that helped us exploit different methodologies and packages on the wine data.\\r\\n\\r\\nS202 Team Eureka\\nJie Yang, Neeral Beladia\\njieyang, beladia AT stanford * edu', 'The attached article discusses in detail the rating system that won the Kaggle competition “Chess Ratings: Elo\\xa0vs the rest of the world”. The competition provided a historical dataset of outcomes for chess games, and\\xa0aimed to discover whether novel approaches can predict the outcomes of future games, more accurately\\xa0than the well-known Elo rating system.\\xa0The major\\xa0component of the winning system is a regularization technique that avoids overfitting.\\r\\n\\r\\nkaggle_win.pdf', 'Graph theory has always been an academic side interest of mine, so I was immediately interested when Kaggle posted the IJCNN social network challenge.\\xa0 Graph-theoretic problems are deceptively accessible and simple in presentation (what other dataset in a data-mining competition can be written as a two-column list?!), but often hide complex, latent relationships in the data.\\r\\n\\r\\n\\r\\n\\r\\nThere’s an old adage among researchers, “a week in the library saves a year in the lab.”\\xa0 Hoping not to reinvent the wheel, I combed through about 20 papers on the link prediction problem, the most helpful & accessible of which was Kleinberg’s “The link prediction problem for social networks.” Unfortunately, at over 1 million x 1 million nodes, the graph size of the Flickr dataset precluded many of the published methods from running on a commodity desktop computer.\\r\\n\\r\\nI programmed about 20 features (Jaccard, Adar, Katz, various neighbor-related metrics, path lengths, unseen bigrams, etc.) along with a framework whereby I would extract local “neighborhood graphs” to make the O(N^2)—or worse—computations feasible.\\xa0 I had this running in parallel in Matlab on an 8-core machine with 10 gigs of RAM and an SSD drive, which really helps keep things moving when the RAM runs out and the pageouts start climbing.\\xa0 I used the Boost Graph Library to speed up graph calculations.\\xa0 \\xa0I spent a good deal of time experimenting with different ways to build the local graphs.\\xa0 Taking the first-level neighbors was possible, but adding the second-level neighbors was intractable.\\xa0 I also tried, among others, methods based on enumerating the simple paths between A and B, using the nodes these paths traverse most often.\\xa0 This proved to be too expensive, so I ended up using the first neighbors of A and B.\\r\\n\\r\\nI was the first to pass the 0.9 public AUC mark when I realized that all the standard similarity features in the literature could be applied to 3 distinct problems:\\r\\n\\r\\n(1) How similar is node A to B?\\r\\n\\r\\n(2) How similar are the outbound neighbors of A to B?\\r\\n\\r\\n(3) How similar are the inbound neighbors of B to A?\\r\\n\\r\\nThis was probably my most novel contribution to the contest and the idea that produced some of my best features.\\xa0 I also found a method from a querying algorithm called Bayesian Sets that worked very well. \\xa0\\xa0As far as I know, this was the first time it has been applied to link prediction.\\r\\n\\r\\nUsing the 60+ features I obtained from applying my features to problems (1), (2) and (3), I tested neural networks, random forests, boosting, and SVMs. I decided the forests were the way to go, since they yielded the best AUC and required no data preprocessing. It was important to do regression rather than strict classification.\\xa0\\xa0 ROC analysis cares only about the relative order of the points, so having “degeneracy” in prediction values is detrimental.\\xa0 Neural networks were almost as good as forests, but their ROCs were just worse enough to make blending neural nets and forests worse than forests alone. With random forests, I was able to score 0.98 and above on my own validation sets, but my public score was paradoxically stuck at 0.92, no matter how many sophisticated features I added or how many training points I churned out.\\xa0 As the competition continued and other teams jumped ahead, I still couldn’t pinpoint the disparity between my private and public scores.\\r\\n\\r\\nAbout the same time, Bo Yang (who was on the 2nd place team in the Netflix prize and had a similar score on the IJCNN leaderboard), approached me about teaming up.\\xa0 It was through my emails with Bo that I realized I had created my validation sets improperly. I was not limiting the outnodes to appear just once in the training data, which meant I was biasing the data towards outnodes with more edges.\\xa0 Once I corrected this, my private AUC dropped from 0.98 to around 0.95, while the public one increased to around the same. What a relief!\\r\\n\\r\\nThe last two weeks of the competition were a frenzy to calculate features, exchange ideas with Bo, and play catch up to IND CCA’s lead.\\xa0 I had the computer running 100% (x 8), 24 hours a day at this point.\\xa0 When IND CCA kept improving, we decided to approach Benjamin Hamner (who had a score near 0.95 too) to make a last effort to merge and overtake.\\xa0\\xa0 Ben agreed to team up. We all exchanged a common set of validation files (which Ben created to precisely match the distribution of the real test set), extracted our own features, and then I combined them all by running several hundred random forests and blending the output.\\xa0 Running this many forests helped to order the points in “no man’s land” around 0.5, which have the highest variance from trial to trial.\\r\\n\\r\\nBen had discovered a group of points he believed were true, based on a discrepancy in the way that Dirk sampled the graph (there should have been a larger number of false B edges with an in-degree of 1). \\xa0\\xa0I also found a set of suspected true edges based on a probabilistic argument.\\xa0\\xa0 If B->A exists and A is the type of user who forms reciprocal friendships, it is highly likely that A->B exists and was removed.\\xa0 Reciprocal edges were fairly rare in the data, and having a path length B->A of 1 was also rare.\\xa0 Thus, having both together in the test set had an extremely low probability of occurring at random.\\r\\n\\r\\nBen and Bo each contributed some very strong features to my own (I’ll leave it to them to explain the details).\\xa0 Ben’s Edgerank feature by itself would have finished in the top 5 of the competition!\\xa0 Bo developed and tuned some very accurate kNN models, several of which scored above 0.9 by themselves.\\r\\n\\r\\nOur best submission was the result of 94 features, some using the entire graph, some calculated on a graph consisting of local neighbors of each node. \\xa0\\xa0Below I attached a table of some of our features and the AUCs they give by themselves on our validation sets.\\xa0 We had 8 validation sets of 4480 points each, half fake and half real.\\xa0 Random forests combined our features to a private AUC of about 0.972, which resulted in a 0.969 public final score.\\r\\n\\n\\n\\nAUC\\nBrief Description\\n\\n\\n0.936\\nEdgerank\\n\\n\\n0.931\\nkNN3\\n\\n\\n0.923\\nkNN4\\n\\n\\n0.907\\nJaccard applied to (2)\\n\\n\\n0.902\\nkNN1\\n\\n\\n0.893\\nAdarB All\\n\\n\\n0.892\\nCommon Neighbors applied to (2)\\n\\n\\n0.889\\nBayesian sets\\n\\n\\n0.889\\nCosine applied to (2)\\n\\n\\n0.888\\nCosine applied to (1)\\n\\n\\n0.88\\nJaccard applied to (1)\\n\\n\\n0.857\\nAdar   applied to (2)\\n\\n\\n0.854\\nSimrank applied to (2)\\n\\n\\n0.854\\nPercent of non-existing edges < SVD(A,B)\\n\\n\\n0.852\\nCommute Time in the local graph\\n\\n\\n0.852\\nGlobal link distance 2\\n\\n\\n0.846\\nTime to 1st similar neighbor in   Bayesian sets\\n\\n\\n0.843\\nkNN2\\n\\n\\n0.84\\nSVD rank 80\\n\\n\\n0.831\\nCommon Neighbors applied to (1)\\n\\n\\n0.831\\nAdarA All\\n\\n\\n0.826\\nSimrank\\n\\n\\n0.812\\nDot product of columns in SVD rank 80 approx\\n\\n\\n0.812\\nSVD values of neighbors\\n\\n\\n0.805\\nSimrank applied to (1)\\n\\n\\n0.794\\nUnseen   bigrams on Simrank\\n\\n\\n0.793\\nKatz   similarity of A and B\\n\\n\\n0.781\\nKatz   applied to (1)\\n\\n\\n0.776\\nBounded   Walks in the local graph\\n\\n\\n0.769\\nMaximum   flow in the local graph\\n\\n\\n0.764\\nShortest   Paths Histogram applied to (1)\\n\\n\\n0.739\\nCommon   neighbors of in2\\n\\n\\n0.73\\nCommon   neighbors\\n\\n\\n0.73\\nCosine   similarity of A and B\\n\\n\\n0.73\\nJaccard   similarity of A and B\\n\\n\\n0.73\\nAdar   similarity of A and B\\n\\n\\n0.727\\nIn   degree of node B\\n\\n\\n0.714\\nBayesian   Sets applied to (1)\\n\\n\\n0.706\\nAdar   applied to (1)\\n\\n\\n0.701\\nPagerank   of node A\\n\\n\\n0.689\\nNumber   of paths between A and B of length 2\\n\\n\\n0.686\\nPower   law exponent of the local graph\\n\\n\\n0.675\\nClustering   Coeff. Node A\\n\\n\\n0.663\\nUnseen   bigrams on Katz\\n\\n\\n0.661\\nPreferential   Attachment\\n\\n\\n0.657\\nBetweenness   Centrality in the local graph\\n\\n\\n0.654\\nClustering   Coeff. Node B\\n\\n\\n0.593\\nCommon   neighbors of in1\\n\\n\\n0.569\\nCommon   neighbors of out1\\n\\n\\n0.56\\nPagerank   of node B\\n\\n\\n0.541\\nKatz   applied to (2)\\n\\n\\n0.538\\nB->A   exists and node A forms mutual edges?\\n\\n\\n0.533\\nCommon   neighbors of out2\\n\\n\\n0.532\\nOut   degree of node B\\n\\n\\n0.521\\nIn   degree of node A\\n\\n\\n0.521\\nOut   degree of node A\\n\\n\\n\\n \\r\\n\\r\\nSpecial thanks to Kaggle and the organizers for sponsoring this competition, as well as my teammates Bo Yang and Benjamin Hamner for their valuable contributions.\\r\\n\\r\\nWilliam Cukierski is a PhD candidate in the Biomedical Engineering Department at Rutgers University. He has a bachelor’s degree in physics from Cornell University.\\r\\n\\r\\nhttp://pleiad.umdnj.edu/~will/', 'First things first: in case anyone is wondering about our team name,  we are all computer scientists, and most of us work in cryptography or related  fields. IND CCA refers to a property of an encryption algorithm. Other than that, no  particular significance.\\r\\n\\r\\nI myself work in computer security and privacy, and my  specialty is de-anonymization. That explains why the other team members (Elaine Shi,  Ben Rubinstein, and Yong J Kil) invited me to join them with the goal of  de-anonymizing the contest graph and combining that with machine learning.\\r\\n\\r\\n\\r\\n\\r\\nTo clarify: our goal was to map the nodes in the training dataset to the real identities in the  social network that was used to create the data. That would allow us to simply look  up the pairs of nodes in the test set in the real graph to see whether or not the  edge exists. There would be a small error rate because some edges may have changed  after the Kaggle crawl was conducted, but we assumed this would be negligible.\\r\\n\\r\\nKnowing that the social network in question is Flickr, we crawled a few million users\\'  contacts from the site. The crawler was written in python, using the curl library,  and was run on a small cluster of 2-4 machines.\\r\\n\\r\\nWhile our crawl covered only a  fraction of Flickr users, it was biased towards high degree nodes (we explicitly  coded such a bias, but even a random walk is biased towards high degree nodes), so we were all set. By the time we crawled 1 million nodes we were hitting a 60-70%  coverage of the 38k nodes in the test set. But more on that later.\\r\\n\\r\\nOur basic approach  to deanonymization is described in my paper with Vitaly Shmatikov. Broadly, there are two steps: “seed finding” and “propagation.” In the former step we somehow  deanonymize a small number of nodes; in the latter step we use these as “anchors” to propagate the deanonymization to more and more nodes. In this step the algorithm  feeds on its own output.\\r\\n\\r\\nLet me first describe propagation because it is simpler. As  the algorithm progresses, it maintains a (partial) mapping between the nodes in the  true Flickr graph and the Kaggle graph. We iteratively try to extend the mapping as  follows: pick an arbitrary as-yet-unmapped node in the Kaggle graph, find the “most similar” node in the Flickr graph, and if they are “sufficiently similar,” they get mapped to each other.\\r\\n\\r\\nSimilarity between a Kaggle node and a Flickr node is  defined as cosine similarity between the already-mapped neighbors of the Kaggle node  and the already-mapped neighbors of the Flickr node (nodes mapped to each other are  treated as identical for the purpose of cosine comparison).\\r\\n\\r\\n\\r\\n\\r\\nIn the diagram, the blue  nodes have already been mapped. The similarity between A and B is 2 / (√3·√3) =  ⅔. Whether or not edges exist between A and A’ or B and B’ is irrelevant.\\r\\n\\r\\nThere \\xa0are many heuristics that go into the “sufficiently similar” criterion, which will be described in our upcoming paper. There are two reasons why the similarity between \\xa0a node and its image may not be 100%: because the contest graph is slightly different from our newer crawled graph, and because the mapping itself might have \\xa0inaccuracies. The /latter is minimal, and in fact the algorithm occasionally revisits \\xa0already-mapped nodes to correct errors in the light of more data.\\r\\n\\r\\nI have elided over \\xa0many details — edge directionality makes the algorithm significantly more complex, \\xa0and there are some gotchas due to the fact that the Kaggle graph is only partially \\xa0available. Overall, however, this was a relatively straightforward adaptation of the \\xa0algorithm in the abovementioned paper with Shmatikov.\\r\\n\\r\\nFinding seeds was much harder. \\xa0Here the fact that the Kaggle graph is partial presented a serious roadblock, and \\xa0rules out the techniques we used in the paper. The idea of looking at the \\xa0highest-degree nodes was obvious enough, but the key observation was that if you look at the nodes by highest in-degree, then the top nodes in the two graphs roughly \\xa0correspond to each other (whereas ordered by out-degree, only about 1/10 of the \\xa0Flickr nodes are in the Kaggle dataset). This is because of the way the Kaggle graph \\xa0is constructed: all the contacts of each of the 38K nodes are reported, and so the \\xa0top in-degree nodes will show up in the dataset whether or not they’re part of the \\xa038K.\\r\\n\\r\\nStill, it’s far from a straightforward mapping if you look at the top 20 in the two graphs. During the contest I found the mapping by hand after staring at two \\xa0matrices of numbers for a couple of hours, but later I was able to automate it using \\xa0simulated annealing. We will describe this in detail in the paper. Once we get 10-20 \\xa0seeds, the propagation kicks off fairly easily.\\r\\n\\r\\nOn to the results. We were able to \\xa0deanonymize about 80% of the nodes, including the vast majority of the high-degree \\xa0nodes (both in- and out-degree.) We’re not sure what the overall error rate is, but for the high-degree nodes it is essentially zero.\\r\\n\\r\\nUnfortunately this translated to \\xa0only about 60% of the edges. This is because the edges in the test set aren’t \\xa0sampled uniformly from the training graph; it is biased towards low-degree nodes, and deanonymization succeeds less often on low-degree nodes.\\r\\n\\r\\nThus, deanonymization alone \\xa0would have been far from sufficient. Fortunately, Elaine, Ben and Yong did some \\xa0pretty cool machine learning, which, while it would not have won the contest on its \\xa0own, would have given the other machine-learning solutions a run for their money.\\r\\n\\r\\nElaine sends me the following description:\\n\\n \\nOur ML algorithm is quite similar to \\xa0what vsh described earlier. However, we implemented fewer features, and spent less \\xa0time fine tuning parameters. That\\'s why our ML performance is a bit lower, with an \\xa0*estimated* AUC of 93.5-94%.\\n(Note that the AUC for ML is not corroborated with \\xa0Kaggle due to the submission quota, but rather, is computed over the ground truth \\xa0from the deanonymization. The estimate is biased, since the deanonymized subset is \\xa0not sampled randomly from the test set. [The 93.5-94% number is after applying \\xa0Elaine’s debiasing heuristic. -- Arvind])\\nWe also used Random Forest over features a bunch of features, including the following (with acknowledgements to the \"social \\xa0network link prediction\" paper):\\r\\n1) whether reverse edge is present\\r\\n2) Adamic\\r\\n3) \\xa0|intersection of neighbors| / |union of neighbors|\\r\\n4) Neighborhood clustering \\xa0coefficient\\r\\n5) Localized random walk 2 - 4 steps\\r\\n6) Degree of n1/n2.\\r\\n7) ... and a few \\xa0other features, most are 1-3 hop neighborhood characteristics.\\nFor some of the \\xa0above-mentioned features, we computed values for the out-graph, in-graph, and the \\xa0bi-directional graph (i.e., union of in-graph and out-graph). We wanted to implement \\xa0more features, but did not due to lack of time\\nThe best feature by itself is the \\xa0localized random walk 3-4 steps. This feature on its own has an *estimated* AUC of \\xa092%. The Random Forest implementation used the Python Milk library. \\xa0 http://pypi.python.org/pypi/milk/\\nCombining DA and ML:\\r\\n\\r\\n1) Naive algorithm:\\r\\nFor \\xa0each test case:\\xa0output DA prediction if DA prediction exists, else ML \\xa0score\\r\\n\\r\\n2) Improved version:\\r\\nFor edge (n1, n2), if n1 and/or n2 has multiple DA \\xa0candidates, and all candidates unanimously vote yes or no, output the corresponding \\xa0prediction.\\nFinally, like every one else, there were some trial-and-error tuning and \\xa0adjustments.\\r\\nOnce we hit No. 1, we emailed the organizers to ask if what we did \\xa0was OK. Fortunately, they said it was cool. Nevertheless, it is interesting to wonder how a future contest might prevent deanonymization-based solutions. There are two \\xa0basic approaches: construct the graph so that it is hard to deanonymize, or require \\xa0the winner to submit the code for human inspection. Neither is foolproof; I’m going to do some thinking about this but I’d like to hear other ideas.\\r\\n\\r\\nLast but not least, a big thanks to Kaggle and IJCNN!', 'I chose to participate in this contest to learn something about graph theory, a field with a huge variety of high-impact applications that I\\'d not had the opportunity to work with before. \\xa0However, I was a late-comer to the competition, downloading the data and submitting my first result right before New Years. \\xa0From other\\'s posts on this contest, it also seems like I\\'m one of the few who didn\\'t read Kleinberg\\'s link prediction paper during it.\\r\\n\\r\\n\\r\\n\\r\\nThere were a few reasons I did not do a literature review while participating in the contest. \\xa0One was time - I had a late start, and I knew I had little time to devote to the contest for its remainder. \\xa0Short on time, I preferred to dive right into the data and see what I could make of it, as opposed to doing an extensive review of the prior literature. \\xa0I did this at the risk of missing something obvious that would help the models or \"reinventing the wheel,\" but with the benefit of having a fresh start.\\r\\n\\r\\nTo begin, I wrote a quick script to generate my own validation sets from the data and a framework to extract features from the data. \\xa0Features I extracted fell into three categories: those pertaining only to the \"A\" node (the source of the potential edge), those pertaining only to the \"B\" node (the destination of the potential edge), and those pertaining to the relation between A and B.\\r\\n\\r\\nNext, I looked at the nodes known to follow B. \\xa0Let \\xa0 denote the set of nodes N follows, and  denote the set of nodes that follow N. \\xa0For each pair A and\\xa0B, I took the following sum as a feature:\\r\\n\\r\\n\\r\\n\\r\\nThis feature by itself worked decently well, and almost got me in the top 10. \\xa0I continued to add features that I thought would be relevant to the problem. \\xa0These included the following:\\r\\n\\r\\n- Mean of values in the above summand- Max of values in the above summand- In-degree of A- Out-degree of A- In-degree of B- Out-degree of B- SVD- BFS-based features on the subgraph limited to the 37,689 nodes with outward links.\\r\\n\\r\\nA Random Forest trained on this combination succeeded in getting a leaderboard AUC above 0.91.\\r\\n\\r\\nNext, I thought about how some edges in the graph may be more important than others. \\xa0If a node follows only a few other nodes, then it is more likely to pay attention to or follow the nodes that those nodes follow. \\xa0I constructed a feature to value each potential edge based on random walks on the graph, starting at the A node. \\xa0Allowing the random walks to traverse the edges in either direction substantially improved this feature, giving it an AUC above 0.90 alone.\\r\\n\\r\\nHowever, the random walk feature was computationally expensive to evaluate and optimize. \\xa0As a result, I used linear algebra to iteratively approximate the solution, and termed it edgeRank after I realized this was similar to Google\\'s PageRank. \\xa0I optimized this feature independently over two parameters: the probability of restarting at the A node during each step in the random walk, and the weights outbound edges were assigned relative to inbound edges. \\xa0By itself, this feature achieved around 0.93 on the leaderboard. \\xa0(After reading Kleinberg\\'s paper, a very similar feature is in there, termed a rooted PageRank).\\r\\n\\r\\nOne issue I ran into was that my predicted AUC scores from my private validation sets were much higher than my leaderboard validation scores. \\xa0As a result, I analyzed any differences between my validation sets and Dirk\\'s test set. \\xa0The primary difference was as follows:\\r\\n\\r\\n#A nodes, fan-out=1, test: 96 \\xa0, valid: ~374#B nodes, \\xa0 fan-in=1, test: 399, valid: ~2569\\r\\n\\r\\nIt ends up Dirk was not considering A nodes with a fan-out of 1 and B nodes with a fan-in of 1 for false edges, substantially altering the test distribution. \\xa0When I corrected for this, the test dataset distribution fell well within the conference intervals predicted by my private validation datasets on all the features I looked at. \\xa0Also, this likely meant that all 493 unique edges in the test set fulfilling this criteria (fan-out A == 1 or fan-in B <= 1) were true edges. \\xa0This information plus the edgeRank feature alone was enough to put my leaderboard AUC above 0.95, and a Random Forest trained on all the features put me in 2nd at the time, behind IND CCA. \\xa0(When the answers were released, I saw that 488 out of 493 of those edges were true edges, so my guess was only 99% right).\\r\\n\\r\\nA couple days before the end of the contest, I received a message from Will (who had just passed me on the leaderboard), seeing if I wanted to join him and Bo on a team to try beating IND CCA. \\xa0Given the magnitude of IND CCA\\'s lead, I decided to. \\xa0As Will has described, we pooled our features, bounced ideas off one another, shared validation sets, and then trained a number of Random Forests for our final submission.\\r\\n\\r\\nI\\'d also like to comment on the applicability of this contest to real-world problems. \\xa0In one sense, the manner the problem was presented made link-prediction almost trivially easy: it was a binary classification problem with balanced classes. \\xa0The true edges almost always belonged to nodes close together on the graph, while the false edges almost always belonged to nodes very far apart on the graph. \\xa0Thus, the AUC scores for this competition were very high. \\xa0In reality of course, there is a huge imbalance between the two classes.\\r\\n\\r\\nA different evaluation procedure could have been providing participants with a list of originating nodes from which one or more edges may have been removed, and then having participants submit a short, ordered list of potential destination nodes for each originating node. \\xa0Participants could be evaluated on the Mean Average Precision of their submitted lists, or a similar metric. This is more applicable to social networks recommending new friends, and it encourages methodologies that could analyze and rank large numbers of potential edges for each originating node in a computationally efficient manner (such as edgeRank).\\r\\n\\r\\nIn another sense, this link prediction problem was far more difficult than it would be from the standpoint of a social network. \\xa0Networks like Facebook have vast quantities of additional information on the types and timing of interactions between nodes (each of which forms an edge), in addition to the friendship graph itself. \\xa0It probably is easier to predict new edges based on features like \"just appeared in a picture together\" in combination with the feature types used in this contest.\\r\\n\\r\\nCongratulations to IND CCA, who won the contest with a very clever de-anonymization scheme coupled with a more standard machine-learning approach. \\xa0I considered trying to do something similar, but didn\\'t have enough time left in the contest to dive into the Flickr API. \\xa0Additionally, I thought that de-anonymizing the data would be, in many ways, a more difficult challenge (as they said, their de-anonymization procedure was not sufficient to win).\\r\\n\\r\\nIn one regard, it\\'s a relief to have been beaten by a team in this manner: they won by tackling a different problem, and my team still had the best machine learning methodology applied to the dataset in the competition (disregarding outside information).\\r\\n\\r\\nMany thanks to my teammates, Will and Bo - y\\'all were great to work with and we made a strong run at the end! \\xa0And many thanks to Anthony and Dirk for running a well-organized contest. \\xa0I know I learned a lot, and I believe many of my competitors did the same!\\r\\n\\r\\nImage credit: Dusk Eagle', 'I first saw kaggle.com in Nov 2010. I looked at the ongoing contests and found the IJCNN Social Network Challenge most interesting and decided to join, mostly because of its possible real-world application due to popularity of online social networks.\\r\\n\\r\\n\\r\\n\\r\\nI cut my teeth on collaborative filtering, prediction engine, etc on the Netflix Prize. At first I was motivated by the one million dollar prize (yay, lottery for geeks !), but found the whole technical aspect very interesting. In addition to the prize, the fact that it was a competition also served as a \"motivation engine\", of course this applies to kaggle.com too.\\r\\n\\r\\nAnyway, I tried some quick-and-dirty methods with various levels of success, and slowly got my score up to .830 or so. At this point I decided to create my own validation data sets. At first I just picked nodes at random and I got a validation set that was very easy to predict, with my validation scores having no bearing on my test scores.\\r\\n\\r\\nPreviously I have asked a question on the discussion forum about the data sets, and in the subsequent discussion, the contest organizer gave some info on the creation of the test data set. Based on that info, I wrote a second validation data set builder, and was able to build validation data sets whose scores were reasonably close to the test scores. So my question has rewarded me greatly, in a totally unexpected way, and I also learned everything I could about the test data set, or so I thought (more on this later).\\r\\n\\r\\nNext I tried some algorithms in earnest, adapting some of my Netflix prize code to this task. Due to limited amount of time available, I read a grand total of 3 research papers, and missed a number of well known algorithms in related fields, but as it turned out, it didn\\'t matter.\\r\\n\\r\\nBy late 2010/early 2011, I got my score to about .930, and making progress was becoming difficult. So I contacted William Cukierski about collaboration and eventual teaming up. William agreed and once we started talking, I was astounded by the breadth of his arsenal. It looked like he had implemented almost all implementable algorithms, given his hardware constraints. In fact, the only thing holding him back was that he had inadvertently created a bad validation data set. Once that was fixed his score shot up to .950 and kept improving.\\r\\n\\r\\nWilliam broke the node similarity problem down into 3 sub-problems:\\r\\n\\r\\n(1) How similar is node A to B?\\r\\n\\r\\n(2) How similar are the outbound neighbors of A to B?\\r\\n\\r\\n(3) How similar are the inbound neighbors of B to A?\\r\\n\\r\\nI improved by score to about .935 based on his input. William also had a random forest overlord that was vastly superior to my own blender.\\r\\n\\r\\nIn the last week of the contest, we contacted Benjamin Hamner about teaming up and Ben agreed. Not surprisingly, Ben had a number of strong predictors, and he shared with us a startling insight about the test data set. It alone boosted my score from .935 to .952 ! But in testimony to the strength of William\\'s random forest, in hardly made any difference there. In the end we fed almost everything into the forest and the final score was about .970.\\r\\n\\r\\nTechnical Details of My Methods\\r\\n\\r\\nUnsurprisingly, some of the methods I tried didn\\'t work, they include a neural network, a SVD/clustering-inspired method, and some node distance-based methods. Their scores were as high as .860, and by \"didn\\'t work\" I mean they didn\\'t contribute meaningfully to my blends. And fortunately, most of the methods that didn\\'t work run much slower than the ones that did.\\r\\n\\r\\nIn the end, I had 3 major parts in my \"solution\":\\r\\n\\r\\n1. A genetic algorithm blender. At first I used my linear regression blender from the Netflix prize. It was designed for RMSE scoring so of course it didn\\'t work well for AUC scoring. I could easily out-blend it by picking the weights manually. Then I decided to try my Netflix GA blender, which was useless for that task, but it worked well enough here. I was still going to do a logistic blender, until I ran into William\\'s random forest, and decided there was no point.\\r\\n\\r\\n2. KNN. There\\'re two keys to this:\\r\\n\\r\\n2A. The graph is very sparse, so to predict link between nodes A and B, you should go beyond 1st-level neighbors and use 2nd-level neighbors. I tried using 3rd-level neighbors too, but at that distance the neighborhood was getting too big.\\r\\n\\r\\n2B. Links should weight less as the link count goes up. Intuitively, imagine one million people following a celebrity, chances are most of them never met each other or the celebrity; on the other hand, if you have 30 contacts in your social network, chances are they\\'re your family and friends and many of them know each other.\\r\\n\\r\\nI tried a number of variations for KNN, for example following only inbound link and different link weighting functions. The best one scored 0.915 by itself, and was created by going to 2nd-level neighbors, but applying link weighting (1/sqrt(LinkCount+1)) only to 1st-level neighbors, and checking how outbound neighbors of A are similar to B (William\\'s 2nd point above).\\r\\n\\r\\n3. Global Link Distance (my own term here). To predict link from A to B, calculate the link distances from A to all other nodes, and the link distances from B to all other nodes. And the prediction is simply:\\r\\n\\r\\n1/SumOverAllNodesN( 2^( LinkDistance(A,N)+LinkDistance(B,N) ) )\\r\\n\\r\\nBy itself, GLD\\'s score was only about .850, but it blended well with KNN. I suspect it\\'s because they complement each other: KNN works on the local neighborhood while GLD works on the entire network.\\r\\n\\r\\nFor my best submission, I blended 4 variations of KNN and 2 variations of GLD. These methods are relatively simple, fast, and effective. They all took about 1 minute to run (2.66GHz CPU, single-threaded), and the blender usually came up with a good set of weights in 15 seconds. All together they could generate a .935 test submission in about 7 minutes.\\r\\n\\r\\nWhich brings me to Ben\\'s insight that boosted by score to .952: as an artifact of the test data set creation process, the link from A to B is almost guaranteed to be true if A has only 1 bound neighbor, or B has 0 or 1 inbound neighbor. Apparently a few other contestants on the leaderboard discovered it too. It\\'s extremely simple, fast, and effective.', \"Background\\r\\n\\r\\nI recently got my Bachelor degree from National Taiwan University (NTU). In NTU, I worked with Prof. Chih-Jen Lin's on large-scale optimization and meta-learning algorithms. Due to my background, I believe that good optimization techniques to solve convex model fast is an important key to achieve high accuracy in many application because we can don't have to worry too much about the models' performance and focusing on data itself.\\r\\n\\r\\n\\r\\n\\r\\nNoticing that the machine learning society lacks software on various kinds of algorithms that may be beneficial to our daily life, I am now developing pieces of tools that compile various state-of-the-art algorithms performing well in many data or contests. One of these pieces is lib-GUNDAM.\\r\\n\\r\\nMethods I tried \\r\\n\\r\\nI entered the competition on Jan 30, 2011. That time, I had a submission just trying to make sure whether my lib-GUNDAM performs well on data sets other than my experimental sets. It ended up that I got 10th in my first submission. I only expanded categorical features,\\r\\ntrained it with Linear-SVM and submit its decision values on the data set.\\r\\n\\r\\nThen, I used lib-GUNDAM to do data scaling and perform parameter search on linear-SVM using the training data set. Shortly, I rose to about 7th place.\\r\\n\\r\\nI somehow got stuck here and I decided to investigate other classifiers. I tried the tree bagger in Matlab, but I found it ran too low on the data. I thought I did not have time to wait for it. Somehow, I noticed that L2 logistic regression(LR) performs better than L1-SVM in most case on the data set. Thus, I switched to using it afterwards. Fortunately, my friend Hsiang-Fu Yu made a study months ago about training L2 LR in short time and incorporated it into LIBLINEAR. By using the software, I conducted parameter search very fast. (Search for positive instance penalty weight and negative instance penalty weight.)\\r\\n\\r\\nAlso I found that explicit 2-degree polynomial expansion (bi-gram expansion), worked well on the data. I began to used it before training on all data after wards,\\r\\n\\r\\nI rose to 4th place by using LR on best cross-validation parameters and using 2-degree polynomial expansion.\\r\\n\\r\\nThen, I had about two days left. I thought I did not have time to train other classifiers. Two ideas came to my mind:\\r\\n1. to boost the L2 LR or\\r\\n2. try to add more features.\\r\\n\\r\\nThus, I began to try random subspace methods and adaboost to boost the performance of L2 LR. Random subspace methods usually needs diverse learners, but I did not have fast learners, so used only liblinear, which might not be diverse enough. In my expectation, it did not work.\\r\\n\\r\\nAdaboost boosted my learners very trivially.\\r\\n\\r\\nIn time, I began to visit the forums to check whatever had been discussed. I noticed two things :\\r\\n1. training and testing data have overlap instances and\\r\\n2. the contest holder seemed to suggest contestants to investigate user-oriented features.\\r\\n\\r\\nTherefore, I hard-coded labels to those testing instances which occurred in the training data. Also, I decided to add features to users. Since we have five features\\r\\n\\r\\n* DependencyCount\\r\\n\\r\\n* SuggestionCount\\r\\n\\r\\n* ImportCount\\r\\n\\r\\n* ViewsIncluding\\r\\n\\r\\n* PackagesMaintaining for packages\\r\\n\\r\\nI added for each user five feature by averaging the individual features his packages own. For example, for DependencyCount, for user i, we add a feature userDependencyCount to the user by \\\\sum_{[i installed j]==1} DependencyCount_j / \\\\sum_{[i installed j]==1}. This yielded very good performance and rose me to the third place.\\r\\n\\r\\n8 hours before the contest ended I came up with the idea that recommendation system problems could be viewed as link prediction problems. Thus, I tried to add some link prediction features. Due to time constraint, I added only preferential attachment. Although it worked better on my cross validation, it did not work better than the previous model. I thought, I might be utilizing too much graph information. (I want to prediction if there is a link between node a and b, but I used the information regrading links between a and b. I think is a kind of\\r\\noverfitting.)\\r\\n\\r\\nThen, my submission quota was full, I did not do further experiments although I was to boost L2 LR again. I ended up in the 3rd place.\\r\\n\\r\\nTools I used\\r\\n\\r\\n1. lib-GUNDAM (http://www.csie.ntu.edu.tw/~b95028/software/lib-gundam/index.php) for feature preprocessing\\r\\n\\r\\n2. LIBLINEAR (www.csie.ntu.edu.tw/~cjlin/liblinear/) for training fast L2 LR\\r\\n\\r\\n3. LIBLINEAR with instance weight (http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances)\\r\\n\\r\\n4. for training adaboost because adaboost needs base learners able to take instance weight into account when learning\\r\\n\\r\\n5. hi-ensemble (http://www.csie.ntu.edu.tw/~b95028/software/hi-ensemble/index.php) for random subspace method\\r\\n\\r\\n6. adaboost\\r\\n\\r\\nFor algorithm details and experimental statistics, please refer to\\r\\nhttp://www.csie.ntu.edu.tw/~b95028/papers/2011kaggleR.pdf \", 'I (David Slate) am a computer scientist with over 48 years of programming experience and more than 25 years doing machine learning and predictive analytics.  Now that I am retired from full-time employment, I have endeavored to keep my skills sharp by participating in machine learning and data mining contests, usually with Peter Frey as team \"Old Dogs With New Tricks\".  Peter decided to sit this one out, so I went into it alone as \"One Old Dog\".\\r\\n\\r\\n\\r\\n\\r\\nFor this contest I used essentially the same core forecasting technology that I\\'ve employed in other contests: a home-grown variant of \"Ensemble Recursive Binary Partitioning\".  This is a robust algorithm that can handle large numbers of records and large numbers\\r\\nof feature (predictor) variables.  Both outcome and feature variables can be boolean (2 classes), categoric (multiple classes), or numeric (real numbers plus a missing value).  For the R contest the outcome was boolean, and the features provided in the training set were a mix of all three variable types.\\r\\n\\r\\nTo help tune modeling parameters and select feature variables, I relied both on a cross-validation procedure and also on feedback from the leaderboard.  For most of the cross-validation runs I partitioned the training data into 5 subsets of roughly equal population, trained a model on the data in 4 of the 5 subsets, tested it on the 5th to produce an AUC score, and then repeated this process 4 more times, rotating the subsets so that each subset got to play the role of test set once.  I then repeated this 5-fold procedure one more time, after scrambling the data to ensure a different partitioning, so as to produce 10 AUC scores altogether.  These were averaged together into a\\xa0composite score for the run.  I also computed a standard deviation and standard error of the mean for the 10 scores to get some idea of their statistical variability.  In the course of the competition I performed a total of 628 of these cross-validation runs.  By the time of my first submission on Dec 11, I had already done 115 of them.\\r\\n\\r\\nAlthough my tests involved a large number of feature variable selections and parameter settings, testing was not systematic enough to conclude that the winning model was in any way optimal.  There were too many moving parts for that.\\r\\n\\r\\nTo produce my first submission I used only the feature variables provided in the training set, but I enhanced the results in two ways. One was to exploit the fact that some records occurred in both the training and test sets, so that their forecasts could simply be copied from the training labels.  The other was to use the package dependency information in the depends.csv file from the supplementary archive johnmyleswhite-r_recommendation_system-36f8569.tar.gz, which, as suggested on the contest \"Data\" page, I downloaded from http://github.com/johnmyleswhite/r_recommendation_system.  For each record whose Package depended on a Package known to be not Installed\\xa0by this User, I produced the forecast 0, and for each record whose Package was depended on by a Package known to be Installed by this User, I produced the forecast 1.\\r\\n\\r\\nAlthough this first submission received the lowest final score (0.983419) of all my 55 submissions, it turned out that unbeknownst to me this would have been just sufficient to win the contest.\\r\\n\\r\\nIn the course of the contest I produced and tested a variety of additional variables, many of them based on other files in the github archive, such as imports.csv, suggests.csv, and views.csv.  I also made use of the one-line package descriptions on the \"Available Packages\" list at cran.r-project.org.  Finally, I created variables from the text in the package index pages acquired by downloading all the pages http://cran.r project.org/web/packages/PKGNAME/index.html, where PKGNAME stands for each package name.\\r\\n\\r\\nI failed to include in my final 5 selections the submission that received the highest final score, 0.988189.  But I did include my 2nd best (0.988157), and I\\'ll describe that submission in some detail. Note that both of these submissions were made the day before the contest ended.\\r\\n\\r\\nThe winning submission model utilized 43 features.  These included the 15 provided in the training file plus 28 synthesized feature\\r\\nvariables.  Although my model-building algorithm will naturally give greater weight to highly-predictive variables, it is also possible to assign an \"a priori\" weight to each variable, and I tried various values of these.  Here is a table of feature variable names, together with their types (B = boolean/binary, C = categoric/class, N = numeric), their assigned or default relative weights, and, in the case of each B or N variable, a crude indication of its utility in the form of its correlation coefficient with the outcome (Installed).  The\\r\\nfinal column contains a brief description of the variable.\\r\\n\\r\\nSeveral of the synthesized features involve some crude text analysis. In the description of those features, a \"word\" refers to a contiguous sequence of alphanumeric characters, and a \"name\" is an upper case letter followed by a contiguous sequence of alphanumeric characters.\\r\\n\\r\\n\\nVariable name\\xa0 Type\\xa0Weight\\xa0 Corr\\xa0 Description or source\\xa0\\nPackage             \\xa0 C\\xa0 0.50\\xa0\\xa0 Training file\\xa0\\nUser                \\xa0 C\\xa0 1.00\\xa0 \\xa0 Training file\\xa0\\nDependencyCount     \\xa0 N\\xa0 1.00\\xa0  0.0722\\xa0 Training file\\xa0\\nSuggestionCount     \\xa0 N\\xa0 1.00\\xa0  0.3856\\xa0 Training file\\xa0\\nImportCount         \\xa0 N\\xa0 1.00\\xa0  0.2849\\xa0 Training file\\xa0\\nViewsIncluding      \\xa0 N\\xa0 1.00\\xa0  0.1603\\xa0 Training file\\xa0\\nCorePackage         \\xa0 B\\xa0 1.00\\xa0  0.0538\\xa0 Training file\\xa0\\nRecommendedPackage  \\xa0 B\\xa0 1.00\\xa0  0.2858\\xa0 Training file\\xa0\\nMaintainer          \\xa0 C\\xa0 0.80\\xa0 \\xa0 Training file, but mapped to lower case and with non-alphnumerics mapped to \\'_\\'\\xa0\\nPackagesMaintaining \\xa0 N\\xa0 1.00\\xa0  0.1379\\xa0 Training file\\xa0\\nLogDependencyCount  \\xa0 N\\xa0 1.00\\xa0  0.4112\\xa0 Training file\\xa0\\nLogSuggestionCount  \\xa0 N\\xa0 1.00\\xa0  0.4526\\xa0 Training file\\xa0\\nLogImportCount      \\xa0 N\\xa0 1.00\\xa0  0.3464\\xa0 Training file\\xa0\\nLogViewsIncluding   \\xa0 N\\xa0 1.00\\xa0  0.1386\\xa0 Training file\\xa0\\nLogPackagesMaintaining\\xa0 N\\xa0 1.00\\xa0  0.1333\\xa0 Training file\\xa0\\nMaintainerName      \\xa0 C\\xa0 0.25\\xa0 \\xa0 Name extracted from Maintainer field\\xa0\\nMaintainerEmail     \\xa0 C\\xa0 0.25\\xa0 \\xa0 Email address extracted from Maintainer field\\xa0\\nCountSuggest2       \\xa0 N\\xa0 1.00\\xa0  0.4184\\xa0 Count of packages installed by User that suggest Package\\xa0\\nCountImport2        \\xa0 N\\xa0 1.00\\xa0  0.2291\\xa0 Count of packages installed by User that import Package\\xa0\\nComPkgDescWordCnt   \\xa0 N\\xa0 1.00\\xa0  0.3443\\xa0 Sum, over all distinct \"words\" of length >= 5 chars in 1-line description of Package, the count of packages installed by User whose 1-line descriptions also contain this \"word\"\\xa0\\nComPkgDescWordCntRat3\\xa0 N\\xa0 1.00\\xa0  0.1779\\xa0 Related to ComPkgDescWordCnt, but takes the ratio of sum of counts per installed package to sum per not installed\\xa0\\nComPkgNamSubMatCnt  \\xa0 N\\xa0 1.00\\xa0  0.1926\\xa0 Count of packages installed by User whose names, mapped to lower case, have at least 1 substring of length >= 5 in common with Package name\\xa0\\nComPkgNamSubMatFracRat2\\xa0 N\\xa0 1.00\\xa0  0.0369\\xa0 Related to ComPkgNamSubMatCnt, but takes the ratio of count of matching substrings per installed package to count per not installed\\xa0\\nComViewsCnt         \\xa0 N\\xa0 1.00\\xa0  0.3384\\xa0 Count of packages installed by User with a view in common with Package\\xa0\\nComViewsFracRat     \\xa0 N\\xa0 1.00\\xa0  0.1647\\xa0 Related to ComViewsCnt, but takes the ratio of count of installed packages to count of not installed\\xa0\\nComViewsCntAll      \\xa0 N\\xa0 1.00\\xa0  0.3384\\xa0 Same as ComViewsCnt due to a bug, but was supposed to be somewhat different\\xa0\\nComViewsFracRatAll  \\xa0 N\\xa0 1.00\\xa0  0.1647\\xa0 Same as ComViewsFracRat due to a bug, but was supposed to be somewhat different\\xa0\\nDependsOnCnt        \\xa0 N\\xa0 1.00\\xa0  0.1024\\xa0 Count of packages installed by User that Package depends on\\xa0\\nSuggestsOnCnt       \\xa0 N\\xa0 1.00\\xa0  0.2266\\xa0 Count of packages installed by User that Package suggests\\xa0\\nPubYear             \\xa0 N\\xa0 1.00\\xa0  0.0612\\xa0 Published year, including fraction, derived from \"Published:\" field in package index page\\xa0\\nComMaintCnt         \\xa0 N\\xa0 1.00\\xa0  0.3406\\xa0 Count of packages installed by User that have same MaintainerEmail address as Package\\xa0\\nPkgIdxTxtLen        \\xa0 N\\xa0 0.60\\xa0  0.2338\\xa0 Length in chars of package index page text\\xa0\\nPkgIdxTxtWordCnt    \\xa0 N\\xa0 0.40\\xa0  0.2506\\xa0 Count of distinct \"words\" of length >= 6 chars in Package index page text\\xa0\\nComPkgTxtWordCnt    \\xa0 N\\xa0 0.40\\xa0  0.5188\\xa0 Sum, over all distinct \"words\" of length >= 6 chars in index page text of Package, the count of packages installed by User whose index page texts also contain this \"word\"\\xa0\\nComPkgTxtWordCntRat \\xa0 N\\xa0 0.40\\xa0  0.0832\\xa0 Related to ComPkgTxtWordCnt, but takes the ratio of sum per installed packages to sum per not installed\\xa0\\nComPkgTxtWordCntFrac\\xa0 N\\xa0 0.40\\xa0  0.4597\\xa0 Ratio of ComPkgTxtWordCnt to PkgIdxTxtWordCnt\\xa0\\nPkgIdxTxtNameCnt    \\xa0 N\\xa0 0.40\\xa0  0.3219\\xa0 Count of distinct \"names\" of length >= 5 chars in Package index page text\\xa0\\nComPkgTxtNameCnt    \\xa0 N\\xa0 0.40\\xa0  0.5147\\xa0 Sum, over all distinct \"names\" of length >= 5 chars in index page text of Package, the count of packages installed by User whose index page texts also contain this \"name\"\\xa0\\nComPkgTxtNameCntRat \\xa0 N\\xa0 0.40\\xa0  0.0691\\xa0 Related to ComPkgTxtNameCnt, but takes the ratio of sum per installed packages to sum per not installed\\xa0\\nComPkgTxtNameCntFrac\\xa0 N\\xa0 0.40\\xa0  0.4571\\xa0 Ratio of ComPkgTxtNameCnt to PkgIdxTxtNameCnt\\xa0\\nDependsOnRecMis     \\xa0 N\\xa0 1.00\\xa0 -0.1335\\xa0 Count of packages not installed by User that Package depends on\\xa0\\nDependsByRecMis     \\xa0 N\\xa0 1.00\\xa0  0.1743\\xa0 Count of packages installed by User that depend on Package\\xa0\\nMaintainerNameOrEmail\\xa0 C\\xa0 0.25\\xa0 \\xa0 MaintainerName unless missing, in which case MaintainerEmail\\xa0\\n\\r\\n\\r\\nVarious other feature variables were tried, but for whatever reasons did not make the final cut.\\r\\n\\r\\nMy computing platform consisted of two workstations powered by multi-core Intel Xeon processors and running the Linux OS.  The core forecasting engine was written in C, but was controlled by a front-end program written in the scripting language Lua using LuaJIT (just-in- time Lua compiler version 2 Beta 5) for efficiency.', 'My background\\r\\n\\r\\nI graduated on Warsaw University of Technology with master thesis about text mining topic (intelligent web crawling methods). I work for Polish IT consulting company (Sollers Consulting), where I develop and design various insurance industry related stuff, (one of them is insurance fraud detection platform). From time to time I try to compete in data mining contests (Netflix, competitions on Kaggle and tunedit.org) - from my perspective it is a very good way to get real data mining experience.\\r\\n\\r\\n\\nWhat I tried\\r\\n\\r\\nAs far as I remember, the basis of the solution I defined at the very beginning: to create separate predictors for each individual loop and time interval. So my solution required me to build 61x10=610 regression models. I was playing with various regression algorithms, but quickly chose linear regression - because the results were good and the computation time was short. I think the key to get quite good result (especially on public RMSE :) ) was the set of attributes used. I used the following attributes for the linear regression for each individual loop&time interval:\\r\\n- number of minutes from 0:00 hours up to current moment (\"now\")\\r\\n- average drive time for given loop&interval\\r\\n- loop times for current moment and some number of historical moments\\r\\nbefore (the number of time points and the loop varied between the\\r\\nmethods)\\r\\n- differences between \"neighboring\" time moments for the above data:\\r\\njust differences or differences transformed with logistic function\\r\\n(1/1+e^-difference). Use of logistic function  gave a jump from public\\r\\nRMSE at about 198 to 189. The idea to use of sigmoid function here was\\r\\njust my intuition inspired by differences distribution.\\r\\n- \"saturations\" for for each loop (except the 2 first loops at both\\r\\ndirections ).\\r\\n\\r\\nI introduced the simple (and very naive) model of traffic growth:\\r\\nIf the speed at given loop is up to 40 km/h - the saturation is 1;\\r\\n\\r\\nIf the difference between the previous loop and the given loop is more than 5 km/h: it is assumed that this road part is partially saturated: there is segment that is moving at 30 km/h and second segment with the same speed as in the loop that is before given loop. The saturation is derived as the proportion of first segment to the whole road part. Each loop detector has its minimal value in RTAData file - after the regression this minimal value was used if predicted value was less than minimum.\\r\\n\\r\\nI did not use historical data at all - I found them useless during the initial tests (maybe too hastily). The only source of data for learning and testing was RTAData and lengths files (also no weekends, holidays, weather conditions).\\r\\n\\r\\nWhat ended up working\\r\\n\\r\\nFor each of 610 regression models the following 3 models were competing. Models were being trained with all data availabe in RTAData\\r\\nfile:\\r\\nModel 1: For all (61) loops: current + 5 times moments before and 5 simple differences - 675 attributes,\\r\\nModel 2: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 simple differences, saturations (for current time moment only) - 204 to 404 atrributes,\\r\\nModel 3: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 sigmoided differences,\\r\\nsaturations (for current time moment only) 204 to 404 atrributes,\\r\\nModel with least RMSE computed on the train file was selected for particular loop.  It is not a very good strategy, however I thought\\r\\nthat generally linear regression was resistant to overfitting (it is not true - as the number of variable grows, the more variance can be explained - this is what I have learnt).\\r\\n\\r\\nThis strategy gave me public RMSE 189.3\\r\\n\\r\\nI added also 4th model, that I just used for 15, 30 minutes predictions arbitrarily:\\r\\nModel 4: For all (61) loops: current + 5 times moments before and 5 sigmoided differences, saturations (for current time moment only) - 614 attributes. This turn gave mi 188.6 public result.\\r\\n\\r\\nWhat is interesting, the best private solution (however not selected by me since I relied to much on public results) was 190.819 (public 197.979) , it was just the model 3 described above combined with model 5 (model 5 was used for 15,30,45,60,90 minutes predictions arbitrarily, rest model 3):\\r\\nModel 5: like model 3 but also loop times are \"sigmoided\" not only differences.\\r\\n\\r\\nWhat tools I used\\r\\n\\r\\nMy solution is written as Java application with Weka linked as library (as always when I try to compete in data mining contests).  Since linear regression requires to solve matrix equation (in this case quite huge), the memory allocated by the program was becoming more and more important issue (3,5GB for one thread) - at the of the competition i was using computer with 4 processors and 12 GB of RAM - with 3 separate threads building and testing the models. The whole computation for my last attempts took about 48 hours of computations.', \"Because I have recently started employment with Kaggle, I am not eligible to win any prizes. Which means the prize-winner for this comp is Quan Sun (team 'student1')! Congratulations!\\r\\n\\r\\nMy approach to this competition was to first analyze the data in Excel pivottables. I looked for groups which had high or low application success rates. In this way, I found a large number of strong predictors - including by date (new years day is a strong predictor, as are applications processed on a Sunday), and for many fields a null value was highly predictive.\\r\\n\\r\\n\\r\\n\\r\\nI then used C# to normalize the data into Grants and Persons objects, and constructed a dataset for modeling including these features: CatCode, NumPerPerson, PersonId, NumOnDate, AnyHasPhd, Country, Dept, DayOfWeek, HasPhd, IsNY, Month, NoClass, NoSpons, RFCD, Role, SEO, Sponsor, ValueBand, HasID, AnyHasID, AnyHasSucc, HasSucc, People.Count, AStarPapers, APapers, BPapers, CPapers, Papers, MaxAStarPapers, MaxCPapers, MaxPapers, NumSucc, NumUnsucc, MinNumSucc, MinNumUnsucc, PctRFCD, PctSEO, MaxYearBirth, MinYearUni, YearBirth, YearUni .\\r\\n\\r\\nMost of these are fairly obvious as to what they mean. Field names starting with 'Any' are true if any person attached to the grant has that feature (e.g. 'AnyHasPhd'). For most fields I had one predictor that just looks at person 1 (e.g. 'APapers' is number of A papers from person 1), and one for the maximum of all people in the application (e.g. 'MaxAPapers').\\r\\n\\r\\nOnce I had created these features, I used a generalization of the random forest algorithm to build a model. I'll try to write some detail about how this algorithm works when I have more time, but really, the difference between it and a regular random forest is not that great.\\r\\n\\r\\nI pre-processed the data before running it through the model by grouping up small groups in categorical variables, and replacing continuous columns with null values with 2 columns (one containing a binary predictor that is true only where the continuous column is null, the other containing the original column, with nulls replaced by the median). Other than the Excel pivottables at the start, all the pre-processing and modelling was done in C#, using libraries I developed during this competition. I hope to document and release these libraries at some point - perhaps after tuning them in future comps.\", 'I participated in the R package recommendation engine competition on Kaggle for two reasons.  First, I use R a lot.  I cannot learn statistics without R.  This competition is my chance to give back to the community a R package recommendation engine.  Second, during my day job as an engineer behind a machine learning service in the cloud, product recommendation is one of the most popular applications our early adopters want to use the web service for.  This competition is my opportunity to stand in users\\' shoes and identify their pain points associated with building a recommendation system.\\r\\n\\r\\n\\r\\n\\r\\nI treat R package recommendation as a binary classification task: a classifier $latex f(u, p)$ takes as input a user $latex u$ and a package $latex p$, and predicts whether the user will install the package or not.  My final submission (team name: Record Me Men) combines four classifiers, labeled as large dots in Figure 1. The four classifiers share the same form of objective function that minimizes loss $latex L$ plus regularizers $latex R$:\\r\\n$latex J(\\\\theta) = \\\\sum_i L(y_i, f(u_i, p_i; \\\\theta)) + \\\\lambda R(\\\\theta)$\\r\\n, where $latex f(u, v)$ is a classification model, $latex \\\\theta$ is the parameters of the classification model, $latex y_i$, $latex u_i$, $latex p_i$ are the class label (package installed or not), user and package of the i-th training example, respectively. $latex \\\\lambda$ controls the penalty from the regularizing function, and is chosen using cross validation.\\r\\n\\r\\nModel 1: Baseline.  Model 1 is example_model_2.R that the competition organizer provides as a baseline.  In this model, a user is encoded as dummy variables $latex \\\\mathbf{u}$, and a package is represented by seven features $latex \\\\mathbf{p} = \\\\{p_1, \\\\dots, p_7\\\\}$ (e.g, the logarithmic count of dependency.  See the R script for the complete list).  The classification model is a linear combination of user dummy variables and package features with weight parameters, $latex f(u, p) = \\\\mathbf{\\\\theta_u}^T \\\\mathbf{u} + \\\\mathbf{\\\\theta_p}^T \\\\mathbf{p}$.  The parameters are $latex \\\\theta_u$ and $latex \\\\theta_p$ (and intercept).  The loss function $latex L$ is negative logistic log likelihood.  There is no regularizer in the model.\\r\\n\\r\\nThe first model establishes a strong baseline, achieving AUC of ~0.94, labeled as M1b2 in Figure 1.  A variant of this model that omits the user feature (see example_model_1.R, also provided by the contest organizers) achieves noticeable lower AUC of 0.81 (not shown in Figure 1).  This suggests that some users are more likely to install R packages than others.  In the next model, I will explore a classification model that incorporates not only user variations but also package variations.\\r\\n\\r\\nModel 2: Latent factor Models. In contrast to Model 1 with features derived from metadata, I focus on the user-package rating matrix.  Model 2 consists of two components: baseline estimates and latent factors.  Baseline estimates are linear combinations of three parts: global (one parameter $latex \\\\mu$), user (one parameter per user $latex \\\\mu_u$), and package (one parameter per package $latex \\\\mu_p$).  For latent factors, I assumes that there are K latent factors for each user, $latex \\\\mathbf{\\\\beta}_u$ and each package $latex \\\\mathbf{\\\\beta}_p$, and the inner product of these two factors captures the interaction between a user and a package.  The classifier model in Model 2 is $latex f(u, p) = \\\\mu + \\\\mu_u + \\\\mu_p + \\\\mathbf{\\\\beta}_u^T \\\\mathbf{\\\\beta}_p$.  This kind of latent factor model, also known as \"singular value decomposition\",  has been reported with great success in previous collaborative filtering studies and at the Netlifx Prize.   I choose an exponential loss function for Model 2,\\xa0$latex L(y, f(u, p)) = \\\\exp(- y f(u, p))$,\\r\\n\\r\\nwhere $latex y_i \\\\in \\\\{1, -1\\\\}$.  I choose exponential loss over squared loss because 1) exponential loss matches 0-1 loss better than squared loss 2) exponential loss is differentiable.  I apply L2 regularizers, $latex R(\\\\cdot) = ||\\\\mu_u||^2 + ||\\\\mu_p||^2 + ||\\\\beta_u||^2 + ||\\\\beta_p||^2$.  I minimize the objective function using stochastic gradient descent.  The number of latent factors, K, is chosen by cross validation.\\r\\n\\r\\nThe latent factor model works very well on the R package recommendation data, achieving AUC of ~0.97 (See the M2 family in Figure 1).  I plot the performance of five latent factor models with different Ks, ranging from 10 to 50, labeled as M2k10, M2k20, ..., M2K50 in Figure 1.  As K increases, the latent factor model becomes more expressive and fit the data better, resulting in higher AUC.\\r\\n\\r\\nModel 3: Package LDA topic.  In Model 3,  I explore new features not used in Model 1 and 2.  The new feature is a package\\'s topic based LDA (Latent Dirichlet Allocation).  The LDA topic of a package is inferred from the word counts of its man pages.  I use the topics.csv, kindly prepared by the contest organizers, to map a R package to one of the 25 LDA topics (See topic_models.R for details on running LDA, provided by the contest organizer).\\r\\n\\r\\nThe classification model of Model 3 is similar to Model 2.  Model 3 replaces user factors in Model 2 with T LDA factors weights $latex \\\\mathbf{t}_u$ (T=25 here), and replaces package factors in Model 3 with T dummy variables $latex \\\\mathbf{p}$.  The classification model is $latex f(u, v) = \\\\mu + \\\\mu_u + \\\\mu_p + \\\\mathbf{t}_u^T \\\\mathbf{p}$.  The loss function is the same as Model 2, and the regularizer is L2, $latex R(\\\\cdot) = ||\\\\mu_u||^2 + ||\\\\mu_p||^2 + ||\\\\mathbf{t}_u||^2 $.\\r\\n\\r\\nModel 3 achieves better AUC than Model 1, resulting in AUC of ~0.97 (labeled as M3u in Figure 1).  Prior to M3u, I explore a simpler model that users share the same weights $latex \\\\mathbf{t}$.  The parameter space becomes smaller, but Model 3 with shared parameters (labeled as M3b in Figure 1) performs slightly worse than Model 3 with user-specific parameters.  This makes sense because it is unlikely every R user shares the same interest in the same set of LDA topics.\\r\\n\\r\\nModel 4: Package task view.   R packages are organized into task views (e.g., high-performance computing,  survival analysis, and time series.  See the page for the complete list of views).  It is possible that a user interested in a particular task would install not just one but many, even all, packages in the same task view (e.g., using install.views() R function).  We could improve recommendation if we know how much a user is interested in a particular task view.  I use the views.csv, provided by the contest organizers, to map a package to its task view.\\r\\n\\r\\nThe classification model of Model 4 is similar to Model 3, except that LDA topics in Model 3  are replaced with task views (T=29, 28 task views plus 1 unknown view).  The performance of Model 4 is labeled as M4u in Figure 1.  Model 4 performs well and better than Model 1.  Similarly, I experiment with a variant of Model 4 that all users share the same task view parameters $latex \\\\mathbf{t}$, labeled as M4b i n Figure 1, and it performs worse than Model 4 with per-user parameters.  The finding is consistent with Model 3, and R users, at least on the R recommendation data set, seem to have different preferences in task views.\\r\\n\\r\\n[caption id=\"attachment_79\" align=\"aligncenter\" width=\"442\" caption=\"Figure 1.  The performance of individual models on the training set.  The x-axis is selected ten models in four model families, and the y-axis is the pooled AUC over 10-fold cross validation on the training set (the higher, the better).  The larger dots are those chosen for ensemble learning.\"]\\r\\n[/caption]\\r\\n\\r\\nEnsemble learning.  I combine four classifiers using logistic regression.  To collect out-of-sample training data for the ensemble learner, I divide the training data into three sets: 80%, 10%, and 10%.  I train individual classifier on the 80%.  I then apply the trained classifiers on the 20%, and combine the scores from individual classifiers as training data for the ensemble learner. Both individual classifiers and the combiner are evaluated on the last 10% data set (as those shown in Figure 1 and Figure 2).  After evaluating one fold, I shift data and conduct another folds until all data are used, resulting in a total of 10 ensemble learners.  The output of these 10 ensemble learners are averaged to produce final predictions.\\r\\n\\r\\nThe ensemble learning works very well, as shown in Figure 2.  Combining M1 and M2 achieves AUC of 0.9721, which is higher than either M1 0.94 or M2 0.9702 alone.  When I combine more models and include M3 and M4, the performance gets even better.  The submission of combining four models achieves best performance on the training set among all models.  On the test set the final model achieves AUC of 0.983289 (after post-processing, see below), the highest among my submissions.  The success of ensemble training is possibly because the individual models are strong performers, and models are diverse with different classification models and features such that they complement each other.\\r\\n \\r\\nFigure 2. The performance of ensemble learning models. The x-axis is three ensembles, ranging from two to four individual models. The y-axis is the pooled AUC of 10-fold cross validation on the training set.  The dash lines are the performance of four individual models. \\nPost-processing.  I apply two post-processing steps before submitting each entry.  First, the label and (user, package) association on the training set is memorized.  For any (user, package) pairs in the test set that are already seen in the training set,  their labels on the training set are recalled and used instead.  Second, I assume when a user install a package P, the user also installs the packages that P depends on.  I record all packages a user installs on the training set as well as their dependent packages.  Given a pair (U, P) on the test set, if the package P is found in the set of the packages on which the user U\\' installed packages depend, I ignore the prediction and output 1.0 instead.  Although the dependency assumption does not completely hold true (there are known contradictory examples on the training set),  the two filters combined increases absolute AUC ~0.004, which matters in a competition with razor-thin margins between leading submissions.\\r\\n\\r\\nI develop the classification training programs for Model 2, 3, and 4 in Python.  I use R to explore data, run logistic regression (glm() in the stats library), calculate AUC (performance() in the ROCR library), and plot results (ggplot() in the ggplot2 library).  All programs are developed and run on a commodity PC with dual-core 2GHz CPU and 4G RAM.  I make the source codes available at github.\\r\\n\\r\\nAlthough my submissions are based on only the data the contest organizers provide and simple linear models, by no means you should stop here.  There are many directions worth exploring, for examples, crawling CRAN to analyze the rich graphs of depends, imports, suggests, and enhances between R packages.   In an open-end contest like this, the sky is the limit.\\r\\n\\r\\nMax Lin is a software engineer with Google Research in New York City office, and the tech lead of the Google Prediction API.  Prior to Google, he published research work in video content analysis, sentiment analysis, machine learning, and cross-lingual information retrieval.  He has a PhD in Computer Science from Carnegie Mellon University.\\nHe would like to thank the organizers of the R package recommendation engine competition for their hard work and efforts in putting this competition together.  He participates the competition as individuals, and writes all codes in his spare time.  Nothing expressed here may or should be attached to his employer.', \"My background\\r\\n\\r\\nI'm a PhD student of the Machine Learning Group in the University of Waikato, Hamilton, New Zealand. I’m also a part-time software developer for 11ants analytics. My PhD research focuses on meta-learning and the full model selection problem. In 2009 and 2010, I participated the UCSD/FICO data mining contests.\\r\\n\\r\\n\\nWhat I tried and What ended up working\\r\\n\\r\\nI tried many different algorithms (mainly weka and matlab implementations) and feature sets in nearly 80 submissions. This report will briefly introduce two approaches that worked for this competition. Each of them will be discussed sequentially in the order of submissions.\\r\\n\\r\\nAfter the first 10 testing submissions, I realised that there was a concept drift happening between 2007 and 2008. The success rates decline gradually from 2007. Also, on the information page of the contest, it states that “In Australia, success rates have fallen to 20-25 per cent…”. To me, this probably means, the decision rules for grant applications were somehow changed during 2007 and 2008. Here are some consequences that I could think of, including but not limited to:\\r\\n\\nThe overall success rates will continue to drop\\nSuccessful applications in 2005/2006 would be declined in 2007/2008, so for 2009/2010\\nSuccess patterns becoming to be “more” random\\nDecision rules for year 2009/2010 will be close to that for 2007/2008, compared with rules for year 2006 and prior.\\n\\r\\nBased on the information and assumptions above, I decided to mainly use data points from 2007 and 2008 for training my classifiers, which turns out to be a reasonable choice.\\r\\n\\r\\nApproach A: Ensemble Selection with transformed feature set (used in the first 20 submissions)\\nData engineering/transformation part\\n\\n\\n\\nOriginal attribute\\nTransformation method\\n\\n\\nStart.date\\nto numeric, year, month,   day in numbers\\n\\n\\nRFCD.Code.X (X=1 to 5)\\nto nominal\\n\\n\\nPerson.ID.X (X=1 to 15)\\nto nominal\\n\\n\\nNumber.of.Grant.X (X=1 to   15)\\nTotal number of successful/unsuccessful   grants per application\\n\\n\\nPublications AA, A, B, C\\nTotal number of AA, A, B, C   publications per application\\n\\n\\nRole.X\\nTotal number\\xa0 of CHIEF_INVESTIGATORs,   PRINCIPAL_SUPERVISORs, DELEGATED_RESEARCHER, EXT_CHIEF_INVESTIGATORs per   application\\n\\n\\nCountry.of.Birth.X\\nTotal number of   Asia_Pacific born, Australia, Great_Britain, Western_Europe, Eastern_Europe,   North_America, New_Zealand, Middle_East_and_Africa per application\\n\\n\\nWith.PHD\\nTotal number of PhDs per application\\n\\n\\nYears.IN.UNI\\nTotal number of people who   has been in the University for more than 5 years\\n\\n\\n\\r\\nAfter all those transformations are done, I also had a java program to transform all nominal attributes to its corresponding frequency. The frequency counting is based on all the available data points. So, the final feature set consists of the original features, transformed features and frequency.\\r\\n\\r\\nModeling part\\r\\n\\r\\nMy main method is called Ensemble Selection, originally proposed by Rich Caruana and co-authors of Cornell University (http://portal.acm.org/citation.cfm?id=1015432). The following pseudocode demonstrates the basic idea of Ensemble Selection:\\r\\n\\r\\n0. Split the data into two parts: The build set and the hillclimb set\\r\\n\\r\\n1. Start with the empty ensemble.\\r\\n\\r\\n2. Add to the ensemble the model (trained on “build” set) in the library that maximizes the ensemble’s performance to the error metric (AUC for this contest) on a “hillclimb” (validation) set.\\r\\n\\r\\n3. Repeat Step 2 for a ﬁxed number of iterations or until all the models have been used.\\r\\n\\r\\n4. Return the ensemble from the nested set of ensembles that has maximum performance on the hillclimb (validation) set.\\r\\n\\r\\nModel library used for my Ensemble Selection system:\\r\\n\\r\\nAdaBoost, LogitBoost, RealAdaBoost, DecisionTable, RotationForest, BayesNet, NaiveBayes, 7 algorithms with different parameters, in total 28 base classifiers.\\r\\n\\r\\nBuilding set and hillclimb set for Ensemble Selection:\\r\\n\\r\\nData points from year 2007 are used as the “build set”\\r\\n\\r\\nData points from year 2008 are used as the “hillclimb set”\\r\\n\\r\\nOr\\r\\n\\r\\nData points from year 2007/01/01 to 2008/04/30 are used as the “build set”\\r\\n\\r\\nData points after year 2008/04/30 are used as the “hillclimb set”\\r\\n\\r\\nBoth setups worked well for the Ensemble Selection approach.\\r\\n\\r\\nIn summary, the final system for Approach A consists of three main components:\\r\\n\\r\\nData points from 2007 for training and 2008 for hillclimbing.\\r\\n\\r\\nEnsemble Selection, num of bags: 10, hillclimb iterations = size of the model library.\\r\\n\\r\\nIn total 352 features.\\r\\n\\r\\nLearderboard AUC: 0.956X, Best final test set AUC: 0.961X\\r\\n\\r\\nFrom submission 20 to the end of the competition, the following features are added to Approach A feature set:\\r\\n\\r\\nNumber of missing values\\r\\n\\r\\nNumber of non-missing values\\r\\n\\r\\nMissing value rate\\r\\n\\r\\nTransform “Contract.Value.Band” to numeric values\\r\\n\\r\\nAverage contract value\\r\\n\\r\\nRFCD.CODE mean, sum, max, min, standard deviation per application based\\r\\n\\r\\nRFCD.PCT mean, sum, max, min, std per application based\\r\\n\\r\\nSEO.CODE mean, sum, max, min, std per application based\\r\\n\\r\\nSEO.PCT mean, sum, max, min, std per application based\\r\\n\\r\\nSuccessful.grant mean, sum, max, min, std per application based\\r\\n\\r\\nUnsuccessful.grant mean, sum, max, min, std per application based\\r\\n\\r\\nSuccessful.grant mean average per application based\\r\\n\\r\\nSuccessful.grant sum average per application based\\r\\n\\r\\nAll the above features for the first three applicants\\r\\n\\r\\nAll the above features for Unsuccessful.grant\\r\\n\\r\\nSuccess rate of applicant 1, applicant 2, and applicant 3 per application based\\r\\n\\r\\nSuccess rate of all applicants per application based\\r\\n\\r\\nMean, max, std success rates of all applicants per application based\\r\\n\\r\\nNumber of publications mean, sum, max, min, std per application based\\r\\n\\r\\nExcept the frequency counting described in Approach A, only “row-based (per-application-based)” statistical features were gradually introduced to my system during the competition, because I thought that, compared with “time based/column based features”, “row-based” statistical features would reduce the chance of overfitting.\\r\\n\\r\\nAlso, the following algorithms (with different/diverse parameter settings) were gradually added to the model library while the competition:\\r\\n\\r\\nRandomForest\\r\\n\\r\\nRacedIncrementalLogitBoost\\r\\n\\r\\nBagging with trees\\r\\n\\r\\nADTree\\r\\n\\r\\nLinear Regression\\r\\n\\r\\nRandomCommittee with Random Trees\\r\\n\\r\\nDagging\\r\\n\\r\\nJ48\\r\\n\\r\\nApproach B: Rotation Forest with the feature set from Approach A\\n \\r\\n\\r\\nI tried using only Rotation forest (http://www.computer.org/portal/web/csdl/doi/10.1109/TPAMI.2006.211) with the following setup:\\r\\n\\r\\nBase classifier: M5P model tree (weka default is J48)\\r\\n\\r\\nRotation method: Random Projection with Gaussian distribution (weka default is PCA)\\r\\n\\r\\nThe Rotation forest classifier was trained on data points from 2007 and 2008 with the feature set from Approach A. Here are the results:\\r\\n\\r\\nLeaderboard AUC: 0.947X, Final test set AUC: 0.962X\\r\\n\\r\\nAveraging the two approaches could improve the final test set AUC to 0.963X.\\r\\n\\r\\n \\n \\nWhat tools I used\\n \\r\\n\\r\\nSoftware/Tools used for modelling and data analysis:\\r\\n\\r\\nWeka 3.7.1 is used for modelling (with my own improved version of the Ensemble Selection algorithm)\\r\\n\\r\\nMatlab and SAS are used for data visualization and statistical analysis\\r\\n\\r\\nJava is used as the main programming language for this project\\r\\n\\r\\nMost experiments were done on my home PC: AMD 6-core, 16G ram on Windows system.\", 'Background \\r\\n\\r\\nI am Yuanchen He, a senior engineer in McAfee lab. I have been working on large data analysis and classification modeling for network security problems.\\r\\n\\r\\nMethod\\r\\n\\r\\nMany thanks to Kaggle for setting up this competition. And congratulations to the winners! I enjoyed it and learned a lot from working on this challenging data and reading the winners\\' posts. \\xa0I am sorry I didn\\'t find free time last week to write this report.\\r\\n\\r\\n\\r\\n\\r\\nThe data came with a lot of categorical features with a high number of values. At the very beginning, I removed useless features (by weka.filters.unsupervised.attribute.RemoveUseless -M 99.0) and removed the features with almost 100% missing values. After that, I tried to transform the categorical features into a group of binary features with each is a yes or no on a specific value. I also generated 4 quarter features and 12 month features from startdate and generated binary indicator features for missing values. The binary features, date-based features, indicator features, as well as other numerical features, after simply filling missing values with mean, were fed into R randomForest classifier for RFE. With that I got 94.9x on the leaderboard. I kept tuning along this way but the accuracy cannot be improved further. Then I started to suspect there were some information loss during the process of feature transformation and feature selection.\\r\\n\\r\\nSo I tried to build classifiers directly on the categorical features without transforming them into binary features. A simple frequency based pre-filtering was applied. For a raw categorical feature, all values presented less than 10 instances in the data were combined into a specific common value \"-1\". However, R randomForest cannot accept a categorical feature with more than 32 values. So I had to split each categorical feature again into \"sub features\", with each has no more than 32 values. The way I split the values into different sub features was sorting the values with information gain first, and then top 31 values were assigned into sub feature 1, the next 31 values were assigned into sub feature 2, and so on. With this feature transformation strategy I got 94.6x on the leaderboard.\\r\\n\\r\\nThe next one I tried was simply combining the top features from the above two methods. The randomForest classifiers on the combined feature sets can improve the leaderboard ROC to 95.1x-95.3x, depending on the instances used for training. The best classifiers were generated from training only on instances after 0606, only on instances after 0612, and only on instances after 0706. Finally, I observed the prediction results from these classifiers were different enough and hence it was worth to make a major voting from them, and I got my best leaderboard AUC 95.555, which generalized to the other 75% test instances with the final AUC 96.1051', 'My background\\r\\n\\r\\nMy name is Junpei Komiyama. I obtain a Master\\'s degree in computational and statistical physics at The University of Tokyo, Japan. I have been working in a team developing a live-streaming website (http://live.nicovideo.jp) for two years, contributing mainly to designing and implementation of DB tables, cache structures, and front-end programs of the site.\\r\\n\\r\\n\\nWhat I tried\\r\\n\\r\\nEach team received three sets of data: Training data, testing data and submission example. The training and the test data are the records of sequential observations of car drivers. The observation data consisted of eight physiological data (P1-P8), eleven environmental data (E1-E11) and eleven vehicular data (V1-V11), recorded in every 100ms. Information about meaning of each data was not given. The training data consisted of 500 car drivers\\' records of observation data associated with judgments as to whether each driver was alert or not. The test data set consisted of another 100 drivers\\' records of observation and had no overlap with the training data. We were requested to estimate whether each driver in the test data set was alert or not. Each team was evaluated by the value of“area under the receiver-operating-characteristics curve (AUC)\". All data in the training set were used for machine training. To solve this problem, I constructed a Support Vector Machine (SVM), which is one of the best tools for classification and regression analysis, using the libSVM package.\\r\\n\\r\\nIn my first attempt, I simply put the training data into libSVM with default settings (C-SVC and RBF kernel) and evaluated the test data with the trained model. This approach took more than 3 hours to complete the training step and yielded a modest AUC score (0.745). Meanwhile, observations were visualized for each person using python/gnuplot. I found some data (P3-P6) were characterized by strong noise, fluctuating more than 50 percent for every 100ms. Also, many environmental and vehicular data showed discrete values continuously increased and decreased. These suggested the necessity of pre-processing the observation data before SVM analysis for better performance.\\r\\n\\r\\nTo improve the AUC score, I took the following approaches:\\r\\n\\r\\n1.To smoothen the observation data by removing extremely small temporal values. Sometimes observation values dropped to nearly zero and then immediately resumed to the former level, presumably due to failure of observation. Therefore, I attempted to remove such instantaneous near-zero values. However, this attempt rather caused slight reduction in the AUC score and so was abandoned.\\r\\n\\r\\n2.To integrate the present and old time points in the observation data. The observation records were taken at every 100 ms (milliseconds), which is fairly more frequent than many events of human activities. I assumed that changes in the alert state might have a certain length of delay after an observation was made. I tested this possibility by averaging each observation datum point with an old datum point collected 100-500 ms before. However, such an attempt of data integration did not improve the AUC score.\\r\\n\\r\\n3.To average several datum points. I found that this simple process not only improved the AUC score and also significantly reduced computing time. Therefore, this approach was taken for further optimization. Since there was the two-submissions-per-day limit for evaluation, it was necessary to examine changes in performance in a local setting through cross-validation of the SVM training data set.\\r\\n\\r\\nWhat ended up working\\r\\n\\r\\nPre-processing before SVM: I attempted to determine the optimal number of datum points to be averaged. Averaging 100 rows into a single row resulted in too coarse data whereas averaging only 3 rows had little effect. I empirically determined that averaging 7 consecutive datum points provided the optimal result, reducing the SVM training time by 86% and increasing the AUC by approximately 0.01.\\r\\n\\r\\nSVM: Among the available options offered by libSVM (i.e., C-SVC, nu-SVC, one-class SVM, epsilon-SVR and nu-SVR), the epsilon-SVR performed best for this problem, enhancing the AUC than C-SVC/option -b by about 0.05. Linear, polynomial, RBF and sigmoid kernels were then tested. I found that kernel choice had relatively minor effects although RBF kernel performed best.\\r\\n\\r\\nWith these optimized SVM type and kernel function, I optimized the SVM parameters namely, cost parameter c, kernel parameter g, and loss function parameter p.\\r\\n\\r\\nIn the final setting, epsilon-SVR, RBF kernel with optimized parameters (c=2, g=1/30 and p=0.1) yielded a greatly improved AUC (0.839).\\r\\n\\r\\nTools I used\\r\\n\\r\\nPython for processing the CSV data.\\r\\nGnuplot for visualizing the observation data.\\r\\nLibSVM for constructing a SVM model, including processes of training and prediction.', 'I was Team “Sali Mali” which won the seasonal part of the online tourism forecasting competition. The aim was to produce the smallest MASE for the 427 quarterly time series and 366 monthly timeseries. In this article, I brieﬂy describe the methods used.\\r\\n\\r\\n\\nBasic algorithms\\r\\n\\r\\nNo new time series forecasting algorithms were developed speciﬁcally for this contest. We basically took algorithms that already existed as ‘building blocks’ and combined their forecasts in speciﬁc ways. Based on Athanasopoulos et al. (2011), and using the R package ‘forecast’ (Hyndman, 2011), four base algorithms were used:\\r\\n\\r\\n1. \\xa0Seasonal Naïve;\\r\\n\\r\\n2. Damped Holt-Winters\\r\\n\\r\\n3. ARIMA;\\r\\n\\r\\n4. ETS.The benchmark to beat was the ETS algorithm.\\r\\n\\r\\nThe benchmark forecasts were replicated and visually inspected and it became clear that the forecasts for some of the series (only one or two) were clearly ‘unlikely’, in the sense that they basically went oﬀ the scale. The other algorithm sseemed to give much more realistic forecasts for these particular series.\\r\\n\\r\\nThe approach then taken was to concentrate on protecting against these ‘disastrous’ forecasts. The mindset was not thinking how to improve the overall accuracy, but how to prevent the worst case events. One way of achieving this is by not putting all your eggs in one basket (i.e. only relying ona single algorithm). This technique is commonly known as ‘ensembling’\\r\\n\\r\\nAn ensemble approach\\r\\n\\r\\nTwo methods were used in the ensembling process:\\r\\n\\r\\n1. \\xa0The last 12 months of the training data was set aside as a holdout set, and the MASE acrossall the series was calculated for each algorithm on this set. Based on these MASE values, aweighting was assigned to each algorithm, with the total of the weightings summing to 1.\\xa0Thus four predictions were made for each series, with the ﬁnal prediction being a weighted av-erage. The weights for each algorithm where consistent across each series within the monthlyand quarterly series types. This global weighted average method gave an improvement overthe baseline method.\\r\\n\\r\\n2. Forecasts for three algorithms (Damped, ARIMA, ETS) were generated using four diﬀerentsized training windows. The Seasonal Naïve forecast was then added, to give 13 forecasts foreach point in each series. The ﬁnal forecast for each point was then the median value of these 13 individual forecasts.\\xa0This local selection method also gave an improvement over the baseline method.\\r\\n\\r\\nThe ﬁnal solution was then a weighted average of methods 1) & 2).\\r\\n\\r\\nThe cheat factor\\r\\n\\r\\nIf the ﬁrst 12 months of the benchmark forecast is simply replicated for the second 12 months, rather than relying on the algorithms forecasts for months 13-24, then the overall benchmark accuracy was improved. This is in line with the organisers’ ﬁndings that as the time horizon increases, the gain in accuracy of certain algorithms over the seasonal naïve method was diminished. The leaderboard was used to determine the ‘year two growth factor’ that should be applied. It was found that just multiplying the ﬁrst year’s predictions by approximately 1.04 gave the best second year predictions, as determined by the leaderboard score. In other words, the forecasts for months 13–24 obtained from the algorithm were completely disregarded and replaced by a simple multiple of the ﬁrst year’s forecasts.\\r\\n\\r\\nComments on the competition\\r\\n\\r\\nThe prediction dates for the time series are more than likely to be the last two years available, meaning that the physical dates are the same for many series (this hypothesis is based on our ﬁnding that a growth rate of 1.04 seemed to work across all series). Because tourism ﬁgures are likely to be aﬀected by global factors (GFC, exchange rates, etc., that are largely unpredictable) it is likely the values will all trend similarly. Thus the conclusion that one algorithm is better for tourism data than another algorithm must be treated with caution as it might just be the algorithm that fortunately got the trend correct for that particular moment in time. \\xa0It is suggested that more generalizable results may have been obtained if the series were deliberately staggered in time.\\r\\n\\r\\nIt seems common practice to report time series on a calendar month basis. \\xa0In reality people operate on a weekly cycle, not a monthly cycle. This causes problems in series that have signiﬁcant daily ﬂuctuations — for example car hire volumes can be very diﬀerent during the week than the weekend. The implication for forecasting is that a big diﬀerence between one month and the next, or the same month in the previous year, can be due to the diﬀerence in having four weekends in a month and ﬁve weekends in a month. Reporting tourism ﬁgures in a four weekly cycle rather than a monthly cycle would lead to improvements in forecast accuracy, and this may give better reward for eﬀort than additional algorithm development.', \"The “Stay Alert!” competition from Ford \\xa0challenged competitors to predict whether a car driver was not alert based on various measured features.\\r\\n\\r\\nThe \\xa0training \\xa0data \\xa0was \\xa0broken \\xa0into \\xa0500 \\xa0trials, \\xa0each \\xa0trial \\xa0consisted \\xa0of a \\xa0sequence \\xa0of \\xa0approximately \\xa01200 \\xa0measurements \\xa0spaced \\xa0by \\xa00.1 \\xa0seconds. Each measurement consisted of 30 features; \\xa0these features were presentedin three sets: \\xa0physiological (P1...P8), environmental (E1...E11) and vehic-ular (V1...V11). \\xa0 Each feature was presented as a real number. \\xa0 For each measurement we were also told whether the driver was alert or not at thattime (a boolean label called IsAlert). \\xa0No more information on the features was available.\\r\\n\\r\\n\\r\\n\\r\\nThe test data consisted of 100 similar trials but with the IsAlert label hidden. 30% of this set was used for the leaderboard during the competition and 70% was reserved for the ﬁnal leaderboard. \\xa0Competitors were invited to submit a real number prediction for each hidden IsAlert label. \\xa0This realprediction should be convertible to a boolean decision by comparison with a threshold.\\r\\n\\r\\nThe accuracy assessment criteria used was “area under the curve” (AUC). \\xa0 The “curve” is the receiver-operating characteristic (ROC) curve where \\xa0the \\xa0true-positive \\xa0rate \\xa0is \\xa0plotted \\xa0against \\xa0false-positive \\xa0rate \\xa0as \\xa0this threshold is varied. \\xa0An AUC value will typically vary between 0.5 (random guessing) and 1 (perfect prediction).\\r\\n\\r\\nSee the full explanation of Inference's method in the attached PDF.\", '\\r\\n\\r\\nWe are Team Irazú, José P. González-Brenes and Matías Cortés. We finished 1st in the RTA  Challenge.\\r\\n\\r\\nWhat didn’t  work\\r\\n\\r\\nWe started off exploring the data by calculating means for  different combinations of time, day of week and month. We plotted these means to  identify patterns in the data. We also explored forecasting based on linear regressions.\\r\\n\\r\\n\\r\\n\\r\\nWe realized that including historical data led to less  accurate predictions, so we attempted using weighted linear regressions where  proportionally higher weights were assigned to more recent datapoints.\\r\\n\\r\\nWe also tried forecasting through autoregressions, where the  preceding n observations (t-1, t-2, …, t-n) are used to make a prediction.\\r\\n\\r\\nIn the end, we decided to work only with the more recent  dataset, and ignore the historical data provided by Kaggle.\\r\\n\\r\\nWhat ended up  working\\r\\n\\r\\nWe used a statistical technique called Ensemble of Decision  Trees (often called Random Forest™).\\xa0 One  of the nice properties of this approach is that it doesn’t assume a linear  relationship between explanatory variables and the predicted outcome.\\r\\n\\r\\nWe used the following explanatory variables: \\n\\nWhat time is it? (Hour + (Minute/60))\\nWhat is the date? (Month + (Day/31))\\nWhat day of the week is it? (Monday, Tuesday…)\\n\\n“All roads are equal,  but some roads are more equal than others!” \\r\\n\\r\\nOur final algorithm combines two methods. Both methods  include time, date, and day of week as explanatory variables.\\r\\n\\r\\n \\nMethod A:\\r\\n\\r\\nAdditionally encodes most recent observation for 2 neighboring routes\\r\\n\\r\\nMethod B:\\r\\n\\r\\nAdditionally encodes most recent observation for 4 neighboring routes and recent trend in travel time\\r\\n\\n\\n\\n\\xa0\\r\\n\\n\\nWe tried Method A and Method B, and discovered that for some  route segments, Method A performed better, while for others, Method B performed  better!\\xa0\\r\\n\\r\\n\\xa0\\r\\n\\r\\nOur final algorithm uses:\\r\\n\\r\\nMethod A for route segments 40105-41160\\r\\n\\r\\nMethod B for route segments 40010-40100 \\n\\n\\n\\r\\n\\xa0\\r\\n\\r\\nWe prepared a PDF providing more details about Random Forests  and our solution. Click here to read  it!\\r\\n\\r\\n', 'Background\\r\\n\\r\\nMy name is Mick Wagner and I worked by myself on this challenge in my free time.\\xa0 I am fairly new to data mining but have been working in Business Intelligence the last 5 years.\\xa0 I am a senior consultant in the Data Management and Business Intelligence practice at Logic20/20 in Seattle, WA.\\xa0 My undergrad degree is in Industrial Engineering with an emphasis on Operations Research and Management Science out of Montana State University.\\xa0 \\xa0This is my second Kaggle competition I have entered.\\r\\n\\r\\n\\r\\n\\r\\nThe Stay Alert challenge was sponsored by Ford to help prevent distracted drivers.\\xa0 The objective of this challenge is to design a detector/classifier that will detect whether the driver is alert or not alert, employing any combination of vehicular, environmental and driver physiological data that are acquired while driving.\\r\\n\\r\\nThe data for this challenge shows the results of a number of \"trials\", each one representing about 2 minutes of sequential data that are recorded every 100 ms during a driving session on the road or in a driving simulator.\\xa0 The trials are samples from some 100 drivers of both genders, and of different ages and ethnic backgrounds.\\xa0 The actual names and measurement units of the physiological, environmental and vehicular data were not disclosed in this challenge. Models which use fewer physiological variables are of particular interest; therefore competitors are encouraged to consider models which require fewer of these variables.\\r\\n\\r\\nTools Used\\r\\n\\r\\nMicrosoft Excel\\r\\n\\r\\nMicrosoft SQL Server Stack (SQL Server Engine, SQL Server Integration Services, SQL Server Analysis Services)\\r\\n\\r\\nData Analysis\\r\\n\\r\\nI spent the majority of my time analyzing the data.\\xa0 I inputted the data into Excel and started examining the data taking note of discrete and continuous values, category based parameters, and simple statistics (mean, median, variance, coefficient of variance).\\xa0 I also looked for extreme outliers.\\r\\n\\r\\nI read through the Kaggle discussion board to see if anything about the challenge had been changed or if Ford provided additional insight into the data.\\r\\n\\r\\nThe most important step I made was to take a step back and holistically examine the problem and the dataset.\\xa0 The data was composed of various trials with different human experimenters.\\xa0 My background in statistical quality control immediately told me that this would have a large impact on the statistical analysis and needed to be factored into the design of the system.\\xa0 To do this, I created my own sets of test and training data.\\xa0 Normally, Microsoft SQL Server randomly derives these sets based off of a HoldOutMaxPercent value that dictates the Test data size.\\xa0 I made the first 150 trials (~30%) be my test data and the remainder be my training dataset (~70%).\\xa0 This single factor had the largest impact on the accuracy of my final model.\\r\\n\\r\\nMy next breakthrough decision was limiting the copious amount of data that went into my algorithm.\\xa0 I was concerned that using the entire data set would create too much noise and lead to inaccuracies in the model.\\xa0 The final goal of the system is to detect the change in the driver from alert to not alert so that the car can self-correct or alert the driver.\\xa0 So I decided to just focus on the data at the initial moment when the driver lost alertness.\\xa0 This reduced my dataset significantly and I repeated my initial Excel analysis of the data.\\xa0 I highlighted which factors consistently had a relatively large delta between status changes.\\r\\n\\r\\nFailed Attempts\\r\\n\\r\\nMy first few attempts were running different types of models with the all the variables and Microsoft’s recommended variables.\\xa0 I used a lift chart to compare their accuracy to each other.\\xa0 My goal was to narrow down the possible algorithms given by Microsoft SQL Server Analysis Services from seven down to 2.\\xa0 Several models (Association Rules, Linear Regression, and Logistic Regression) did not make sense to use because of the data types, structure of the data, and desired binary output.\\xa0 The Decision Tree and Neural Network scored the highest on my lift chart.\\r\\n\\r\\nWhat Ended Up Working\\r\\n\\r\\nAfter testing the Decision Tree and Neural Network algorithms against each other and submitting models to Kaggle, I found the Neural Network model to be more accurate.\\r\\n\\r\\nAfter initially trying the variables recommended by SSAS (SQL Server Analysis Services), I augmented the solution with the variables I found key in my initial analysis.\\xa0 These variables included: E4, E5, E6, E7, E8, E9, E10, P6, V4, V6, V10, and V11.\\r\\n\\r\\nAs recommended by Ford, I tried to avoid relying on physiological variables.\\xa0 I did find P6 to be helpful though.\\r\\n\\r\\nThe key strengths of my model are ease to build, ease to deploy and maintain in an industry setting with transactional data, and scalability within SQL Server.\\xa0 My model uses an out of the box algorithm that is well understood and respected.\\xa0 It also took me significantly less attempts (and time) to scale to the top of the standings:\\r\\n\\r\\n1st place: 24 entries\\r\\n\\r\\n2nd place (me): 8 entries\\r\\n\\r\\n3rd place: 39 entries\\r\\n\\r\\n4th place: 25 entries\\r\\n\\r\\n\\xa0', 'At the core of our method was a system called oriented Basic Image Feature columns (oBIF columns). This system has shown good results in several character recognition tasks, but this was the first time we had tested it on author identification. As we are a computational vision group, our focus was on the visual features rather than on the machine learning, and we used a simple Nearest Neighbour classifier for our experiments and entries.\\n\\noBIF Columns\\nThe description of oBIFs begins with Basic Image Features (BIFs). In this system every location in an image is assigned to one of seven classes according to local symmetry type, which can be dark line on light, light line on dark, dark rotational, light rotational, slop, saddle-like or flat. The class is calculated from the output of six Derivative-of-Gaussian filters. The algorithm takes two parameters. First, the scale parameters, which determines the size of the filters and second, a threshold that determines the likelihood that a location will de classified as flat.\\xa0An extension to the BIF system is to include local orientation, to produce oriented Basic Image Features (oBIFs). The local orientation that can be assigned depends on the local symmetry type. For the slope type, a directed orientation (arrow like) makes sense, whereas for the line and saddle-like types an undirected orientation (double-ended arrow) applies. The rotational and flat types have no orientation assigned.\\xa0There is an extra parameter associated with oBIFs, that determines the level of orientation quantization. In previous work we have found that a set of 23 oBIFs is sufficient for most applications. Examples of an oBIF encoded image are shown in the figure, with the original section of image at the top, the oBIF encoding at a fine scale and the oBIF encoding at a coarser scale. The colours indicate symmetry type and the lines indicate orientation.\\n\\n\\xa0\\nIn order to increase the discriminative power of the system, we then combine oBIFs at different scales to produce the oBIF column features. Typically only two scales are required to produce a sharp increase in discriminative power, whereby the set of features is increased to 232. There is an extra parameter associated with oBIF columns, which is the ratio between the scales used.\\xa0Our basic approach to describing images using oBIFs, is to count the occurrences of each of the 232 types of feature and discard the locations, hence our descriptor is a 232-bin histogram. Previous experience has shown that we generally get the best results using rooted-normalized histograms( i.e. we start with histograms whose values add up to one, and square root each of the values) but for clarity we will simply refer to these as histograms in the remainder.\\xa0Applying\\nApplying oBIF Columns to Author Identification\\nIn previous work on character recognition, we have used histograms of oBIF columns directly with a Nearest Neighbour classifier. This works in general because the encoding for a particular letter ‘A’, in oBIF column space, is close to that for other ‘A’s. However, author identification is clearly a different problem as the aim is to spot differences in the style of the text, rather than recognise the content of the text. Thus, for the case of the ‘A’, the aim is to recognise a particular deviation from the average ‘A’ rather than recognise that it is an ‘A’.\\xa0Therefore after calculating oBIF column histograms for all the images, we calculated the deviations from the mean histograms for training. These were then used to build a Nearest Neighbour classifier, with the Euclidean Distance as the metric.\\nParameter Tuning\\nWe had four parameters to tune, the scale, the ratio between the scales, the threshold and the orientation quantization. This was done using the training set and the optimal values were found to be very similar as we’ve used for other applications.\\nIdentifying Missing Authors\\nA difficult aspect of the competition problem was the possibility that some of the images in the test set had authors that were not in the training set. In order to tackle this problem we first looked at the distribution of the test images in oBIF column histogram space to see if any were especially close (at least 3 standard deviations below the mean distance.) If none were (as was the case), the conclusion would be that no two test images were written by the same author. Then each test image was assigned to its nearest training image in oBIF column space. This process left three test images unclassified.\\nOur Entries\\nWe made three entries.\\nEntry 1\\nFor the first entry we were unaware that one of the columns (for author 040) had to be removed prior to submission so we got a large error.\\nEntry 2\\nFor the second entry we assigned the three unclassified images as UNKNOWN, which produced an error on the public leaderboard equivalent to misclassifying one image.\\nEntry 3\\nFor the third entry, we looked at the training authors that had not been assigned to any of the test images. We then looked at the next nearest neighbours for the three unclassified images to see whether any of the unassigned train authors featured there. If they did, they would be assigned to that test image. As it turned out, one of the unassigned training authors was the 2nd nearest neighbour of one of the unclassified test images, and so this single classification was changed. This gave us a public error of 0 and, as it turned out, both entries two and three got a final error of 0.\\n', 'My name is Tim Salimans and I am a PhD candidate in Econometrics at Erasmus University Rotterdam. For my job I constantly work with data, models, and algorithms, and the Kaggle competitions seemed like a fun way of using these skills in a competitive and social environment. The Deloitte/FIDE Chess Rating Challenge was the first Kaggle contest I entered and I was very fortunate to end up taking first place. During the same period I also used Kaggle-in-class to host a prediction contest for an undergraduate course in Econometrics for which I was the teaching assistant. Both proved to be a lot of fun. This post is a nontechnical account of my experiences in the chess rating challenge. For more details, including my code, see my web page.\\r\\n\\r\\n\\nChess rating systems\\r\\n\\r\\nThe first thing to do when tackling a new problem is to look up what other people have done before you. Since Kaggle had already organized an earlier chess rating competition, the blog posts of the winners were a logical place to start. After reading those posts and some of the academic literature, I found that chess rating systems usually work by assuming that each player’s characteristics can be described by a single rating number. The predicted result for a match between two players is then taken to be some function of the difference between their ratings. Yannis Sismanis, the winner of the first competition, used a logistic curve for this purpose and estimated the rating numbers by minimizing a regularized version of the model fit. Jeremy Howard, the runner-up, instead used the TrueSkill model, which uses a Gaussian cumulative density function and estimates the ratings using approximate Bayesian inference.\\r\\n\\r\\nI decided to start out with the TrueSkill model and to extend it by shrinking each player’s rating to that of their recent opponents, similar to what Yannis Sismanis had done in the first competition. In addition, I introduced weights into the algorithm which allowed me to put most emphasis on the matches that were played most recently. After some initial experimentation using the excellent Infer.NET package, I programmed everything in Matlab.\\r\\n\\r\\nUsing the match schedule\\r\\n\\r\\nThe predictions of my base model scored very well on the leaderboard of the competition, but they were not yet good enough to put me in first place. It was at this time that I realized that the match schedule itself contained useful information for predicting the results, something that had already been noticed by some of the other competitors. In chess, most tournaments are organized using to the Swiss system, in which in each round players are paired with other players that have achieved a comparable performance in earlier rounds. If in a Swiss system tournament player A has encountered better opponents than player B, this most likely means that player A has won a larger percentage of his/her matches in that tournament.\\r\\n\\r\\nIn order to incorporate the information present in the match schedule, I generated out-of-sample predictions for the last 1.5 years of the data using a rolling 3-month prediction window. I then performed two post-processing steps using these predictions and the realized match outcomes. The first step used standard logistic regression and the second step used a locally weighted variant of logistic regression. The most important variables used in the post-processing procedure were:\\r\\n\\n- the predictions of the base model\\n- the ratings of the players\\n- the number of matches played by each player\\n- the ratings of the opponents encountered by each player\\n- the variation in the quality of the opponents encountered\\n- the average predicted win percentage over all matches in the same month for each player\\n- the predictions of a random forest using these variables\\n\\r\\n\\xa0\\r\\n\\r\\nThis post-processing dramatically improved my score and put me well ahead of the competition for some time. Later, other competitors made similar improvements and the final weeks of the competition were very exciting. After a long weekend away towards the end of the competition I came back to find that I had been surpassed on the leaderboard by team PlanetThanet. By tweaking my approach I was able to crawl back up during the next few days, after which I had to leave for a conference in the USA. Upon arrival I learned that I was again surpassed, now by Shang Tsung. Only by making my last submissions from my hotel room in St. Louis was I finally able to secure first place.\\r\\n\\r\\nConclusions\\r\\n\\r\\nMuch of the contest came down to how to use the information in the match schedule. Although interesting in its own right, this was less than ideal for the original goal of finding a good rating system. To my relief, the follow-up data set that Jeff Sonas made available showed that my model also makes good predictions without using this information. Finally, I would like to thank both the organizers and the competitors for a great competition!', \"Sergey Yurgenson finished second the Mapping Dark Matter challenge and agreed to answer a few 'How I Did It' questions for our first post in the Mapping Dark Matter series. \\xa0Over the next few weeks we will be posting regular interviews with more of the top competitors.\\nWhat was your background prior to entering Mapping Dark Matter?\\r\\nI have a PhD in Physics from Leningrad (now St.Petersburg) State University, Russia. \\xa0I work at Harvard University developing software and hardware for neurobiology research and data analysis.\\r\\nMy first attempt at a data mining competition was the Netflix Prize. \\xa0I learned about it somewhere in the middle of the competition and spent several weeks building my model. \\xa0I managed to just barely beat the Netflix benchmark and realized that it required more time and hardware power than I was able to dedicate at the time.\\r\\n\\r\\nFortunately, many Kaggle competitions have a more manageable scope and can be done as a hobby rather than full time job.\\xa0 Mapping Dark Matter was my third Kaggle competition; before that I came second in the RTA competition and made one submission in the Chess Rating challenge.\\r\\n\\r\\nWhat was your most important insight into the dataset?\\r\\nInitially, I was trying to use a modified quadruple moments formula and fitting procedure. \\xa0However, I was not satisfied with the result.\\xa0 My mind was constantly coming back to neural networks. \\xa0The only question was ‘what kind of parameters to use as inputs’? \\xa0The number of those parameters should be reasonable, and they need to describe images well. \\xa0Thus, the images’ principal components looked like the logical choice. \\xa0I calculated the positions of galaxies and stars and re-centered all the images, creating image stacks for galaxies and stars separately. \\xa0I then calculated the principle components for those stacks.\\r\\n\\r\\nTo my surprise, many of the principle components were easy to understand. \\xa0Here are components #2 and #3 and a scatter plot where x and y are amplitudes of components #2 and #3 and color corresponds to galaxy orientations:\\r\\n\\r\\n[gallery]\\r\\n\\r\\nComponents #2 and #3 are quadruples with shifted phase and definitely reflect the orientation of elongated galaxies.\\xa0 Many other components were also easy to interpret. \\xa0I used some of them to improve the center of object calculation. \\xa0The amplitudes of principle components for galaxies and stars served as inputs to the neural network. \\xa0I played with the network parameters until I found a good combination of the number of parameters and the network configuration. \\xa0In the end, I combined the results of multiple networks to calculate my final submissions.\\r\\n\\r\\nWhich tools did you use?\\r\\nAll calculations were done using Matlab.\\r\\n\\r\\nThank you Sergey!\", \"Daniel Margala and David Kirkby (as team DeepZot) placed first in the Mapping Dark Matter challenge. \\xa0Daniel agreed to answer a few questions for No Free Hunch as part of our series of posts on the best Mapping Dark Matter entries. \\xa0\\nWhat was your background prior to entering Mapping Dark Matter?\\r\\n\\r\\nI graduated from the University of California, Los Angeles in 2009 with a B.S. in Physics. In the course of my studies at UCLA, I learned Linux system administration and various scripting languages managing a cluster of servers and data archive for an astro-particle research group. I became interested in numerical analysis while investigating the polarity and momentum of muons produced in the atmosphere by incident cosmic rays. Currently, I am a PhD student in the Physics and Astronomy Department at the University of California, Irvine. My advisor (and DeepZot team member), Prof. David Kirkby, and I are using the Baryon Oscillation Spectroscopic Survey (BOSS) to study the distribution of matter in our universe at the largest volumes. My work with BOSS has primarily focused on the operations software at the telescope, specifically, with the interfaces to the BOSS spectrograph, located at Apache Point Observatory in New Mexico.\\r\\n\\r\\nHow did you come to form a team together?\\r\\n\\r\\nI became interesting in working with David during a conversation that included an avid discussion of programming languages at a department event (where I was lured by the prospect of free food and drink, the perfect bait for graduate students). I began working on a variety of projects with David for about half a year, ranging from cosmology to electrical engineering, before we started working on the related GREAT10 challenge.\\r\\n\\r\\nWhat made you decide to enter?\\r\\n\\r\\nThe GREAT10 challenge was a perfect opportunity for me to bring my freshly developed proficiency in numerical analysis to bear. As a student looking to gain experience, this was also a chance to contribute to the forefront of analysis techniques employed by the weak lensing community. The comparatively compact size and similarity between data sets made participating in the MDM challenge very attractive. The ellipticity measurement (galaxy shape) in the MDM competition was a critical step in our GREAT10 analysis, where the goal is to disentangle the shear (due to dark matter) from the intrinsic galaxy shape.\\r\\n\\r\\nWhat was your most important insight into the dataset?\\r\\n\\r\\nThe most important insight was that the pixel-level residuals are a powerful tool for finding the best-fit models for galaxy and star images. We were able to assess the quality of various image models and parameters studying distributions of residuals across sets of images. This was essential to our method, which consisted of a pixel-level maximum-likelihood fit to each star and galaxy image.\\r\\n\\r\\nThe images below demonstrate our fit for a single galaxy image. From left to right, we see the original image (zoomed in on the center), our fitted model with the same resolution, and a higher resolution version of the fitted model:\\r\\n\\r\\n[gallery]\\r\\n\\r\\nWere you surprised by any of your insights?\\r\\n\\r\\nWe used an artificial neural network to improve the ellipticity measurements from the fitting procedure. The neural network was trained to provide corrections using a non-obvious subset of the of the likelihood output. For example, feeding the centroid position for a galaxy image to the neural network worsened the predicted correction. Without the neural network, our best entry would have ranked 8th.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nOur code consists of C++ libraries that we developed to perform the following tasks: generate images by convolving galaxy and point spread function models, access GREAT10 and MDM images, carry-out maximum-likelihood fit of images, do KSB measurement (moment inspired technique commonly used by astrophysicists) of ellipticity, and provide an ellipticity correction via machine learning algorithms.\\r\\n\\r\\nThe code utilizes a fit minimization engine (Minuit) and neural network engine (TMVA) that are both available as part of the open source (LGPL) ROOT data analysis framework (http://root.cern.ch), which is widely used by particle physicists.\\r\\n\\r\\nWhat have you taken away from the competition?\\r\\n\\r\\nI am intrigued by the tight cluster of scores near the top of the leaderboard despite the wide variety of methods applied. This suggests that those methods are making use of the maximum amount of information available in the images in the presence of pixelation and noise.\\r\\n\\r\\nWinning the MDM competition gave us confidence in our strategy for measuring the shapes of galaxies, which we have put to use in GREAT10.\\r\\n\\r\\nMost importantly, why 'DeepZot'?\\r\\n\\r\\nThe name DeepZot was formed by merging 'Deep Thought', the name of a fictional computer from Douglas Adams’ Hitchhiker’s Guide to the Galaxy, and 'Zot!', the battle cry of our school’s mascot, Peter the Anteater.\\r\\n\\r\\nCongratulations to Daniel and David!\", 'For the last of our series of interviews with the top Mapping Dark Matter competitors, Ali Hassaïne and Eu Jin Lok pulled out the stops to give some generous insight into their techniques.\\nWhat was your background prior to entering Mapping Dark Matter?\\nAli: I graduated from Center of Mathematical morphology / Mines-ParisTech in 2009 with a PhD in morphological image processing. I worked within my thesis (under the supervision of Etienne Decencière and Bernard Besserer) on the restoration of optical soundtracks of old movies. I am currently a postdoctoral researcher at Qatar University working on writer identification and signature verification with Somaya Al-Ma\\'adeed. Earlier this year, I hosted the ICDAR2011 Arabic Writer Identification Contest on Kaggle.\\r\\nEu Jin: I have a masters degree in Econometrics and am currently an analyst at Deloitte Australia, working in data analytics. I\\'ve been with Kaggle for almost a year and have competed in many competitions, one of them being the writer identification contest hosted by Ali [Kaggle note - Eu Jin came in the top 5 in that contest as well!].\\r\\n\\nHow did you come to form a team together?\\nAli: I contacted Eu Jin because he showed very interesting ways of combining the features we provided in the writer identification contest. These same features have also been used in this contest along with many others.\\r\\nWhat was your most important insight into the dataset?\\nAli: I combined several methods, some are inspired from my PhD thesis on soundtrack restoration. Other methods are inspired from my current research on writer identification and signature verification. I also tried some methods which were specifically developed for this problem. I will briefly describe here the methods I liked the most:\\r\\nMethods inspired from soundtrack restoration (the morphological approach)\\r\\nFor an introduction to mathematical morphology, Wikipedia\\'s page is a good starting point. This instructional video about optical soundtracks might also be of interest.\\r\\nThe soundtrack (the part of the film stock which contains the audio information) is sometimes badly exposed due to light diffusion during the copying process.\\r\\n\\r\\nFigure 1 Location of soundtrack on film stockThe effect of bad exposure is clearly visible on the peaks and valleys of optical soundtracks.\\r\\n\\r\\nFigure 2 (a) overexposureFigure 2 (b) normal exposureFigure 2 (c) underexposure.The effect of under-exposure is visually very similar to a morphological dilation with a certain structuring element. Given the physical process that causes under-exposure, it can be safely assumed that this structuring element is a disk. Therefore, the under-exposure might be restored by applying the dual morphological operator which is a morphological erosion with the same structuring element. Similarly, the over-exposure is very similar to a morphological erosion and might be corrected by applying a dilation [1] (references below).\\r\\nIn the same way, it can be assumed that the effect of lensing (or more generally, the application of a PSF) is very similar to a morphological operation with a certain structuring element. The structuring element is no longer a disk, however its shape can be computed from the provided star images as explained in following scheme.\\r\\n\\r\\n[caption id=\"attachment_1231\" align=\"aligncenter\" width=\"362\" caption=\"Figure 3 Computing basic morphological operations from galaxy and star images\"][/caption]\\r\\n\\r\\nThe quadrupole moments are then computed from all the resulting images and used as predictors.\\r\\n\\r\\nMethods inspired from signature verification and writer identification\\r\\nSeveral geometrical features previously used in signature verification and writer identification happen also to be very powerful predictors for computing the ellipticity. These features are also computed on the thresholded galaxy and star images. The following figure illustrates some of them.\\r\\n\\r\\n[caption id=\"attachment_1252\" align=\"aligncenter\" width=\"300\" caption=\"Figure 4 Geometrical features\"][/caption]\\r\\n\\r\\nAll these predictors, along with many others, have been combined using a linear fit. The strength of this method lies in the large number of diverse predictors it combines. For those of you who might be interested in trying other models, I’ve put all the data together here.\\r\\nWhat tools did you use?\\r\\nThe code is written in C++ with a strong dependency on the excellent OpenCV library. I will hopefully make the code available soon.\\r\\nThanks and congratulations to Ali and Eu Jin!\\nReferences:\\r\\n[1] J. Taquet, B. Besserer, A. Hassaïne and E. Decencière, Detection and correction of under/overexposed optical soundtracks by coupling image and audio signal processing, EURASIP Journal on Advances in Signal Processing, Vol. 2008.\\r\\n[2] S. Al-Ma’adeed, E. Mohammed and D. Al Kassis, Writer identi?cation using edge-based directional probability distribution features for Arabic words, International Conference on Computer Systems and Applications (AICCSA), pp. 582-590, 2008.\\r\\n[3] A. Hassaïne, V. Eglin and S. Bres, Une méthode de compression sans perte pour les images de documents basée sur la séparation en couches', \"Keith T. Herring placed second in the Wikipedia Participation Challenge with just three entries on the board and agreed to talk to Kaggle about his process. Read on for the first in a great series of interviews with the top competitors from the Wikipedia challenge.\\nWhat was your background prior to entering the Wikipedia Participation Challenge?\\r\\nI have a computer science degree from my home state, University of Illinois Urbana-Champaign (UIUC)... I then headed to Boston to get a Masters and Doctorate from MIT in Electrical Engineering and Computer Science. I now reside and work in the Queen City (official Seattle nickname from 1869 to 1982, ref: Wikipedia). I’ve been fortunate to have been allowed to get my hands dirty in Robotics, Wireless Communications, Remote Sensing, Machine Learning, Network Security, Financial Markets, Casino Arbitrage, and the prediction of infinitesimally small subsets of the future universe, although I don’t feel obligated to call myself a futurist or appear on the Discovery Channel as such.\\r\\n\\nWhy did you decide to enter the Challenge?\\r\\nTwo Reasons.\\r\\nFirst, I have a lot of respect for what Wikipedia has done for the accessibility of information. Any small contribution I can make to that cause is in my opinion time well spent.\\r\\nSecond, a new data set is to me what a new sheet of bubble wrap is to Larry David. I can’t wait to dive in and pop all the bubbles/bits of information I can find! So this is where I give props to Kaggle: they’ve done a great job building on the success and excitement of the Netflix Prize. It's a win-win for data enthusiasts like myself and organizations like Wikipedia that have a lot of data and questions.\\r\\n\\r\\nWhich tools did you use?\\r\\nMy strategy was as follows:\\r\\n\\nCompile a representative training set of Wikipedia Editing behavior. An interesting feature of this competition was that it involved a public data set. I wrote web scrapers to extract the editing history of approximately 1 million Wikipedia editors.\\nTransform the raw edit data into a representative feature set for prediction of future editing volume. My final predictor operated on 206 features derived from editor attributes such as: edit timing, edit volume, name-space contributions, article concentration, article creation, edit automation, commenting behavior, etc.\\nLearn a diverse set of future edit predictors. Each model/predictor I considered was a randomly constructed decision tree. I made use of an implementation (ref: Abhishek Jaiantilal and Andy Liaw) of Breiman and Cutler’s Random Forest algorithm for constructing the individual random decision trees. Further diversity was achieved by randomizing the random forest parameters for each individual tree rather than using a single optimized set of parameter values for all trees. Randomized parameters included: feature set cardinality per decision node (weak vs strong learner), in-to-out-of-bag ratio, stop- ping conditions (under vs over fitting).\\nMy final future edits predictor was formed as an optimized ensemble of the models in (3). I wrote an iterative quadratic optimizer for approximating the optimal model weighting using the out-of-bag samples, which varied across candidate models/trees. Out of the approximately 3000 models generated, 34 informative non-redundant models were retained in the final optimized ensemble.\\n\\r\\n\\r\\nI used the following tools/setup to implement this strategy: Ubuntu Linux, Python, MySQL, C++, and Matlab.\\r\\n\\r\\nWhat was your most important insight into the dataset?\\r\\nA randomly selected Wikipedia editor that has been active in the past year has approximately an 85 percent probability of being inactive (no new edits) in the next 5 months. The most informative features (wrt the features I considered) captured both the edit timing and volume of an editor. More specifically the exponentially weighted edit volume of a user (edit weight decreases exponentially with increased time between the edit and the end of the observation period) with a half-life of 80 days provided the most predictive capability among the 206 features included in the model.\\r\\nOther attributes of the edit history, such as uniqueness of articles, article creation, comment behavior, etc. provided some additional useful information, although roughly an order of magnitude or less than the edit timing and volume when measured as global impact across the full non-conditioned editor universe.\\r\\nAn opportunity for future analysis would be to consider data relevant to any community political dynamics that may exist. Specifically edit reversion behavior and associated attributes.\\r\\n\\r\\nThanks Keith, and congratulations on a fantastic performance!\", \"Long-time Kaggle competitor D'yakonov Alexander won the dunnhumby Shopper Challenge ahead of 537 other entrants who submitted a grand total of 2029 entries.  In addition to releasing his code and a description of his method, D'yakonov agreed to answer some background questions for us:\\nWhat was your background prior to the dunnhumby Shopper Challenge?\\r\\nI received a PhD in Mathematics from Moscow State University, Russia (my supervisor was academician Yuri Ivanovich Zhuravlev), where I now work. I like different contests and this is my fifth Kaggle competition.\\r\\n\\nWhat approaches did you try, and what worked best?\\r\\nAt first I tried to use simple heuristics to understand the 'logic of the problem'. My main idea was to split the problem into two parts: the date prediction and the dollar spend prediction. For the first task I initially thought to use probability theory. But I soon found out that it was useless to predict the date if we couldn't predict the spend. Therefore I calculated not only probabilities of visits, but also the stability of users’ behavior (see Alexander's detailed description of his winning method). For that task, I used a kernel density (Parzen) estimator. But it was necessary to take account of the fact that 'fresh' data is more useful than 'old' data, so I used a weighted Parzen scheme to give greater weight to more recent data points. Then I hung parameters on my heuristics and performed my optimization.\\r\\n\\r\\nWhat tools did you use?\\r\\nI use MATLAB for all my Kaggle entries, just the basic M-language without any libraries.\\r\\n\\r\\nCongratulations D'yakonov Alexander on a fantastic result.  D'yakonov's winning method description and code are available to download from his website: \\nClick here for his method description \\nClick here for his winning code\\r\\n(startsolution2.m to run)\\r\\n\\r\\n\", \"William Cukierski finished fourth in the dunnhumby Shopper Challenge, backing up his previous second-place finish in the IJCNN Social Network Challenge (you can read his write-up of that challenge here). At the time of writing, Will has just WON/FINISHED SECOND in the Semi-Supervised Feature Learning.\\nWhat was your background prior to entering the dunnhumby Shopper Challenge?\\r\\n\\r\\nI studied physics at Cornell and am now finishing a PhD in biomedical engineering at Rutgers. During my day job, I look at ways to apply machine learning and data mining to cancer diagnosis in pathology images. During my night job (once my fiancée has gone to bed and the coast is clear) I fire up Matlab and trade hours of sleep for a chance at Kaggle glory.\\r\\n\\nWhat made you decide to enter?\\r\\n\\r\\nSimple data sets always catch my eye. I like to be able to get right down to the analytics, without spending hours poring over data dictionaries and mucking with strings and categorical variables and all that business. I do enough of that in my day job!\\r\\n\\r\\nWhat was your most important insight into the dataset?\\r\\n\\r\\nLike forecasting weather, this was a very “local” prediction contest. Most customers returned to the store quite soon, meaning that forecasting the date/spend too far out provided vastly diminishing returns. There was no Easter holiday to account for (which would have been a tricky detail had the competition been a year earlier). I polled friends for ideas on what might make people go shopping. Despite some good leads (one suggested that people might shop after getting a paycheck on the 1st or 15th of the month), I couldn't find any such macro trends that affected the short prediction time frame. It may seem obvious that an individual's decision to go shopping has very little to do with how much the store has taken in that day, or how many others have gone that day, but one can never ignore these possibilities in a data mining competition where fractions of a percent matter.\\r\\n\\r\\nDue to the nature of the scoring method (namely, that you had to get the date exactly right for the spend estimate to even matter), I focused almost entirely on predicting the date. I extracted features on the historical date information alone (ignoring how much was spent and ignoring the training data after April 1st, 2011). There were strong weekday patterns, which meant many of the features worked best when computed “modulo 7”.\\r\\n\\r\\nThere are three generic classes of features one can consider in a problem like this:\\r\\n\\r\\n1. User features: prior number of visits, mode time since visit, PCA, SVDs, etc.\\r\\n2. User-Date features: prior probability of visiting on a given weekday, days since visiting, empirical probability distribution of having x days pass without a visit, etc.\\r\\n3. Date features: ignored... global information not very important!\\r\\n\\r\\nI performed logistic regression to obtain a probability of the customer visiting on each day for 2 weeks after the start of the prediction window. Each day has a different feature matrix due to the inclusion of the user-date features. The predicted date was then the one with the highest probability. For the spend, I used the “modal window” concept, which I hope other contestants will describe in more detail.\\r\\n\\r\\nWere you surprised by any of your insights?\\r\\n\\r\\nI was most surprised by the seemingly endless list of things which didn't work on this data! In most data mining problems, if you have method A which does well and method B which does well, you can combine them and watch your score improve. This one was tough because if A says “Tuesday” and B says “Thursday”, you can't average them and say “Wednesday.” This would have improved your score if something like RMSE was used, but it doesn't fly for the exact error metric. For all you know, that person has Yoga class and never goes shopping on Wednesday. Similarly, you can't toss the £2 gum purchases in with the £200 weekly shops and guess the person will spend £100.\\r\\n\\r\\nTo alleviate this problem, I had to be careful about the types of features used for the regression. In metaphorical terms, I tried to take modes where I normally would have taken means. This required careful attention to statistical support. The mode is a deceitful beast in that the “most common” pattern of past behavior can range from “this person comes in every 7 days without fail” all the way to “this person comes in whenever they feel like it, and it just so happens that 7 was the number that won by chance.” I experimented with a number of ways to use confidence estimates to weight, blend, or downplay features for customers who were strangers to the supermarket.\\r\\n\\r\\nThanks William - now we're looking forward to reading about your success in the Semi-Supervised Feature Learning competition!\", 'Neil Schneider placed second in the dunnhumby Shopper Challenge with a breakout performance. Read on for some insights into his methodology, and visit the links at the bottom to view his code.\\nWhat was your background prior to entering the dunnhumby Shopper Challenge?\\r\\nI am an Associate Actuary with Milliman\\'s Indianapolis office. I have two degrees from Purdue University in Actuarial Science and Statistics. I graduated in December of 2007 and joined Milliman. Most of my experience is in creating actuarial models for pricing and reserving. During my time with Milliman, I have become proficient in SAS and JMP. Lately, I have completed some data analyses using R. I understand enough R to run various functions, but still rely on SAS for all my data manipulations. Our office has recently completed some work on a new method for reserve projections, based on published robust time series statistics. A lot of the research was applied from the inventory control field.\\r\\n\\nWhat made you decide to enter?\\r\\nWe found Kaggle during our reserve methodology research, because of the Heritage Health Prize. I have competed in the \"Don\\'t Overfit!\" and \"Mapping Dark Matter\" competitions prior to dunnhumby\\'s challenge. While most of the competitions sound intriguing, I only have so much free time to devote to a competition. I thought this was an excellent example of sparse time series data and was hoping to leverage knowledge from our own model to predict the outcomes. As it turned out, this was not the case.\\r\\n\\r\\nWhat was your most important insight into the dataset?\\r\\nOne insight was that shoppers are habitual. They will tend to have their day of week to do their shopping. A customer may visit the store on different days of the week, but you can see that for different weekdays a customer will spend different amounts. Example: A customer typically spends $100-$150 on Saturday, but will visit the store just as often on Monday, but only spends $40-$60. Developing separate projections for the spend amount by weekday was important to correctly predicting the next spend.\\r\\n\\r\\nWere you surprised by any of your findings?\\r\\nI was surprised that the time series models for inventory controls performed so poorly on these projections. I was also surprised at how poorly regression models fit the spend amounts. This is probably due to the test statistic for the evaluation. Out of the box regressions will optimize predictions for the mean or quartiles of the dependant variable. This competition needed an optimization of the highest density area.\\r\\n\\r\\nWhich tools did you use?\\r\\nI used SAS for most of the heavy data work. This included proc sql statements to develop the maximum density visit_spend ranges.\\r\\nJMP was invaluable for visualizing the data and optimizing choices. I mainly used histograms, X vs Y plots and partition models.\\r\\nFinally, I used R to run more advanced statistical models (Generalized Boosting Regression Models - Package \"gbm\").\\r\\n\\r\\nWhat have you taken away from the competition?\\r\\nI learned that GBMs are indeed powerful for predictions, but the interpretation of coefficients for independent variables can be meaningless. This leads me to question what methods would be the most useful for modeling, but producing quality coefficient estimates. Maybe a future competition?\\r\\n\\r\\nCongratulations Neil on a fantastic performance! Neil has posted his methodology in more detail, along with his SAS and R code, on the Kaggle forum.', 'Dell Zhang placed third in Wikipedia\\'s Participation Challenge and agreed to give us a peek into his process.\\nWhat was your background prior to entering the Wikipedia Participation challenge?\\r\\nI got my PhD in Computer Science from the Southeast University in China 9 years ago, and then moved to Singapore to work as a postdoc research fellow under the supervision of Prof Wee Sun Lee. It was very kind of him to send me to the first-ever Machine Learning Summer School in 2002 - that\\'s when I discovered the fascinating world of machine learning. Since then my research has been centred around using statistical machine learning techniques to improve information retrieval and organisation. I am currently a Senior Lecturer in Computer Science at Birkbeck, University of London. I have also joined the Royal Statistical Society and learned loads of interesting stuff done by statisticians.\\r\\n\\nWhat made you decide to enter?\\r\\nBeing a busy academic I can no longer spend much time on coding as I used to, but I still enjoy getting my hands dirty and playing with data now and again. So when I got some time this summer, I decided to take part in a Kaggle competition to brush up on my rusty coding skills. The Wikipedia Participation Challenge attracted me most because as a heavy user of Wikipedia I felt obliged to do something helpful for the community.\\r\\n\\r\\nWhat was your most important insight into the dataset?\\r\\nThe most important insight was that most user\\'s future behaviour can be largely determined by his or her recent behavioural dynamics - how the number of edits and the number of edited articles change in the last period of time.\\r\\n\\r\\nWere you surprised by any of your insights?\\r\\nI am a bit surprised that dynamics features alone can go such a long way when we choose proper temporal scales and employ a powerful machine learning method.\\r\\n\\r\\nWhich tools did you use?\\r\\nI used Python to write small programs for analysing data and making predictions. I like the simplicity of Python - simple is beautiful. The machine learning methods that I have tried all come from two open-source Python modules: one is scikit-learn, and the other is OpenCV. Finally, Gradient Boosted Trees (implemented in OpenCV) outperformed the other methods. I must mention that the parameter tuning was carried out on a big validation dataset shared to all participants by Twan van Laarhoven. \"Such a nice guy\", indeed!\\r\\n\\r\\nWhat have you taken away from the competition?\\r\\nA lot of fun, and a few lessons :-) I cannot wait to see others\\' secret weapons for tackling this problem. Long live Wikipedia!\\r\\n\\r\\nThanks very much Dell and congratulations on a great performance!\\nUpdate\\r\\nDell has posted a detailed write-up at:\\r\\nhttp://arxiv.org/abs/1110.5051\\r\\nAnd \\xa0source code at:\\r\\nhttp://www.dcs.bbk.ac.uk/~dell/publications/wikichallenge_zeditor.zip', \"A quick interview today with Dr Gareth Campbell, who placed fourth in Wikipedia's Participation Challege.\\nWhat was your background prior to entering the Wikipedia Challenge?\\nI have a PhD in Finance.\\nWhat made you decide to enter?\\nI wanted to test how good my econometric skills were.\\nWhat was your most important insight into the dataset?\\nThe number of edits by Wikipedia users was almost entirely dependent on the number of edits made by them in the past few months. The number of reversions, the type of article edited, comments etc. had little predictive power.\\nWere you surprised by any of your insights?\\nA little surprised that the number of reversions had such little impact.\\nWhich tools did you use?\\nStata.\\nWhat have you taken away from the competition?\\nTesting lots of different small steps can lead to substantial improvements in predictive power.\\nThanks Gareth!\", \"This week, we were thrilled to welcome to the Kaggle team Ben Hamner, winner of the Semi-Supervised Learning Competition and one of our most successful competitors to date. Ben recently placed third in dunnhumby's Shopper Challenge, and had the following to say about the experience.\\nWhat was your background prior to entering this challenge?\\nI graduated from Duke University in 2010 with a bachelors in biomedical engineering, electrical and computer engineering, and mathematics. For the past year, I applied machine learning to improve non-invasive brain-computer interfaces as a Whitaker Fellow at EPFL. On the side, I’ve participated in various predictive analytics competitions.\\nWhat made you decide to enter?\\nThe dataset was deceptively simple. It simply consisted of a list of customers’ visits and spend amounts for the past year, whereas many datasets have a much higher dimensionality and require substantially more preprocessing. This simplicity provided a good competitive testbed for more statistically oriented methodologies.\\nWhat was your most important insight into the data?\\nTwo patterns in the data were key: periodic weekly behavior dominated when a customer visited a store (as opposed to the time since his last visit), and a customer’s recent shopping behavior was more predictive than his past behavior. Simple models using these patterns performed very well. My best “simple” date predictor took the most commonly visited day of the week, weighted by how recent the visits were. My best “simple” spend predictor took the mode of a weighted kernel density estimate of the customer’s previous spend behavior. As with most machine learning competitions, more complex methods that incorporated these simple models were necessary for bleeding edge performance.\\nWere you surprised by any of your insights?\\nStandard algorithms performed poorly on this task due to the atypical cost function. Many supervised regression methods optimize the mean squared error, but the spend prediction was evaluated with a binary metric: whether the predicted spend was with $10 of the actual spend. This meant that our task was to predict the most likely customer behavior, as opposed to the average customer behavior. All gradient based approaches I applied to this task performed relatively poorly, even when the cost function was modified to be a differentiable approximation of the binary evaluation metric. On the other hand, successful approaches to this competition performed poorly on the mean squared error metric.\\nWhich tools did you use?\\nI used Matlab and Python.\\nWhat have you taken away from this competition?\\nCarefully analyzing and optimizing according to the evaluation metric is crucial for competitors. The metric can dramatically affect which models perform well and which perform poorly, especially at the frontier of what is possible given the data. In my case, I optimized the model based on date prediction, and then on spend prediction given the predicted date. I optimized each of these over the individual evaluation metrics. I ran out of time to jointly optimize the date and spend prediction according to the final evaluation metric, and ultimately got beat by a model that did.\\nFrom a competition host’s perspective, the evaluation metric may not be as crucial. If the goal is to get the best models possible on a very well defined problem, then choosing the appropriate metric is absolutely vital. If the goal is to discover what is possible given an underlying set of data, or what useful patterns are hidden in the data, then the precise metric may not be as important as other considerations.\", 'What was your background prior to entering this challenge?\\nI\\'m a software developer and got started in machine learning in the Netflix prize.\\nWhat made you decide to enter?\\nThe locations and lists of words seem to offer many possibilities. The data is clean with no missing values. And this was a short contest so I couldn\\'t spend too much time on it.\\nWhat preprocessing and supervised learning methods did you use?\\nMy best result was a mix of random forest, GBM, and two forms of logistic regression. I put the raw data into a database and built many derived variables. I also used many raw & derived variables from external, location-based data. I wrote some Javascript code to call Google Maps API and retrieved:\\n\\nElevation at each latitude-longitude coordinate.\\nCountry and administrative area. This could not be determined at some locations, they were all in the middle of the ocean, on small islands or boats I guess.\\nNumber of \"places\" within 50 KM and 10 KM radius of each location, and users\\' ratings of these places.\\n\\nI downloaded a bunch of World Development Indicators data from\\xa0worldbank.org. Buried among these were 10 country-tourism data which I injected into my database.\\nI downloaded population density data from\\xa0http://sedac.ciesin.columbia.edu/gpw/\\xa0and made a rough album per capita index: albumCount/populationDensity. I figured locations that score high on this index are remote, scenic places, and those that scored low are boring urban areas.\\nAll these external data helped, but only a little. The raw and derived variables were fed to algorithms in different combinations.\\nWhat was your most important insight into the data?\\nI don\\'t think I have any, and I\\'m actually very curious about Jason Tigg\\'s insight. One day Jason suddenly gained a huge lead over everyone else, and I was convinced he found a great insight and/or external data. For the remainder of the contest, I was obsessed and went on a wild-goose chase after this insight, this \"one ring to rule them all (imagine Gollum hissing in the cave)\".\\nMy most useful variables were simple and well known: average numbers of \\'good\\' albums for each word, weighted with global average based on how many albums the word appear in. This was done separately for album name, album description, photo caption, and one merged word list. Then for each album and location, the average of word averages were calculated.\\nWere you surprised by any of your insights?\\nWell,\\xa0I was surprised I couldn\\'t get\\xa0any signal\\xa0out of word pairs.\\nWhich tools did you use?\\nSQL, R, C++, C#, Javascript, Google web services, and Excel.\\nWhat have you taken away from this competition?\\nIt\\'s probably not worth it to spend too much time on external data, as chances are any especially useful data are already included. Time can be better spent on algorithms and included variables. For example I didn\\'t even try to use the number of times a word appeared in each album.', \"Alexander D'yakonov placed third in the Photo Quality Prediction competition and agreed to give us a peek into his process.\\nWhat was your background prior to entering this challenge?\\nI’m an Associate Professor at Moscow State University. \\xa0I like different data mining problems and have participated in many Kaggle contests.\\nWhat made you decide to enter?\\nThe problem has few features, so it did not look very complicated, and it had interesting lists of words. There were many participants, so a good result would be more challenging.\\nWhat preprocessing and supervised learning methods did you use?\\nI combined random forests with a weighted k-NN. The combination used a weighted square root of the sum of squares of the predictions, with coefficients tuned by gradient descent. I did not use any external information.\\nWhat was your most important insight into the data?\\nNothing! I used Random Forests with some simple additional features. \\xa0For example, these included the “ratio” (the width of the image divided by the height of the image) and “area” (the number of pixels in the photo). \\xa0I also used a merged word list (words from album name, album description, photo caption).\\nWere you surprised by any of your insights?\\nI was surprised that I couldn’t build good features from the word lists. All my engineered features were worse than I expected.\\nWhich tools did you use?\\nMATLAB and R.\\nWhat have you taken away from this competition?\\nIt is necessary to be careful when you build the final decision. I made one mistake: instead of averaging six algorithms, I used only three algorithms and had worse performance.\", 'Joe Malicki placed third in Give Me Some Credit despite health problems preventing him from submitting for the majority of the competition.\\nHow does it feel to have done so well in a competition with almost 1000 teams?\\nGreat! This was my first serious attempt at Kaggle - I\\'ve been doing data modeling for a while, and wanted to try cutting my teeth at a real competition for the first time.\\nWhat was your background prior to entering this challenge?\\nI have a computer science (CS) degree and have done some graduate work in CS, statistics, and applied math. I currently work for Nokia doing machine learning for local search ranking.\\nWhat preprocessing and supervised learning methods did you use?\\nI tried quite a bit of preprocessing, mentioned below, and in the code I posted to the forums.\\nI used random forests, gradient boosted decision trees, logistic regression, and SVMs, with various subsampling of the data for various positive/negative feature balancing. In the end, using 50/50 balanced boosting, and 10/90 balance random forests, and averaging them, won.\\nThis competition had a fairly simple data set and relatively few features – did that affect how you went about things?\\nAbsolutely! One of the big problems in this competition was a large and imbalanced dataset - defaults are rare. I used stratified sampling for classes to produce my training set.\\nTransformations of the data were key. To expand the number of features, I tried to use my prior knowledge of credit scoring and personal finance to expand the feature set.\\nFor instance, knowing that people above age 60 are likely to qualify for social security, which makes their income more stable, and that 33% (for the mortgage) and 43% (for the mortgage plus other debt) are often magic debt to income (DTI) numbers was very useful. I feel strongly that knowing what the data elements actually represent in the real world, rather than numbers, is huge for modelling.\\nRandom forests are a great learning algorithm, but they deal poorly where transforms of features are very important. So I identified some things that looked important - combining income and DTI to estimate debt and combining number of dependents and income as a proxy for disposable income/constraints. The latter is important since struggling to barely support dependents makes default more likely.\\nAlso, trying to notice \"interesting\" incomes divisible by 1000 was useful - I was guessing that fraud might be more likely for these, and/or they may signal new jobs where a person hasn\\'t had percent raises. I was going to try a Benford\\'s law-inspired income feature, to help detect fraud, but had to leave the competition before I got a chance.\\nWhat was your most important insight into the data?\\nForum discussion pointed out that ‘number of times past due’ in the 90\\'s seemed like a special code, and should be treated differently. Also, noticing that DTI for those with missing income data seemed weird was critical.\\nWere you surprised by any of your insights?\\nI had tried keeping a small hold-out data set, and then combining all of the models I trained via a logistic regression on that data set, using the model probabilities as features, but found that only taking the best models of various classes worked better.\\nWhich tools did you use?\\nR. It\\'s a memory hog, but it has first-class algorithm implementations.\\nWhat have you taken away from this competition?\\nThe results I saw on the public test set differed dramatically from any results I could get on any held-out portion of the training set. In the end, the results on the private testing data set for all of the models I submitted were extremely close to my private evaluations, far closer than to the performance on the public set. This highlighted to me the importance of relying on cross-validation on large samples.', \"Xavier Conort came second in Give Me Some Credit and let us in on some of the tricks of the trade.\\nHow does it feel to have done so well in a contest with almost 1000 teams?\\nI feel great because Machine Learning is not part of my natural toolkit. I now look forward to exploiting this insight in my professional life and exploring new ideas and techniques in other competitions.\\nWhat was your background prior to entering this challenge?\\nI am an actuary and set up a consultancy called Gear Analytics a few months ago. It’s based in Singapore, and helps companies to build internal capabilities in predictive modeling and risk management using R. Previously, I worked in France, Brazil, China and Singapore holding different roles (actuary, CFO, risk manager) in the Life and Non-Life Insurance industry.\\nWhat preprocessing and supervised learning methods did you use?\\nI didn't spend much time on preprocessing. My most important input was to create a variable which estimates the likelihood of being late by more than 90 days.\\nI used a mix of 15 models including Gradient Boosting Machine (GBM), weighted GBMs, Random Forest, balanced Random Forest, GAM, weighted GAM, Support Vector Machine (SVM) and bagged ensemble of SVMs. My best score, however, was an ensemble without SVMs.\\nThis competition had a fairly simple data set and relatively few features – did that affect how you went about things?\\nThe data was simple yet messy. I found off-the-shelf techniques such as GBM could handle it. The relative simplicity of the data allowed me to allocate more time to trying different models and ensembling my individual models.\\nWhat was your most important insight into the data?\\nThe likelihood of being late was by far the most important predictor in my GBM and its inclusion as a predictor improved my individual fits accuracy.\\nWere you surprised by any of your insights?\\nI've always believed that people can benefit from diversity, but I was surprised to see how much data science can also benefit from it (through ensembling techniques). The strong performance achieved by\\xa0\\xa0Alec, Eu Jin and Nathaniel (Perfect Storm) also shows that teamwork matters.\\nMy best individual fit was a weighted GBM which scored 0.86877 in the private set. Without ensembling weaker models, my rank would have been 25.\\nWhich tools did you use?\\nR, the excellent book The Elements of Statistical Learning and the great online Stanford course by Andrew Ng. All are free.\\nWhat have you taken away from this competition?\\nWhen I entered the competition, I was still unfamiliar with Machine Learning techniques as they are rarely used in the insurance industry. I was amazed by the capacity of Gradient Boosting Machine (also called Boosted Regression Trees) to learn non-linear functions (including interactions) and accommodate missing values and outliers. It is definitely something that I will include in my toolbox in the future.\", \"The Perfect Storm, comprising Alec Stephenson, Eu Jin Lok and Nathaniel Ramm, brought home first prize in Give Me Some Credit. We caught up with Alec and Eu Jin.\\nHow does it feel to have done so well in a contest with almost 1000 teams?\\nEJ: Pretty amazing, especially when it was such an intense competition with so many good competitors. Personally, I felt a strong sense of achievement together as a team.\\nAS: It feels great, particularly because we won by such a well-defined margin. The gap between first and second place was the largest gap in the top 500 placings.\\nWhat were your backgrounds prior to entering this challenge?\\nEJ: My background is in statistics and econometric modelling. More recently I've worked in data mining and machine learning for Deloitte Analytics Australia, where I am a Senior Analyst.\\nAS: My formal background is in mathematics and statistics. I am a largely self-taught programmer, and have written a number of R packages. I do not work in data mining, but have picked up an interest in it over the last year or so, mainly due to Kaggle! I am an academic, originally from London, and have studied or worked at universities in England, Singapore, China and Australia.\\nWhat preprocessing and supervised learning methods did you use?\\nAS: We tried many different supervised learning methods, but we decided to keep our ensemble to only those things that we knew would improve our score through cross-validation evaluations. In the end we only used five supervised learning methods: a random forest of classification trees, a random forest of regression trees, a classification tree boosting algorithm, a regression tree boosting algorithm, and a neural network.\\nThis competition had a fairly simple data set and relatively few features – did that affect how you went about things?\\nEJ: It meant that the barrier to entry was low, competition would be very intense and everyone would eventually arrive at similar results and methods. Before we formed a team, I knew that I would have to work extra hard and be really innovative in my approach to solving this problem. Collaboration was the last ace and as the competition started to hit the ceiling, I decided to play that card.\\nWhat was your most important insight into the data?\\nEJ: I discovered 2 key features, the first being the total number of late days, and second the difference between income and expense. They turned out to be very predictive!\\nWere you surprised by any of your insights?\\nAS: I was surprised at how well neural networks performed. They certainly gave a good improvement over and above more modern approaches based on bagging and boosting. I have tried neural networks in other competitions where they did not perform as well.\\nHow did working in a team help you?\\nTOGETHER: As individuals, we were unlikely to win. But with Nathaniel's expertise in credit scoring, Alec's expertise in algorithms and Eu Jin's knowledge in data mining, we had something completely different to offer that was really powerful. In a literal sense, we stormed our way up to the top.\\nWhich tools did you use?\\nTOGETHER: SQL, SAS, R, Viscovery and even Excel!\\nWhat have you taken away from this competition?\\nAS: That data mining is fun when you are in a team, and also how effective a team can be if the skills of its members complement each other. You can learn a lot from the people that you work with.\", 'Jason Tigg came third in the Claim Prediction Challenge and caught up with us afterwards.\\nWhat was your background prior to entering the Prediction Claim challenge?\\nAs I child I was interested in machine intelligence and when I was 14 I wrote my first \"intelligent\" program in assembler on my Dragon 32 computer to play Othello, inspired by a wonderful book \"Computer Gamemanship\" by David Levy. Through Kaggle I have made contact with David Slate of the team of \"Old Dogs with New Tricks\" who I have discovered was instrumental in pioneering the field of computer chess back in the 1970s. I studied at Oxford University where I obtained a doctorate in Elementary Particle Physics which made extensive use of an early version of Mathematica to solve\\xa0some fairly\\xa0complicated integral equations. Since then I have been working writing financial software for both trading and risk management. I previously entered a fascinating chess challenge on Kaggle, so this was my second competition.\\nWhat made you decide to enter?\\nI entered the competition mostly for the fun of the challenge. The leaderboard on Kaggle is addictive and gives a real sense of competition as well as giving you a sense of how well you are understanding the algorithms and the data.\\nWhat was your most important insight into the dataset?\\nI would say the most important insight was technical not algorithmic. The dataset was so large it required some compression to hold in RAM and some interesting iterator code to walk through one household at a time. Examining data clustered by household turned out to be particularly important.\\nWhy the name Planet Thanet?\\nI was born and grew up in the beautiful seaside town of Ramsgate in the Isle of Thanet in South East England.\\nWhich tools did you use?\\nNot many to be honest. All the code was written in Java and no third party libraries were used.\\nWhat have you taken away from the competition?\\nThe top two teams were some distance above me so clearly I must have missed something. Hopefully I will discover what that is!', 'Anil Thomas, Chris \"Swedish Chef\" Hefele and Will Cukierski came 4th in the Algorithmic Trading Challenge.\\xa0 We caught up with them afterwards.\\nWhat was your background prior to entering this challenge?\\nAnil: I am a Technical Leader at Cisco Systems, where I work on building multimedia server software. I was introduced to machine learning when I participated in the Netflix Prize competition. Other than Netflix Prize where I was able to eke out an improvement of 7% in recommendation accuracy, I have no significant data mining experience to speak of.\\nChris: \\xa0I have a MS in electrical engineering, but I have no formal background in machine learning. \\xa0My first data-mining contest was the Netflix Prize, and I learned a tremendous amount by being part of the team that came in 2nd place. \\xa0Since then, I’ve been hooked by these competitions, and have entered several Kaggle contests in my spare time. \\xa0During the day, though, I work on Voice-over-IP projects at AT&T Labs, where I’m a systems engineer.\\nWill: I studied physics at Cornell and am in the final stages of a PhD in biomedical engineering at Rutgers. \\xa0Like Chris and Anil, my first data mining contest was the Netflix prize, where I placed somewhere around 30,000th (the leaderboard doesn’t go past 1000, but what’s a few thousand places among friends). \\xa0Several years and many competitions later, I am lucky to rub elbows with the clever minds and talented folks on Kaggle.\\nWill and Chris formed a team from the start, while Anil climbed the leaderboard separately. \\xa0In the closing days of the competition, the two teams agreed to merge in order to better their respective chances at the top and only prize. \\xa0Following a long weekend of furious model blending, they ended up in 4th. All three participants wish to thank Capital Markets Cooperative Research Centre and Kaggle.com for hosting this competition.\\nWhat made you decide to enter?\\nAnil: I had just completed the excellent online course on machine learning taught by Prof. Andrew Ng of Stanford and was looking for a challenge that goes beyond routine homework problems. This competition was a perfect fit. I have always found stock market data to be intriguing and this looked like a good opportunity to try my hand at analytics.\\nWill: This was my first contest with financial data and a nice opportunity to peek into the world of short-term market dynamics, spreads, order books, etc.\\nChris: \\xa0I have always been interested in “quant” finance topics. Also, I had some success in the INFORMS 2010 contest on Kaggle, which involved predicting short-term price movements of securities. \\xa0I thought some of the lessons I learned in that contest might be helpful in this one.\\nWhat preprocessing and supervised learning methods did you use?\\nWill: I had initial success with a kNN model and spent the majority of the competition convinced I could improve this model. \\xa0My initial feature set was picked by hand using feedback from the probe set (the last 50k trades of the train set). \\xa0Most of the features were basic transformations of the spread just before the liquidity shock. \\xa0Querying for neighbors within each security generally outperformed querying across all securities, but we did find that the combination of the two worked best. \\xa0I spent many weeks attempting to implement a more rigorous way to pick the feature space. \\xa0There are many published methods on how to learn a custom Mahalanobis distance metric using supervised labels (that is, to find S in the equation below such that trades with similar reactions would have similar distances in feature space).\\n$$ D_M(x) = \\\\sqrt{ (x - \\\\mu)^T S^{-1} (x - \\\\mu)}$$\\n(Note that a diagonal S is the same thing as weighting each feature separately in Euclidean space)\\nHowever, this contest was not a traditional supervised classification problem in the sense that we had a measure of dissimilarity (the RMSE between the bid/ask responses), as opposed to neat-and-tidy class labels. \\xa0Despite numerous promising modifications and a last minute multidimensional scaling idea, I ran out of time to find a suitable Mahalanobis matrix that beat the RMSE of the initial hand-picked feature set.\\nChris: \\xa0I tried to keep it simple, and stuck with creating multiple variations on basic linear regression models. \\xa0I created more than 30 features derived from the original data (consisting mostly of the min/max/median/std.deviations of prices & spreads & the trade/quote arrival rate). \\xa0I fed those features plus the original data to a LASSO regression, which selected 18 variables. \\xa0Separate regressions were used for bids vs asks, and buys vs sells. \\xa0I also had models that predicted prices for each time period individually, as well as other models that predicted time-invariant, average prices. \\xa0Furthermore, the characteristics of the the testing & training sets differed, so I tried a variety of ways to weight each row of data to correct for those differences. In the end, I just weighted each row by the ratio of how often each security appeared in the testing vs training sets.\\nAnil: The model that worked best for me was linear regression on various indirect predictors derived from the training data. I also tried Random Forest, k-NN, k-means and SVM regression techniques. As for preprocessing, I found it advantageous to set the base predictions to match the general trajectory of the prices and then model the residuals.\\nWhat was your most important insight into the data?\\nWill: This data was very fussy, and the use of un-normalized RMSE to score the competition made for a very skewed error distribution. Chris and Anil did some great due diligence into the quirks of this dataset, so I defer to them for details.\\nChris: \\xa0The market open (at 8AM) was very extremely unpredictable, and contributed a disproportionally large amount of error. \\xa0For one model, I found 12% of squared error for the entire trading day occurred in the first minute of trading. To combat this, I trained some separate models for the market open, since it seemed so different (the naive benchmark model worked better than my regressions at the market open, for example).\\nAdditionally, the farther away you got from the “liquidity shock” trade, the more unpredictable the prices were. \\xa0Looking backward in time from that “liquidity shock” trade, \\xa0my variable-selection algorithms dropped all historical bid/ask prices except those immediately before the trade, since those prices did not provide enough predictive value. As you moved forward in time from that trade, bid/ask prices got progressively harder to predict. \\xa0Using time-averages & PCAs, though, you could see two common patterns in the noise: \\xa0for buys, bid/ask prices jumped up sharply \\xa0& then rose slowly; for sells, bid/ask prices jumped down sharply, and then fell slowly. \\xa0Thus the “liquidity shock” trades seemed to have a permanent impact on prices, \\xa0\\xa0rather than a temporary, mean-reverting one.\\nAnil: Categorizing and plotting the data clearly showed that the bid and ask prices followed separate paths and their trajectories differed depending on who initiated the trade - buyer or seller. Performing regression separately for each category led to dramatic improvement in prediction accuracy.\\nWere you surprised by any of your insights?\\nWill: I’m unconvinced that I had noteworthy insights outside of the usual techniques to gain ground in a data mining competition. \\xa0RMSE falls by 3 methods: creating many models and blending them, better data/features, or better methods. \\xa0Chris and Anil each brought a nice blender and several models to the table. \\xa0I did what I could to make a better kNN method, but perhaps my time would have been better spent coming up with features or looking for outliers.\\nAnil: The conventional wisdom seems to suggest that an ensemble of reasonably good models perform better than a finely tuned individual model. As a team, we had a great variety of models, but looking back, I think we would have fared better if we spent more efforts to tune the individual models. The models that used SVM and k-means with mediocre prediction accuracy ended up contributing almost nothing to the final blended result.\\nChris: \\xa0I knew the market open & outliers would be important, but I was really surprised by how much of an impact they had on one’s RMSE. \\xa0I was also surprised to find no mean-reversion in the bid/ask prices, since that differed from the examples that the contest organizers gave.\\nWhich tools did you use?\\nAnil: I am a minimalist and typically use little more than gVim, gcc and gnuplot. For this competition, I picked up some R and was impressed by its capabilities. One could just grab an off-the-shelf package, let it loose on the data and end up with decent results. I still think lower level languages have a place in data analytics because of the flexibility that they offer. Knowing what happens under the hood can give you an edge. Sometimes small tweaks to the underlying mechanism can give you a big boost when you desperately need it. My best model was written entirely in C++ without using any 3rd party libraries. This made it possible to mold the model well enough to fit the quirks within the data.\\nChris: \\xa0I used R, mostly working with the glmnet package. \\xa0I also used Python (with \\xa0the numpy package) for blending prediction sets together.\\nWill: \\xa0Matlab\\nWhat have you taken away from this competition?\\nWill: \\xa0Collaborating with new teammates was a nice experience. \\xa0Teammates bring different backgrounds, fresh ideas, and code in different ways. \\xa0Working alone it is easy to get stuck trying the same hammer on every nail, even if that nail happens to be a screw. That’s when a teammate can step in and tell you to stop smashing with the hammer and try a screwdriver. \\xa0Witnessing another person dissect the same data problem is a great way to pick up new tools and skills.\\nChris: \\xa0One lesson I learned from this competition is that one should always identify outliers as specifically as possible & decide how to best deal with them. \\xa0Also, it’s really helpful to have teammates to bounce ideas off of, especially when you’re stuck or losing motivation.\\nAnil: The discussions on the forum, especially post-contest have been illuminating. I don\\'t think any contestant individually had quite a handle on the data. The details that contestants have shared about their findings and methods have shed light on various aspects of the data. One can actually see the pieces coming together in the giant jigsaw puzzle. I am also thankful that I got to collaborate with Chris and Will, who are first-rate data scientists and fantastic people to work with.', 'Ildefons Magrans is the winner of the Algorithmic Trading Challenge.\\xa0 He explains why he chose to measure himself against the market.\\nWhat was your background prior to entering this challenge?\\nI hold a Masters in Computer Science, a Masters in Electrical Engineering and a PhD in Electrical Engineering.\\xa0 My first machine learning experience was with fuzzy logic clustering algorithms during the final project of MsC in CS.\\xa0 Recently, I have been working on two applied research projects: developing of a human-like dialog turn-taking model with a continuous-time Hidden Markov Model, and developing a classification system for a prosthetic ankle to infer the presence of stairs.\\nWhat made you decide to enter?\\nI have been interested in algorithmic trading since I finished my PhD 3 years ago. I have been studying market micro-structure, arbitrage opportunities at different frequencies, contributing to open-source algo trading infrastructure and so on. But I never dared to use real money.\\xa0 I was not sure about my skills compared to other people working in the field. This challenge was a wonderful opportunity to test myself.\\nWhat preprocessing and supervised learning methods did you use?\\nI tried many techniques: (SVM, LR, GBM, RF). Finally, I chose to use a random forest.\\nWhat was your most important insight into the data?\\nThe training set was a nice example of how stock market conditions are extremely volatile.\\xa0 Different samples of the training set could fit very different models. Lots of fun!\\nWere you surprised by any of your insights?\\nI was not surprised by the difficulty level. High frequency trading is a very competitive field full of smart people trying to fish small inefficiencies.\\nWhich tools did you use?\\nI did everything with R, without a database, on an i7 laptop with 16 Gbytes of RAM.\\nWhat have you taken away from this competition?\\nI have had to improve my parallel programming skills in R.', 'Owen Zhang, who passed the 6 CAS exams \"just for fun\", discusses placing 2nd in the Claim Prediction Challenge\\nWhy did you decide to participate in the Claim Prediction Challenge?\\nTo continue improving and evaluating my predicative modeling knowledge and skills.\\nApart from monetary incentives, did anything else motivate you to participate in the competition?\\nTo master cutting-edge analytical methodology in the context of a real world business problem, and to see where I stand in insurance modeling.\\nHow many entries did you submit?\\nWhat drove you to continue submitting new entries?\\xa0\\xa0 I submitted 20 entries. The reason to keep submitting is to find out if the tricks that appeared to have worked on my own validation data would work on the 4th year data as well. Another purpose is, obviously, to \"catch\" those who were in front of me. In retrospect, my 3rd serious submission would have got the same 2nd place, but I didn\\'t know then.\\nHow would you characterize your competitors in this contest?\\nI kind of \"know\" some of them (such as \"old dogs with new tricks\") through other modeling/data mining competitions. I feel this is a very diverse group of modelers. Some are obviously seasoned professionals and some have apparently just started learning. I also have the impression that many competitors are not from P&C insurance background.\\xa0 I guess we have more machine learners here than statisticians.\\nWhat did you enjoy most about the competition?\\nTrying to come up business stories behind partially anonymized data.\\nWhat got you interested in actuarial science?\\nI see myself as more a predictive modeler/data miner than an actuary (although I did pass 6 CAS exams just for fun), so this question doesn\\'t really apply to me. I am interested in predictive modeling primarily because I find it is extremely intellectually stimulating AND I appear to be reasonably good at it.', 'Marcin Pionnier, member of the Don\\'t Get Kicked winning team Sollers & Gxav, along with Xavier Conort, gives us some tips on how they blended different models to produce the best results.\\nWhat made you decide to enter?\\nThis particular competition was classical - it was easy to start and produce first submissions. Also the domain is something that I can understand - common sense is enough to interprete variable values and to create new ones.\\nWhat was your background prior to entering this challenge?\\nI graduated from Warsaw University of Technology (Poland), my master thesis was related to text/web mining. Currently I work as software architect/programmer for Sollers Consulting (we operate mainly in Europe) \\xa0in the insurance area, so preparation data sets from transactional systems for risk estimation is one of my typical everyday tasks. Also I think that participating in Kaggle challenges is giving me a lot of very valuable experience. \\nHave you ever bought a used car?\\nYes, but from authorized car dealer to mitigate the risk. It was \"Good Buy\" transaction - not to be confused with \"good bye\" :)\\nWhat preprocessing and supervised learning methods did you use?\\nI did not use any external data sources. However, I added some variables to the initial dataset. The most important among them were:\\n\\nVarious differences between the prices given\\nResults of regression models \\xa0built on training set: prediction of OdoVeh (it was then subtracted with just OdoVeh value), WarrantyPrice, Engine (in some cases it was possible to extract engine from text fields, for the rest it was made with this regression)\\n\\nMy part of our joint solution (simply blended with Xavier Conort\\'s result) is average of seven internal models:\\n\\nSix variants of LogitBoost algorithm that internally use Dagging Algorithm - training set is divided into stratified folds and the internal models are averaged. As weak learners for Dagging method simple DecisionStump (one node decision tree) and some up to 3-level decision tree based on DecisionStump were used.\\nSeventh model was Alternating Decision Tree which is also similar to boosting.\\n\\nWhat was your most important insight into the data?\\nTo be honest there was no data transformation/additional data generated that gave me important progress. Various data enrichment approaches were improving my score only a little. I think that in this competition the most important task was to choose algorithms that could aggregate many weak predictors in an efficient way.\\nWere you surprised by any of your insights?\\nI am surprised with the power of boosting weak learners - their good performance is well-known fact, however I was using this technique for the first time.\\nWhich tools did you use?\\nI used Weka as library of algorithms linked with my own Java code for data pre-procesing and learning algorithms configuration. I think that such an approach gives the possibility of quick prototyping for initial solutions, and also it is possible to modify existing algorithms by copying the sources and introduce changes when needed.\\nDo you have any advice for other Kaggle competitors?\\nFusion of the R model developed by Xavier with my modeling ideas prepared on Weka-based tools gave us a big improvement on the leaderboard, so using different Machine Learning packages together with R (which seems to be most popular tool amongst Kagglers) might be good strategy.', \"Vladimir Nikulin, winner of 2nd prize in the Don't Get Kicked competition, shares some of his insights and tells us why Poland is the place-to-be for machine learning.\\nWhat made you decide to enter?\\nBoth Challenges (Give Me Some Credit and Don't Get Kicked) could be regarded as classic and are very similar. That's why, I think, they were extremely popular. I have proper experience, and participated in the relevant Contests (see, for example, PAKDD07 and PAKDD10) in the past. In addition, the financial applications are directly relevant to the interests of my Department of Mathematical Methods in Economy at the Vyatka State University, Kirov, Russia.\\nWhat was your background prior to entering this challenge?\\nI have a PhD in mathematical statistics from the Moscow State University. By the way, I shall be visiting MSU in the middle of this February. Since 2005, I participated in many DM Challenges. In particular, some readers might be interested to consider text of my interview in Warsaw, Poland:\\r\\nhttp://blog.tunedit.org/2010/07/20/no-alternatives-to-data-mining/\\r\\nThis interview was given in June 2010. At that time, Kaggle was at the most early stages of development. \\xa0Also, I would like to use this opportunity to express my very high impression about support and recognition of the area of data mining in Poland.\\nHave you ever bought a used car?\\nYes, I bought three used cars while in Australia:\\n\\nToyota-Corona: {1978/1993/1995}\\nToyota-Camry: {1992/1996/2000}\\nToyota-Camry: {1999/2000/2011}\\n\\nwhere the meaning of the years is {made/bought/sold}.\\nWhat preprocessing and supervised learning methods did you use?\\nOn the pre-processing: it was necessary to transfer textual values to the numerical format. I used Perl to do that task. Also, I created secondary synthetic variables by comparing different Prices/Costs. On the supervised learning methods: Neural Nets (CLOP, Matlab) and GBM in R. No other classifiers were user in order to produce my best result.\\nNote that the NNs were used only for the calculation of the weighting coefficient in the blending model. Blending itself was conducted not around the different classifiers, but around the different training datasets with the same classifier. I derived this idea during last few days of the Contest, and it produced very good improvement (in both public and private).\\nWhat was your most important insight into the data?\\nRelations between the prices are much more informative compared to the prices themselves.\\xa0 The next step was to rank and treat the relations in accordance to their importance.\\nWere you surprised by any of your insights?\\nYes, there was a huge jump from 0.26023 to 0.26608 in public, when I included in the model all the differences between Costs/Prices. I expected a jump, but not so big. On another occasion, I created two promising new variables, and thought it will produce some modest improvement at least. Instead, I observed deterioration.\\nWhich tools did you use?\\nPerl, Matlab, NNs in CLOP and GBM in R.\\nDo you have any advice for other Kaggle competitors?\\nBe flexible and patient. Do not worry too much about the LeaderBoard. Try to concentrate on the science and fundamentals, but not on how to win.\\nAnything else that you would like to tell us about the competition?\\nCurrently, I am working on the detailed description of my method, and would like to share an excerpt from the Introduction:\\nSelection bias or overfitting represents a very important and challenging problem. As it was noticed in [1], if the improvement of a quantitativecriterion such as the error rate is the main contribution of a paper, the superiority of a new algorithms should always be demonstrated on independent validation data. In this sense, the importance of the data mining contests is unquestionable. The rapid popularity growth of the data mining challenges demonstrates with confidence that it is the best known way to evaluate different models and systems. Based on our own experience, cross-validation (CV) maybe easily overfit as a consequence of the intensive experiments. Further developments such as nested CV maybe overfitted as well. Besides, they are computationally too expensive [1], and should not be used until it is absolutely necessary, because nested CV may generate secondary serious problems as a result of 1) the dealing with an intense computations, and 2) very complex software (and, consequently, high level of probability to make some mistakes) used for the implementation of the nested CV. Moreover, we do believe that in most of the cases scientific results produced with the nested CV are not reproducible (in the sense of an absolutely fresh data, which were not used prior).\\n[1] Jelizarow, M., Guillemot, V., Tenenhaus, A., Strimmer, K. and Boulesteix, A.-L. (2010) Over-optimism in bioinformatics: an illustration, Bioinformatics, Vol.26, No.16, pp.1990-1998.\", 'Tim Veitch, the 4th prize winner of used car prediction challenge Don\\'t Get Kicked!, catches up with us about finishing in the money on his second Kaggle outing.\\r\\n\\nWhat made you decide to enter?\\nCuriosity, really! \\xa0Kaggle combines two of my favourite things: solving difficult problems and competition.\\xa0 I had a bit of spare time over Christmas, so I thought I\\'d give it a go.\\xa0 I\\'m also hoping to meet some interesting people from the Kaggle community - so feel free to get in touch!\\nWhat was your background prior to entering this challenge?\\nI work in my family\\'s travel-modelling consultancy (Veitch Lister Consulting).\\xa0 My work involves trying to predict the daily travel made by the millions of people living in Australia\\'s urban areas. \\xa0This has exposed me to fairly advanced choice modelling techniques (among them logistic regression), which has proved useful on Kaggle.\\nHave you ever bought a used car?\\nI drive a used car...but I can\\'t say that I bought it. \\xa0It was a \\'hand me down\\' from my Mum...Love You Mum! \\xa0I do, however, feel well qualified to buy a used car thanks to this competition!\\nWhat preprocessing and supervised learning methods did you use?\\nI used logistic regression to begin with. \\xa0This meant constructing ordinal variables from each of the numeric variables (e.g. the odometer), and adding some interesting variable interactions, particularly involving the MMR variables. \\xa0I also found some interesting temporal effects, and included a dummy variable for each month in the dataset. \\xa0I then extended my simple logit model by building \"logit trees\" - ie. binary splits (to a level of 1 or 2), with a logistic regression on each leaf. Late in the process I added two data driven approaches - random forests and GBMs, which used standard packages in R. \\xa0The GBM turned out to be my highest scoring individual model, with the logit forest second.\\nWhat was your most important insight into the data?\\nProbably the temporal effects. \\xa0My basic logit model suggested that the eight months from January to August 2009 were the eight months with lowest \\'kick likelihood\\', all other things being equal. \\xa0I don\\'t yet know the cause, but I think it would be very interesting to investigate why that period was such a good period for buying used cars. \\xa0If I\\'d gotten to the bottom of it, I\\'m sure it would have improved my model, as the effect probably varies spatially. \\xa0And it would certainly help with real life prediction.\\nWere you surprised by any of your insights?\\nI was continually surprised by the variables which proved important: wheel type, the month, or a lack of change in the MMR price (current - acquired). \\xa0It was surprising how relatively unimportant the make, model and vehicle type were.\\nWhich tools did you use?\\nI used my own C++ library for logistic regression, and the standard Random Forest and GBM packages in R (though I did try to implement my own GBM implementation on the last night, which didn\\'t quite work as well as the R version). \\xa0I used the Ruby scripting language to tie it all together, and Excel pivot tables / charts to analyse the data.\\nDo you have any advice for other Kaggle competitors?\\nKaggle has really reinforced to me the importance of cross validation. \\xa0I\\'ve also found getting to know the inner workings of each algorithm very rewarding - it\\'s interesting, and it helps. \\xa0I was surprised by how well GBMs worked...that\\'s a key learning for me. \\xa0And drink lots of coffee...but not too much!\\r\\n\\xa0', 'At long last, we catch up with Matthew Carle, the solo winner of the Claim Prediction Challenge.\\nWhy did you decide to participate in the Claim Prediction Challenge?\\nAs an actuary, I have worked on claims models in the past, and the Claim Prediction Challenge allowed me to see how my modelling skills compare with those of other modelling experts. It also provided a way to improve modelling skills and try new techniques.\\nApart from monetary incentives, did anything else motivate you to participate in the competition?\\nNormally when building predictive models it is not easy to understand how much better your model could be. The ability to rank your models against those of your competitors provides additional insight into how good your modelling skills actually are.\\nHow many entries did you submit? \\xa0What drove you to continue submitting new entries?\\nI made a total of 35 entries. The improvements made by other participants provided the incentive to continue making new submissions, particularly when in competition for the top places.\\nWhat did you enjoy most about the competition?\\nThe ability to compare model performance with those of other modelling experts, and improve modelling skills through testing of different techniques.\\nWhat got you interested in actuarial science?\\nI have always had an interest in mathematics and statistics, and in particular their application to real-world problems. The business context of actuarial science was also appealing.', 'Congratulations to Alec Stephenson, who was recently announced as winner of the FIDE Prize in the Deloitte/FIDE Chess Rating Challenge! This prize was awarded to the submission which was the most promising practical chess rating system (the criteria can be found here). The World Chess Federation (FIDE) has administered the world championship for over 60 years and manages the world chess rating system.\\r\\n\\r\\nHere at Kaggle we\\'re very excited about Alec\\'s achievement. This is a major breakthrough in an area which has been extensively studied by some of the world\\'s best minds. Alec wins a trip to the FIDE meeting to be held in Warsaw this April, where he will present his winning method. The next world chess rating system could be based on his model!\\r\\n\\r\\nWorld chess ratings have always used the Elo system, but in the last few years there has been a movement to make the rating system more dynamic. One approach is to modify the Elo system by adjusting the so-called \\'K-factors\\', which determine how quickly individual match results change the overall rankings. Professor Mark Glickman, chairman of the United States Chess Federation ranking committee, has proposed the Glicko system, which was a key inspiration behind Microsoft\\'s TrueSkill algorithm. Jeff Sonas, with the backing of FIDE, initiated this Kaggle contest to bring in fresh ideas. He says \"of all the things learned during the contest, the one that I am most excited about is the degree to which Alec was able to improve the accuracy of the well-established Glicko model without significantly increasing its complexity.\"\\r\\n\\r\\nWe interviewed Alec after his big win...\\nWhat made you decide to enter?\\nI make a couple of submissions in most competitions and then decide from that point whether my interest is sufficient to spend the time competing seriously. What I liked about the chess competition was that, unlike more traditional data mining competitions, the data was extremely simple, containing just player identifiers and results. This meant that the competition was more theoretical than is usually the case, which benefited me as a mathematician.\\nWhat was your background prior to entering this challenge?\\nMy background is in mathematics and statistics. I am currently an academic, teaching courses in R, SAS and SPSS, and have worked in a number places including The National University of Singapore and Swinburne University in Australia. I will soon be taking a position at CSIRO, Australia\\'s national science agency.\\nWhat preprocessing and supervised learning methods did you use?\\nBecause of the simplicity of the data I took the view that the best approach would be to build upon methods that already exist in the literature. I took the Glicko sytem of Mark Glickman, added a couple of ideas from Yannis Sismanis and then used a data driven approach to inform further modifications. The Glicko system is based on a Bayesian statistical model; I took this and then let predictive performance, rather than statistical theory, determine my final scheme. I suspect my approach is less useful for other types of two-player games as it was essentially optimized for the chess data.\\nWhat was your most important insight?\\nThe most important and suprising thing was how competitive an iteratively updated ratings scheme could be in terms of predictive performance. It got in the top 20 overall, which was a great surprise to me, particularly given that the unrestricted schemes obtained an additional advantage from using future information that would not be applicable in practice.\\nDo you have any advice for other Kaggle competitors?\\nMy three tips are (1) Have a go! Start with some random numbers and progress from there. (2) Concentrate on learning new skills rather than the leaderboard. (3) Beware of anything that takes more than 10 minutes to run.\\nWhich tools did you use?\\nMy usual tool set is R, C, Perl and SQL, but for this competition I just used R with compiled C code incorporated via the .C interface. I\\'m currently working on an R package allowing users to examine different iteratively updated rating schemes for themselves. Hopefully it will also allow me to make my method a bit simpler without losing predictive performance, which may make it more palatable to the FIDE.\\nWhat have you taken away from this competition?\\nAn interest in methods for modelling two-player games, and a motivation to learn how to play chess! It\\'s my second win in public Kaggle competitions, which is a nice personal achievement.', 'This week we catch up with the winners of the Grockit \\'What Do You Know?\\' Competition, which ended on Feb 29th. \\xa0The challenge was to predict which review questions a student would answer correctly when studying for the GMAT, SAT or ACT. Pankaj Mishra placed 3rd, in his first ever Kaggle competition, and offers some great tips for how to get started.\\nWhat was your background prior to entering this challenge?\\nI am a Software Developer with an undergraduate degree in Aeronautics. I learned machine learning from the free Stanford Machine Learning class at ml-class.org and the AI class at ai-class.com. Big thanks to Andrew Ng, Sebastian Thrun, and Peter Norvig for teaching those classes so well!\\nWhat made you decide to enter?\\nI participated to gain experience in machine learning. I was excited to get access to high quality real-world data from Grockit and use it to solve a concrete problem.\\nWhat preprocessing and supervised learning methods did you use?\\nPreprocessing:\\nI used Java to create a training file with one row (training example) for each user who answered five or more questions. Each row corresponds to the last question for a user. I added many columns to capture fraction-of-correct-answers-by-user, question-difficulty, etc.\\nSupervised learning methods:\\nMy solution is a mixture of a Neural Network ensemble and a Gradient Boosting Machine (GBM) ensemble. Both of them were trained on the same training data. The training data included columns that themselves were predicted values from other models such as various Collaborative Filtering models, IRT Model, Rasch Model, and the LMER benchmark. This approach to blending is inspired by the paper \"Collaborative Filtering Applied to Educational Data Mining\"\\xa0 [see Section(3): Blending].\\xa0 None of my individual models scored very well on the leader board, but they did much better when combined together.\\nWhat was your most important insight into the data?\\nThere were some columns in the training data (e.g. number of players for a question) that I did not use in my earlier models because I had thought they would have no effect on whether a user got the question right. I was surprised to discover later that adding them to the model did improve prediction accuracy. So I think the lesson is to not listen to your intuition; let data speak for itself. In practice, though, there is not enough time to try every possible feature, so we do have to go by intuition to a degree.\\nWere you surprised by any of your insights?\\nI was surprised by the low correlation between (1) an individual model’s performance by itself and (2) the amount of performance improvement of an ensemble when the individual model is added to it.\\xa0 As an example, I had some very good individual models that when added to an ensemble barely improved the performance of the ensemble. By contrast, I also had some nearly hopeless models that when added to an ensemble significantly improved the performance of the ensemble.\\nTherefore, we need lots of diverse models in the ensemble for good performance, not necessarily the best performing models. I think that is well known, but I was still surprised to observe it first hand.\\nWhich tools did you use?\\nJava and R.\\nI used Java for pre-processing and building various Collaborative Filtering models and IRT models.\\nI used R\\'s nnet and gbm packages for Neural Networks and Gradient Boosting Machine respectively.\\nWhat have you taken away from this competition?\\n\\nThe most fun way to get better at machine learning is to work really, really hard to win a machine learning competition. When I started in December, the leaderboard had many players with much better score than mine. However, for me, the knowledge that a much better model existed was a strong motivator for finding it, and I spent virtually all my free time researching and looking for better models and techniques.\\nKaggle\\'s forums for various competitions have top quality user generated content containing practical machine learning techniques that one does not find in text-books. I learned at least 2-3 techniques that helped me improve my score.\\nOne can find readily applicable techniques and models in the papers from winners of other competitions. I read almost all the papers from the Netflix challenge, Heritage Health Prize, and KDD Cup 2010. The papers from the winners of \"KDD Cup 2010 Educational Data Mining Challenge\" contain a wealth of information relevant to the Grockit competition.\\n', 'We caught up with all time top-ranked Kaggle competitor, Alexander D\\'yakonov, on his experience with the Grockit \"What Do You Know\" Competition.\\nWhat was your background prior to entering this challenge?\\nI’m an Associate Professor at Moscow State University. Participating in Kaggle challenges is giving me a lot of valuable experience. I write popular scientific lectures about data mining.\\xa0 In the lectures I tell about my experiences. For example,\\xa0 Introduction to Data Mining\\xa0 and Tricks in Data Mining (both in Russian).\\nWhat made you decide to enter?\\nIn the last three competitions, I took the first, third and fourth places.\\xa0 Therefore I looked for a competition to take the second place. :) And I found it!\\nWhat preprocessing and supervised learning methods did you use?\\nMy approach was to reduce this problem to a standard classification problem. I generated feature description of every pair “student – question”. I used pairs from valid_test.csv for tuning the algorithms. Here are some examples of features: an average student score, an average student score today, his time of the answering, the weighed average score (with different weighted schemes), the question difficulty, the question difficulty today, etc. There were also some features from SVD. I also added some linear combinations of the features (which increased performance). I blended GBMs (from R), GLM (from MATLAB) and neural nets (from CLOP library in MATLAB).\\nWhat was your most important insight into the data?\\nNothing, I solved it as a standard classification problem and did not look at the data.\\nWere you surprised by any of your insights?\\nI was surprised that Random Forests were essentially worse than GBMs and didn\\'t increase performance in blending.\\nWhich tools did you use?\\nR and MATLAB (with CLOP library)\\nWhat have you taken away from this competition?\\nI really liked the winner’s method. And I should admit that the method is more effective than my method. But when I solved the problem, I checked a hypothesis that it could be solved as a usual classification problem. I think that my hypothesis has proved to be true.', 'Grockit competition winner, Steffen Rendle, shares his Factorization Machine technique.\\xa0 In his own words, \"The combination of FMs and Bayesian learning was very handy as I didn\\'t had to search for any regularization hyperparameters.\"\\nWhat was your background prior to entering this challenge?\\nI am an assistant professor in computer science at the University of Konstanz.\\nWhat made you decide to enter?\\nI wanted to study factorization machines on a competitive setting and get some empirical evidence that they work well. The Grockit challenge raised my interest because the dataset is of reasonable size (not too small) and has interesting variables.\\nWhat preprocessing and supervised learning methods did you use?\\nI used factorization machines (FM) which is a model class that I have developed over the last years. The model allows one to use feature engineering the same way as most standard machine learning tools (e.g. linear regression or SVMs) but it uses factorized variable interactions. For learning, I extended the Bayesian approach that was developed by Christoph Freudenthaler and me with classification capabilities.\\nI used a simple preprocessing that removes cases (student-question-combinations) that appear very often.\\nWhat was your most important insight into the data?\\nThe most important predictors were the student, question and the time taken for answering a question. I also derived a variable that states how often a student has tried to answer a question before. Besides this I only used the subtrack name and the game type. All other variables didn\\'t help -- at least not in my experiments.\\nWere you surprised by any of your insights?\\nEnsembles didn\\'t work at all. I couldn\\'t find any improvement by ensembling several models -- neither on my validation set, the public or private leaderboard. I didn\\'t expect large improvements as I tried to have strong single models, however I was surprised to find no improvement at all.\\nWhich tools did you use?\\nI used libFM which is an implementation of factorization machines. The software is written in C++ and can be obtained from my website http://www.libfm.org/\\nFor preprocessing, I used Perl.\\nWhat have you taken away from this competition?\\nI think the format of machine learning challenges gives very valuable feedback to researchers. When conducting experiments for a research paper, one cannot compare to so many different approaches as you have in a Kaggle competition.', 'We catch up with Yanir Seroussi, a graduate student in Computer Science, on how he took third place in the ICFHR 2012 - Arabic Writer Identification Competition.\\xa0 After signing up for a Kaggle account over a year ago, he finally decided to give one of the competitions \\'just a quick try\\'.\\xa0 Famous last words...\\nWhat was your background prior to entering this challenge?\\nI\\'m currently in the final phases of my PhD, which is in the areas of natural language processing and user modelling. Even though I address some predictive modelling problems in my thesis, I\\'ve never done any image processing work, though it did help to have some background knowledge in machine learning and statistics.\\nWhat made you decide to enter?\\nI signed up to Kaggle over a year ago but never used my account. Recently, I started thinking about what I want to do once I graduate, and somehow bumped into Phil Brierley\\'s blog. This inspired me to give one of the smaller competitions \"just a quick try\", which ended up consuming a lot of my free time...\\nWhat preprocessing and supervised learning methods did you use?\\nMy\\xa0most successful submission was based on SVMs. I joined the competition quite late and didn\\'t have time to play with blending techniques, which seem to be a key component of many winners\\' approaches.\\nAs to preprocessing, I don\\'t have any prior knowledge about image processing, so I only briefly experimented with one idea that didn\\'t require much knowledge: converting all the images to texts with freely-available OCR software, which is based on the idea that the same OCR errors would appear for the same writers. While using this as a standalone feature yielded some interesting results, it didn\\'t improve accuracy when used in conjunction with the provided features.\\nWhat was your most important insight into the data?\\nAt first I played around with SVMs and the commonly-used kernels (linear, polynomial, RBF and sigmoid). Then I remembered a recent paper that was presented at the ACL conference, about using character histograms for authorship attribution (http://aclweb.org/anthology-new/P/P11/P11-1030.pdf). Since the provided features were given in the form of histograms, I figured that the same techniques would be applicable here. And indeed, using SVMs with a diffusion kernel proved to yield the most significant performance boost.\\nWere you surprised by any of your insights?\\nI wouldn\\'t call it \"surprised\", but I was a bit frustrated by the apparent lack of correlation between cross-validation results on the training data and the accuracy on the validation set. This is probably because each of the 204 writers had only two training paragraphs (all containing the same text), while the test instances were a third paragraph with different content. So any form of cross validation yielded a training subset that was very different from the full training set, and a test subset that obviously couldn\\'t contain the \"third paragraphs\".\\nWhich tools did you use?\\nMostly LibSVM and SVMLight (for some brief experiments with transductive SVMs that didn\\'t go well). I used Python to parse the feature files, run the libraries, and produce the final results.\\nWhat have you taken away from this competition?\\nA better understanding of SVMs and the conclusion that I still have a lot to learn :-)', \"Wayne Zhang, the winner of the ICFHR 2012 - Arabic Writer Identification Competition shares his thoughts on pushing for the frontiers in hand-writing recognition.\\nWhat was your background prior to entering this challenge?\\nI'm pursuing my PhD in pattern recognition and machine learning. I have interests in many problems of this field, such as classification, clustering, semi-supervised learning and generative models.\\nWhat made you decide to enter?\\nTo test my knowledge on real-world problems, to compete with smart people, and to contribute in real-life prediction tasks.\\nWhat preprocessing and supervised learning methods did you use?\\nI used the provided features. The writer identification problem is a multi-class classification problem, and linear discriminant analysis is suitable for this task.\\nWhat was your most important insight into the data?\\nBoth the training and test set are of a small size, I had to be careful about the generalization ability of the model.\\nWhich tools did you use?\\nI used LDA, which was popular and successful in face recognition ten years ago. It appeared to have surprisingly good results on writer identification, possibly because the two tasks are similar. I implemented my code in Matlab, because of its superior matrix computation support.\\nWhat have you taken away from this competition?\\nTo work on real-world problems, you had to be careful about the overfitting problem. It is different from academic research. In real problems we need to consider many details to make a perfect system. One challenge of Kaggle competitions is that the discrepancy between the public and private scores. It makes me consider more about what the situation will be like in real world. You always have limited training data and validation data, but the test data usually are unbounded.\\xa0 How to generalize your model to the unbounded data could be a problem.\", \"We catch up with Ben Hamner, a data scientist at Kaggle, after he won Kaggle's Air Quality Prediction Hackathon. As a Kaggle employee, he is ineligible for prizes.\\nWhat was your background prior to entering this challenge?\\nI graduated from Duke University in 2010 with a bachelors in biomedical engineering, electrical and computer engineering, and mathematics. For the next year, I applied machine learning to improve non-invasive brain-computer interfaces as a Whitaker Fellow at EPFL. On the side, I participated in or won a number of machine learning competitions. Since November 2011, I have designed and structured a variety of competitions as a Kaggle data scientist.\\nWhat made you decide to enter?\\nI was hanging out at Splunk (one of the SF venues hosting the hackathon). Anthony asked me some questions about extracting features from the data, which prompted me to open it up and look at it in the afternoon.\\nWhat preprocessing and supervised learning methods did you use?\\nI took the lagging N components from the full time series (N=8 for the winning submission, which was selected arbitrarily) as features, then each of the 10 prediction times and 39 pollutant measures as targets. I then trained 390 Random Forests over the entire training data, one for each predicted offset time-pollutant combination. The Random Forest parameters were selected so that the models would be quick to train. The code for creating the winning model is available here.\\nSome straightforward approaches to improving this model include\\n\\nOptimizing the parameters for model performance as opposed to training time.\\nDirectly optimizing for the error metric (mean absolute error) instead of RMSE.\\nUsing a data-driven approach to select the number of previous time points to include.\\n\\nWhat was your most important insight into the data?\\nI don’t believe I had any specific insights on the data - I barely looked at it before training the model.\\nWere you surprised by any of your insights?\\nI was surprised that domain insight wasn’t necessary to win the hackathon. Key insights have been crucial in many of our longer-running competitions.\\nWhich tools did you use?\\nOnce I decided to fiddle with the data, I asked David (a fellow Kaggle data scientist) to pick a random number between one and three. He picked two, and I used MATLAB. (If he said one I would have used R, and three would have been Python).\\nWhat have you taken away from this competition?\\nTaking all the features and chucking them into a Random Forest works surprisingly well on a variety of real-world problems. This is demonstrated more empirically in this paper. I'm very interested in domains such as CV and NLP where this doesn't hold true, or where the problem can't be simply formulated in the standard supervised machine learning framework.\\nWhat did you think of the 24 hour hackathon format?\\nIt was a lot of fun! I especially enjoyed seeing Kagglers in venues all over the world collaborating and competing on this problem. I'm curious to see how much better the results would be if we ran this as a standard competition over a couple months, and whether the work in the first day would comprise the majority of the improvement over the benchmark.\", \"The EMC Data Science Global Hackathon prize was awarded to James Petterson.\\xa0 Check out his webpage for a more detailed description and the source code: http://users.cecs.anu.edu.au/~jpetterson/\\nWhat was your background prior to entering this challenge?\\nI am currently finishing my PhD in machine learning at ANU. Before that I worked as a software engineer for the telecom industry for many years.\\nWhat made you decide to enter?\\nThe challenge of kaggle competitions always attracted me - I took part in two other ones in the past (What Do You Know and Heritage Health Prize). I was abstaining from entering new ones as I know how time consuming this can be, but when I heard about this 24h one I couldn't resist.\\nWhat preprocessing and supervised learning methods did you use?\\nI computed a set of training instances based on:\\n\\n- mean of all variables for each prediction time\\n- mean of all variables for each prediction time and chunkID\\n- most recent value of all variables for each chunkID\\n\\nI did some bootstrapping to increase the size and variety of the training data, using a 24-hour moving window. I then trained 390 Generalised Boosted Regression models, one for each combination of target variable and prediction time.\\nWhat was your most important insight into the data?\\nI didn't spent much time looking at the data, so I can't think of any particular insight.\\nWere you surprised by any of your insights?\\nI was surprised that I had a good result without spending much time trying to understand the data. I suspect that wouldn't be the case in a longer competition, though.\\nWhich tools did you use?\\nOnly R.\\nWhat have you taken away from this competition?\\nI saw once again how powerful boosting methods are. Even though this was essentially a time series problem, a standard boosting regression method performed quite well.\\nWhat did you think of the 24-hour hackathon format?\\nNormally competitions take 3 months or more, which tends to favour those that can spend more time on them. The 24-hour format was great in the sense that it gave a chance to those that are more time constrained. And, of course, it was a lot of fun!\\r\\nI hope we will have more of these in the future.\\nPhoto by\\xa0ninahale\", 'For the first of our interviews with top finishers in the Hewlett Automated Essay Scoring Challenge, we catch up with 6th place finisher and polymath Martin O\\'Leary (@mewo2).\\xa0 You can also check out his blog at\\xa0 http://mewo2.github.com/\\nWhat was your background prior to entering this challenge?\\nI\\'m a mathematician turned glaciologist, working as a research fellow at the University of Michigan. I\\'ve been involved with Kaggle for about a year now, and have had a few good finishes. I have a habit of doing well in the early part of competitions, which has got me some publicity, but doesn\\'t translate well into final results.\\nI\\'ve always had an interest in linguistics (at one point I considered it as a career), but this was the most serious text mining I\\'ve ever done.\\n\\nWhat made you decide to enter?\\nMomchil Georgiev. He approached me early on about possibly collaborating, and we decided to produce individual entries first. Somehow we never got around to teaming up, and by the end he\\'d assembled a big enough team that I decided I\\'d rather try for a solo run than try to merge. I feel a little bit like Pete Best, who left the Beatles before they became famous.\\nMore seriously, I liked the problem because it\\'s an interesting dataset, and a problem which comes down to a lot more than just number-crunching.\\nWhat preprocessing and supervised learning methods did you use?\\nA lot of the difficulty in this problem was in finding meaningful features in the essays. I spent a lot of time on topic modelling, and looking at distributions of syntactic features. For the final prediction, I used a fairly large ensemble of different methods. Some of the essay sets worked better with boosted approaches, while others were more susceptible to neural nets.\\nWhat was your most important insight into the data?\\nThe choice of error metric is really important! Most algorithms are tuned to a particular notion of error, and it helps a lot to tweak things so that you\\'re actually optimising for your target metric. In this case that meant some customisation, as the quadratic kappa used is a little unusual.\\nWere you surprised by any of your insights?\\nI was quite surprised how little measures of spelling and grammar \"correctness\" mattered. Except in one case where the grading rubric explicitly mentioned it, they didn\\'t seem to matter much at all. It warms my descriptivist heart to see that teachers are grading on more than just who can use a spellchecker and a semicolon.\\nWhich tools did you use?\\nI started out using just R, but introduced Python fairly quickly because of its stronger NLP libraries. There\\'s a good reason that NLTK is popular. I recycled a lot of old R code for various tasks, and used a mixture of custom and pre-packaged models.\\nWhat have you taken away from this competition?\\nThe benefits of multiple approaches. I think the winning teams did so well because they were able to combine several independently created models. Also, you can\\'t take a month off from a competition and expect to still be winning when you get back.', 'We catch up with Alfonso Nieto-Castanon, the winner of Round 1 of the CHALEARN Gesture Challenge.\\xa0 This fascinating series of 4 competitions revolves around gesture and sign language recognition using a Microsoft Kinect camera.\\xa0\\xa0 A must-read for anyone planning to throw their hat in the ring for CHALEARN Round 2.\\nWhat was your background prior to entering this challenge?\\nMy background is on computational neuroscience (Ph.D. Cognitive and Neural Systems, Boston University) and engineering (B.S./M.S. Telecommunication Engineering, Universidad de Valladolid). I work freelance as a research consultant and my latest projects range from development of functional connectivity MRI software and analysis methods, to brain computer interfaces for speech restoration in subjects with locked-in syndrome.\\nWhat made you decide to enter?\\nThe Chalearn dataset and goals were too interesting to pass up. I just had to give it a try.\\n\\nWhat preprocessing and supervised learning methods did you use?\\nI did not implement any learning strategy but used instead a combination of ad hoc features from the depth videos (somewhat inspired by neural processes in the visual system) with a Bayesian network model for recognition.\\nWhat was your most important insight into the data?\\nThinking of gestures as a form of communication, and realizing that the subjects in those videos were already doing what they thought would work best in order for us to interpret and recognize those gestures correctly. I imagined that a system that would mimic the specificities of the human visual system would be most likely to pick up those helpful cues from the video sequences correctly.\\nWhich tools did you use?\\nI used Matlab (just the matlab base set, no specific toolboxes other than the nice set of functions provided by the contest organizers to browse the data and create a sample submission)\\nWhat have you taken away from this competition?\\nI enjoy developing problem-specific algorithms rather than using a combination of off-the-shelf procedures. This contest gave me the chance to do just that while working in one of those (few) areas where humans still outperform machines (and\\xa0I am curious to see if we can further bridge that gap!)', \"3 top competitors, who met during Kaggle's first ever private competition, teamed up to win the public Boehringer Ingelheim Predicting a Biological Response competition.\\xa0 Team 'Winter is Coming' ( Jeremy Achin and Tom DeGodoy, props for the name) joined forces with Sergey Yurgenson, exchanging 349 emails over 45 days, to build their winning bioresponse model.\\n\\nWhat was your background prior to entering this challenge?\\nTom and I met while we were both studying Math and Physics at the University of Massachusetts at Lowell and have been friends and colleagues ever since. We both have 7+ years experience doing applied predictive analytics. Most recently we were both Directors of Research and Development at Travelers Insurance (Tom on the Business Insurance side and me on the Personal Insurance side). A couple months ago, we decided to quit our jobs to start our own Data Science company. We plan to use winnings from private Kaggle competitions as our initial source of seed funding.\\nWe met Sergey while competing against him in Kaggle’s first ever invite-only private competition. He has a background in Physics and currently works for Harvard Medical School. The Boehringer Ingelheim contest was his 8th Kaggle competition.\\nI think it's safe to say that all 3 of us are obsessed with Data Science to an extent that I’m not sure is healthy.\\nWhat made you decide to enter?\\nTom and I were looking for our next competition and heard that Sergey was looking for teammates for this problem. We thought this contest had the potential to be big in terms of the quantity and quality of competitors (which definitely turned out to be the case). Sergey’s reason for entering the competition is analogous to the reason fish “decide” to swim--it’s required for survival. :) In Sergey’s words, he “just cannot stop competing.”\\nWhat preprocessing and supervised learning methods did you use?\\nWe used Random Forests for feature ranking & selection. Methodologies explored for various roles included Random Forests, GBM, KNN, Neural Nets, Elastic Net, GLM, GAM, and SVM. Simple transformations & splines were used for some models.\\nWhat was your most important insight into the data?\\nI would say that the most important insight into the data was obtaining an accurate ranking of the relative importance of the variables. Eliminating variables reduced model training time (allowing us to try more things) and improved performance considerably.\\nAnother important insight was recognizing the danger of overfitting inherent in this problem. This led us to design a testing framework that helped to ensure we didn’t overfit the public leaderboard.\\nWere you surprised by any of your insights?\\nBy plotting the data points using the 2nd and 3rd principal components, you can see 4 very separated clusters of points. We thought this was going to be a very important finding, but it didn’t turn out to help us.\\nWhich tools did you use?\\nR & Matlab. Rstudio Server is awesome if you are using multiple computers--I love having a browser open with many tabs each interfacing with a different RStudio Server.\\nWhat have you taken away from this competition?\\nFirst and foremost, having a team with diversity and relentless tenacity is extremely important. I’ll quote Shea Parkes because I couldn’t possibly phrase it any better: “It’s quite obvious how much an ensemble of viewpoints can contribute above and beyond an ensemble of algorithms.”\\nThe ability to collaborate effectively is critical. We exchanged 349 emails over the 45 days that we worked on the competition, and we were in sync enough to be able to share, test, and improve on each other’s work. At one point, Sergey took over 17,000 files we generated using R and combined them with his own results in Matlab allowing us to reliably test out many different blends.\\nAlso, if you are going to run long jobs (20+ hours), make sure to put a sign up in your house to remind people not to plug an iron into the same circuit your computers are on. I went into shock when all of a sudden the monitors went black and the computers went silent. It was like someone yanked the Matrix data probe out of the back of my head.\\n\", 'Before we dive in to the slew of interviews with the winners of the many recently finished contests, we take a sec to catch up with Vik P. and Sergey\\xa0 E., the 3rd place team from the Benchmark Bond Trade Price Challenge back in May.\\xa0 What do you get when you combine an American diplomat with a Russian physicist?\\xa0 Read more to find out.\\nWhat was your background prior to entering this challenge?\\nVik: I have a bit of a strange background for a Kaggle participant. I was actually a member of the U.S. Foreign Service 8 months ago, and was serving as a diplomat in South America. I have also worked in operations management, and have a degree in American History.\\nSergey: I graduated from Moscow Institute of Physics and Technology in 2005 with MS degree in applied physics and mathematics. Next years I’ve been working as a senior software engineer in several different companies in Russia. I’ve always been interested in AI and it’s application for different type of real life problems.\\n\\nWhat made you decide to enter?\\nVik: I had previously entered the algorithmic trading challenge on Kaggle, and was eager to apply some of what I had learned to a problem with some clear similarities. After I entered, the fact that the data had a lot of angles and a lot of potential for score improvement kept me motivated.\\nSergey: I’ve been studying machine learning for a while and wanted to test my skills. This is my first kaggle contest and I really enjoyed it.\\nWhat preprocessing and supervised learning methods did you use?\\nVik: I actually did not do any real preprocessing. The missing values for some of the previous trades could have been dealt with using preprocessing, but I dealt with them in feature extraction by using average values over the whole 10 trade range and the most recent 5 trade range.\\nAs far as the learning algorithms, I used 2 slightly different random forest models, which I blended using stacked generalization. Due to time constraints, which prevented me from fully exploring parameter tuning, I was unable to get good results when I blended in GBM and linear models.\\nSergey: I’ve constructed a lot of features manually. The most important idea was data normalization. I tried to get rid of absolute values such as dollar prices. Then I simply used random forest implementation in R to generate predictions.\\nWhat was your most important insight into the data?\\nVik: My most important insight was probably how irregular the distribution of the target variable is. Various normalization methods really helped in terms of producing a stronger model. The maximum size of the terminal nodes for the random forest was also an important parameter. As a lot of the rows were related due to how the time series data was constructed, setting it too low meant that extremely similar trades were split into different nodes, which actually seemed to reduce accuracy.\\nSergey: It was all about how good you construct your features. I spend too much time re-implementingrandom forest in Java specifically for this problem but it didn’t give me any advantage. While some simple features build in a couple of minutes lead to significant improvement.\\nWere you surprised by any of your insights?\\nVik: I was very surprised by how different the prices were when dealers dealt with each other versus when dealers dealt with customers. The type of trade (dealer to dealer versus customer buy versus customer sell) was actually the single most important variable to the random forest. Although it certainly is to be expected, seeing it illustrated so starkly was interesting.\\nSergey: Then I split data by trade_type and is_callable I got unexpectedly big gain in prediction performance.\\nWhich tools did you use?\\nVik: I solely used R.\\nSergey: R for prediction and Java for feature extraction.\\nWhat have you taken away from this competition?\\nVik: The key takeaway that I took from this competition was to not get bogged down with any one idea for too long. These competitions take a lot of creativity, and with that creativity can come fixation on one concept, and the need to get it to work. In the algorithmic trading competition that I participated in previously, I got fixated on the idea of using a linear model, and never really wavered from that fixation. Here, I iterated through a lot of possibilities, used what worked, and didn’t get stuck in the minutiae.\\nSergey: Being in team is of crucial importance. Wish we joined our efforts a bit earlier.\\nphoto by kate hiscock', \"What\\xa0was\\xa0your\\xa0background\\xa0prior\\xa0to\\xa0entering\\xa0this\\xa0competition?\\nWe\\xa0are\\xa0a\\xa0team\\xa0focused\\xa0on\\xa0data\\xa0mining\\xa0from\\xa0Shanda\\xa0Innovations,\\xa0a\\xa0tech\\xa0incubator\\xa0of\\xa0Shanda\\xa0Corporation\\xa0from\\xa0China.\\xa0It’s\\xa0a\\xa0global\\xa0leading\\xa0interactive\\xa0entertainment\\xa0media\\xa0group\\xa0.\\xa0We\\xa0all\\xa0graduated\\xa0from\\xa0top\\xa0tier\\xa0universities\\xa0in\\xa0China,\\xa0majored\\xa0in\\xa0Computer\\xa0Science,\\xa0and\\xa0then\\xa0started\\xa0our\\xa0career\\xa0in\\xa0Chinese\\xa0IT\\xa0companies.\\xa0Right\\xa0before\\xa0the\\xa0EMI\\xa0Competition,\\xa0we\\xa0was\\xa0awarded\\xa0second\\xa0place\\xa0in\\xa0ACM\\xa0KDD-Cup\\xa02012.\\n\\nWas your strategy any different for competing in a 24-hour hackathon vs. the longer running KDD cup? \\xa0Any advice for future hackathon participants on how to win the 'sprint' rather than the 'marathon'?\\n\\nOur strategy for the longer running KDD-Cup and the 24-hour hackathon was very different. The 24 hour hackathon is a very intensive competition, so that what the participants need to do is to find the key features in a much faster way. This means the participants need to take simpler and more effective methods for preprocessing and post-processing. The hackathon is more demanding with getting quick reaction and making the right priorities. Given a chance, we will give the above advice for future participants.\\n\\nThe KDD-Cup lasting for several months is more demanding with the continuous concentration as well as better endurance of participants. Better skills in time management skill as well as project management are also necessary in this longer competition. Last but not least, participants also need to take care of each other’s motivation for people tend to lose their eagerness to participate when the time goes by and it is hard to always keep a strong motivation.\\n\\nWhat\\xa0made\\xa0you\\xa0decide\\xa0to\\xa0enter?\\nWe\\xa0would\\xa0love\\xa0to\\xa0put\\xa0ourselves\\xa0on\\xa0the\\xa0international\\xa0stage\\xa0and\\xa0to\\xa0compete\\xa0as\\xa0well\\xa0as\\xa0to\\xa0share\\xa0our\\xa0knowledge\\xa0of\\xa0data\\xa0mining\\xa0with\\xa0peers\\xa0from\\xa0all\\xa0over\\xa0the\\xa0world.\\xa0We\\xa0believe\\xa0participating\\xa0in\\xa0this\\xa0competition\\xa0would\\xa0be\\xa0a\\xa0precious\\xa0opportunity\\xa0to\\xa0“meet”\\xa0all\\xa0the\\xa0talents\\xa0in\\xa0this\\xa0field.\\xa0In\\xa0addition,\\xa0the\\xa0competition\\xa0was\\xa0very\\xa0exciting\\xa0and\\xa0challenging.\\xa0Given\\xa0a\\xa0tight\\xa0time\\xa0constraint\\xa0of\\xa0the\\xa0competition,\\xa0we\\xa0viewed\\xa0it\\xa0as\\xa0a\\xa0chance\\xa0for\\xa0overnight\\xa0team\\xa0building.\\nWhat\\xa0preprocessing\\xa0and\\xa0machine\\xa0learning\\xa0methods\\xa0did\\xa0you\\xa0use?\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nSome\\xa0preprocessing\\xa0was\\xa0given\\xa0to\\xa0Words.txt.\\xa0We\\xa0mapped\\xa0the\\xa0words\\xa0that\\xa0users\\xa0chose\\xa0to\\xa0describe\\xa0artists\\xa0to\\xa0some\\xa0keyword\\xa0IDs\\xa0and\\xa0used\\xa0these\\xa0IDs\\xa0in\\xa0the\\xa0logistic\\xa0regression\\xa0model,\\xa0which\\xa0greatly\\xa0improved\\xa0the\\xa0performance.\\nThe\\xa0main\\xa0machine\\xa0learning\\xa0methods\\xa0were\\xa0SVD++\\xa0and\\xa0Logistic\\xa0Regression.\\xa0\\nWhat\\xa0was\\xa0your\\xa0most\\xa0important\\xa0insight\\xa0into\\xa0the\\xa0data?\\nFor\\xa0most\\xa0users\\xa0the\\xa0training\\xa0data\\xa0was\\xa0very\\xa0sparse.\\xa0Therefore,\\xa0we\\xa0should\\xa0integrate\\xa0more\\xa0features\\xa0from\\xa0other\\xa0aspects.\\xa0For\\xa0instance,\\xa0the\\xa0user\\xa0profiles,\\xa0words\\xa0they\\xa0chose,\\xa0and\\xa0survey\\xa0results\\xa0would\\xa0be\\xa0very\\xa0valuable.\\nSometimes\\xa0it\\xa0is\\xa0easy\\xa0for\\xa0us\\xa0to\\xa0trap\\xa0in\\xa0a\\xa0fixed\\xa0mindset\\xa0and\\xa0may\\xa0ignore\\xa0some\\xa0potential\\xa0important\\xa0indicators.\\xa0Keeping\\xa0our\\xa0mind\\xa0open\\xa0is\\xa0easier\\xa0said\\xa0than\\xa0done.\\nWere\\xa0you\\xa0surprised\\xa0by\\xa0any\\xa0of\\xa0your\\xa0insights?\\nWe\\xa0were\\xa0very\\xa0surprised\\xa0to\\xa0find\\xa0that\\xa0the\\xa0variation\\xa0of\\xa0the\\xa0track\\xa0scores\\xa0given\\xa0by\\xa0different\\xa0people\\xa0was\\xa0a\\xa0lot\\xa0more\\xa0than\\xa0we\\xa0expected.\\xa0For\\xa0instance,\\xa0User\\xa0ID\\xa041072\\xa0scored\\xa0100\\xa0to\\xa0track\\xa0156\\xa0whereas\\xa0User\\xa0ID\\xa041286\\xa0gave\\xa0merely\\xa04\\xa0to\\xa0the\\xa0same\\xa0track!\\xa0It\\xa0was\\xa0very\\xa0interesting\\xa0to\\xa0find\\xa0that\\xa0people\\xa0were\\xa0so\\xa0different\\xa0in\\xa0music\\xa0preference\\xa0and\\xa0we\\xa0believed\\xa0that\\xa0was\\xa0why\\xa0so\\xa0many\\xa0different\\xa0types\\xa0of\\xa0music\\xa0existed.\\xa0By\\xa0making\\xa0a\\xa0further\\xa0in-depth\\xa0data\\xa0analysis\\xa0we\\xa0may\\xa0discover\\xa0more\\xa0on\\xa0the\\xa0music\\xa0interests\\xa0of\\xa0people.\\xa0\\nWhich\\xa0tools\\xa0and\\xa0programming\\xa0language\\xa0did\\xa0you\\xa0use?\\nThe\\xa0programming\\xa0languages\\xa0include\\xa0C++\\xa0and\\xa0Python.\\xa0We\\xa0would\\xa0like\\xa0to\\xa0express\\xa0our\\xa0gratitude\\xa0to\\xa0APEX\\xa0team,\\xa0the\\xa0author\\xa0of\\xa0an\\xa0open\\xa0source\\xa0toolkit\\xa0called\\xa0SVDFeature\\xa0used\\xa0in\\xa0our\\xa0solution.\\nWhat\\xa0have\\xa0you\\xa0taken\\xa0away\\xa0from\\xa0this\\xa0competition?\\nWe\\xa0had\\xa0a\\xa0lot\\xa0of\\xa0fun\\xa0and\\xa0our\\xa0team\\xa0became\\xa0more\\xa0united\\xa0.\\xa0What\\xa0is\\xa0more\\xa0important\\xa0is\\xa0that\\xa0we\\xa0came\\xa0to\\xa0think\\xa0about\\xa0a\\xa0broader\\xa0area\\xa0of\\xa0data\\xa0mining\\xa0application.\\xa0It\\xa0is\\xa0interesting\\xa0to\\xa0see\\xa0our\\xa0technology\\xa0being\\xa0used\\xa0in\\xa0other\\xa0industries,\\xa0for\\xa0example\\xa0the\\xa0music\\xa0and\\xa0entertainment\\xa0industry\\xa0this\\xa0time.\\xa0This\\xa0is\\xa0an\\xa0eye-opening\\xa0experience\\xa0that\\xa0brought\\xa0a\\xa0lot\\xa0of\\xa0sparkles\\xa0to\\xa0our\\xa0routine\\xa0work.\\xa0\", \"We check in with the 1st, 2nd, and 3rd place teams in the Practice Fusion Diabetes Classification Challenge\\xa0( based on\\xa0Shea Parkes' top voted\\xa0submission in the Prospect round). \\xa0As an experiment, we've decided to group all the winners interviews together in one post to really highlight the diversity of backgrounds among successful data scientists.\\nWhat are your backgrounds prior to entering this competition?\\n1st place:\\xa0Jose Antonio Guerrero\\xa0aka 'blind ape',\\xa0Sevilla, Spain:\\xa0My degrees are in mathematics, statistics and operations research. I’m worked in the public health sector for 25 years as researcher, IT technician and senior manager. \\xa0A year ago, when I turned 50, decided it was a good age for return at\\xa0my professional\\xa0origin, so I went to Virgen del Rocio Universitary\\xa0Hospital, the flagship hospital in the region.\\xa0I'm working with large size (8 figures) clinic records databases, grouping\\xa0clinical cases and with quality and research issues\\r\\n\\r\\n2nd:\\xa0Matt Berseth\\xa0aka 'mtb', Jacksonville, FL, USA:\\xa0I have a Bachelor's degree in computer science and a Master's degree in software engineering - both from North Dakota State University. I started my career as an intern with Microsoft and worked there full time for three years. Since leaving Microsoft, I have been working as a full stack developer with primarily Microsoft technologies for the last ten years. I have been fortunate to work in a variety of interesting areas including: automotive marketing, transportation management / logistics and healthcare IT.\\r\\n\\r\\n3rd:\\xa0Shashi Godbole\\xa0aka 'An apple a day',\\xa0Mumbai, India: Data mining has been the focus of my work for the past six odd years. Earlier this year, I started a consulting firm with a friend from my alma mater. I had worked on a few other Kaggle competitions in the past (including the Heritage Health Prize which is still going on) but had done it mostly for fun. These earlier competitions allowed me to brush up my skills and learn the latest advances in machine learning. I also turned to R as my primary tool for analysis.\\r\\n\\r\\nWhat made you decide to enter?\\nJose:\\xa0My experience in health sector. I’m used to clinical databases.\\r\\n\\r\\nMatt:\\xa0I studied machine learning as an undergraduate, but that was over ten years ago. So last fall when Coursera launched their machine learning course I decided to take the opportunity to get back up to speed. I enjoyed the course and took Daphane Koller's graphical models course this spring as a follow-up. With all of that theory under my belt, I decided I should apply it to something tangible like one of the Kaggle competitions.\\r\\n\\r\\nShashi:\\xa0Healthcare is one of the core focus areas of my team at the consulting firm we are building. The problem statement of this particular competition resonated strongly with the kind of problems we are looking to solve for healthcare providers, payers and other stakeholders.\\r\\n\\r\\nWhat preprocessing and supervised learning methods did you use?\\nJose:\\xa0Main work was data cleaning and feature creation. Grouping diagnostics and actives principles was crucial.\\xa0As an advance of my solution, I based it in a hard preprocessing and\\xa0feature creation work:\\r\\n\\nTranslating each medication to its active principles, route of\\xa0administration.\\nGrouping principles\\xa0active by chemical families / clinical indication. In some cases, as statins,\\xa0adjusting dose equivalences. Choosing for each group between number of\\xa0prescriptions, dose or binary flag for feature creation.\\nGrouping the diagnoses in base CCS and my personal experience.\\nAnd much\\xa0more...\\n\\r\\nAfter, the methods used were the well known gbm and randomforest and later stacking in a generalized additive model.\\r\\n\\r\\nMatt:\\xa0I spent a fair amount of time generating features from the ICD9, NDC and lab data. I used wikipedia heavily to learn more about diabetes and create features from the diagnoses and treatments that are related to diabetes.\\r\\n\\r\\nAll of the models I selected for my final submissions were boosted trees. I used anywhere from 5 to 13 different models and blended/combined their predictions to create my submissions.\\r\\n\\r\\nShashi:\\xa0The preprocessing was limited to missing value imputation for a few fields in the data tables. I used random forests, gradient boosting and neural networks to build several models which were finally stacked together to generate a final solution.\\r\\n\\r\\nWhat was your most important insight into the data?\\nJose:\\xa0The great numbers of comorbidities and symptoms associated with diabetes.\\r\\n\\r\\nMatt:\\xa0The ICD9 data is rich. There is information in the hierarchy of the codes (i.e. what level a specific code belongs to), information regarding the health of the individuals family (i.e. any of the 'family history of' codes) and information regarding the individuals behavior (i.e. any of the 'history of non-compliance' codes). And of course the conditions that are associated with each of the codes.\\r\\n\\r\\nFor the first month or so of the competition I focused almost solely on feature generation. And a majority of these features were derived from the ICD9 codes.\\r\\n\\r\\nShashi:\\xa0One big insight was that the formats for some key fields were different in train and test datasets. This was causing a much larger error on test data than that on training data. I could not think of any explanation for this for quite a while. Fixing the formatting discrepancy resolved this issue and made the train and test errors consistent.\\r\\n\\r\\nWhere you surprised by any of your insights or any key features?\\nJose:\\xa0I’m surprised by gender overall impact. In the data, diabetes was much more prevalent in male (15%) than female (11%) but when fitting the model, the gender influence fell to 0.17%. Probably other comorbidities associated to gender would explain this reduction.\\r\\n\\r\\nMatt:\\xa0I was most surprised by the area's that I did not find interesting features. I did not find the lab data very useful and I thought the drug information would be more useful than it was. That surprised me. I would be interested in seeing what features the other competitors found in this area.\\r\\n\\r\\nShashi:\\xa0I created several new features by taking ratios of different pairs of features. I was surprised by the extent to which these features improved my model. I created these features towards the very end of the competition. It helped me jump up a couple of places on the leaderboard.\\r\\n\\r\\nWhich tools did you use?\\nJose:\\xa0R\\r\\n\\r\\nMatt:\\xa0\\n\\n.Net / C# for writing the logic that generates the features\\nSQL Server for storing the data as well as basic analysis (just sql queries from management studio)\\nPython and scikit-learn for training, testing and evaluating the models\\nR for graphical analysis (ggplot2)\\n\\nShashi: \\xa0I used only R to do all the data processing and the modeling. Excel was used just a little bit to do some quick plots on the data.\\r\\n\\r\\nWhat have you taken away from this competition?\\nJose:\\xa0I learned a lot about active principles and their interactions with diabetes.\\r\\n\\r\\nMatt:\\xa0Trust your cross validation scores and use the public leaderboard as a measurement of competitiveness. When I selected my final models for submission, I picked the models with the lowest cross-validation scores, not the ones with the lowest public leaderboard scores. This worked well for me in this competition, using this formula, I ended up picking my five best models.\\r\\n\\r\\nShashi:\\xa0The most important learning was that feature engineering is of utmost importance. Perhaps even more than any fine-tuning of modeling algorithms. Allocating a lot of time to just review the data and create useful features can result in significant performance improvement.\", \"We catch up with Indy Actuary Shea Parkes\\xa0on his prize-winning Word Tornado entry to the Practice Fusion Open Challenge. \\xa0Shea also had the winning entry to the prospect phase of the predictive challenge, which was the source of the Practice Fusion Diabetes Classification contest (in which he placed 5th with NSchneider). \\xa0These dudes know their healthcare data.\\nWhat was your background prior to entering this competition?\\r\\n\\r\\nI'm a health actuary with Milliman, Inc. I do some traditional services\\xa0like pricing and reserving, but I also focus on applied statistics and\\xa0statistical graphing. I have worked with EHR data for some client projects\\xa0before. Neil Schneider and I have teamed up on many of the Predictive\\xa0Modeling contests on Kaggle over the last couple years. You'll find us\\xa0consistently just outside the money and loitering in the forums.\\r\\n\\r\\nWhat made you decide to enter?\\r\\n\\r\\nI enjoy making visualizations. I don't have a lot of experience with\\xa0data-heavy dashboards, but I constantly make graphs to tell stories. As a\\xa0part of any Predictive Modeling contest I enter, I create mountains of\\xa0visualizations. I don't really believe a result until I can see the\\xa0result. Given this, I enjoyed the novelty of a contest just about\\xa0visualization.\\r\\n\\r\\nWhat preprocessing methods did you use to study the data?\\r\\n\\r\\nMostly I dealt with flattening the data to one observation per patient. I\\xa0focused on Age/Gender, Diagnosis, Medications, Smoking Status and some of\\xa0the Biometrics. For the Diagnosis and Medication prevalence I did some\\xa0Bayesian shrinkage since some patients were seen so rarely I did not\\xa0believe their complete health status had been captured. I used non-linear\\xa0expansions of the continuous variables since linear assumptions are tenuous\\xa0at best with real data. My entry goes into more details about the\\xa0particular form of dimensionality reduction I applied next. Lastly, I did\\xa0a few supervised learning steps to make the Data.gov mash-up.\\r\\n\\r\\nHow did you decide what aspects of the data to use?\\r\\n\\r\\nI focused on the portions that could tell a good story of redundancy. I\\xa0also avoided the portions that would require heavy imputation. I initially\\xa0chased down physician specialty, but there was too little variation to be\\xa0interesting in an unsupervised environment. For the Data.gov portion I\\xa0focused mainly on state-level information since that was the most likely\\xa0shared key. EHR data is commonly sparse in socioeconomic information, so I\\xa0chose income data.\\r\\n\\r\\nWere you surprised by any of your insights or any key features?\\r\\n\\r\\nI was surprised at how strong the Smoking Status was in the principal\\xa0component analysis. I expected Age/Gender to drive the major data\\xa0divisions, but Smoking Status was as important if not more so. Smokers\\xa0just utilize a very different set of services than non-smokers.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nI did all of my analysis in R (http://cran.r-project.org/). I cite all of\\xa0the great packages I used in my report. It was the first time I had tried\\xa0flattening a relational database via R, and it went much smoother than I\\xa0expected. It was also the first time using the Markdown language, which\\xa0was also surprisingly easy.\\r\\n\\r\\nWhat have you taken away from this analysis\\r\\n\\r\\nA much better understanding of the Markdown language and the RStudio /Knitr implementation in particular. I will definitely be using that in my\\xa0consulting work going forward.\", 'Crossposted from Overkill Analytics, the newly launched extra-curricular data science blog by Gigaom-Wordpress Challenge winner Carter S.\\xa0 You can also read more about his \\'overkill\\' philosophy on Gigaom.\\r\\n\\r\\nI’d like to start this blog by discussing my first\\xa0Kaggle\\xa0data science competition – specifically, the “GigaOM WordPress Challenge”. \\xa0\\xa0This was a competition to design a recommendation engine for WordPress blog users; i.e. predict which blog posts a WordPress user would ‘like’ based on prior user activity and blog content.\\xa0\\xa0 This \\xa0post will focus on how my engine used the WordPress social graph to find candidate blogs that were not in the user’s direct ‘like history’ but were central in their ‘like network.’\\r\\nMy Approach\\r\\nMy general approach – consistent with my\\xa0overkill analytics\\xa0philosophy – was to abandon any notions of elegance and instead blindly throw multiple tactics at the problem. \\xa0 In practical terms, this means I hastily\\xa0wrote ugly Python scripts to create data features, and I used oversized RAM and CPU from an Amazon EC2 spot instance to avoid any memory or performance issues from inefficient code.\\xa0\\xa0 I then tossed all of the resulting features into a glm and a random forest, averaged the results, and hoped for the best. \\xa0 It wasn’t subtle, but it was effective.\\xa0(Full code can be found\\xa0here\\xa0if interested.)\\r\\nThe WordPress Social Graph\\r\\nFrom my brief review of other winning entries, I believe one unique quality of my submission was its limited use of the WordPress social graph.\\xa0\\xa0 (Fair warning:\\xa0 I may abuse the term ‘social graph,’ as it is not something I have worked with previously.) \\xa0 Specifically, a user ‘liking’ a blog post creates a link (or edge) between user nodes and blog nodes, and these links construct a graph connecting users to blogs outside their current reading list:\\r\\n\\r\\n\\r\\nDefining user-blog relationships by this ‘like graph’ opens up a host of available tools and metrics used for social networking and other graph-related problems.\\r\\nNode Distance, a.k.a. Two Degrees of Separation\\r\\nThe simplest of these graph metrics is the concept of\\xa0node distance\\xa0within graphs.\\xa0 In this case, node distance is the smallest number of likes required to traverse between a particular user node and a particular blog node.\\xa0\\xa0 In the diagram above, for example, User A and Blog 4 have a node distance of 3, while User C and Blog 5 have a distance of 5.\\r\\n\\r\\nThe chart below breaks down likes from the last week of the final competition training data (week 5) by the node distance between the user and the liked blog within their prior ‘like graph’ (training data weeks 1-4):\\r\\n\\r\\n\\r\\n\\r\\nAs you can see, nearly 50% of all new likes are from blogs one ‘edge’ from the user – i.e., blogs the user had already liked in the prior four weeks.\\xa0\\xa0\\xa0 These ‘like history’ blogs are a small, easily manageable population for a recommendation engine, and there are many relevant features that can be extracted based on the user’s history with the blog.\\xa0\\xa0 Therefore, the like history set was the primary focus of most contest submissions (including mine).\\r\\n\\r\\nHowever, expanding the search for candidates one more level – to a distance of 3 edges/likes traversed – encompasses 90% of all new likes.\\xa0\\xa0 A ‘distance 3’ blog wold be a blog that is not in the subject’s immediate like history but that is in the history of another user who had liked at least one blog in common with the subject.\\xa0\\xa0 This is admittedly a large space (see below), but I think it significant that >90% of a user’s liked blogs in a given week can be found by traversing through just one common reader in the WordPress graph.\\xa0\\xa0 Finding the key common readers and common blogs, therefore, is a promising avenue for finding recommendation candidates that are new to the subject user.\\r\\nNode Centrality, a.k.a. Finding The Common Thread\\r\\nAs referenced above, the main problem with using the distance 3 blogs as recommendation candidates is that the search space is far too large – most users have tens of thousands of blogs in their distance 3 sets:\\r\\n\\r\\n\\r\\n\\r\\nAs seen from the above chart, while a significant portion of users (~20%) have a manageable distance 3 blog set (1,000 to 2,000 blogs), the vast majority have tens of thousands of blogs within\\xa0that\\xa0distance.\\xa0\\xa0 (Some post-competition inspection shows that many of these larger networks are caused by a few ‘hyper-active’ users in the distance 3 paths.\\xa0 Eliminating these outliers could be a reasonable way to create a more compact distance 3 search set.)\\r\\n\\r\\nOne could just ignore the volume issues and run thousands of distance 3 blog candidates per user through the recommendation engine.\\xa0 However, calculating the features and training the models for this many candidate blogs would be computationally intractable (even given my inclination for overkill).\\xa0 To get a manageable search space, one needs to find a basic, easily calculable feature that identifies the most probable liked blogs in the set.\\r\\n\\r\\nThe metric I used was one designed to represent\\xa0node centrality, a measure of how important a node is within a social graph.\\xa0 There are many\\xa0sophisticated, theoretically sound ways to measure node centrality, but implementing them would have required minutes of exhaustive wikipedia reading.\\xa0\\xa0 Instead, I applied a highly simplified calculation designed to measure a blog node’s three-step centrality within a specific user’s social graph:\\r\\n\\nStep 1(a):\\xa0 Calculate all three-step paths from the subject user in the graph (counting multiple likes between a user and blog and multiple possible paths);\\nStep 1(b): Aggregate the paths by the end-point blog; and\\nStep 1(c): Divide the aggregated paths by the total paths in step 1(a).\\nStep 2: ???\\nStep 3: Profit.\\n\\r\\nThe metric is equivalent to the probability of reaching a blog in three steps from the subject user, assuming that at each outbound like/edge has an equal probability of being followed.\\xa0\\xa0 It is akin to Google’s original\\xa0PageRank, except only the starting user node receives an initial ‘score’ and only three steps are allowed when traversing the graph.\\r\\n\\r\\nI don’t know if this is correct or theoretically sound, but it worked reasonably well for me – substantially lifting the number of likes found when selecting candidates from the distance 3 graph:\\r\\n\\r\\n\\r\\n\\r\\nAs shown above, if you examine the first 500 distance 3 blogs by this node centrality metric, you can find over 20% of all the likes in the distance 3 blog set.\\xa0\\xa0 If you selected 500 candidates by random sample, however, only 3% of the likes from this population would be found.\\xa0\\xa0 While I am certain this metric could be improved greatly by using more sophisticated centrality calculations, the algorithm above serves as a very useful first cut.\\r\\nSome Very Simple Code\\r\\nI’d feel remiss not putting any code in this post. \\xa0 Unfortunately, there was a lot of bulky data handling code I used in this competition to get to the point where I could run the analysis above, so posting the code that produced this data would require a lot of extra files. \\xa0I’d happily send it all to anyone interested, of course, just e-mail me.\\r\\n\\r\\nHowever, in the interest of providing something, below is a quick node distance algorithm in Python that I used after the fact to calculate node distances in the like graph. \\xa0This is just a basic breadth-first search implemented in Python, with the input graph represented as a dictionary with node names as keys and sets of connected node names as values:\\r\\n\\r\\n[sourcecode language=\"python\" wraplines=\"false\"]\\r\\n\\r\\ndef distance(graph, start, end):\\r\\n\\r\\n  # return codes for cases where either the start point\\r\\n  # or end point are not in the graph at all\\r\\n   if start not in graph: return -2\\r\\n   if end not in graph: return -3\\r\\n\\r\\n# set up a marked dictionary to identify nodes already searched\\r\\n   marked = dict((k, False) for k in graph)\\r\\n   marked[start] = True\\r\\n\\r\\n# set a FIFO queue (just a python list) of (node, distance) tuples\\r\\n   queue = [(start, 0)]\\r\\n\\r\\n# as long as the queue is full...\\r\\n  while len(queue):\\r\\n     node = queue.pop(0)\\r\\n\\r\\n    # if the next candidate is a match, return the candidate\\'s distance\\r\\n     if node[0] == end:\\r\\n        return node[1]\\r\\n\\r\\n    # otherwise, add all the nodes connected to the candidate if not already searched\\r\\n    # mark them as searched (added to queue) and associate them with candidate distance + 1\\r\\n     else:\\r\\n        nextnodes = [nn for nn in graph.get(node[0], set()) if marked[nn] == False]\\r\\n        queue.extend((nn, node[1]+1) for nn in nextnodes)\\r\\n        marked.update(dict((nn, True) for nn in nextnodes))\\r\\n\\r\\n  # if you fall through, return a code to show no connection\\r\\n  return -1\\r\\n\\r\\n[/sourcecode]\\r\\n\\r\\nConclusion\\r\\n\\r\\nIn the end, this node centrality calculation served as a feature in my recommendation engine’s ranking and – more importantly – as a method of identifying the candidates to be fed into the engine.\\xa0\\xa0 I have not done the work to see how much this feature and selection method added to my score, but I know as a feature it added 2%-3% to my validation score, a larger jump than many other features.\\xa0\\xa0 Moreover, my brief review of the other winner’s code leads me to think this may have been a unique aspect of my entry – many of the other features I used were closely correlated elements in the other’s code.\\r\\n\\r\\nMore crucially, for actual implementation by Automattic the ‘like graph’ is a more practical avenue for a recommendation engine, and is probably what the company uses to recommend new posts.\\xa0\\xa0 Most of the work we did in the competition\\xa0differentiated\\xa0between posts from blogs the user was already reading – useful, but not a huge value-add to the WordPress user.\\xa0\\xa0 Finding\\xa0unseen\\xa0blog posts in which a user may have interest would be a more relevant and valuable tool, and finding new blogs for the user with high centrality in their social graph is a reasonable way to find them.\\xa0\\xa0 From my work on the competition, I believe these methods would be more promising avenues than NLP and topic-oriented methods.\\r\\n\\r\\nThe above posts covers all of my thinking in applying network concepts to the WordPress challenge problem, but I am certain\\xa0 I only scratched the surface.\\xa0 \\xa0There are a host of other metrics that make sense to apply (such as\\xa0eccentricity,\\xa0closeness\\xa0and\\xa0betweenness).\\xa0\\xa0 If you work for WordPress/Automattic (and that is the only conceivable reason you made it this far), I’d be happy to discuss additional ideas, either on this blog or in person.\\r\\n\\r\\nPhoto Credit:\\xa0karindalziel', \"First time Kaggler Yasmin Lucero aka Yolio took home 2nd place in the Practice Fusion Open Challenge\\xa0by combining Electronic Health Records with general population data. \\xa0Also, lots of good tips on using R for visualizations ( Go ggplot2! )\\nWhat was your background prior to entering this competition?\\r\\n\\r\\nI earned my PhD doing mathematical biology and statistics in the field of\\xa0marine fisheries science. I have done analytical work on a variety of\\xa0problems in environmental science, mostly working for NOAA (National\\xa0Oceanographic and Atmospheric Administration).\\r\\n\\nWhat made you decide to enter?\\r\\n\\r\\nIt was a chance for me to build my portfolio. I recently decided that I\\xa0want to move into doing data analytics work for a tech company in the\\xa0health and wellness sphere. I need opportunities to demonstrate how my\\xa0skills transfer to this new area and how I add value. This project was\\xa0perfect for that.\\r\\n\\nWhat preprocessing methods did you use to study the data?\\r\\n\\r\\nI work primarily in R. I used a package called RSQLite to access the SQL\\xa0database. I did this partly because I thought I would be doing a lot of\\xa0joins, but I found that practice fusion had already provided tables with\\xa0almost everything I wanted (the data was in great shape). I did eventually\\xa0write a few SQLite queries of my own: I wanted the number of doctor visits,\\xa0medications and diagnoses for each patient. You can sort of do this in R\\xa0with the aggregate function, but the database was large enough for it to be\\xa0quite slow. SQL is really good at doing that sort of thing fast.\\r\\n\\r\\nOnce the data was in R, I did exploratory analysis using the functions str\\xa0and table. Then, I made many histograms and other plots. I spent lots of\\xa0time studying the metadata/schema that was provided in a pdf file. I did a\\xa0bit of recoding. I recoded dmIndicator to a logical variate (TRUE/FALSE). I\\xa0recoded the NIST smoking codes. That was tricky; I had to dig around the\\xa0internet quite a bit to make sense of the metadata that was provided. I\\xa0eventually was able to recode the NIST codes into a binary logical variate\\xa0called Smoker. I also came across some obvious measurement error: there\\xa0were several weight measurements for greater than a 1000 pounds and height\\xa0measurements for people greater than 9 feet tall. I ended up cutting out\\xa0lots of unrealistic weight/height measures. I converted Year of Birth to\\xa0age. I ended up removing all of the data for Puerto Rico, since I couldn't\\xa0get comparative general population data.\\r\\n\\r\\nHow did you decide what aspects of the data to use?\\r\\n\\r\\nI wanted to find out how representative this medical population was of the\\xa0general U.S. population. So, I was interested in any data that I could get\\xa0both for the EHR and the general population, via the census or other\\xa0surveys. I searched the internet for general population data for anything\\xa0that was also in this data. This boiled down: obesity rates, smoking\\xa0status, diabetes status, state of residence, age and gender.\\r\\n\\nWere you surprised by any of your insights or any key features?\\r\\n\\r\\nYes, I thought that the spatial distribution of the patient population\\xa0would drive the overall population characteristics, but this didn't happen.\\xa0It seemed that the EHR population represented a specific demographic slice\\xa0that was the same regardless of which state they were in.\\r\\n\\nWhich tools did you use?\\r\\n\\r\\nThe graphics are all made with the ggplot2 package in R. I made a lot of\\xa0use of the reshape2 package as well, to prepare the data for plotting. And\\xa0I used RSQLite to get the data into R, and to implement a few queries. And\\xa0I used the knitr package to generate the markdown report.\\r\\n\\nWhat have you taken away from this analysis?\\r\\n\\r\\nI think that my favorite result was the age/bmi distribution for diabetics\\xa0vs non-diabetics. Most diabetics are older, but young diabetics are much\\xa0more prone to be very overweight. Older diabetics have a weight\\xa0distribution that isn't that different from older non-diabetics. I also\\xa0thought it was interesting that while there is a strong relationship\\xa0between diabetes and BMI, most overweight people are not diabetic. Even at\\xa0extremely high BMI, 2/3 of people are not diabetic.\", 'For the final entry in our How I Did It series on the\\xa0\\xa0Practice Fusion Open Challenge, we spoke with the winner, Ryan Pedela, the CEO and co-founder of medical info search engine Datalanche ( currently in Private Beta, but you can check it out with the login info provided in his contest submission)\\nWhat was your background prior to entering this competition?\\r\\n\\r\\nOur team at Datalanche has experience and expertise in computer science, computer graphics, gaming, and data science.\\r\\n\\nWhat made you decide to enter?\\n\\r\\nWe had already started work on a medical information search engine powered by de-identified patient records. The competition and Practice Fusion’s data set aligned perfectly with our goals. The competition gave us an opportunity to build and test our search engine using a high-quality, real-world data set of 10,000 de-identified patient records.\\r\\n\\nWhat preprocessing methods did you use to study the data?\\n\\r\\nOne of our goals is to give users a dynamic, near real-time experience based on their input. This allows them to quickly see how a patient’s demographics, medical history, etc affect any given medical statistic. Practice Fusion’s data set was re-organized so that we could quickly compute statistics based on the user’s input.\\r\\n\\r\\nWe use MedlinePlus Connect, an API from the National Library of Medicine, to provide our users encyclopedia information for every medical topic in our database. The API returns encyclopedia information formatted as HTML, but we needed a different HTML formatting than the one provided. We wrote Python scripts to reformat the encyclopedia information.\\r\\n\\nHow did you decide what aspects of the data to use?\\n\\r\\nAccording to a 2010 study by the Pew Research Center [1], the most commonly searched medical topics are symptoms, medical conditions, and treatments. We decided to focus on medical conditions and medications since they are commonly searched and a significant percentage of Practice Fusion’s data set was devoted to those medical topics.\\r\\n\\r\\nreferences:\\r\\n1.\\xa0http://pewinternet.org/Reports/2011/HealthTopics.aspx\\n\\nWere you surprised by any of your insights or any key features?\\n\\r\\nWe were surprised by the breadth of medical conditions and medications represented in Practice Fusion’s data set given its relatively small size. With only 10,000 patients in the data set, 21 patients had been diagnosed with multiple sclerosis, a rare disease, and some had been treated with interferon, a medication prescribed for treatment of multiple sclerosis. Several other relatively rare medical conditions and medications are also represented in the data set. To us, that shows Practice Fusion’s data set has great coverage for medical conditions and medications.\\r\\n\\nWhich tools did you use?\\n\\r\\nThe website is built with Javascript, HTML5, CSS3 on the client. On the server, we use Node.js for the web server, our relational database is MySQL, and Apache Solr for search. Data preprocessing scripts are written in Python.\\r\\n\\nWhat have you taken away from this analysis?\\n\\r\\nWe believe improving everyone\\'s access to reliable medical information will improve health care. Our mission is to find medical correlations, trends, facts or \"insights\" which can only be found by analyzing large amounts of anonymous medical data, then intuitively showcase those insights. This competition showed us that the medical data necessary to accomplish our mission is available.\\r\\n\\r\\nPhoto Credit:\\xa0sgillies', 'Cross-post from Peekaboo,\\xa0Andreas Mueller\\'s computer vision and machine learning blog. \\xa0This post documents his experience in the Impermium Detecting Insults in Social Commentary competition, but rest of the blog is well worth a read, especially for those interested in computer vision and Python scikit-learn and -image.\\r\\n\\r\\nRecently I entered my first\\xa0kaggle\\xa0competition - for those who don\\'t know it, it is a site running machine learning competitions. A data set and time frame is provided and the best submission gets a money prize, often something between 5000$ and 50000$.\\r\\n\\r\\nI found the approach quite interesting and could definitely use a new laptop, so I entered\\xa0Detecting Insults in Social Commentary.\\xa0\\r\\n\\r\\nMy weapon of choice was Python with\\xa0scikit-learn\\xa0- for those who haven\\'t read my blog before: I am one of the core devs of the project and never shut up about it.\\r\\n\\r\\nDuring the competition I was visiting Microsoft Reseach, so this is where most of my time and energy went, in particular in the end of the competition, as it was also the end of my internship. And there was also the\\xa0scikit-learn release\\xa0in between. Maybe I can spent a bit more time on the next competition.\\r\\nThe Task\\r\\nThe task was to classify forum posts / comments into \"insult\" and \"not insult\".\\r\\nThe original data set was very\\xa0 small, ~3500 comments, each usually between 1 and 5 sentences.\\r\\nOne week before the deadline, another ~3500 data points where released (the story is a bit more complicated but doesn\\'t matter so much). Some data points had timestamps (mostly missing in training but available in the second set and the final validation).\\r\\nThe Result (Spoiler alert)\\r\\nI made 6th place.\\xa0Vivek Sharma\\xa0won.\\r\\nFrom some mail exchanges, comments in my blog and a\\xa0thread I opened in the competition forum, I know that at least places 1, 2, 4, 5 and 6 (me) used\\xa0scikit-learn\\xa0for classification and / or feature extraction. This seems like a huge success for the project! I haven\\'t heard from the third place, yet, btw.\\r\\n\\r\\nEnough blabla, now to the interesting part:\\r\\nFirst\\xa0my code on github. Probably not so easy to run. Try my \"working\" branch of sklearn\\xa0if you are interested.\\r\\nThings That worked\\r\\nMy two best performing models are actually quite simple, so I\\'ll just paste them here.\\r\\nThe first uses character n-grams, some handcrafted features (in BadWordCounter), chi squared and logistic regression (output had to be probabilities):\\r\\n\\r\\n[sourcecode language=\"python\" wraplines=\"false\"]\\r\\nselect = SelectPercentile(score_func=chi2, percentile=18)\\r\\nclf = LogisticRegression(tol=1e-8, penalty=\\'l2\\', C=7)\\r\\ncountvect_char = TfidfVectorizer(ngram_range=(1, 5), analyzer=\"char\", binary=False)\\r\\nbadwords = BadWordCounter()\\r\\nft = FeatureStacker([(\"badwords\", badwords), (\"chars\", countvect_char), ])\\r\\nchar_model = Pipeline([(\\'vect\\', ft), (\\'select\\', select), (\\'logr\\', clf)])\\r\\n[/sourcecode]\\r\\n\\r\\nThe the second is very similar, but also used word-ngrams and actually preformed a little better on the final evaluation:\\r\\n\\r\\n[sourcecode language=\"python\" wraplines=\"false\"]\\r\\nselect = SelectPercentile(score_func=chi2, percentile=16)\\r\\n\\r\\nclf = LogisticRegression(tol=1e-8, penalty=\\'l2\\', C=4)\\r\\ncountvect_char = TfidfVectorizer(ngram_range=(1, 5), analyzer=\"char\", binary=False)\\r\\ncountvect_word = TfidfVectorizer(ngram_range=(1, 3), analyzer=\"word\", binary=False, min_df=3)\\r\\nbadwords = BadWordCounter()\\r\\n\\r\\nft = FeatureStacker([(\"badwords\", badwords), (\"chars\", countvect_char),(\"words\", countvect_word)])\\r\\nchar_word_model = Pipeline([(\\'vect\\', ft), (\\'select\\', select), (\\'logr\\', clf)])\\r\\n[/sourcecode]\\r\\n\\r\\nMy final submission contained two more models and also the combination of all four. As expected, the combination performed better than any single model, but the improvement over char_model_word was not large (0.82590 AUC vs 0.82988 AUC, the winner had 0.84249).\\r\\n\\r\\nBasically all parameters here are crazily cross-validated, but many are quite robust (C= 12 and percentile=4 will give about the same results).\\r\\n\\r\\nSome of the magic happens obviously in BadWordCounter. You can see the implementation\\xa0here, but I think the most significant features are \"number of words in a badlist\", \"ratio of words that is in badlist\", \"ratio of words in ALL CAPS\".\\r\\n\\r\\nHere is a visualization of the largest coefficients of three of my model. Blue means positive sign (insult), red negative (not insult):\\r\\n\\nFull sized image\\r\\nMost of the used features are quite intuitive, which I guess is a nice result (bad_ratio is the fraction of \"bad\" words, n_bad is the number).\\r\\n\\r\\nBut in particular the character plot looks pretty redundant, with most of the high positives detecting whether someone is a moron or idiot or maybe retarded...\\r\\nStill it performs quite well (and of course these are only 100 of over 10,000 used features).\\r\\n\\r\\nFor the list of bad words, I used one that allegedly is also used by google.\\r\\nAs this will include \"motherfucker\" but not \"idiot\" or \"moron\" (two VERY important words in the training / leaderboard set), I extended the list with these and whatever the thesaurus said was \"stupid\".\\r\\nInterestingly in some models, the word \"fuck\" had a very large negative weight.\\r\\nI speculate this is caused by n_bad (the number of bad words) having a high weight and \"fuck\" not actually indicating insults.\\r\\n\\r\\nAs a side note: for the parameter selection, I used the ShuffleSplit (as\\xa0Olivier\\xa0suggested), as StratifiedKFold didn\\'t seem to be very stable. I have no idea why.\\r\\nI discovered very close to the end that there were some duplicates in the training set (I think one comment was present 5 times), which might have been messing with the cross-validation.\\r\\nThings that didn\\'t work\\nFeature selection:\\r\\nI tried L1 features selection with logistic regression followed by L2 penalized Logistic regression, though it was worse than univariate selection in all cases.\\r\\nI also tried RFE, but didn\\'t really get it to work. I am not so familiar with it and didn\\'t know how to adjust the step-size to work in reasonable time with so many features.\\r\\nI also gave the randomized logistic regression feature selection a shot (only briefly though), also without much success.\\r\\nClassifiers:\\r\\nOne of my submissions used elastic net penalized SGD, but that also turned out to be a bit worse than Logistic Regression.\\r\\nI also tried Bernoulli naive Bayes, KNN, and random forests (after L1 feature selection) to no avail.\\r\\nWhat surprised me most was that I couldn\\'t get SVC (LibSVM) to work.\\r\\nThe logistic regression I used (from LibLinear) was a lot better than the LibSVM with Platt-scaling. Therefore I didn\\'t really try any fancy kernels.\\r\\nFeatures:\\r\\nI tried to use features from PCA and K-Means (distance to centers).\\r\\nI also tried to use the chi squared kernel approximation in RandomizedChi2,\\r\\nas this often worked very well for bag of visual words, but didn\\'t see any improvement.\\r\\nI also played with\\xa0jellyfish, which does some word stemming and standardization, but couldn\\'t see an improvement.\\r\\n\\r\\nA long complicated pipeline:\\r\\n\\r\\nI also tried to put more effort into handcrafting the features and parsing the text.\\r\\nI used sentence and word tokenizers from\\xa0nltk, used collocations, extracted features\\xa0using regex, even tried to count and correct spelling mistakes.\\r\\nI briefly used part-of-speech tag histograms, but gave up on POS-tagging as it was very slow.\\r\\nYou can look up the details of what I tried\\xa0here.\\r\\nThe model using these features was by far the worst. I didn\\'t use any character features, but many many handcrafted ones. And it didn\\'t really overfit.\\r\\nIt was also pretty bad on the cross-validation on which I designed the features.\\r\\nApparently I didn\\'t really find the features I was missing.\\r\\nI also used a database of positive and negative connotated words.\\r\\n\\r\\nI should probably have tried to combine each of these features with the other classifiers, though I wanted to avoid building to similar models (as I wanted to average them). Also I didn\\'t really invest enough time to do that (my internship was more important to me).\\r\\nThings I implemented\\r\\nI made several additions to scikit-learn particularly for this competition.\\r\\nThey basically focused on text feature extraction, parameter selection with grid search and feature selection.\\r\\n\\r\\nThese are:\\r\\nMerged\\n\\nEnable grid searches using Recursive Feature Elimination. (PR)\\nAdd minimum document frequency option to CountVectorizer (n-gram based text feature extraction) (PR)\\nSparse Matrix support in Recursive Feature Elimination. (PR)\\nSparse Matrix support in Univariate Feature Selection. (PR)\\nEnhanced grid search for n-gram extraction. (PR)\\nAdd AUC scoring function. (PR)\\nMinMaxScaler: Scale data feature-wise between given values (i.e. 0-1). (PR)\\n\\nNot merged (yet)\\n\\nFeatureUnion: use several feature extraction methods and concatenate features. (PR)\\nSparse matrix support in randomized logistic regression (PR).\\nEnhanced visualization and analysis of grid searches. (PR)\\nAllow grid search using AUC scores. (PR)\\n\\nThings I learned\\r\\nI learned a lot about how to process text. I never worked with any text data before and I think now I have a pretty good grip on the general idea. The data was quite small for this kind of application but still I think I got a little feel.\\r\\nAlso, it seems to me that the simplest model worked best, feature selection and feature extraction are very important, though hand-crafting features is very non-trivial.\\r\\nTo recap: my best single model was the \"char_word_model\",\\xa0 which can be constructed in 7 lines of sklearn stuff,\\xa0 together with 30 lines for custom feature extraction. I think if I had added also the date, I might have had a good chance.\\r\\nThings that worked for others\\r\\nMost contestants used similar models as I did, i.e. linear classifiers,\\r\\nword and character n-grams and some form of counting swearwords.\\r\\nVivek, who won, found that SVMs worked better for him than logistic regression. Chris Brew, who came in fourth, only used character n-grams\\r\\nand a customized SGD classifier. So even with very simple features, you can\\r\\nget very far.\\r\\nIt seems most people didn\\'t use feature selection, which I tried a lot.\\r\\n\\r\\nThe most commonly used software was scikit-learn, as I said above, R, and\\xa0software from the Stanford NLP\\xa0group.\\r\\n\\r\\nFor details on what others used, see the discussion in the\\xa0kaggle forum.\\r\\nFinal Comments\\r\\nAfter the first version of this blog-post (which I now shamelessly rewrote), I got a huge amount (relatively speaking) of feedback from other competitors.\\r\\nThanks to everybody who shared there methods - in the comments, at kaggle, and at the\\xa0scikit-learn mailing list\\xa0- and even\\xa0their code!\\r\\n\\r\\nI feel it is great that even though this is a competition and money is involved, we can openly discuss what we use and what works. I think this will help push the \"data science\" community and also will help us create better tools.\\r\\n\\r\\nThere where several thing that seemed a bit weird about the competition.\\r\\nI know the competitions are generally still somewhat in a beta, phase, but there are some things that could be improved:\\r\\n\\r\\nThe scores from the leader board dropped significantly, from\\xa0\\xa0around 91 AUC\\xa0to\\xa0around 83 AUC\\xa0on the final evaluation. I\\'m pretty sure I did not overfit (in particular the leader board score was always close to my cross validation score and I only scored on the leader board 4 times). Some discussion about this is\\xa0here. Generally speaking, some sanity tests on the data sets would be great.\\r\\n\\r\\nI was a bit disappointed during the competition as cross-validation seemed very noisy and my standard deviation captured the scores of the first 15 places.\\r\\nThat also made it hard to see which changes actually helped.\\r\\nAlso, there seemed to be a high amount of label noise.\\r\\n\\r\\nFor example most of my models had this false positive:\\r\\n\\r\\nAre you retarded faggot lol If you are blind and dont use widgets then that doesnt mean everyone else does n\\'t use them Widgets is one of the reasons people like android and prefer it agains iOS You can have any types of widgets for weather seeing your twitter and stuff and on ios you scroll like an idiot like a minute and when you finally found the apps you still have to click a couple of times before you see what you need Android 2:0 iOS ; ]\\xa0\\r\\n\\r\\nHope you enjoyed this lengthy post :)', \"We catch up with the team of undergrads who took 1st place in the CPROD (Consumer Products) Challenge. \\xa0They'll be presenting their results this December at the\\xa0ICDM-2012 conference.\\nWhat was your background prior to entering this competition?\\r\\nWe are undergraduate students from Tsinghua University, China. Before entering the competition, we have some experience about developing software and applications using techniques from machine learning and nature language processing. What’s more, we attended KDD Cup 2012 Track 1 with the same team name “ISSSID” and ranked 8th finally.\\r\\nWhat made you decide to enter?\\r\\nWe found that the problem was both challenging and research-oriented. In addition, the competition is a part of the ICDM 2012 conference.\\r\\nWhat preprocessing and supervise learning methods did you use?\\r\\nPreprocessing: (1) JSON format to plain text format (2) cleaning the data by deleting all useless characters and symbols. (3) Change all uppercase for few products to lowercase\\r\\n\\r\\nSupervision learning: We employed “Conditional Random Field” Model. We choose this algorithm because it converges faster and is easy to implement. We used tool MALLET for this purpose.\\r\\nWhat was your most important insight into the data?\\r\\nThe specific characters of products naming (Example: iPhone – mixture of both uppercase and lowercase) and human sematic behavior analysis (Example: my iPhone or <action> by <company name>) are the most important insight that helped us to improve the precision overall. We finally took voting approach (to find which category the product belongs to) based on our experimental results.\\r\\nWere you surprised by any of your insights or any key features?\\r\\nWhen we merged the Conditional Random Field Model to other two models (Standard and Rule Template) we have, the performance we achieved significantly increased. We got approximately 3% improvement in F1 score.\\r\\n\\r\\nNotably combinations of CRF models achieved the highest score in the private leaderboard, but not in the public leaderboard.\\r\\nWhich tools did you use?\\r\\nLanguages: C++, Python, Perl\\r\\n\\r\\nTool: Mallet\\r\\nWhat have you taken away from this competition?\\r\\nReal Life problem challenges because of the following reason:\\r\\n\\r\\n1)\\xa0\\xa0\\xa0 Data is from heterogeneous dataset.\\r\\n\\r\\n2)\\xa0\\xa0\\xa0 Generally entity resolution is quite difficult task.\\r\\n\\r\\n3)\\xa0\\xa0\\xa0 Huge product name list.\\r\\n\\r\\n4)\\xa0\\xa0\\xa0 Semantics used in the forum environment poses challenge.\\r\\n\\r\\nEven we entered the competition very late, we have devised several approaches and ran quiet good amount of experiments to move in the right direction. We learnt working on real life problem poses lot many challenges. Working on this problem improved our approach, creativity and knowledge. Now we look forward to work more on such real life dataset problems. Finally we are glad to win the competition in a popular conference, and of course bucks!\", 'We check in with the 2nd place winner of the Impermium \"Troll-dar\" Competition. \\xa0He\\'s also published his\\xa0code\\xa0and a more detailed\\xa0explanation\\xa0of his approach on github.\\nWhat was your background prior to entering this challenge?\\r\\n\\r\\nI used to work in Yandex (Russian N1 search engine) on text classification\\r\\nproblems. I also finished great online courses: ML class by Andrew Ng and\\r\\nNLP class by Manning and Jurafsky. Actually I am not a strong ML hacker, I\\r\\nthink my advantage was in variety in extracted features and text processing\\r\\nmethods.\\r\\n\\r\\nWhat made you decide to enter?\\r\\n\\r\\nI recognized this Kaggle competition as an opportunity to experiment with\\r\\ntext processing tasks and to learn more about machine learning techniques.\\r\\n\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nI used stemming and dependency parsing in preprocessing. I also used\\r\\nlanguage model code which I wrote during studying at the NLP class. As for\\r\\nlearning methods - I used logistic regression for basic classifiers and\\r\\nrandom forest for the final ensemble.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nSentence level features. After examining classification errors I realized\\r\\nthat many insulting posts were one sentence posts, and in bigger posts\\r\\nthere were one insulting sentence.\\r\\n\\r\\nWere you surprised by any of your insights?\\r\\n\\r\\nOne of the most surprising thing for me was that the simple stem-based\\r\\nfeatures (subsequnces and ngrams) work much better in the final ensemble\\r\\nthan complex features based on parser results and POS tags.\\r\\nSyntax features (features build around dependency parser results) alone\\r\\ngave me pretty great AUC, but got very low feature importance in the final\\r\\nensemble.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nStanford POS tagger and parser for preprocessing. scikit-learn for learning.\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\n\\r\\nI learned how to build ensembles using stacking (I said - I am not a ML\\r\\nhacker ;). Also got some insight how to use different NLP features.\\r\\nThis competition definitely gave me great opportunity to stretch my\\r\\nknowledge about ML and NLP. Eager to participate in next competition, just\\r\\nneed make up for the lost sleep ;)\\r\\n\\r\\nPhoto Credit:\\xa0Howard Dickins', 'Team DataRobot explains how to take on the Merck Molecular Activity Challenge using smoke alarms and airplanes.\\nWhat was your background prior to entering this challenge?\\nXavier: I run a consultancy Gear Analytics specialized in predictive analytics in Singapore. Previously, I worked in France, Brazil, China and Singapore holding different roles (actuary, CFO, risk manager) in the life and non-life insurance industry.\\r\\n\\r\\nJeremy and Tom: We met while we were both studying Math and Physics at the University of Massachusetts at Lowell and have been friends and colleagues ever since. We both have 7+ years experience doing applied predictive analytics. Most recently we were both Directors of Research and Modeling at Travelers Insurance (Tom on the Business Insurance side and Jeremy on the Personal Insurance side). Earlier this year, we decided to quit our jobs to start our own Data Science company (DataRobot). In our previous three kaggle competitions, we placed 3rd (private), 1st (Bio) and 4th (Diabetes).\\r\\n\\r\\nWhat made you decide to enter?\\r\\n\\r\\nXavier: I was looking for a competition to team up with my buddies Tom and Jeremy, who I met through Kaggle. We previously teamed up with Sergey Yurgenson for the \"Practice Fusion Diabetes Classification\" competition. We got good results but failed to finish in the top 3 (4th place). We also knew that we had good chance to win the Merck competition as we did quite well for the \"Biological Response\" (1st and 5th place).\\r\\n\\r\\nJeremy and Tom: We were looking for a competition to team up with our buddy Xavier, who we met through Kaggle, and we thought we’d be able to leverage what we had learned during the BioResponse competition which we placed 1st in. Also, because we quit our jobs earlier this year, we were hoping to place in the top 3 and win some money to pay for a few more months of ramen noodles and canned tuna fish. If we placed 1st or 2nd we thought we might even be able to turn our internet back on--if our neighbor changes his wifi password on us again, we are screwed.\\r\\n\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nMethodologies explored for various roles include Random Forests, PCA, GBM, KNN, Neural Nets, Elastic Net, GAM, and SVM.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nFor most problems, GBM and SVM had similar predictive power and produced similar predictions. However, for problem 5, SVM\\'s predictions deviated significantly from GBM\\'s predictions and scored badly on the public leaderboard. This confirmed the importance of having at least 2 very different models in one\\'s toolkit. We also improved the accuracy slightly by capping the predictions of a few problems.\\r\\n\\r\\nWere you surprised by any of your insights?\\r\\n\\r\\nThe most surprising thing was that almost all attempts to use subject matter knowledge or insights drawn from data visualization led to drastically worse results. We actually arranged a 2 hour whiteboard lecture from a very talented biochemist and came up with some ideas based on what we learned, but none of them worked out. Also, the visualizations that were shared as part of the visualization part of the competition were incredible (thanks to all who contributed!). We drew much insight from them which led us to try some new approaches that we were absolutely sure would work. However, most of these approaches failed to improve results, and many of them drastically decreased our public leaderboard scores. The visualizations did make us take a second look at capping which helped a bit.\\r\\n\\r\\nWe were also surprised to see how well our internal CV scores correlated with the public (and private) leaderboard scores. This was unexpected because of all the evidence suggesting the test sets were very different from the training sets for some problems. Only for problem 4 did the public leaderboard give us faulty feedback, but since problem 4 was so small, we knew to include a submission that ignored the public leaderboard feedback and included our best CV model for problem 4.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nWe used R, Python, and a lot of computing power. We really didn’t start working on the problem until around 2 weeks before the deadline, so we had to cram lots of cpu cycles into a short amount of time. \\xa0We used our 9 Ubuntu servers, Amazon, plus Xaviers magic macbook which he somehow gets to perform like it is a 32 core machine with 256GB of RAM. I (Jeremy) was sure the thing would combust at any moment, so I made sure all the smoke alarms in the house had fresh batteries.\\r\\n\\r\\nWe also used airplanes--Xavier came to the US and worked with us in person for 4 days which allowed us to get a good head start on the problem and make a solid two week plan (which we stuck to for the most part).\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\n\\r\\nWe have some new techniques we need to learn if we want to compete for first place in future competitions.\\r\\n\\r\\nKaggle is a great place to meet people with the same interest and great results can come from this: friendship + powerful models!\\r\\n\\r\\n\\n', \"So, what's with the punctuation mark for a team name?\\nEu Jin Lok: Apologies for the team name, I know it’s annoying. If you were wondering, I chose it for its functionality: (1) It’s hard for people to notice; (2) It’s hard for people to click (if they want to find out our names).\\nWhat was your background prior to entering this challenge?\\nZach Mayer: I've got an undergraduate degree in biology, and a professional background in applied statistics and predictive modeling. \\xa0I currently work for management consulting firm AlixPartners.\\nEu Jin Lok: I majored in Marketing and Econometrics in University, and like Zach, I'm currently working for Deloitte in the data analytics unit as a senior consultant.\\nAlexander Larko: \\xa0I have a Master's degree in computer science - from South Russian State Technical University (Novocherkassk Polytechnic Institute). I started my career as an engineer with Scientific Research Institute of the city of Donetsk and worked there for three years. After that, I left to join a manufacturing firm and spent the next 25 years of my career as researcher, IT - engineer and senior manager for the firm. Now, I'm working for a small IT company as a technical director\\nWhat made you decide to enter?\\nAL: I liked the challenge, because it's an interesting set of data.\\nZM: Well it seemed like an easy regression problem at first, at least until we realized how much the size of the data sets varied. \\xa0Self-evaluation on this problem was especially challenging.\\nEJ: Zach and I have been working together for a while on the HHP contest. On day, Zach mentioned the competition to me in passing so I thought why not give it a go. its a pretty interesting problem to solve...and also not to mention the prize was attractive.\\nHow did you guys meet each other?\\nZM: Eu Jin and I met when we started collaborating on the Heritage prize, and we liked working together, so we entered a couple other competitions.\\nHow did you decide to start working together on the Merck comp?\\nEJ:\\xa0One day, all of a sudden, there was a raft of new competitions on Kaggle and they were all really interesting, but deadline is so close together, like the MERCK and US Census. So I decided that the best strategy is to work with Zach whom I'm already working with on the HHP contest. We entered the MERCK and US Census competition together, doing a tag team, swamping and changing as we get new ideas. Half way through the contest, our work/career ate into our Kaggle time, so I invited Alex to join us as I've seen him competing for 2 years now and thought it would be nice to work together with him.\\nWhat preprocessing and supervised learning methods did you use?\\nEJ: I used SVD to reduce features which was used as training data. For models, I tried everything from GBMs to PLS but it came down to just SVM and Random Forest.\\nAL: The key success factor was selecting the significant variables, and for this I used a gradient increase as a feature selector. I used SVMs, gradient increases and neural networks to build several models which was subsequently put together to create the final submission.\\nZM: I created a glmnet model that used sparse matrix representations of each data set. Unfortunately, my approach did not crystalise to a strong solution. So, for the rest of the time, I helped Eu Jin with SVD and PCA when his laptop ran out of RAM!\\nWhat was your most important insight into the data?\\nZM: Alex discovered that a GBM run on a sample of the data could be used to select features and greatly speed up the full model.\\nEJ:I was surprised by Alex's approach wherein he ran a GBM on a sample of the data which he then used to select features. Did not expect that to work well but it did so that was an insight for me.\\nAL: Yea me too! But also the temporal effect of the dataset, which was prevalent in the activity of the molecules.\\nWere you surprised by any of your insights?\\nZM: Not particularly. \\xa0A big portion of this competition was the technical challenge of pre-processing and modeling on large data sets.\\nAL:\\xa0I was surprised by the coincidence of errors in different parts of the test suite (public error, private error).\\nEJ: I didn't really have any insights, couldn't be more surprised.....\\nWhich tools did you use?\\nTogether: R.\\r\\nAL: And open office too.\\r\\nEJ: And excel too, for graphs.\\nWhat have you taken away from this competition?\\nEJ: Alot of what we have learnt on the MERCK contest, I will take it to the US Census and the HHP. On a serious note, try everything and don't give up. Every tiny effort you put in will bring you closer to the top.\\nZM: RAM is cheap and you should have a lot on your prototyping machine! \\xa0I personally couldn't afford to keep an m2.4xlarge EC2 instance running for a month or 2...\\nAL: The benefits of multiple approaches.\", 'What was your background prior to entering this challenge?\\r\\n\\r\\nWe are a team of computer science and statistics academics. Ruslan Salakhutdinov and Geoff Hinton are professors at the University of Toronto. George Dahl and Navdeep Jaitly are Ph.D. students working with Professor Hinton. Christopher \"Gomez\" Jordan-Squire is in the mathematics Ph.D. program at the University of Washington, studying (constrained) optimization applied to statistics and machine learning.\\r\\n\\r\\nWith the exception of Chris, whose research interests are somewhat different, we are highly active researchers in the burgeoning subfield of machine learning known as deep learning, a sub-field revived by Professor Hinton in 2006. George and Navdeep, along with collaborators in academia and industry, brought deep learning techniques to automatic speech recognition. Systems using these techniques are being commercialized by companies around the world, including Microsoft, IBM, and Google.\\r\\n\\r\\nWhat made you decide to enter?\\r\\n\\r\\nWe wanted to show the Kaggle community the effectiveness of neural networks that use the latest techniques from the academic machine learning community, even when used on problems with relatively scarce data, such as the one from this competition. Neural nets similar to the ones we used have recently demonstrated a lot of success in computer vision, speech recognition, and other application domains.\\r\\n\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nSince our goal was to demonstrate the power of our models, we did no feature engineering and only minimal preprocessing. The only preprocessing we did was occasionally, for some models, to log-transform each individual input feature/covariate. Whenever possible, we prefer to learn features rather than engineer them. This preference probably gives us a disadvantage relative to other Kaggle competitors who have more practice doing effective feature engineering. In this case, however, it worked out well. We probably should have explored more feature engineering and preprocessing possibilities since they might have given us a better solution.\\r\\n\\r\\nAs far as supervised learning goes, our solution had three essential components: single-task neural networks, multi-task neural networks, and Gaussian process regression. The neural nets typically had multiple hidden layers, used rectified linear hidden units, and used \"dropout\" to prevent overfitting. No random forests were harmed (or used) in the creation of our solution. We used simple, greedy, equally-weighted averaging of these three basic model types. At the very end we began experimenting with gradient boosted decision-tree ensembles to hedge our solution against what we believed other competitors would be using and improve our averages a bit. We didn\\'t have a lot of time to explore these models, but they seemed to make very different predictions from our other models and were thus more useful in our averages than their often weaker individual performances would suggest. For similar reasons, we suspect that averaging our models with the models from other top teams could improve performance quite a bit.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nOur single most important insight was that the similarity between the fifteen tasks could be exploited well by a neural network using all inputs from all tasks and with an output layer with fifteen different output units. This architecture allows the network to reuse features it has learned in multiple tasks and share statistical strength between tasks. Since we can only assume that Merck is interested in even more than the fifteen molecular targets in the competition data, it should be possible to gain even more benefits from combining more and more targets.\\r\\n\\r\\nWere you surprised by any of your insights?\\r\\n\\r\\nWe were somewhat surprised that using ridge regression for model averaging did not provide any detectable improvement over simple equally-weighted averaging.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nWe used Matlab code released by Carl Rassmussen and Chris Williams to accompany their Gaussian processes book. For the neural nets we used a lot of our own research code (in python) and wrote some new neural net code specifically for the competition. Our research code is designed to run on GPUs using CUDA. The GPU component uses Tijmen Tieleman\\'s gnumpy library. Gnumpy runs on top of Volodymyr Mnih\\'s cudamat library. We also used scikits.learn for a variety of utility functions, our last minute experiments with gradient boosted decision trees, and our ill-fated attempts at more sophisticated model averaging.\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\n\\r\\nOur experience has confirmed our opinion that training procedures for deep neural networks have now reached a stage where they can outperform other methods on a variety of tasks, not just speech and vision. In the Netflix competition, the Toronto group publicized their novel use of restricted Boltzmann machines for collaborative filtering, and the winners used this method to create several of the models that were averaged to produce the winning solution. In this competition we decided not to share our neural network methods before the close of the competition, which may have helped us win.', 'We spoke with the\\xa0Merck Visualization Challenge\\xa0winner about his technique. \\xa0All algorithms and visualizations were produced using Matlab R2011a. Implementations of t-SNE (in Matlab, Python, R, and C) are available from\\xa0the t-SNE website.\\nWhat was your background prior to entering this challenge?\\r\\n\\r\\nI am a post-doctoral researcher at Delft University of Technology (The Netherlands), working on various topics in machine learning and computer vision. In particular, I focus on developing new techniques for dimensionality reduction, embedding, structured prediction, regularization, face recognition, and object tracking.\\r\\n\\r\\nWhat made you decide to enter?\\r\\n\\r\\nI entered the visualization challenge to test the effectiveness of an embedding technique, called t-Distributed Stochastic Neighbor Embedding (t-SNE), that Geoffrey Hinton and I developed a few years ago (building on earlier work by Geoffrey Hinton and Sam Roweis).\\r\\n\\r\\nWhat preprocessing or data munging methods did you use?\\n\\r\\nThe main ingredient of my visualization approach is formed by t-SNE (L.J.P. van der Maaten and G.E. Hinton. Visualizing Data using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008). T-SNE represents each object by a point in a two-dimensional scatter plot, and arranges the points in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points. When you construct such a map using t-SNE, you typically get much better results than when you construct the map using something like principal components analysis or classical multidimensional scaling, because (1) t-SNE mainly focuses on appropriately modeling small pairwise distances, i.e. local structure, in the map and (2) because t-SNE has a way to correct for the enormous difference in volume of a high-dimensional feature space and a two-dimensional map. As a result of these two characteristics, t-SNE generally produces maps that provide much clearer insight into the underlying (cluster) structure of the data than alternative techniques.\\r\\n\\r\\nTo produce the visualizations I submitted to the challenge, I ran t-SNE on the raw data and I plotted the resulting two-dimensional map as a scatter plot, coloring the points according to either their index in the data set or according to their activity value. The first coloring provides insight into how the data distributions changes over time (using index as a surrogate for time), whereas the second coloring provides insight into how well the activity values may be predicted from the raw data. I also constructed the similar plots in which I also included the test data in the t-SNE analysis and colored the test points in a neutral gray color, to obtain insight in the difference between the training and the test distributions.\\r\\n\\r\\nWhat was your most important insight into the data?\\n\\r\\nOne of the key insights that my visualizations give into the data distribution is that it changes enormously over time. When coloring the points according to their index, for many data sets, distinct colored clusters can be identified that suggest the data comprises batches of very different measurements. Maps that include the test data (depicted in a neutral gray color) reveal that the test distribution is very different from the training distribution for many data sets. I confirmed this finding using a simple experiment: I trained logistic regressors to discriminate the training from the test data (as is often done in importance-weighting approaches to covariate shift), and found that these logistic regressor have zero error for almost all data sets. This suggests the support of the training and test distribution are almost completely disjoint.\\r\\n\\r\\nWere you surprised by any of your insights?\\n\\r\\nThe enormous difference between the training and test distributions was quite surprising: the difference is so large, that standard importance-weighting techniques for covariate shift will completely fail (because nearly all training points obtain an infinitesimal weight). I am curious to see how the contestants in the prediction challenge have dealt with this problem. I am also interested to know what the underlying phenomenon is that leads to the enormous shift in the data distribution (perhaps such knowledge suggests a preprocessing of the data that would reduce the shift).\\r\\n\\r\\nAnother surprising result was that the individual data sets appear to have quite different structure. This suggests that different data sets may be best modeled by different prediction models.\\r\\n\\r\\nWhat have you taken away from this competition?\\n\\r\\nAlways visualize your data first, before you start to train predictors on the data! Oftentimes, visualizations such as the ones I made provide insight into the data distribution that may help you in determining what types of prediction models to try.\\r\\n\\r\\nBrief Bio\\r\\n\\r\\nI studied computer science at Maastricht University (The Netherlands), and obtained my Ph.D. from Tilburg University (The Netherlands) in 2009 for a thesis that used machine learning and computer vision techniques to analyze archaeological data. As a Ph.D. student, I became interested in using dimensionality reduction to visualize high-dimensional data and whilst visiting Geoffrey Hinton\\'s lab at University of Toronto, Geoffrey and I developed t-SNE. After being doctored, I became a post-doctoral researcher at University of California San Diego, where I studied new algorithms for structured prediction and online learning, and where I worked on machine-learning applications in software engineering and face analysis. At present, I am a post-doctoral researcher at Delft University of Technology (The Netherlands), where I work on a range of topics including embedding, structured prediction, regularization, face recognition, and object tracking.\\r\\n\\r\\nI decided the enter the competition to test out the effectiveness of t-SNE on the challenging Merck data sets. Because I do not have any prior knowledge of the underlying process that generated the data, I had no means of \"helping\" t-SNE to produce an appropriate map of the data: visualizing this data really was a \"blind\" test of t-SNE. I am happy to see that t-SNE was effective on the Merck data in that it was helpful in building an intuition for the underlying data distribution.\\r\\n\\r\\nI like the Kaggle platform a lot because it provides a very fair way to compare different learning approaches. This makes the platform a very valuable addition to the experimental evaluations that are done in machine-learning papers (in such papers, experimental conditions may have been used that favor the approach developed by the authors of those papers; on Kaggle, such subtle ways of \"cheating\" are impossible). A downside of many Kaggle competitions is that the best performance is typically obtained by an ensemble of a large number of blended predictors. This makes it hard for individual machine-learning researchers to be very competitive.', \"Cross-posted from Iain Murray's homepage. He's also sharing his code.\\r\\n\\r\\nIn December 2012 I entered the\\xa0Kaggle/Winton Observing Dark Worlds\\xa0competition. My\\xa0predictions placed 2nd out of 357 teams. This short note describes my approach.\\r\\nModeling the data, and making predictions\\r\\nI took the “obvious” Bayesian approach to this challenge, although didn’t implement it quite as carefully as possible. This style of solution involves three steps: 1)\\xa0Build a probabilistic model of the data; 2)\\xa0Infer underlying explanations of the test data; 3)\\xa0Fit the ‘best’ estimates that minimize the expected error (measured by the metric used on the leaderboard).\\r\\n\\r\\n1) The model:\\xa0The observed data were positions of galaxies and their measured ellipticities. The ellipticities were clearly Gaussian-distributed. The dark matter halos exert a “force” on the galaxies, locally shifting the mean of the distribution over ellipticities. The example maximum likelihood code, provided by the organizers, gave me the equations to describe how the ellipticities are shifted by a force. That only left me to model the force as a function of displacement from a halo center.\\r\\n\\r\\nBased on a quick visualization, I picked a force term of\\xa0m/max(r,r0), where\\xa0m\\xa0is proportional to the halo mass,\\xa0r\\xa0is the radial distance from the halo center, and\\xa0r0\\xa0is a core radius inside which the force doesn’t increase. I doubted that this model was quite right, so I also increased the variance of the ellipticities inside the halo core, hoping to increase the robustness of the model.\\r\\n\\r\\n(I was a little negligent: I should have considered more flexible models of the forces, with more free parameters, and learned whether and how to use them from the data. I didn’t make time to do that, or even test properly whether increasing the within-core variance was a good idea. I’d be more careful in a real collaboration.)\\r\\n\\r\\n2) Inference:\\xa0Given the model, I applied a standard method to simulate plausible explanations of the test skies. I used Slice Sampling, an easy-to-use Markov chain Monte Carlo (MCMC) method, and didn’t have to tune any tweak parameters. My results on the training skies seemed to be similar if I initialized the dark matter halos at their true locations, or randomly, so I assumed that the sampler was working well enough for the competition. The inference stage was by far the easiest: very little coding or computer time, and no tweaking required.\\r\\n\\r\\nIf you would like to try out slice sampling, you could play with my\\xa0Octave/Matlab-based MCMC tutorial exercise. The\\xa0original paper (Neal, 2003)\\xa0is excellent, or for a shorter introduction, see Chapter\\xa029 ofDavid MacKay's book.\\r\\n\\r\\nSampling gives me a bag of samples: a set of plausible dark matter halo positions and masses. If I showed you a movie, stepping through the different plausible halo locations, you would probably understand roughly what we can and can’t know. You might say something like “I can see that there’s definitely a halo in the top right, but the other one could be nearly anywhere”. Unfortunately I’m not able to cram a bag of samples, or your verbal explanation, into the\\xa0.csv\\xa0submission format defined by the competition rules. There are a plethora of possible ways to make a single guess, including: picking a random sample, using the most probable sample, or using the mean of the samples. But these methods shouldn’t be used without justification.\\r\\n\\r\\n3) Making a leaderboard submission:\\xa0A sensible way to make a guess, when forced to, is to minimize the expected size of your mistakes. In this competition, the size of a mistake was measured in a complicated way: the “Dark Worlds Metric”. I’d like to say that I minimized the expected value of this metric, but it was hard to deal with. The best prediction for a sky depends on the mistakes made in other skies, and I didn’t know which skies would be in the final test set. Rather than considering possible test set splits (and possibly inferring the split!), I used a pragmatic hack. I optimized the prediction for each sky using the Dark Worlds Metric applied to a fictitious test set containing multiple copies of the same sky with different halo locations given by the MCMC samples. I hoped that this procedure was close enough to the ideal procedure that I would do well.\\r\\nDiscussion\\r\\nGiven the setup of this competition, I doubt that it is possible to do much better than the style of approach described here. Tim Salimans, who won, followed\\xa0a very similar approach. He might have solved the optimization problem better (I should have used gradient-based optimization, but didn’t). Tim fixed his\\xa0r0\\xa0parameters, based on the training data, whereas I left them uncertain. I didn’t work hard enough to notice that these parameters were always close to one of two fixed values (an artifact of the competition if true). Hierarchical learning of the distribution of such tweak parameters is important in real problems, and would be the way to go in future.\\r\\n\\r\\nIf the organizers ran MCMC on the model that they actually used to generate the data, that would find the best reasonable performance: what could be achieved without getting lucky, or fitting to the test data with multiple submissions. I have\\xa0described in a separate Forum posthow it would also be possible for the organizers to tell whether entrants to the competition were just lucky, by testing their expected performance over all reasonable explanations of the test set, rather than just one.\\r\\n\\r\\nI know from the\\xa0Forum\\xa0that some of the competitors near the top of the leaderboard didn't use Bayesian approaches: there are other ways to get good predictions, and my approaches to machine learning are not always Bayesian either. However, in this problem, the Bayesian approach was natural and straightforward. Unlike some methods discussed on the Forum, no careful tuning, special consideration of edge effects, or choosing of grids was required. The result of inference (given enough samples) is a complete description of what is known about the sky. The resulting samples aren’t tied to one evaluation metric, but can be used to find an estimate that will work well under any.\\r\\n\\r\\nThanks very much to the organizers: Tom Kitching, David Harvey, Kaggle, and Winton for a fun competition. Congratulations\\xa0to Tim Salimans for coming first!\\r\\n\\r\\nI will release my code. Check back later if you want it. Email me if it isn't here by January 2013.\\r\\n\\r\\n\\xa0\\r\\n\\r\\nCode is now available here.\\r\\n\\r\\nHeader Image: \\xa0Iain's 'Research in a Nutshell' video\", \"Cross-posted from Tim Salimans on Data Analysis. \\xa0He'll\\xa0post the Matlab code for his solution sometime later this week\\r\\n\\r\\nKaggle recently ran another\\xa0great competition, which I was very fortunate to win. The goal of this competition: detect clouds of dark matter floating around the universe through their effect on the light emitted by background galaxies.\\r\\n\\r\\nFrom the competition website:\\r\\nThere is more to the Universe than meets the eye. Out in the cosmos exists a\\xa0form of matter that outnumbers the stuff we can see by almost 7 to 1, and we don’t know what it is. What we do know is that it does not emit or absorb light, so we call it\\xa0Dark Matter.\\xa0Such a vast amount of aggregated matter does not go unnoticed. In fact we observe that this stuff aggregates and forms massive structures called\\xa0Dark Matter Halos.\\xa0Although dark, it warps and bends spacetime such that any light from a background galaxy which passes close to the\\xa0Dark Matter\\xa0will\\xa0have its path altered and changed.\\xa0This bending causes the galaxy to appear as an ellipse in the sky.\\r\\nThe task is then to use this “bending of light” to estimate where in the sky this dark matter is located.\\r\\n\\r\\nAlthough the description makes this sound like a physics problem, it is really one of statistics: given the noisy data (the elliptical galaxies) recover the model and parameters (position and mass of the dark matter) that generated them. After recovering these dark matter halos, their positions could then be uploaded to the Kaggle website where a complicated loss function was used to calculate the accuracy of our estimates.\\r\\n\\r\\nBayesian analysis provided the winning recipe for solving this problem:\\r\\n\\nConstruct a prior distribution for the halo positions\\xa0, i.e. formulate our expectations about the halo positions before looking at the data.\\nConstruct a probabilistic model for the data (observed ellipticities of the galaxies) given the positions of the dark matter halos:\\xa0.\\nUse Bayes’ rule to get the posterior distribution of the halo positions:\\xa0, i.e. use to the data to guess where the dark matter halos might be.\\nMinimize the expected loss with respect to the posterior distribution over the predictions for the halo positions:\\xa0, i.e. tune our predictions to be as good as possible for the given error metric.\\n\\r\\nFor step 1. I simply assumed that the dark matter halos were distributed uniformly at random across the sky. Step 2 is more complicated. Fortunately the competition organizers provided us with a set of training skies for which the positions of the dark matter halos was known, as well as a summary of the physics behind it all. After reading through the tutorials and forum posts it became clear that the following model should be reasonable:\\r\\n\\r\\n,\\r\\n\\r\\nwhere\\xa0\\xa0denotes the normal distribution,\\xa0\\xa0is the\\xa0tangential direction, i.e. the direction in which halo\\xa0\\xa0bends the light of galaxy\\xa0,\\xa0\\xa0is the mass of halo\\xa0, and\\xa0\\xa0is a decreasing function in the euclidean distance\\xa0\\xa0between galaxy\\xa0\\xa0and halo\\xa0.\\r\\n\\r\\nAfter looking at the data I fixed the variance of the Gaussian distribution\\xa0\\xa0at 0.05. Like most competitors I also noticed that all skies seemed to have a single large halo, and that the other halos were much smaller. For the large halo I assigned the halo mass\\xa0\\xa0a log-uniform distribution between 40 and 180, and I set\\xa0. For the small halos I fixed the mass at 20, and I used\\xa0. The resulting model is likely to be overly simplistic but it seems to capture most of the signal that is present in the data. In addition, keeping the model simple protected me against overfitting the data. Note that I assumed that the galaxy positions were independent of the halo positions, although it turns out this may not have been completely accurate.\\r\\n\\r\\nAfter completing step 1 and 2, step 3 and 4 are simply a matter of implementation: I choose to use a simple random-walk Metropolis Hastings sampler to approximate the posterior distribution in step 3. The optimization in step 4 was done using standard gradient-based optimization, with random restarts to avoid local minima.\\r\\n\\r\\nLike I remarked in the competition forums, the outcome of this competition was more noisy than is usual: final prediction accuracy was judged on a set of only 90 cases, with an evaluation metric that is very sensitive to small (angular) perturbations of the predictions. The public leaderboard standings were even more random, being based on only 30 cases. In fact, the 1.05 public score of my winning submission was only about average on the public leaderboard. All of this means I was very lucky indeed to win this competition. Nevertheless, the runner-up seems to have taken a\\xa0very similar approach, suggesting there is at least something to be said for looking at this kind of problem from a Bayesian perspective.\\r\\n\\r\\nFinally, I would like to thank the organizers Dave and Tom, and sponsor Winton Capital for organizing a great competition. Looking at a problem different from the standard regression/classification problems was very refreshing.\\r\\n\\r\\nPhoto Credit: Dark Matter Visualization for LBC Dataset,\\xa0NPACI Visualization Services -\\xa0Amit Chourasia,\\xa0Steve Cutchin. \\xa0San Diego Supercomputer Center, ENZO Early Universe Simulation\\n\", 'Cross-posted from rouli.net.\\r\\n\\r\\nNothing could have prepared me for the\\xa0pleasant\\xa0surprise awaiting me last Thursday\\'s morning. Just a few hours earlier I\\'ve submitted my final predictions on\\xa0Kaggle\\'s Event Recommendation Engine Challenge\\xa0(in short - the goal is to predict which events will be interesting to a given user). The public leader board was frozen a week earlier, and at the time I was at the 11th place, after falling six places in a couple of days. Moreover, during the board-freeze session, I didn\\'t find any feature or modeling technique that improved significantly my score on my own validation set. I was hoping to be in the top 10%, but woke up to find that I won the third place, reaching my best ranking ever.\\r\\n\\r\\nI strive to learn from each competition I participate in, and this one is no different. However, my take-aways from this challenge don\\'t involve a new\\xa0algorithm\\xa0or feature selection insights. Rather, they are lessons about handling data on Kaggle challenges and in real life :). But first, the boring stuff:\\r\\n\\nI used pandas, which I really wanted to try out (great, but I\\'m missing some of R\\'s data exploration methods), scikit-learn and\\xa0iPython notebook (fantastic!).\\nI\\'ve treated the challenge as a classification problem, and ranked the events by their predicted probability to be classified as interesting.\\nMy model is basically an average between a Random Forest and a Gradient Bosting Classifier, with a sprinkle of Naive Bayes on the events\\' descriptions.\\nThe top feature for me (as judged by the random forest), was the time between when the user was presented with the event and the event\\'s date, or delta for short. Next come the ratio of invited users that decided not to attend an event (surprisingly, the higher the ratio, the more interesting is the event), \\xa0the total number of attendees, and the estimated distance between the user and the event (more on this later).\\n\\n\\nLesson One: Over Fitting is a Bitch\\r\\nAs you can see in the graph below, the public score was a very bad predictor for the private (and final) score.\\r\\n\\n\\n\\r\\nLuckily, I\\'ve trusted more my five-fold validation averaged score than the scores I got on the public leader board. But boy, was that frustrating. A few times during the competition I\\'ve submitted my predictions after fixing some bug or improving the reliability of some feature just to get a lower public score. Even worst, in the last couple of days before the board freeze, I was unable to improve my public score and was watching new-comers getting far better scores than mine. I was exiled from the warm comfort of the fourth place to the 11th place. It wasn\\'t easy to ignore the public score and not to optimize against it, but luckily I did so.\\r\\n\\n\\n\\nLesson Two: Don\\'t Believe Random Forest\\'s Hype\\n\\r\\nIt was my first time successfully employing a random forest classifier as the main predictive mode (usually, logistic regression works better for me), and I believed all the hype about random forests being better at avoiding over fitting. However, the telltale this isn\\'t the case here was observing that adding features to the model sometimes decreased my validation score.\\r\\n\\r\\nI\\'ve combated that behavior by limiting the trees in the random forest to a certain maximum depth and a minimum number of samples at each leaf, and averaging with the GB classifier (which I haven\\'t tweaked).\\r\\n\\r\\nKnowing what I know now, I would have also dropped some features that I\\'ve added to the model just because they seemed relevant and I believed\\xa0Overkill Analytics\\' approach of adding as many features as possible to a random forest, and let it sort them out.\\r\\n\\n\\n\\nLesson Three: There\\'s Always Some Data Leakage\\n\\n\\r\\nMidway through the competition I\\'ve discovered a feature with a very strong predictive power. Turns out that in many cases you could guess whether a user in the train set is interested in a specific event just by looking at the timestamp when he observed that event. If a given user observed several events at timestamp X, and another one at timestamp Y>X (even if those are just a few seconds apart), that other \"later\" event was probably marked as interesting.\\r\\n\\r\\nObviously this was some sort of a bug in the train set, and once I\\'ve proved that it was exploitable by reaching the third spot, I\\'ve (foolishly, since I didn\\'t get any \"finders fee\" :)) alerted Kaggle about it. [Kaggle note: \\xa0Much appreciated!!] Fortunately, I got back to third place without such dirty tricks.\\r\\n\\r\\nHowever, in the way the train and test sets were assembled, there\\xa0was some leakage that could (and should) be exploited. First, for each user we had a list of about six events, from which exactly one was marked as interesting. That is, we are given a lot of information in comparison of the classical classification problem. I\\'ve exploited that by having a feature that compared the delta of each event displayed to a user against the delta of the earliest event. This proved to have more predictive power than the number of friends a user had in the event.\\r\\n\\r\\nMoreover, we can assume that startup behind the competition already propose only relevant events to its users. For example, users from Indonesia were rarely presented with events happening in the US. They probably do this by examining IP address which they chose not to share in the train set.\\r\\n\\r\\nThis led to my first attempt and still most successful at creating a \"distance between user and event\" feature; I simply calculated the median of the locations of the events presented to the user, and for each event calculated its distance from that median, as though the median was a substitute to the user\\'s location. This worked better than deriving the user coordinates by looking at events\\xa0occurring\\xa0at the same city as the one found in the user\\'s profile, and averaging between them.\\r\\n\\n\\nLesson Four: Always (Always!) Use Random Seeds\\r\\nSome of my features are derived from graphs, such that a pagerank score for events according to their\\xa0attendance\\xa0graph (where events share an edge with a weight that depends on the Jaccard\\xa0similarity\\xa0between the users that attended them), or, like many other did, events and users clusters. Sadly, because my hardware isn\\'t powerful enough, I had to prune the graphs and I did so by deleting edges at random.\\r\\n\\r\\nBig mistake. Now I cannot recreate my final submission, and after all the work I put into it, I may not see the prize money. I was smart enough to set a random seed for my random forest, but too lazy or stupid to that when creating the graph. Gah!\\r\\n\\nLesson Five: Things that didn\\'t Work\\r\\nI\\'ve tried a lot of things during this competition, most of them didn\\'t work, or at least I couldn\\'t prove them to be working. However, they are good things to consider in future competitions:\\r\\n\\n\\nI\\'ve tried using\\xa0ELO rating\\xa0as an extra event-rating technique over the classification ones. It showed some (a lot!) of promise in the public test set, but failed in my 5 fold validation sets.\\nI\\'ve tried calculating the \"distance\" between a user and an event by considering a graph with edges between users according to their friendship status, and between users and events according to their\\xa0attendance. This was a complete failure.\\nNaive Bayes (or more precisely, Multinomial Naive Bayes) on the event\\'s descriptions was very disappointing. I then splited the users according to geographies and had a different NB classifier for each, which made this technique only slightly disappointing.\\nPage-ranks of events (and even more so, page-ranks of the events organizers) didn\\'t contribute much.\\nI\\'ve tried filling up missing users and events locations by using an iterative approach; I guessed the location of each user by averaging the\\xa0coordinates\\xa0of the events \\xa0she attended, and the location of each event by averaging the coordinates of the users who attended it. I did this repeatedly until I failed to discover the locations of any new events or users. This actually made my performance worse.\\n\\nEpilogue\\n\\r\\nWow, did you really read this very long blog post? If so, you may be interested in following me on\\xa0Twitter. Hope you enjoyed it, and feel free to comment, especially if you notice any grammatical or spelling mistakes :).', 'Xavier Conort is currently the number 1 ranked Kaggle data scientist and member of team \"Gxav &*\", winners of\\xa0Flight Quest.\\r\\n\\r\\nQ: What is your background? What did you study in school, and what has your career path been like?\\nXavier Conort: I am a French actuary with more than 15 years of working experience in France, Brazil, China, and Singapore. I studied actuarial science and statistics in ENSAE Paris Tech and University Paris Denis Diderot. Before becoming a data science enthusiast, I held different roles in the insurance industry (actuary, CFO, and risk manager).\\r\\n\\r\\nI currently work in the Data Analytics department of I2R (Institute for Infocomm Research, a research institute under the A*STAR family in Singapore) and develop analytics techniques and solutions together with my teammates of the GE Flight Quest. Our department has around 40 data scientists and serves several major clients like Visa and Boeing. We are one of Singapore’s largest R&D teams of data scientists.\\r\\n\\r\\nMy teammates Hong Cao, Hon Nian Chua, Clifton Phua, and Ghim Eng Yap have PhDs in various areas of data analytics. They were all trained in Singapore, except Clifton who was trained in Australia. Recently, Hon Nian completed his post-doc stints in the University of Toronto and Harvard University, and Clifton left our department and joined SAS.\\r\\n\\r\\nQ: How long have you been competing on Kaggle?\\r\\n\\r\\nI started to compete about 18 months ago but am already considered a veteran.\\r\\n\\r\\nQ: What other kinds of challenges have you solved for companies through Kaggle?\\r\\n\\r\\nThe problems I solved for companies through Kaggle were very diverse. I, with Marcin Pionnier,\\xa0detected if a car purchased at auction is a good buy or a lemon in “Don’t Get Kicked\"\\xa0(1st). I predicted with my teammates from DataRobot biological activities of different molecules given numerical descriptors generated from their chemical structures in the “Merck Molecular Activity Challenge\"\\xa0(2nd). I forecasted monthly online sales in “Online Product Sales\"\\xa0(2nd). \\xa0I modeled the probability that somebody will experience financial distress in “Give Some Credit\"\\xa0(2nd). I developed scoring engines to support the grading of student written essays in the 2 challenges hosted by the Hewlett Foundation (4th). I predicted customer retention for Allstate in “Will I Stay or Will I Go?\" (4th). And I identified patients diagnosed with Type 2 Diabetes in “Practice Fusion Diabetes Classification\"\\xa0(4th).\\r\\n\\r\\nMy teammates for GE Flight Quest have also won academic data mining competitions (outside Kaggle) together with various colleagues from I2R. They placed 1st in PAKDD 2012 Churn Prediction, ACML 2012 Fraud Detection in Mobile Advertising, and Opportunity’s 2011 Mobile Activity Recognition Challenge. In addition, they have achieved top-5 positions in many other competitions.\\r\\n\\r\\nQ: What do you like best about these competitions? Why do you think they’re successful at solving problems for businesses and other organizations?\\r\\n\\r\\nI like the diversity of problems to solve and I enjoy getting live feedback from the public leaderboard. It makes the fight for the best model very concrete.\\r\\n\\r\\nI believe that the competition framework is a win-win scenario. Competitors get access to real-world data to test their algorithms and their modeling skills. Competition hosts benefit by bringing out the best from us, obtain very strong accuracy benchmarks and get the opportunity to implement innovative solutions coming from different industries.\\r\\n\\r\\nQ: What skills do you think are important for a successful data scientist? Did you learn these skills in school, on the job, or on your own?\\r\\n\\r\\nI think that what makes a good data scientist is more of the right attitude than skills. Besides a strong background in statistics or computer science, a good data scientist is a person who loves to solve problems. (S)he is not afraid of putting is (possibly) unrecognized hard work because short cuts rarely produce good results from data. And (s)he is open-minded and is excited to learn new things.\\r\\n\\r\\nI personally discovered machine learning 2 years ago, thanks to Andrew Y. Ng’s Coursera course and Hastie et al’s book titled “The Elements of Statistical Learning,” but learned to really make sense from data when I was working for the insurance industry as an actuary and CFO, and in university when I studied statistics.\\r\\n\\r\\nMy wife (also an actuary) tells me I don\\'t think like a normal person (usually after I\\'ve given her a long complicated answer to what she thinks is a 30 second question), but she thinks that\\'s mainly because I\\'m French.\\r\\n\\r\\nQ: Why do you think your algorithm/predictive model was able to improve on aviation industry benchmarks?\\r\\n\\r\\nIt is certainly due to the fact that many industries work in isolation. Companies like Kaggle, with its large community of data scientists and I2R (my current workplace) are changing the game by bringing new solutions for those industries.\\r\\n\\r\\nQ: What was your process in developing Flight Quest algorithm/predictive model?\\r\\n\\r\\nThe algorithms we used are very standard for Kagglers. We used Gradient Boosting Machine and Random Forest, which have proved to work very well in other competitions too.\\r\\n\\r\\nWe spent most of our efforts in feature engineering. Our final feature selection is a collection of flight statistics and attributes, weather information during the flights, traffic in airports and weather conditions at arrival. We were also very careful to discard features likely to expose us to the risk of over-fitting our model.\\r\\n\\r\\nQ: Based on the data you were given, what challenges did you encounter when developing your model? Was there anything outside of the data you had to consider?\\r\\n\\r\\nUnlike the usual competitions, we did not have standard structured data that we could use to produce a quick first solution. We spent a tremendous time \\xa0exploring the numerous datasets, visualizing the data, understanding which data could bring value, and elaborating a strategy to convert this insight in usable features before producing a first model.\\r\\n\\r\\nQ: What was the most challenging part of this data quest?\\r\\n\\r\\nThe timeline of the competition was our biggest challenge. The most critical deadline of the competition was just a few days after Chinese New Year. Chinese New Year is a 4-day period during which you are supposed to spend time with your family, not with data and algorithms!\\r\\n\\r\\nQ: What is your definition of a data scientist? What impact will data science and data scientists have on the aviation industry?\\r\\n\\r\\nI will consider myself a fully qualified data scientist when I am able to build a one-stop solution that produces high accuracy for very large data sets.\\r\\n\\r\\nProliferation of the use of sensor networks and low-cost communications generate large volumes of operational data in the aviation and other industries. This opens up tremendous opportunities for data scientists to contribute in various aspects. Our department is already working with aircraft manufacturers and suppliers to apply data science to the areas of manufacturing equipment health monitoring, fuselage integrity monitoring and engine airflow optimization.', \"What was your background prior to entering this challenge?\\r\\n\\r\\nI had been working on wireless communication and signal processing for over 10 years and was well established. I received the 2010 IEEE Stephen O. Rice Prize (best paper award for communications), and was serving as an editor for IEEE Transaction on Wireless Communications. It was my wife who told me about the Netflix prize two years ago. Since then, I'm more interested in data science. Of course, participating in Kaggle challenges gives me valuable experience.\\r\\n\\r\\n\\xa0What made you decide to enter?\\r\\n\\r\\nBen's benchmark code already established the pipeline that avoids a lot work on data IO. It was extremely attractive to me at that time since I was very exhausted with the GE flight quest. I started working on the problem two weeks before the deadline. Therefore, I would like to thank Ben for his initial work. Technically speaking, most text mining problems belong to classification; I wanted to gain some experience of regression with text mining.\\r\\n\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nTypical text feature extraction techniques are applied to the raw data, such as text normalization, stop words, n-grams, TF-IDF. I tried ridge regression, SGD, random forests and also converted the regression problem into a classification one, for which I tried native Bayes, SVM, and logistic regression. Finally, I blended the SGD regression and logistic regression based predictor.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nSince salaries are not distributed smoothly, some models that can explore local properties would outperform linear regression. My background in information theory also helped me discover that 4~5 bits good enough to quantize salary values, which benefits computational complexity reduction.\\r\\n\\r\\n\\xa0Were you surprised by any of your insights?\\r\\n\\r\\nNo surprise on the score of each submission made a surprise to me. Overfitting didn't bother me with most methodologies I tried. The results are very consistent in cross-validation and two leader boards.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nPython, scitkit-learn.\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\n\\r\\nIn this competition, there are no significant features at all. It is not surprising that the first and second winners all use neural networks. More interestingly, my model can be regarded as a neural network with a manually created hidden layer. It does help me understand neural networks / deep learning better.\\r\\n\\r\\n-------------------------------------------------------------------------------\\r\\n\\r\\n\\nGuocong Song\\xa0placed third in the Adzuna Job Salary Prediction competition. He received his\\xa0PhD in Electrical and Computer Engineering from Georgia Institute of Technology MS, and his BS in Electrical Engineering from Tsinghua University Aside from data science, his expertise is in: Signal processing, stochastic optimization, wireless networks and devices He has received the IEEE Stephen O. Rice Prize Paper Award, and the best paper award in IEEE Transactions on Communications in 2010. He lives in\\xa0Cupertino, CA.\", \"What was your background prior to entering this challenge?\\r\\n\\r\\nI just completed a PhD in Machine Learning at the University of Toronto, where\\xa0Geoffrey Hinton\\xa0was my advisor. Most of my work is on applying deep learning techniques to aerial image analysis, so I have a lot of experience in training neural networks with tens of millions of parameters on big datasets.\\r\\n\\r\\nWhy did you enter?\\r\\n\\r\\nI had a bit more spare time after completing my thesis so I decided to do a quick project before leaving Toronto.\\xa0 I chose this particular competition because it involved text data and, while that is not something I had a lot of experience with, it seemed like a problem where neural nets should do well (and indeed the 2nd place finisher also used a neural net).\\r\\n\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nI did relatively little preprocessing and feature engineering.\\xa0 I used separate bags of words for the job title, description, and the raw location.\\xa0 I also found that stemming the words in the title and description using the Porter stemmer and encoding them using tf-idf slightly improved the performance. The other fields, like the category, contract, and source, were represented using a 1-of-K encoding.\\xa0 The resulting input representation had between 10000 and 15000 features depending on how many of the top words I used.\\xa0 I did experiment with a number of alternative features and encodings but I did not get any noticeable improvements.\\r\\n\\r\\nFor the supervised learning part, I used deep neural networks implemented on a GPU.\\xa0 I trained the neural nets by optimizing mean absolute error (the evaluation metric for this contest) using minibatch stochastic (sub)-gradient descent and used dropout in order to help avoid overfitting.\\xa0 My best single neural network achieved a score of about 3475 on the public leaderboard, but my final submission averaged the predictions of three neural networks to get down to about 3435.\\xa0 I did not combine neural networks with any other learning methods.\\r\\n\\r\\nThis approach might sound familiar to readers of this blog because my office mates,\\xa0George Dahl\\xa0and\\xa0Navdeep Jaitly, and their team mates recently used a nearly identical architecture in their winning entry for the Merck Molecular Activity Challenge, although there are some differences due to the particulars of that contest.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nMy most important insight was to simply train a powerful and flexible model by directly optimizing the loss function used to determine the winner. Some competitors used complicated ensembles of many disparate models, most of which were not optimizing the correct objective. These people needed to use\\xa0leaderboard and validation error feedback much more heavily than I did since their model selection process was the only part of their pipeline that directly optimized the evaluation metric.\\r\\n\\r\\nWere you surprised by any of your insights?\\r\\n\\r\\nI was somewhat surprised by how little improvement I got from my attempts to engineer better features.\\xa0 For example, I didn't get any improvement from using bigrams or from adding information derived from the normalized location or location tree.\\xa0 Since other competitors have reported noticeable gains in performance from using these features on the competition forum, I suspect that the deep nets I trained were able to learn some of these features automatically.\\xa0 While this is definitely a pleasing result, it is a little surprising even to neural network experts because neural nets are generally considered to be quite sensitive to the input representation.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nI used Python along with a number of open-source Python packages.\\xa0 I used pandas for loading and exploring the data and scikit-learn for its feature extraction pipeline, although I ended up implementing my own text vectorizers for improved memory efficiency.\\xa0 I also used NLTK for its implementation of the\\r\\n\\r\\nPorter stemmer.\\xa0 Finally, I used my own implementation of deep neural networks which relies on\\xa0Tijmen Tieleman's\\xa0gnumpy\\xa0library and my own\\xa0cudamat\\xa0library for GPU support.\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\n\\r\\nI learned quite a bit about how feature engineering interacts with different neural network architectures.\\xa0 In particular, I thought it was really interesting that\\xa0Vlado Boza\\xa0placed 2nd with a completely different neural network architecture and set of features.\\r\\n\\r\\nVlad Mnih\\xa0is a machine learning researcher based in London, England. \\xa0He holds a PhD in Machine Learning from\\xa0the University of Toronto and an MSc in Machine Learning from the University of Alberta.\", \"What was your background prior to entering this challenge?\\r\\n\\r\\nI am finishing my Master’s degree in computer science. I was a software engineering intern at Google working on some machine learning problems.\\xa0I've\\xa0also entered several Kaggle competitions during the last year. I am the founder of Black Swan Rational - a Slovak company specialized in predictive analytics.\\r\\n\\r\\nWhat made you decide to enter?\\r\\n\\r\\nI had some spare time, so I decided it to spend it on some Kaggle competition. At that time there were three competitions running: Job Salary Prediction, Blue Book for Bulldozers, and Whale Detection. Whale Detection already had quite impressive submissions and I\\xa0didn't\\xa0want to spent time just by tweaking a model to get a 0.001 % difference. With Blue \\xa0Book, I thought that there\\xa0would\\xa0be no significant difference between the random forest benchmark and the best submission and it would end up as a big ensemble fight. The Job salary data seemed to be pretty clean and easy to work with. And also there were a lots of possible approaches.\\r\\n\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nI extracted simple binary text features from title and description and also used categorical features for location, company, and source. My whole model was just an old-school neural network with two small hidden layers trained by back propagation. Before that I used nearest neighbor model which was quite successful (got error around 4200).\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nDuring one point I found out that there are too many similar ads and that their salary differs on average by 2000. I used this in my nearest\\xa0neighbor\\xa0model. But neural network could handle this even better without any hacks.\\r\\n\\r\\nWere you surprised by any of your insights?\\r\\n\\r\\nAd similarity was the only thing.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nI have coded all of my algorithms in C++ (I did small preprocessing in Python). I tried to use scikit-learn but it\\xa0didn't\\xa0lead to any big success.\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\n\\r\\nI have to improve my coding practices.\\xa0I've\\xa0made many stupid bugs just because of this. And I also should start to use some versioning system better than “do backup sometimes”.\\r\\n\\r\\n\\xa0\\r\\n\\r\\n------------------------------------------------------------------------------\\r\\n\\r\\n\\nVlado Boza\\xa0won Second Prize in the Adzuna Job Salary Prediction Competition. He is finishing his Master's studies of computer science at Comenius University in Bratislava.\\xa0He spent two summers as Software engineering intern at Google working on machine learning problems.\\xa0His interests include building fast and effective algorithms, hard optimization problems and machine learning.\", 'Posting a summary on behalf of Cornell researchers. From my side I would like to add, that Marinexplore has partnered with Cornell University to develop acoustics related capabilities of our spatio-temporal data platform. Improved analytics of acoustic data is relevant not only to shipping industry, but also to other businesses like offshore industry. Globally there are many public acoustic datasets yet to be integrated with\\xa0marinexplore.org\\xa0as well.\\r\\n\\r\\nThank you everyone for participating in our challenge and pushing the boundaries together.\\xa0Feel free to contact me directly should you want to use our solutions in your organization, explore collaboration options, join our team or just learn more about Marinexplore.\\r\\n\\r\\nMeanwhile we posted a summary of the competition in\\xa0our blog\\xa0and launched an exploratory data challenge for finding the best use of public ocean data with a\\xa0prize of $3000.\\r\\n\\r\\nAndré Karpištšenko\\nCo-founder at Marinexplore, Chief Scientist\\nandre@marinexplore.com\\r\\nskype:andre\\n\\r\\n\\r\\nThe Bioacoustic research program (BRP) at Cornell University has had the honor to co-host with Marinexplore the first ever North Atlantic right whale call-classification competition. Thank you all for contributing your time and never-ending brainstorms, and for making the competition exciting, interesting, intellectually rewarding and totally successful.\\r\\n\\r\\nWe received the documents and source codes from the top two winning Kaggle participants. Many participants also kindly share their insightful thoughts and even source codes on the competition’s message board. We are currently building a new automated right whale detection-classification system, which will include the algorithms from the Kaggle competition and will apply it to a 44-month, continuous recording dataset. We expect that this system will yield a greater understanding of right whale calling behavior, such as their daily & seasonal communication patterns, as well a deeper understanding of the influences of human noise on the whales’ acoustic communication and habitat. You, the participants in this competition, have been and still are the most important partners in our efforts to save right whales.\\r\\n\\r\\nMethods\\r\\n\\r\\nBoth winners used an approach that defines a frequency-time “tight box” bounding the occurrence of the right whale call in a spectrogram, followed by extraction of a customized set of features for each tight box. The 1st place winning team used a multiple template matching approach, while the 2nd place winning team used a Viterbi algorithm to find the exact trajectories of frequency up-sweeps. The tight boxes make the features more consistent and robust and thus more frequency-invariant and/or time-invariant.\\r\\n\\r\\nBoth winning methods also designed several feature vectors from different perspectives to incorporate information from either the spectrum, the temporal dynamics of a call’s frequency-modulation, and even the temporal ordering of labeling (positive or negative). The last variable, temporal ordering, emerged from the ordering and numbering of the files and labels identifying the calls in the dataset. As a result, many positive classification events appear consecutively. This temporal clustering feature in this dataset might not be something reliable that we could use in our updated automated detection system. However, this feature could be useful to discriminate between right whale up-calls, which almost always occur as individual transients, and humpback whale frequency-modulated upsweeps, which are either notes within a song or produced as a series of calls.\\r\\n\\r\\nMany participants applied a deep learning approach (in particular, a convolutional network) and achieved high scores (e.g. contestants ranked #3, #4, and #6). In our understanding of their deep learning approach, the spectrogram of a right whale call is treated as an image in much the same way as a handwritten digit.\\r\\n\\r\\nMany contestants used Python as the preferred programming language, reflecting the fact that modules of Python, such as Sci-Kit-learn, Sci-py, Num-py, have become standards in the world of data analysis. Accordingly, several classifiers, for example gradient boosting and random forest, were preferred over others by the participants.\\r\\n\\r\\nData integrity\\r\\n\\r\\nSeveral participants expressed concerns about data integrity. To some participants some of the audio clips tagged as right whale up-calls did not sound like an up-call, and vice versa. The following are two additional results we need to keep in mind for the particular dataset used in this competition:\\r\\n\\r\\n(i) Some audio clips had very low signal-to-noise ratio (SNR).\\r\\n\\r\\n(ii) An audio clip tagged as a right whale up-call might actually be a non-biological sound or a sound from a different species.\\r\\n\\r\\nWhen both (i) and (ii) occur simultaneously, things can get tricky. The energy from a right whale call might be much lower than the energy from the other sound object in the sound sample. On the other hand, some audio clips tagged as “no-call” sounded like and could appear similar to an up-call in a spectrogram. One possible explanation for this conundrum is that humpback whales, which are renown for their vocal virtuosity, are responsible for these confounding calls. However when humpbacks produce up-call like sounds, they typically produce them in a repetitive sequence. Thus, if a longer acoustic sample had been provided, instead of just the 2-sec clip, discrimination between a single call occurrence (i.e. a right whale up-call) and a sequence (i.e., a humpback song note or call sequence) might have been more obvious, thereby improving correct classification of the sound.\\r\\n\\r\\nFuture\\r\\n\\r\\nWe are going to apply the top two winning methods, along with other methods developed in the Bioacoustic Research Program, to improve our abilities to automatically detect and classify right whale calls. The suite of new methods will also include deep learning and computer-vision-based techniques. All of these methods will be a core part of our new, automated acoustic detection-classification system for large-scale analysis for endangered species, including whales, elephants and birds. One of the first technical challenges is to have the automatic detection-classification process operate on a continuous, long-duration audio stream (e.g. months to years). We’re investigating methods from computer vision and image processing that will locate connected regions, as well as an efficient method for applying a sliding window, by which classification is repeatedly applied along a continuous audio stream. Presently a comprehensive performance evaluation is ongoing using an 8-day dataset. One goal in the next few months is to apply methods from this competition on a 44-month, continuous underwater sound recording. Another very important goal is to use the source code that you all have produced to improve automatic detection-classification systems that listen for whales in order reduce the chances of whales being killed by ships (e.g. right whales in the shipping lanes off Boston, USA,\\xa0www.listenforwhales.com).\\r\\n\\r\\nIt is very obvious from the energy and productivity of the participants in this competition that this was not just about prize money. It was about how a group of smart, motivated people, who were strangers, could work as a group of competitive altruists, to produce software that will have a real benefit for the natural world and the ocean environment, and especially for improving the chances of survival for a species that is near extinction. A huge, huge thank you to all the participants of this excellent competition.\\r\\n\\r\\nAnd a huge, huge thank to Kaggle and Marinexplore for enabling this to become reality.', 'We caught up with the winner of the immensely popular Amazon Access Challenge\\xa0to see how he edged out thousands of competitors to predict which employees should have access to which resources.\\nWhat was your background prior to entering this challenge? What did you study in school, and what has your career path been like?\\nMy background is a bit eclectic; I spent my time in undergrad multitasking between three universities (UC Berkeley, Sciences Po Paris, and the Sorbonne), where I studied mathematics, econom(etr)ics, and social sciences. I’ve since been self-learning machine learning and programming on the job -- the engineers I work with would tell you that I still have a long way to go regarding the latter.\\nI recently moved from France to San Francisco. I currently work as a Data Scientist at Eventbrite, where I am in charge of building the fraud and spam detection models. I also teach data science on occasion, most recently at Zipfian Academy.\\nWhy did you enter?\\nFirst, I was curious to see how well I could place using the knowledge I had recently acquired. Given of the huge number of participants in the Amazon challenge, this was the ideal competition to enter.\\nSecond, the fact you’re studying the same dataset as many other people makes for great dialogue opportunities. This is why I tried sharing as much as possible during the competition, and the response has been very inspiring. People were starting interesting discussions left and right, notably Miroslaw Horbal who sparked a great exchange about feature selection.\\nWhat preprocessing and supervised learning methods did you use?\\nI used an ensemble of linear and tree-based models that were each trained on a slightly different feature set. The features themselves were extracted by cross-tabulating each categorical variable so as to get an idea of how rare each combination is -- because most requests end up being approved and because the number of different categories made up for a lot of noise, I chose to treat the problem more as an outlier detection problem and I created my features as such.\\nThe models were then combined by using their output as an input for a modified linear regression. This second-stage model also incorporated the size of the support for each category as meta features in order to dynamically decide which base model to trust the most in different situations. I ultimately teamed up with Benjamin Solecki, who used a very similar method with slightly different features, which further improved our score when incorporated into the ensemble.\\nYou can find the code on Github\\xa0and a more detailed explanation of the methodology on the forums.\\nWhat was your most important insight into the data?\\nNot spending too much time in feature selection vs. feature engineering. \\xa0Because there was a lot of noise in the dataset and the variance seemed to be high depending on how I would split my train/cross-validation sets, I focused mostly on improving the generalization power of my algorithms by creating classifiers with different strengths.\\nWere you surprised by any of your insights?\\nI noticed that fine tuning (both in terms of feature selection and hyperparameter optimization) didn’t seem as critical in the context of ensembles of different classifiers. In fact, I would sometimes notice that changes that improved the performance of each of my individual models would actually decrease the performance of the overall ensemble!\\nWhich tools did you use?\\nI used mostly Python with the scikit-learn library, with a mix of pandas and R for data exploration.\\nWhat have you taken away from this competition?\\nI learned quite a bit about how to find a balance between feature engineering and feature selection, both in the context of single models and more complex ensembles. The fact the data only consisted of categorical variable was an added challenge as well.\\nI think what is great about Kaggle competitions is the fact they are self-contained: you have a well-defined problem, a well-defined dataset, and a clear evaluation metric. As such, they are an ideal testing ground for new ideas and algorithms. In real life, things are unfortunately not as easy. You often end up having to figure out how you want to build your model, what data you want to use (and how to get it), and the optimization objectives all at once -- sometimes for problems that don’t even exist yet.', \"What was your background prior to entering this challenge?\\r\\n\\r\\nI have a BSc + MSc in Mathematics, Statistic and Operations Research and postgraduate in Scientific Programming. I'm a health data analyst in a university hospital. I discovered Kaggle when Heritage Health Prize was launched and since then I've participated in several challenges.\\r\\n\\r\\nWhat made you decide to enter?\\r\\n\\r\\nI couldn't participate in\\xa0the\\xa0hackathon, so I think this was like a second opportunity. The problem was very attractive and I thought the idea of citizen participation for detect community issues was very interesting.\\r\\n\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nAfter reading the hackathon forum and seeing how the better results were obtained with a short training data set with most recent observations, I decided\\xa0as a priority to use as much data as possible for giving the model enough robustness.\\r\\n\\r\\nThe main problem was dealing with the time anomalies, so I forced the models to learn without using absolute time features (day the issue was sent).\\r\\n\\r\\nThe hypotheses were:\\r\\n\\nThe response to an issue depends (directly or inversely) of number of recent issues and similar issues (time dimension).\\nThe response to an issue depends (directly or inversely) of number of issues and similar issues reported close (geographic dimension).\\nThere are geographic zones more sensitive to some issues (geographic dimension).\\n\\r\\nWith that in mind I defined three time windows -- short, middle and long -- and three epsilon parameters for using them in a radial basis distance-weighted average for each issue.\\r\\n\\r\\nThe selection of these values were for adjusting the decay shape in a way the weights represent city, district and neighbour ambits.\\r\\n\\r\\nFor each issue I computed a 3x3 grid of features for each tag group, using radial basis weights respects the distance in kms between issues.\\r\\n\\r\\nFor a period of last 150 days and for each issue, I computed the LOO (Leave One Out) weighted radial basis average for comments, votes and views for city, district and neighbour parameters.\\r\\n\\r\\nFor summary feature I created a binary bag of more frequents words.\\r\\n\\r\\nI fitted several models: boosted trees, random forest and general linear models and ensembled them with a ridge regression to calibrate the estimations.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nThe source of the issues had a great importance in the responses, and in each city the features' range of values and variability were very different. This explains why using stratified models worked so well. The preprocessing of the data was crucial for fitting models for all the cities at a time.\\r\\n\\r\\nWere you surprised by any of your insights?\\r\\n\\r\\nIn my models, bag of words of summary had a small influence in predictions. I think probably a binary bag of words is\\xa0too simple and using high order tuples would be necessary.\\r\\n\\r\\nThe models trained with the grid of features got extract the time and geographical information without using an absolute time feature nor longitude and latitude.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nR packages gbm, randomForest and glmnet.\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\n\\r\\nI'm surprised how well the big column approach (train the responses all together and stacking the dataset one time for each one of them) works in this case.\\r\\n\\r\\nAnd with this competition I got the #1 spot in Kaggle rankings. I'll never forget that!\\r\\n\\r\\n------------------------------------------------------------------------------\\r\\n\\r\\n\\nJosé\\xa0Guerrero won First\\xa0Place in the See Click Predict Fix Competition. He has worked more than 25 years in the\\xa0health sector in Spain in epidemiology, research, electronic medical records, and\\xa0senior management\\xa0at a university hospital.\\xa0 He is currently crunching\\xa0big databases at the region's main hospital.\", \"What was your background prior to entering this challenge?\\r\\nI studied Electrical Engineering during undergraduate school, and worked as a software engineer in the telecom industry for several years. Later on I moved to Australia to pursue a PhD in Machine Learning at ANU/NICTA, which I finished a couple of years ago. I'm currently working as a Data Scientist at Commonwealth Bank.\\r\\nWhat made you decide to enter?\\r\\nI'm currently refraining from participating in long competitions, given how time consuming they can be, but since I had good results in the See Click Predict Fix - Hackathon I thought it would be worth trying this one.\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nI combined several methods in an ensemble, the main ones being boosting trees (GBM) and linear regression. I also tried random forests and neural networks, but I didn't invest much time in them.\\r\\nWhat was your most important insight into the data?\\r\\nProbably the most important insight was the fact that the distributions of the features and the labels were highly dependant on physical location and time.\\r\\n\\r\\nThe first aspect is easy to model, either by building separate models for each city, or by using an algorithm that can capture feature interactions.\\r\\n\\r\\nThe temporal aspect, however, is harder to deal with. Assuming the conditional distribution of the labels given the features is the same across all periods, we can apply covariate shift corrections. I tried a few methods, such as Kernel Mean Matching, but the performance gains were negligible.\\r\\n\\r\\nWhat did work was applying a simple constant scaling to the predictions (one for each city and each target variable). It only worked well, however, because I used feedback from the leaderboard to adjust these constants. This is not ideal, as we won't be able to do that in a real life situation, but given the way the competition was designed, it is unlikely that it would be possible for anyone to win without resorting to this kind of adjustment.\\r\\nWere you surprised by any of your insights?\\r\\nI was surprised that I couldn't extract much information from the description texts, as I thought that would be one of the richest sources of information.\\r\\nWhich tools did you use?\\r\\nMost of the work was done in R. For linear models I used vowpal wabbit, and to compute vector representations of the description texts I tried word2vec.\\r\\nWhat have you taken away from this competition?\\r\\nIt reminded me once again of the power of building several models together. That was the winner's big column approach (described here), where all three target variables (comments, views and votes) where trained in a single model.\", \"What was your background prior to entering this challenge?\\r\\nMy professional background is in business intelligence and analytics/reporting and Miroslaw’s background is in mathematics, so neither of us has a formal background in machine learning. However, we have both taken multiple online classes in machine learning topics, including Andrew Ng’s excellent StanfordX Machine Learning course.\\r\\nWe also have both competed in quite a few Kaggle competitions in the past year, steadily improving our skills and knowledge with each finish. Kaggle and its community have proven to be a goldmine for anyone interested in learning real-world machine learning techniques outside of academia.\\r\\nWhat made you decide to enter?\\r\\nMiroslaw and I both competed independently in the first portion of the SeeClickFix competition, a 24-hour Hackathon, and we performed well. We really enjoyed the dataset and the objective, so competing in the second portion of the contest seemed a natural progression for us both.\\r\\n\\r\\nIn the second portion of the contest, we both competed independently until about two weeks before the contest completion. At that time, we had both been steadily climbing the leaderboard with myself hitting 1st place on Nov. 16th and Miroslaw right behind me at 4th place. However, J.A. Guerrero had just joined the contest, and he and other top competitors were making steady gains on the leaderboard as well. At that time we decided it was best to hedge our bets and improve our odds of placing in the money by combining our models into a powerful ensemble.\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nMy approach was composed of a segmentation ensemble. This consisted of a distinct base model trained independently on each city and with remote API sourced issues trained together separately as well -- for a total of five segments. Because the variables for each of the cities seemed quite distinct (a lot of variable interaction present), this data seemed ideally suited to a segmentation ensemble. One advantage of segmentation is that it greatly reduces the number of samples and the dimensionality that each base model must deal with, so I was able to utilize gradient boosted regressors (GBMs) for most of the base models.\\r\\n\\r\\nMiroslaw’s approach was more focused on the text analysis side of the data. Using text from the issue summary and description fields, he created a tri-gram TFIDF vector from the entire training set, then combined it with other features that we had engineered. Because of the high dimensionality, only linear models were practical for this approach, and of those he found that ridge regression performed best.\\r\\n\\r\\nAfter we teamed up, our first week was spent strengthening our individual models by integrating some of the stronger features from each other’s code into our own and measuring the improvement, based on a combination of cross-validation score and leaderboard feedback. Our intuition was that while we did want to keep our models distinct to reduce bias within our ensemble, if a feature was proven to improve both cross-validation and leaderboard scores significantly, than its signal was powerful enough to justify inclusion in both base models.\\r\\n\\r\\nThen our final week was spent determining the ideal weights for averaging our models’ predictions together. A simple 50/50 blend made a surprisingly large gain on the leaderboard, but we then went more in-depth and performed segment based weighting for each of the models. For this we used a linear regression model to derive weights based on optimal cross-validation scores for each segment. Not surprisingly, the findings were that in some segments and targets Miroslaw's model performed better and needed to be weighted higher, while on others mine performed better. Lastly, to avoid overfitting the cross-validation test set we reduced weights to be less extreme if the linear model weighted either of our individual models too strongly.\\r\\n\\r\\nThe features used in our models were largely similar with other top competitors. We found that some of the best signals in the data came from description length, geographic location, issue tag type, issue source, summary/description text, and whether the issue was created on a weekend. From these, we created various binary and one hot encoded features, and we also used a reverse-geocoding service to derive zipcodes and neighborhoods from the issue’s latitude/longitude fields given in the data. This ended up paying off for us as neighborhoods in particular ended up being powerful predictors, more powerful than simply using latitude and longitude to approximate location.\\r\\nWhat was your most important insight into the data?\\r\\nBecause this contest was temporal in nature, using time-series models to make future predictions, most competitors quickly realized that proper calibration of predictions was a major factor in reducing error. Even during the initial Hackathon stage of the contest, it became well known on the competition forum that one needed to apply scalars to predictions in order to optimize leaderboard scores.\\r\\n\\r\\nBut while scaling was common knowledge, our most important insight came in applying our segmentation approach to the scalars. For example, rather than apply one optimized scalar to all predicted views for the entire test set, we applied optimized scalars for each distinct segment of the test set (the remote API sourced issues and the four cities). We then optimized the scalars using a combination of leaderboard feedback and cross-validation scores. What we found was that each segment responded differently to scaling-- so trying to apply one scalar to all issues, as many of our competitors were doing, was not optimal.\\r\\nWere you surprised by any of your insights?\\r\\nBy far our biggest surprise was the effectiveness of creating an ensemble model from our individual models. Initially, before deciding to team up, we were concerned that we may see minimal gain from combining our predictions, but we quickly realized that this was not the case. Even a simple average of the two gave a huge decrease in error.\\r\\n\\r\\nThe important lesson for us was that two distinctly developed models will yield huge gains when combined together, particularly when the two have a high degree of diversity (variance in errors) which is critical for an ensemble to perform well. We were fortunate that while our models shared many of the same features, Miroslaw’s had been designed to take more advantage of the text-based features and mine had been designed more to take advantage of the segments in the data.\\r\\nWhich tools did you use?\\r\\nWe both used the same stack of Python tools: scikit-learn, PANDAS, and NumPy. This ended up being a fortunate coincidence as it made it easy for us to share code snippets with each other.\\r\\nWhat have you taken away from this competition?\\r\\nFirst, it was a great experience collaborating together as a team. This was our first experience working on a team in a Kaggle contest, and we both agreed that it will not be the last. Working together allowed us to see the same problem from new angles and we learned many new techniques in just the two weeks we worked together. It was enlightening seeing the many creative solutions and insights that we each had developed. Our only regret is not teaming up earlier in the contest.\\r\\n\\r\\nSecond, we both learned the power of ensembles. Prior to this contest, neither of us had utilized higher level ensembles in previous competitions, always instead focusing on improving one strong model. No longer. Going forward, ensembles will be an important and often used tool in our toolkit. In fact, we were so motivated by our results that Miroslaw and I are developing a Python module to help facilitate the creation and use of ensembles.\\r\\n\\r\\nLastly, we thoroughly enjoyed participating in this contest, and we greatly appreciate SeeClickFix.com, David Eaves, and Kaggle for graciously providing us the opportunity to work with their data. Working with this data set was a blast and we had a great time learning from it and gaining new insight.\\r\\n\\r\\n------------------------------------------------------------------------------\\r\\n\\r\\n\\nBryan Gregory holds a B.S. in Information Systems from Texas A&M University and an M.B.A. with a concentration in Information Technology from Baylor University. He is a frequent Kaggler with an interest in the practical application of machine learning and business intelligence.\\nMiroslaw Horbal has a B.S. in Mathematics with a specialization in Combinatorics and Optimization from the University of Waterloo. He is a self-taught machine learning enthusiast with an addiction to data science competitions.\", \"We're seeing the discussion of many various models emerge on the forum of the recently closed Dogs vs Cats competition from the Kaggle Playground.... but this blog post from the 8th place team\\xa0fastml.com is worth a repost on No Free Hunch:\\r\\n\\nYesterday a kaggler, today a Kaggle master: a wrap-up of the cats and dogs competition\\nOut of 215 contestants, we placed 8th in the Cats and Dogs competition at Kaggle. The top ten finish gave us the master badge. The competition was about discerning the animals in images and here’s how we did it.\", 'What was your background prior to entering this challenge?\\r\\nWe\\'re a team of four. Christophe Bourguignat is a telecommunication engineer during the day, but he becomes a serial Kaggler at night, Kenji Lefèvre has a PhD in Mathematics and his background shows dangerous similarities with that of Baron Münchhausen.\\xa0Finally, Matthieu Scordia and I, Paul Masurel, are normal, healthy, happy, model employees of Dataiku (www.dataiku.com), respectively as Data scientist and Software Engineer. We all share a great interest in data science.\\r\\nWhat made you decide to enter?\\r\\nAt Dataiku, we\\'re building the perfect platform for Data Science. Florian (our CEO) saw in this competition an opportunity to test whether our product would make it possible for four data scientists to work together efficiently on a complex project, so he asked me to lead a team to compete in this challenge. (By the way, Dataiku would like to sponsor other teams on future Kaggle challenges, and provide them with the Studio and adapted computing power. If you’re interested, please contact us.)\\r\\n\\r\\nOn the other hand, Christophe had already developed a plain addiction to Kaggle and data science before knowing us. Finally Kenji saw this challenge as an accelerated introduction to a field that was brand new to him.\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nA lot of our features consisted of family of counters that would express how the user reacted in a past similar situation. For instance, for each occurrence where the same user had been offered the same URL in the past,\\xa0we labelled the outcome as one of the five following possibilities:\\r\\n\\nthe user skipped the URL, meaning he did not click on the URL but clicked on an URL which was on a lower rank.\\nthe user missed the URL, meaning he did not click on the URL and did not click on any URL with a lower rank.\\nthe user clicked on the url with a satisfaction of 0, 1, or 2\\n\\r\\nWe normalized the counter of each of the labels using additive smoothing with an arbitrary prior.\\r\\n\\r\\nFor supervised learning methods, our final solution was using Lambda Mart, an algorithm considered as the state of the art for Learning-To-Rank. Unfortunately, it relies on Gradient Boosting Trees, which do not parallelize. Our best submission could not take advantage of our 12 cores, and took around 30 hours to compute.\\r\\n\\r\\nIn order to quickly study the effect of the different feature, we preferred using scikit-learn\\'s Random Forest and a point-wise classification approach.\\r\\n\\r\\nHere are a couple of Grandma\\'s Tricks:\\r\\n\\r\\nWe tuned our hyperparameter (min_samples_leaf) to directly maximize the NDCG (the formula used for scoring solutions in this contest) on our cross validation set. To do so, we reinvented without knowing it a poorer version of an algorithm called golden section search.\\r\\n\\r\\nAlso, in order to compare different models we seeded the random selection of our cross validation set.\\r\\nWhat was your most important insight into the data?\\r\\nReading related papers on the subject, we kind of knew that even though collaborative filtering techniques came to mind for this problem, they weren\\'t the actual meat of the data. We primarily focused on fully mining more straightforward information: has the user already visited the URL? The domain? Was it for the same query? etc.\\r\\n\\r\\nAt the end of the contest we put more effort in trying to use more collaborative information. Our regularized SVD on domains just gave a score increase of 2.10-5 which was very disappointing.\\r\\n\\r\\nThough spending more time on collaborative information was probably the key to beat Yandex\\'s pampampampam team, retrospectively I\\'m still happy we did not focus on it too early.\\r\\nWere you surprised by any of your insights?\\r\\nWe worked a lot at the beginning of the contest to build a perfect reproduction of Yandex\\'s test dataset to avoid any bias in our training. But the default baseline we measured was way over the one announced by Yandex. The difference measured could not be explained by the score estimator variance.\\r\\n\\r\\nWe wondered whether we could explain this discrepancy by some seasonality in Yandex\\'s score: people search different things during the weekend, or even a weekly scheduling of some backend scoring process at Yandex could be the culprit.\\r\\n\\r\\nIn any case, we did notice a strong day-of-the-week seasonality: Yandex\\'s initial ranking was not as good during the weekend. Unfortunately, this was working against explaining the gap we had between their baseline score and ours. As of today, we still do not understand the inconsistency.\\r\\n\\r\\nThis however helped us understand that Day 1 was a Tuesday, which was also confirmed by the seasonality of the user\\'s requests.\\r\\nWhich tools did you use?\\r\\nObviously Dataiku Data Science Studio. The studio is language agnostic and allows data scientists to work in R, SQL, Hive, Pig... you name it. But I\\'m a Python advocate and all our code was written in Python. We also used a Java library called Ranklib for LambdaMart. Finally the random tree forest implementation was that of scikit-learn.\\r\\n\\r\\nMost probably because of Python\\'s design (google \"GIL\" for more information), the current version of scikit-learn parallelization is based on multi-processing. It means that taking advantage of the 12 cores of our computer would have required 12 times as much RAM.\\r\\n\\r\\nFor this reason, we used a fork of joblib from Olivier Grisel that fixes this issue by making job processes share memory. Scikit-learn is pretty popular among Kagglers so they will be happy to know that in future versions of scikit-learn, all these problems will be solved in an even more elegant fashion.\\r\\nWhat have you taken away from this competition?\\r\\nWe did not expect to do so well. The top of the leaderboard is full of former winners with a far more impressive academic pedigree than ours. At the risk of sounding cheesy, we attribute our result to teamwork. None of us would have reached top-10 ranks individually. In our case, the benefits of teamwork were not about the conjunction of different expertises! Your mates spot your bugs faster than you, they point out your fallacies, and they think about the feature you would have missed. Working in team is an efficient safety net, and a great timesaver. Finally, teamwork offers sheer emulation. Teamwork and Data Science are just by nature a perfect fit, and Dataiku Science Studio did a perfect job making it possible.\\r\\n\\r\\n------------------------------------------------------------------------------\\r\\n\\r\\n\\nTeammates of Dataiku Data Science Studio are based in Paris, France.', 'Sander won First Place in The Galaxy Challenge, sponsored by GalaxyZoo and Winton Capital. Although he already published a fantastic write-up on his own blog, Sander sat down for No Free Hunch to answer more\\xa0questions for the Kaggle community.\\r\\n\\r\\n[caption id=\"\" align=\"alignnone\" width=\"512\"]Image Credit: NASA and European Space Agency[/caption]\\r\\nWhat was your background prior to entering this challenge?\\r\\nI\\'m a PhD student in the Reservoir Lab of Prof. Benjamin Schrauwen at Ghent University in Belgium. My main research focus is applying deep learning and feature learning techniques to music information retrieval (MIR) problems, e.g. audio-based music classification, automatic tagging and music recommendation. I\\'d previously participated in the Million Song Dataset challenge and the whale detection challenge on Kaggle.\\r\\nWhat made you decide to enter?\\r\\nI thought the problem was an excellent match for a feature learning approach. There was lots of image data, and feature learning techniques are known to work particularly well in this setting. But more importantly, it was atypical image data: images of galaxies have quite different statistical properties compared to typical \\'natural\\' images. This made a feature learning approach all the more attractive, because a lot of common knowledge about image features does not apply here.\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nI used raw pixel data as input to my models, but I did do some preprocessing in the sense that I downsampled and cropped the images to reduce the dimensionality, and applied random perturbations to artificially increase the amount of training data (data augmentation). This was necessary to reduce overfitting. All preprocessing was done on the fly during training.\\r\\n\\r\\nI used convolutional neural networks with up to seven layers (4 convolutional layers and 3 fully connected layers). The goal of the competition was to predict a set of weighted probabilities, which adhered to certain constraints. I incorporated these constraints into the networks.\\r\\n\\r\\nI also modified the network architecture to increase parameter sharing, by taking advantage of the rotation invariance property of galaxy images. I cut the images into several overlapping parts and rotated them, so that the network would be able to apply its learned filters in various orientations.\\r\\n\\r\\nI trained the networks with stochastic gradient descent and momentum, using dropout in the fully connected layers for regularisation. I used rectified linear units in all convolutional layers, and maxout nonlinearities in the fully connected hidden layers.\\r\\n\\r\\nMy best single model had 7 layers and about 42 million parameters. Of course it was overfitting significantly, but despite that it still achieved the best score on the validation set.\\r\\n\\r\\nIn the end I trained 17 different models, with different architectures that were variations of the 7-layer architecture of the best model. This helped increase variance, which lead to a nice improvement when the predictions of all these models were averaged. I also averaged predictions across various transformed (i.e. rotated, zoomed) versions of the input images. These two levels of averaging gave my score a nice boost in the last few days of the competition.\\r\\n\\r\\nI wrote a detailed account of my approach on my blog and the code and documentation here on GitHub.\\r\\nWhat was your most important insight into the data?\\r\\nExploiting invariances in the data using data augmentation and modifications to the network architecture proved to be instrumental to get a good result. This goes to show that using a feature learning approach does not excuse you from having to get to know the data. You\\'re not doing any feature engineering, but you still have to do some engineering. It just happens at a higher level of abstraction.\\r\\nWere you surprised by any of your insights?\\r\\nOriginally I didn\\'t include image flipping in the data augmentation process. I figured it wouldn\\'t make much of a difference-- after all, it only doubles the effective number of examples the network sees. But when I eventually added it, my score jumped quite significantly. In the end this makes sense: rotation and scale invariance are much easier for the network to learn than invariance to flipping, because flipping an image displaces its features to a much larger extent.\\r\\nWhich tools did you use?\\r\\nI used Python and implemented the convolutional neural networks in Theano. Its symbolic differentiation support allowed me to experiment with a lot of different approaches, without having to recalculate the gradients every time. Theano also makes it really easy to use GPU acceleration, which proved essential to be able to train the networks in a reasonable amount of time.\\r\\n\\r\\nI used Theano wrappers for the cuda-convnet GPU convolution implementation by Alex Krizhevsky, which are included in the pylearn2 library. This gave a nice speed boost over Theano\\'s own implementation. I used scikit-image for data augmentation.\\r\\n\\r\\nI also used sextractor, a tool to extract properties of objects from astronomical images, and used this data to recenter and rescale the images. This didn\\'t improve results, but I included a few models with this recentering and rescaling in the final ensemble to increase variance.\\r\\nWhat have you taken away from this competition?\\r\\nFor problems like this, I believe that it\\'s better to make your model able to deal with invariances, rather than to try and remove the invariances by normalising the data first. For example, instead of rotating all the galaxy images so their major axes are aligned, it\\'s better to try and make the model work for all possible rotations. You\\'ll need a bigger model, but the end result will be more robust.\\r\\n\\r\\nI also learned that overfitting can happen at many levels: once a network has learned to be rotation invariant in its lower layers, it can overfit much more easily in the higher layers because it has figured out that you\\'re repeatedly showing it rotated versions of the same image. So preventing overfitting in the lower layers can actually make it worse in the higher layers.\\r\\n\\r\\nEven though data augmentation helped a lot, nothing beats having more training data. The competition organisers mentioned that the competition data is only a subset of what\\'s available, so I think it would be interesting to train a network on a larger set of images. I believe there is still a lot of room for improvement.\\r\\n\\r\\n------------------------------------------------------------------------------\\r\\n\\r\\n\\nSander Dieleman is a PhD student in the Reservoir Lab of Prof. Schrauwen at Ghent University in Belgium. His main research focus is applying deep learning and feature learning techniques to music information retrieval (MIR) problems, such as audio-based music classification, automatic tagging and music recommendation.', \"Gregory\\xa0Matthews\\xa0 and Michael Lopez are the members of team One shining MGF who climbed up to first place during a raucous ride on the leaderboard of Kaggle's March Machine Learning Mania. After all predictive models were frozen on March 19, things unfolded to real-world game results in the 2014 NCAA Tournament [see the\\xa0other blog posts tagged as\\xa0march-mania]. We asked Greg and Mike to tell us how they approached the problem,\\xa0working together for the first time on Kaggle.\\r\\nWhat was your background prior to entering this challenge?\\r\\nGreg: I have a Ph.D. in statistics from the University of Connecticut, completed a post-doc at the University of Massachusetts-Amherst, and in the fall will be an Assistant Professor of statistics at Loyola University Chicago. This was my first Kaggle contest.\\r\\nMike: I am a Ph.D. candidate in biostatistics from Brown University, and in the fall will be an Assistant Professor of statistics at Skidmore College. This was also my first Kaggle contest.\\nWhat made you decide to enter?\\r\\nGreg: I’ve always been interested in evaluating and predicting sports using statistical methods. I am particularly interested in professional football, professional baseball, and college basketball. This contest seemed right up my alley.\\r\\nMike: Greg emailed me.\\nWhat preprocessing and supervised learning methods did you use?\\r\\nGreg & Mike: Our winning submission was the combination of two models, a margin-of-victory based model (MOV) and an efficiency model using Ken Pomeroy’s data (KP). For first round games, the MOV model used the actual spread posted in Las Vegas for each game; for future games, we used previous game outcomes to predict a margin of victory. At the end, the spread (or the expected margin) was converged into a probability using logistic regression. For the KP model, we tried different regression models using different team-wide efficiency metrics, eventually settling one that minimized our loss function on the training data. At the end, we used a weighted average of the two probabilities (one from the MOV model, one from the KP model) as our final submission.\\r\\nWhat was your most important insight into the data?\\r\\nGreg: The Las Vegas line is absolutely incredible at predicting games. As they say, if you can’t beat them, use their data in a Kaggle contest. Also, when training the models, we didn’t just try to predict the tournament games, as there is a relatively small number of those types of games. Instead, we also trained or models on regular season data, too.\\r\\nMike: Like Greg said, it seemed silly to only train our models on a sample of 63 tournament games each season, when, in fact, there are hundreds of games played each week. Not sure it helped us, but we also ignored tournament specific information (i.e. a team’s seed number).\\nWere you surprised by any of your insights?\\r\\nGreg: I was surprised by how well our simple models performed. Using the right data was MUCH more important to our models performing well than using more sophisticated models.\\r\\nMike: I think we gave ourselves a chance with a good model, but there was probably a decent amount of luck involved, too. Also, identifying the specific loss function for this Kaggle contest, and where it comes from, seemed to help our model.\\nWhich tools did you use?\\r\\nGreg & Mike: R and R Studio\\r\\nWhat have you taken away from this competition?\\r\\nGreg: Don’t cheat.\\r\\nMike: I should always say yes when Greg emails me.\\n\\r\\n\\r\\n------------------------------------------------------------------------------\\r\\n\\r\\nMichael Lopez is a 4th-year Ph.D. student in the Department of Biostatistics at Brown University. Mike's website is statsbylopez.com\\nGregory J. Matthews is currently a lecturer and the Associate Director of the Institute for Computational Biology, Biostatistics, and Bioinformatics in the Department of Public Health at UMass-Amherst. Greg's website is statsinthewild.com\", 'Cross-posted from GEreports.com:\\r\\n\\r\\n\\nOne afternoon a year ago, Sergey Kozub, a software developer in the Russian city of Kursk, was scrolling through messages on the popular programming forum\\xa0topcoder\\xa0when he hit on a link to\\xa0Kaggle. Kaggle, the world’s largest open community of data scientists, had just partnered with GE and Alaska Airlines and challenged the public to come up with software that would reduce flight delays and make airlines more efficient and profitable.\\nKozub spent the next several months writing algorithms and crunching data after work to come up with a solution. Today, he became\\xa0one of the winners\\xa0of the second leg of the\\xa0Industrial Internet Flight Quest\\xa0contest. “It was a very challenging competition,” Kozub says. “In the end, the difference between the scores of the top competitors was about five basis points.” In other words, miniscule.\\nCommercial airlines spend an estimated $22 billion annually managing flight plan efficiency. “Flight plans don’t always stick to a schedule,” says John Gough, director for Fuel and Carbon Solutions at GE Aviation. “There are gate conflicts, flight delays and unexpected fuel consumption. All these factors add up quickly.”\\nGE estimates that if every scheduled flight worldwide was able to reduce the distance it flew by only 10 miles, airlines could potentially cut annual fuel consumption by 360 million gallons and save the industry over $3 billion each year.\\nGough says that the Flight Quest challenge, which received 6,800 submissions from 58 countries, is looking for an algorithm that could provide real-time, flight plan intelligence to the pilots “so they can make smarter decisions in the cockpit.”\\nKozub, as team charango, won the 2nd prize in the second phase of Flight Quest and $50,000. He built a flight optimization model using dynamic programming to find a rough estimate of a flight route between airports and then make it more efficient. “It’s a custom-made solution,” he says. “The general approaches are well known, but the actual implementation requires a lot of domain-specific knowledge and attention to detail.”\\nHis algorithms analyzed weather data from the National Oceanic and Atmospheric Administration, airport ground conditions and also flight statistics.\\nThe winners of the second stage of the Flight Quest challenge will share $250,000 – the same amount set aside for the first stage winners, who were\\xa0announced last year. (Kozub finished 4th during the first stage. He is the only contestant to win a prize in both legs.)\\nKozub says that “there was some element of luck to winning” since the final results were very tight. “The chance of spending a full month without a reward was very high,” he says.\\nSee the original story on GEreports.com\\xa0»', 'The leader of team anttip in this year\\'s Large Scale Hierarchical Text Classification challenge was Antti Puurula. He\\'s a PhD student in the Machine Learning Group at the University of Waikato, supervised by Prof. Ian Witten. His current interests include text mining, information retrieval, machine learning and graphical models. We asked him about his first place performance with teammates jread and Albert. That competition asked\\xa0participants to classify Wikipedia documents into one of 325,056 categories.\\r\\nWhat was your background prior to entering this challenge?\\r\\nI have a background in natural language processing and speech recognition research. Currently I\\'m finishing my PhD thesis on text mining using generative models, that proposes models using sparse computation as a solution for scalability in text mining.\\r\\nWhat made you decide to enter?\\r\\nI found out about LSHTC2 early in my studies, and decided to give it a try. By LSHTC3 my team was getting results close to the top.We had an ensemble framework that we used to participate in LSHTC3, and we thought it would be easy to set it up for LSHTC4 and see how far we got. I asked my earlier teammate Albert Bifet about trying this, and he brought Jesse Read along as well.\\r\\nWhich tools did you use?\\r\\nFor the base-classifiers we used the SGMWeka toolkit I\\'ve developed mostly for personal research use. On top of this we used Weka, Meka for one of the base-classifiers, and a number of Python and Unix shell scripts for model optimization and processing files. We used a handful of quad-core CPUs with 16GB RAM. Having only 16GB machines for use actually hurt our score quite a bit, and we had to prune and sub-sample the data to use any more complex models.\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nOverall we used quite a big selection of various methods in our base-classifiers and the ensemble, since the toolkit could be used to try out different ideas. Using a large number of ideas was good for ensemble modeling, since it diversified the base-classifiers for model combination. In terms of machine learning the base-classifiers were mostly based on extensions of Multinomial Naive Bayes, and the ensemble used a variant of Feature-Weighted Linear Stacking.\\r\\nWhat was your most important insight into the data?\\r\\nThere were a couple big ones. The biggest one is that you should always understand how the competition measure works and make sure your solution optimizes it. We realized that optimizing the Macro-averaged F-score measure becomes problematic with the very large number of labels used in the competition (325K). Other people on the competition forum noticed this as well. Our earlier system optimized other measures such as Micro-averaged Fscore, and we were far behind the leading participants before we started to think about this issue. Simple corrections such as surrogate measures and post-processing seemed to help a little, but our final solution worked best: instead of predicting the labels for documents, we predicted the documents for labels. This type of \"transposed prediction\" gave us a huge improvement in terms of Macro-averaged F-score, but it could have other uses as well.\\r\\nWere you surprised by any of your insights?\\r\\nThe problem with the competition measure was surprising, and made me reconsider how to approach the task, and how to apply classification to datasets in general.\\r\\n\\r\\nThere were many other surprises. The commonly used feature weights and similarity measures for text data needed considerable modification to work optimally for this dataset. This might be the general case when working with text data, but there is little research work on this. I was also surprised by how scalable classification with inverted indices worked out to be, after some further optimizations to use safe pruning of parameters and multi-threading in classification. We used a handful of commodity cluster machines to optimize and test the tens of base-classifiers in our ensemble, while the other participants seemed to use a single base-classifier and post-processing. Some competitors opted out of the competition altogether, since they were not familiar with methods that scale to LSHTC.\\r\\nWhat have you taken away from this competition?\\r\\nComing from a research background, I learned that competitions are different from research in many ways, and they can be highly rewarding if you take the participation seriously. Typically in research you start from the theory, look at a number of measures and datasets, and you ignore details such as feature pre-processing and similarity measures. In a competition you need to start from the data, look at the measure that matters on that dataset, and get the details right. The theory-first mindset can keep you from making new discoveries about the data and reaching a good score. Improving the score can lead to breaking some commonly accepted ways of doing things, and this can open new perspectives on the theory as well.', \"What was your background prior to entering this challenge?\\nJiří Materna: We are colleagues from CGI Prague data science team, and we come both from mathematical and IT backgrounds. I have a Masters degree in Computer Science and over 15 years of experience in IT –\\xa0mostly in telco and banking. Most of the time I have been involved in projects with large data processing (migration, cleansing, integration, predictive modelling, etc). So far I have participated in about 20 Kaggle competitions, and for the rest of the team this was their first Kaggle competition.\\r\\n\\r\\nLukáš Drápal and Jana Papoušková are math geeks of the team. Lukáš has a Masters degree in Stochastics & Financial Mathematics from Vrije University Amsterdam, and Jana got her Masters degree in Mathematics from Czech Technical University. They love to apply their deep mathematical knowledge to solve practical problems.\\r\\n\\r\\nIn addition, Emil Škultéty and Nomindalai Naranbaatar are the IT experts of the team. They both received a Masters degree in IT and have a wide range of practical experience.\\r\\nWhat made you decide to enter?\\nJiří: From my experience from previous Kaggle competitions I knew that Kaggle is a great place to learn. One not only touches upon the quirks and perks of algorithms, but the community sharing (especially post-competition) provides a way to improve your skills and to learn from the very best.\\r\\n\\r\\nDuring one of our meetings we decided that we would like to participate in\\xa0a competition altogether. With an enormous number of participating teams, the Allstate Purchase Prediction competition seemed like a true challenge.\\r\\nWhat was your most important insight into the data?\\nLukáš: The greatest influence on our chosen approach was the chosen metrics. If even one of the seven predicted policy settings was wrong, it was a complete miss. With this metric the provided last quoted benchmark (LQB) was darn good. We wanted to be really sure when making any changes to LQB – sort of a belt and braces approach.\\nJiří: We built three different models that predicted the complete policy and changed a record from LQB only if these coincided. On top of this we've built a classifier that stated whether the LQB is likely to work for the given record. So, sometimes even when all three models suggested a change from the LQB, we still kept it. In the end, only about 2000 out of 55000 records were changed from the LQB. Most of these changes were done for cases with little information about customer behavior – for those with only two records.\\r\\nJana: Another challenge was to determine how to group policy options together.\\nJiří: Yes, building an individual model for each of the policy option was bad as this lacked interaction between chosen policies. On the other hand, there were not enough data to build a model with all possible policy combination as outcomes. Moreover, this would be computationally too demanding. We settled for a compromise – we have modeled jointly groups (typically pairs) of variables that had the highest correlation.\\r\\nHow did you preprocess the data?\\nJiří: The data used for each model were different. The first model was simply just using the data provided – information about a customer and previous offers. The second model added more variables – a variable with mean cost in each state and each location. Moreover, after prediction of the first variable, a variable stating whether the model output is different from the LQB was added. The last pair was then modeled with information whether any other variable has changed. The third model was the most complex – many features regarding changes in customer’s previous offers (ratios of current and previous costs, time differences, etc) have been added.\\r\\nWhich tools did you use?\\nJiří: We have used Oracle XE as data storage for some basic exploratory data analysis and feature transformations. Otherwise, we used R for everything. Three different models were built using multinomial gradient boosting machine (gbm library in R) algorithm, the classifier used adaboost metric which seems to be reasonable proxy to AUC metric. Along the way, we have also fixed an issue within multinomial gbm for large number of classed, which we hope will be on GitHub soon. As our major improvements came late in the competition we did not have time to try the same approach with other algorithms like random forests.\\r\\nWere you surprised by any of your insights?\\nJiří: We were often surprised how changes from the LQB did not work the way we have expected. This led us to the conservative approach described above. Also, given the score on the public leaderboard (11th) we were surprised to see us on the top of the private leaderboard. We believe that the main reason for this jump was the conservative approach that didn’t overfit that much (together with a portion of luck that is always needed in such a tight competition).\\r\\nWhat have you taken away from this competition?\\nEmil: First of all, this competition convinced me again how important it is to properly understand provided datasets. I learned that feature engineering brings greater benefit than algorithm parameters tuning. And finally, it is always worth to try different approaches, because it supports creative ideas and because proper blending of different solutions together is almost always beneficial.\\r\\nLukáš: Absolutely. I have realized how important it is to understand the evaluation metrics and how it can greatly affect the right approach.\\nJana: Yes, I totally agree. Also as this was my first competition I have learned that the results on public and private leaderboard can be really different. There might be no reason to be afraid of rivals who seem more capable than you. :)\\r\\n\\r\\n------------------------------------------------------------------------------\\r\\n\\r\\n\\nThe team name Prazaci comes from all members being based in the great city of Prague in the Czech Republic.\", \"Alexander D'yakonov\\xa0won the competition Greek Media Monitoring Multilabel Classification which is associated with the WISE 2014 conference in Thessaloniki, Greece. Alexander\\xa0has quite a few winning posts on No Free Hunch,\\xa0and we again asked him to share some insights with Kaggle:\\nWhat was your background prior to entering this challenge?\\r\\nI am a professor at Lomonosov Moscow State University and a Kaggle member since 2010. I try to popularize data mining in Russia. For example, last year I organized a special seminar for students and young scientists — several tasks for them were to participate in Kaggle contests. This seminar was very popular and I’ll try to do something better this autumn.\\r\\nWhat made you decide to enter?\\r\\nI wanted to compete and chose several contests, but I did not have much spare time… so got a final solution only in WISE 2014. There was a quite simple problem: input data were real vectors with unit L2-norm and labels. The only difficulty was that one vector might have several labels. I already had solved similar problems. And my previous Kaggle contest (LSHTC) was related with multi-label text classification like this one. It is interesting that the two highest teams\\xa0on the leaderboard in these two contests are the same.\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nI tried to generate new features, use SVD, and transform initial data, but it only slightly increased performance. My final solution did not use all these tricks. I realized that linear methods (ridge regression and logistic regression) were more suitable for this problem than kNN and naïve bayes. In my final blending I used all these linear methods and kNN. My algorithm consisted of two parts: linear combinations of regressors for each label and a binary decision rule. Such algorithms are very popular in Russia, for example in «the algebraic approach to classification». This technique had been developing by academician Yuri Zhuravlev and his scientific school since 1978 and is unknown in Europe and USA.\\r\\nWhat was your most important insight into the data?\\r\\nThe same vectors had different lists of labels. It was very strange. I didn’t use cv, instead the first texts \\xad– for training, and the last ones – for local tests.\\r\\nWhich tools did you use?\\r\\nI used python and scikit-learn. In my previous contests my main tools were Matlab and R.\\r\\nWhat have you taken away from this competition?\\r\\nI was in sixth place during the last week of the contest and did not have any new ideas. Suddenly I thought up my model 4 hours before the end. I tried the model in my local tests and it sufficiently increased the performance. I ran the model on the whole training set. It took almost 4 hours to build regressors and tune parameters, so I made my final submissions several minutes before the end. I was a lucky that I didn’t make a mistake in the code. I took away that it was possible to win the contest in 4 hours.\", \"Guest contributor David Kofoed Wind is a PhD student in Cognitive Systems at The Technical University of Denmark (DTU):  As a part of my master's thesis\\xa0on competitive machine learning, I talked to a series of Kaggle Masters to try to understand how they were consistently performing well in competitions. What I learned was a mixture of rather well-known\\xa0tactics, and less obvious tricks-of-the-trade. In this blog post, I have picked\\xa0some of their answers to my questions in an attempt to\\xa0outline some of the strategies which are useful for performing well on\\xa0Kaggle. As the name of this blog suggests, there is no free hunch, and reading this blog post will not make you a Kaggle Master overnight. Yet following the steps described below will most likely help with getting respectable results on the leaderboards. I have partitioned the\\xa0answers I got into\\xa0a series of broad topics, together with a list of miscellaneous advice\\xa0in the end.\\r\\nFeature engineering is often the most important part\\r\\nWith the extensive amount of free tools and libraries available for data analysis, everybody has the possibility of trying out advanced statistical models in a competition. As a consequence of this, what gives you most “bang for the buck” is rarely the statistical method you apply, but rather the features you apply it to. By feature engineering, I mean using domain specific knowledge or automatic methods for generating, extracting, removing or altering features in the data set.\\r\\nFor most Kaggle competitions the most important part is feature engineering, which is pretty easy to learn how to do. (Tim Salimans)\\xa0\\nThe features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering. (Luca Massaron)\\nFeature engineering is certainly one of the most important aspects in Kaggle competitions and it is the part where one should spend the most time on. There are often some hidden features in the data which can improve your performance by a lot and if you want to get a good place on the leaderboard you have to find them. If you screw up here you mostly can’t win anymore; there is always one guy who finds all the secrets.\\xa0However, there are also other important parts, like how you formulate the problem. Will you use a regression model or classification model or even combine both or is some kind of ranking needed. This, and feature engineering, are crucial to achieve a good result in those competitions.\\xa0There are also some competitions where (manual) feature engineering is not needed anymore; like in image processing competitions. Current state of the art deep learning algorithms can do that for you. (Josef Feigl)\\r\\nOn the contrary, sometimes the winning solutions are those which go a non-intuitive way and simply use a black-box approach. An example of this is the Solar Energy competition where the Top-3 entries almost did not use any feature engineering (even though this seemed like an\\xa0intuitive approach for the competition) – and simply combined the entire data set into one big table and used a complex black-box model (for example an ensemble of gradient boosting regressors).\\r\\nSimple models will get you far\\r\\nWhen looking through the descriptions of different\\xa0solutions after a competition has ended, there is often a surprising number of very simple solutions obtaining good results. What is also (initially) surprising, is that the simplest approaches are often described by some of the most prominent competitors.\\r\\nI think beginners sometimes just start to “throw” algorithms at a problem without first getting to know the data. I also think that beginners sometimes also go too-complex-too-soon. There is a view among some people that you are smarter if you create something really complex. I “try” to follow Albert Einsteins advice when he said, “Any intelligent fool can make things bigger and more complex. It takes a touch of genius – and a lot of courage – to move in the opposite direction”. (Steve Donoho)\\nMy first few submissions are usually just “baseline” submissions of extremely simple models – like “guess the\\xa0average” or “guess the average segmented by variable X”. These are simply to establish what is possible with very simple models. You’d be surprised that you can sometimes come very close to the score of someone doing something very complex by just using a simple model. (Steve Donoho)\\nYou can go very far [with simple models], if you use them well, but likely you cannot win a competition by a simple model alone. Simple models are easy to train and to understand and they can provide you with more insight than more complex black boxes. They are also easy to be modified and adapted to different situations. They also force you to work more on the data itself (feature engineering, data cleaning, missing data estimation). On the other hand, being simple, they suffer from high bias, so they likely cannot catch a complex mapping of your unknown function. (Luca Massaron)\\xa0\\nOverfitting the leaderboard is a real issue\\r\\nDuring a competition,\\xa0you\\xa0have the possibility\\xa0of submitting to\\xa0the leaderboard. By submitting a solution to the\\xa0leaderboard you get back an evaluation of your model on the\\xa0public part of the test set. It is clear that obtaining evaluations from\\xa0the leaderboard gives you additional information/data, but it also\\xa0introduces the possibility of overfitting to the leaderboard-scores. Two fairly recent examples of competitions with overfitting to the leaderboard, were Big Data Combine and StumbleUpon Evergreen Classification Challenge. In the following table the top-10 entries on the public leaderboard for the StumbleUpon Challenge are shown together with their respective rankings on the private leaderboard.\\r\\n\\n\\n\\nUsername\\nPublic rank\\nPrivate rank\\n\\n\\nJared Huling\\n1\\n283\\n\\n\\nYevgeniy\\n2\\n7\\n\\n\\nAttila Balogh\\n3\\n231\\n\\n\\nAbhishek\\n4\\n6\\n\\n\\nIssam Laradji\\n5\\n9\\n\\n\\nAnkush Shah\\n6\\n11\\n\\n\\nGrothendieck\\n7\\n50\\n\\n\\nThakur Raj Anand\\n8\\n247\\n\\n\\nManuel Días\\n9\\n316\\n\\n\\nJuventino\\n10\\n27\\n\\n\\n\\r\\nThis challenge had 7,395 samples and it was generally observed that the data were fairly noisy.\\xa0In the Big Data Combine competition, the task was to predict the value of stocks\\xa0multiple hours into the future, which is generally thought to be extremely difficult. The extreme jumps on the leaderboard are\\xa0most likely due to the sheer difficulty of\\xa0predicting stocks, combined with overfitting.\\r\\nThe leaderboard definitely contains information. Especially when the leaderboard has data from a different time period than the training data (such as with the heritage health prize). You can use this information to do model selection and hyperparameter tuning. (Tim Salimans)\\xa0\\nThe public leaderboard is some help, [...] but one needs to be careful to not overfit to it especially on small datasets. Some masters I have talked to pick their final submission based on a weighted average of their leaderboard score and their CV score (weighted by data size). Kaggle makes the dangers of overfit painfully real. There is nothing quite like moving from a good rank on the public leaderboard to a bad rank on the private leaderboard to teach a person to be extra, extra careful to not overfit. (Steve Donoho)\\nOverfitting to the leaderboard is always a major problem. The best way to avoid it is to completely ignore the leaderboard score and trust only your cross-validation score. The main problem here is that your cross-validation has to be correct and that there is a clear correlation between your cv-score and the leaderboard score (e.g. improvement in your cv-score lead to improvement on the leaderboard). If that’s the case for a given competition, then it’s easy to avoid overfitting. This works usually well if the test set is large enough.\\xa0If the test set is only small in size and if there is no clear correlation, then it’s very difficult to only trust your cv-score. This can be the case if the test set is taken from another distribution than the train set. (Josef Feigl)\\xa0\\nEnsembling is a winning strategy\\r\\nIf one looks at the winning entries in previous competitions, a general trend is that most of the prize-winning models are ensembles of multiple models. The power of ensembling can also be justified mathematically\\xa0(links to paid article).\\r\\nNo matter how faithful and well tuned your individual models are, you are likely to improve the accuracy with ensembling. Ensembling works best when the individual models are less correlated. Throwing a multitude of mediocre models into a blender can be counterproductive. Combining a few well constructed models is likely to work better. Having said that, it is also possible to overtune an individual model to the detriment of the overall result. The tricky part is finding the right balance. (Anil Thomas)\\nI am a big believer in ensembles. They do improve accuracy. BUT I usually do that as a very last step. I usually try to squeeze all that I can out of creating derived variables and using individual algorithms. After I feel like I have done all that I can on that front, I try out ensembles. (Steve Donoho)\\xa0\\nEnsembling is a no-brainer. You should do it in every competition since it usually improves your score. However, for me it is usually the last thing I do in a competition and I don’t spend too much time on it. (Josef Feigl)\\nPredicting the right thing is important\\r\\nOne task that is sometimes trivial, and other times not, is that of “predicting the right thing”. It seems quite trivial to state that it is important to predict the right thing, but it is not always a simple matter in practice.\\r\\nA next step is to ask, “What should I actually be predicting?”. This is an important step that is often missed by many – they just throw the raw dependent variable into their favorite algorithm and hope for the best. But sometimes you want to create a derived dependent variable. I’ll use the GE Flight Quest as an example: you don't want to predict the actual time the airplane will land; you want to predict the length of the flight; and maybe the best way to do that is to use the\\xa0ratio of how long the flight actually was to how long it was originally estimated to be and then multiply that times the original estimate. (Steve Donoho)\\xa0\\r\\nThere are two ways to address the problem of predicting the right thing: The first way is the one addressed in the quote from Steve Donoho above about predicting the correct derived variable. The other is to train the statistical models using the appropriate loss function.\\r\\nJust moving from RMSE to MAE can drastically change the coefficients of a simple model such as a linear regression. Optimizing for the correct metric can really allow you to rank higher in the LB, especially if there is variable selection involved. (Luca Massaron)\\nUsually it makes sense to optimize the correct metric (especially in your cv-score). [...] However, you don’t have to do that. For example one year ago, I’ve won the Event Recommendation Engine Challenge which metric was MAP. I never used this metric and evaluated all my models using LogLoss. It worked well there. (Josef Feigl)\\r\\nAs an example of why using the wrong loss function might give rise to issues, look\\xa0at the following simple example: Say you want to fit the simplest possible regression\\xa0model, namely just an intercept a to the data:\\r\\n\\r\\n$$x = \\\\left(0.1,\\\\;\\\\; 0.2,\\\\;\\\\; 0.4,\\\\;\\\\; 0.2,\\\\;\\\\; 0.2,\\\\;\\\\; 0.1,\\\\;\\\\; 0.3,\\\\;\\\\; 0.2,\\\\;\\\\; 0.3,\\\\;\\\\; 0.1,\\\\;\\\\; 100\\\\right)$$\\r\\n\\r\\nIf we let \\\\(a_{\\\\text{MSE}}\\\\) denote the \\\\(a\\\\) minimizing the mean squared error, and let \\\\(a_{\\\\text{MAE}}\\\\) denote the \\\\(a\\\\) minimizing the mean absolute error, we get the following\\r\\n\\r\\n$$a_{\\\\text{MSE}} \\\\approx 9.2818$$\\r\\n\\r\\n$$a_{\\\\text{MAE}} \\\\approx 0.2000$$\\r\\n\\r\\nIf we now compute the MSE and MAE using both estimates of \\\\(a\\\\), we get the following results:\\r\\n\\r\\n$$\\\\frac{1}{11} \\\\sum_i \\\\left| x_i - a_{\\\\text{MAE}} \\\\right| = 9.5909$$\\r\\n\\r\\n$$\\\\frac{1}{11} \\\\sum_i \\\\left| x_i - a_{\\\\text{MSE}} \\\\right| = 16.4942$$\\r\\n\\r\\n$$\\\\frac{1}{11} \\\\sum_i \\\\left( x_i - a_{\\\\text{MAE}} \\\\right)^2 = 905.4660$$\\r\\n\\r\\n$$\\\\frac{1}{11} \\\\sum_i \\\\left( x_i - a_{\\\\text{MSE}} \\\\right)^2 = 822.9869$$\\r\\n\\r\\nWe see (as expected) that for each loss function (MAE and MSE), the parameter which was fitted to minimize that loss function achieves a lower error. This should come as no surprise, but when the loss functions and statistical methods become very complicated, it is not always as trivial to see if one is actually optimizing the correct thing.\\r\\nAdditional and personal advice\\r\\nOne of the most important things I have personally taken away from the Kaggle competitions I have participated in, is to get started immediately and to get something on the leaderboard as fast as possible. It is easy to underestimate the\\xa0amount of\\xa0work it takes to build a complete pipeline from reading in the data to outputting a submission file in the right format. Getting a simple benchmark on the leaderboard is a good way to get started, and if you are not able to replicate a benchmark score, then that should be the first step before trying out advanced approaches.\\r\\nMy most surprising experience was to see the consistently good results of Friedman’s gradient boosting machine. It does not turn out from the literature that this method shines in practice. (Gábor Takács)\\r\\nAs a fresh Kaggler, it is very tempting to try out the biggest baddest model first. Ideally, one should start by allocating a fair amount of time to looking at, and playing with the data. Trying out simple models and plotting different variables together is a very important part of getting good results on Kaggle. Starting out with a complex model will slow you down since training and testing time will be higher - and this means that you do not have time to try as many different things. Even though I have personally entered quite a few Kaggle competitions, allocating enough time to simply look at the data is still one\\xa0the things I am struggling with the most.\\r\\nThe more tools you have in your toolbox, the better prepared you are to solve a problem. If I only have a hammer in my toolbox, and you have a toolbox full of tools, you are probably going to build a better house than I am. Having said that, some people have a lot of tools in their toolbox, but they don’t know *when* to use *which* tool. I think knowing when to use which tool is very important. Some people get a bunch of tools in their toolbox, but then they just start randomly throwing a bunch of tools at their problem without asking, “Which tool is best suited for this problem?” (Steve Donoho)\\r\\nA tip that many of the top-performers mention is to make heavy use of the Kaggle forums. During the competitions, many participants write interesting questions which highlight features and quirks in the data set, and some participants even publish well-performing benchmarks with code on the forums. After the competitions, it is common for the winners to share their winning solutions. Reading those carefully will almost surely give you a good idea to try out the next time.\\r\\nThe best tip for a newcomer is to\\xa0read the forums. You can find a lot of good\\xa0advice\\xa0there\\xa0and\\xa0nowadays\\xa0also some code to get you started. Also, one\\xa0shouldn't\\xa0spend too\\xa0much time on optimizing\\xa0the parameters of the model at the beginning of the competition. There is enough time for that at the end of a competition.  (Josef Feigl)\\nIn each competition I learn a bit more from the winners. A\\xa0competition\\xa0is not won by one insight, usually it is\\xa0won by several careful steps towards a good modelling approach. Everything play its role, so there is no secret formula here, just several lessons learned applied together. I think new kagglers would benefit more of carefully reading the forums and the past competitions winning posts. Kaggle masters\\xa0aren't\\xa0cheap on advice!  (Lucas S.)\", \"Kiran placed 3rd in the KDD Cup and shared this interview with No Free Hunch:\\nWhat was your background prior to entering this challenge?\\r\\nI am a computer science engineer and management post-grad, heading marketing analytics, mobile analytics and customer analytics for Flipkart.com (the 'Amazon' of India), where I use data sciences in my work. Prior to this I was at Amazon.com and Dell. I have spent several years at Dell.com in a variety of roles in digital analytics leveraging data sciences.\\r\\n\\r\\nI am self-taught and learnt most things on my own/on-the-job in the field of machine learning starting off with SAS (closed source) and transitioning to R & Python (open source) over the last 5 years. I have participated in several Kaggle competitions to try, use and learn new techniques and was at one point ranked among the top 10 WW data miners. I have freelanced with US startups via the Konnect program helping solve problems like predicting multiple sclerosis recurrence, recommendation engine for music labels, among others. In my professional life, I have leveraged data sciences to solve the multi-touch attribution problem in e-commerce, to solve the store optimization problem of ranking configurator modules (both at Dell), to build an email rules engine and a world-class segmentation engine that was scaled to the entire customer database (both at Flipkart).\\r\\nWhat made you decide to enter?\\r\\nThere were basically two reasons:\\r\\n\\nKDD is the #1 conference for data miners worldwide and I wanted to participate in the competition\\nThe nature of the problem is very interesting. It has all the nuances of a difficult data sciences problem -- namely time based cross-validation, imbalanced dataset, sparsity, huge data size and high dimensionality (as a result of the text data).\\n\\nWhat preprocessing and supervised learning methods did you use?\\r\\nUsing visualization I found out early on that the dataset prior to 2010 did not have any labels and excluded that from training. I also added variables when data was missing for certain features -- to see if they could make a difference with the patterns of missing values.\\r\\n\\r\\nThe supervised learning methods I tried out for this competition were gradient boosting machines, vowpal wabbit, large scale regularized logistic regression & Support vector machines (liblinear), random forest and a bayesian regularized neural network.\\r\\nWhat was your most important insight into the data?\\r\\nThe following were key insights:\\r\\n\\nSome of the recently posted projects did not have sufficient time to be interesting projects\\nThe text features - i.e. the essay content, the title, description of the project were not very useful in prediction - but were useful for ensembling models\\nPart of speech features were useful\\nTime of the year is an important feature\\nSome donors are likely to donate more than other donors\\nThe location of the school requesting donation is important as there are people who like to donate in a specific region\\n\\nWere you surprised by any of your insights?\\r\\nI did not expect parts of speech features to be useful and I expected the text mining to yield stronger results. Time based cross-validation was very important to not overfit the leaderboard.\\r\\nWhich tools did you use?\\r\\nI used R and Python - both open source - leveraging the rich libraries that these languages provide. Besides these there are excellent libraries of vowpal wabbit, xgboost and liblinear that can be called from the command line. All these on my Ubuntu desktop were very powerful.\\r\\n\\r\\nTreetagger is very useful to get parts of speech features with unstructured text data.\\r\\nWhat have you taken away from this competition?\\r\\nThere are several intricacies with different algorithms that enable them to function well.\\r\\nExample: GBM implementation in python gives very good results when we treat factor variables as integers instead of dummy coding them; Random Forest with undersampling is very powerful.\\r\\n\\r\\nIt is important to be able to run algorithms on multiple cores (parallel processing) to get fast results.\\r\\n\\r\\nFinally, having a good repository/your own library of code that you can leverage saves a lot of time.\\r\\n\\r\\n------------------------------------------------------------------------------\\r\\n\\r\\n\\nKiran is a data sciences and business analytics leader working with Flipkart.com. Prior to this he worked at Amazon.com, was one of the first/oldest members in the e-biz team at Dell.com where he spent many years. His research interests span the areas of imbalanced datasets, large dimensionality and text mining with a specific strong interest in the field of digital analytics and e-business/e-commerce.\", \"(Cross-posted from MLWave.com)\\r\\nKaggle hosted a contest together with\\xa0Avito.ru. The task was to automatically detect illicit content in the advertisements on their site.\\r\\n\\r\\n\\r\\nMany competitors were using Vowpal Wabbit for\\xa0this challenge. Some aided by the benchmark from Foxtrot, others by starting out the challenge with it. The highest ranking model using VW for a base was yr's implementation. This #4 spot used the benchmark provided by Avito as part of the pipeline.\\r\\n\\r\\nOur team (Jules van Ligtenberg, Phil Culliton and me, Triskelion) ended up in 8th place with an average precision of ~0.985. A team of Russian moderators had an average precision of ~0.988 when labeling the dataset. Our team did not speak Russian, just English, Dutch and MurmurHash.\\r\\nIt is truly amazing that so many international teams that have no knowledge of Russian language made it to the top. –Ivan Guz, PhD, Competition\\xa0Sponsor\\nInsights\\nWhat did work\\n\\nEnsembling Vowpal Wabbit models. By simply averaging the ranks of different submission files one could up the score. Combining a squared, logistic and hinge loss model this way gave a score of ~0.982, while all individual models scored around ~0.977.\\nUsing an illicit score. This changes the problem from classification to regression. Instead of training models on labels of [illicit, non-illicit], we used the provided “closing hours” and “is proved” variables to create an “illicit score”. The worst offenders for this model are ads that are “blocked” by a moderator, “proved” by an experienced moderator, and “closed” within minutes of being published on the site.\\nAll loss functions gave good results. Initially we gravitated towards logistic loss and hinge loss. Later we added a squared loss and a quantile loss. For example averaging the ranked outputs of both a logistic and a hinge loss model, with all the parameters and data the same, gave a ~0.003 increase in score. We will study these “hybrid” loss functions better.\\n...\\n\\nRead the full post from Triskelion on\\xa0MLWave.com\\xa0! And look for the three winners' interviews coming soon.\", \"What was your background prior to entering this challenge?\\nGiulio: I hold Masters in Statistics and Biostatistics and have worked 15 years in HealthCare Insurance as a Statistician and Data Scientist. While I do pretty much everything from munging and exploration of large, complex, noisy data, to creating presentation for executives, my focus and passion remain on advanced analytics and applied machine learning. I’ve been programming in SAS for my whole career but picked up Python and R after I started competing on Kaggle.\\r\\n\\r\\nbarisumog: I have a BS in civil engineering. I've worked as an analyst for an international cement company for 10 years. My responsibilities included gaining insights from raw reports from a variety of departments (Marketing & Sales, Credit & Risk, and Operations), discovering actionable items for the management level, and offering strategic planning recommendations for the executive level.\\r\\n\\r\\nWhat made you decide to enter?\\nGiulio: My industry, healthcare, offers great opportunities, through\\xa0analytics, to impact and help people in very difficult times in their life. From\\xa0that perspective my work is very rewarding. However this industry is just\\xa0starting to catch up with many cutting edge application of data science, big\\xa0data and machine learning. In that sense, competitions that provide the\\xa0most diverse experience from healthcare and insurance are those I can learn the most from and thus more rewarding. Furthermore I really wanted\\xa0to try to do well in a text mining competition and this one had a very\\xa0sizable portion of text data.\\r\\n\\r\\nbarisumog: I've been studying machine learning on my own for over a year now. I've entered numerous competitions on Kaggle before. I find the competitive spirit very motivating during the competitions. And after the competition, there's always helpful discussion on the forums. I’ve always been interested in natural language processing. I used to code chatbots when I was younger. The fact that the data in this competition was in Russian, which I don’t speak, intrigued me enough to give it a shot.\\r\\n\\r\\nWhat preprocessing and supervised learning methods did you use?\\nGiulio: One thing I found out soon was that Russian text did not really need any special preprocessing and I was able to easily improve the benchmark code using no preprocessing at all. For the text portion of the data I used a series of Stochastic Gradient Descent models on various parts of the text features (title,description, attributes) and fed those predictions into a Random Forest along with additional dense features (category and subcategory being the most important). Since plain accuracy was so high across the train dataset, I then used semi\\xad-supervised learning to score the test set and retrain the algorithm on train and test combined.\\r\\n\\r\\nbarisumog: First of all, I worked on a category and subcategory basis, instead of working with the data as a whole. I concatenated the text fields (title, description, attributes), and created 3 different tf\\xadidf matrices for each category / subcategory. One tf\\xadidf used the raw text, one applied stemming, and one used stop\\xad words. The main reason behind this was to introduce some diversity, which was a key element in this competition. I only used textual features, as I couldn't provide additional value from any non\\xadtext features I tried. I trained Support Vector Classifiers on each tf\\xadidf for every category / subcategory. I also exploited Giulio's semi-\\xadsupervised approach, which worked quite well.\\r\\n\\r\\nWhat was your most important insight into the data?\\nGiulio & barisumog: The following were key insights:\\r\\n\\nThe Real Estate category, a large portion of the data but with very few blocked posts, added no value to the models, it actually made them worse. All of our models do not even bother scoring this category.\\nNo need to do fancy preprocessing on text features.\\nA blend of two very high scoring models did not necessarily translate into a higher overall score. Diversity was much more important.\\n\\nWere you surprised by any of your insights?\\nGiulio: I did expect feature engineering to add lots of value by extracting pieces of text that could be flags of blocked posts. For example, counts of mixed language words which are often used by fraudsters to bypass fraud detection algorithms. But none of what I have tried really added much value.\\r\\n\\r\\nbarisumog: It was moderately easy to get above 0.975 on the public leaderboard, and I initially thought the top ranks would be close to perfect in the end. But once we reached 0.985, it became harder than I expected to improve.\\r\\n\\r\\n\\xa0Which tools did you use?\\nGiulio: I used Python for the whole competition. Mostly scikit learn. Google translator came in handy as I was looking into misclassified observations.\\r\\n\\r\\nbarisumog: Python and scikit learn.\\r\\n\\r\\nWhat have you taken away from this competition?\\nGiulio: Some techniques and methods are generalizable in a much more flexible way than I had imagined. I was somewhat surprised that I could get so much out of Russian text without doing any preprocessing on it. Also, simple algorithms and creative approaches can go a long way.\\r\\n\\r\\nbarisumog: The main route in most text heavy data is tf\\xadidf. A good majority of teams probably had some component based on that. What makes the difference is the creative little things you mix in. In our case, the two main ones were applying semi\\xad-supervised learning, and ignoring a sizable category of posts. This was also my first competition I worked in a team, and I have learned a lot from Giulio in that respect.\", 'What was your background prior to entering this challenge?\\nMikhail: I\\'m a student of Moscow Institute of Physics and Technology. I also do have some background in applied math and CS. Now I\"m getting Master degree. My bachelor thesis was \\'active learning.\\' Started just a year ago, began with reading machine learning course by K. Vorontsov and attending Alexandr Dyakonov\"s seminars. Suppose it was quite good introduction into data science.\\r\\n\\r\\nDmitry: I\\'m graduate of MIPT (same university as Mikhail). I knew about Kaggle from the very beginning but started to compete after having Machine Learning course at School of Data Analysis (where I\"m currently getting my Master Degree). This course gave me required tools, techniques and experience to compete at a good level. Lately I worked at Yandex doing feature engineering related to ad CTR prediction and Yandex web-ranking tasks.\\r\\nWhat made you decide to enter?\\nMikhail: I wanted to learn how to process raw text data. Also had a desire to try a bunch of ideas learned from my previous contests and my former team-mates. However, AVITO.ru is a Russian company so this contest was some kind of a challenge among locals, Russian data scientists.\\r\\n\\r\\nDmitry: An idea to compete where you can do more. My key interests are\\xa0feature engineering possibilities, big data and russian language of course.\\r\\nWhat preprocessing and supervised learning methods did you use?\\nMikhail & Dmitry: We used sklearn and LibFM and found LibFM quite powerful tool for such a task. We were surprized by the fact that preprocessing ( stemming and removing stopwords) gave no profit at all. Two-levels model became our solution: outputs of SVM and LibFM were ensembled by Random Forest. Such technique is widely used and it was one of the ideas Mikhail wanted to try when entering the competition.\\r\\nWhat was your most important insight into the data?\\nMikhail & Dmitry:\\xa0We found that human error rate of labeling was very significant. In fact, the task was not to block illegal contect, but to redict moderator\\'s verdict. This difference in importante, especially if we want to get score 0.98+. Also we were surprised that Random Forest was pretty good for blending. We spent a lot of time tring linear ensembling models in thougs that blending method MUST be as simple as possible. But RF outperformed all linear models from first submission.\\r\\nWhich tools did you use?\\nMikhail & Dmitry:\\xa0We used python, scikit-learn and LibFM. We found that LinearSVC works perfect and Random Forest in sklearn 0.15 works much faster than former 0.14 version =)\\r\\nWhat have you taken away from this competition?\\nMikhail & Dmitry:That you should learn from the best - all ideas are public and you can find them in solutions of past contests. Blending is a real power! And not only linear combinations work well. Also, teamwork gets a lot of advantages - mainly, ideas.', 'Recently Kaggle hosted a competition on the CIFAR-10 dataset. The CIFAR-10 dataset consists of 60k 32x32 colour images in 10 classes. This dataset was collected by Alex\\r\\nKrizhevsky, Vinod Nair, and Geoffrey Hinton.\\r\\n\\r\\nMany contestants used convolutional nets to tackle this competition. Some resulted in scores that beat human performance on this classification task. In this blog series we will interview three contestants and also a founding father of convolutional nets: Yann LeCun.\\r\\n\\r\\n[caption id=\"attachment_4725\" align=\"aligncenter\" width=\"297\"] Example of cifar-10 dataset[/caption]\\r\\nYann LeCun\\r\\nYann Lecun is currently Director of AI Research at Facebook and a professor at NYU.\\r\\nWhich other scientists should be named and celebrated for the successes of convolutional nets?\\r\\nCertainly, Kunihiko Fukushima\\'s work on the Neo-Cognitron was an inspiration. Although the early forms of convnets owed little to the Neo-Cognitron, the version we settled on (with pooling layers) did.\\r\\n\\r\\n[caption id=\"attachment_4726\" align=\"aligncenter\" width=\"437\"] A schematic diagram illustrating the interconnections between layers in the Neo-Cognitron. From Fukushima K. (1980) Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.[/caption]\\r\\nCan you recount an aha!-moment or theoretical breakthrough during your early research into convolutional nets?\\r\\nNot really. It was the logical thing to do. I had been playing with multi-layer nets with local connections since 1982 or so (not having the right learning algorithm though. Backprop didn\\'t exist). I started experimenting with shared weight nets while I was a postdoc in Toronto in 1988.\\r\\n\\r\\nThe reason for not trying earlier is simply that I didn\\'t have the software nor the data. Once I arrived at Bell Labs, I had access to a large dataset and fast computers (for the time). So I could try full-size convnets, and it worked amazingly well (though it required 2 weeks of training).\\r\\nWhat is your opinion on the recent popularity of convolutional nets for object recognition? Did you expect it?\\r\\nYes. I knew it had to happen. It was a matter of time until the datasets become large enough and the computers powerful enough for deep learning algorithms to become better than human engineers at designing vision systems.\\r\\n\\r\\nThere was a symposium entitled \"frontiers in computer vision\" at MIT in August 2011. The title of my talk was “5 years from now, everyone will learn their features (you might as well start now)”. David Lowe\\xa0(the inventor of SIFT) said the same thing.\\r\\n\\r\\n[caption id=\"attachment_4727\" align=\"aligncenter\" width=\"498\"] A slide from the talk LeCun Y. (2011) “5 years from now, everyone will learn their features (you might as well start now)”.[/caption]\\r\\n\\r\\nStill, I was surprised by how fast the revolution happened and how much better convnets are, compared to other approaches. I would have expected the transition to be more gradual. Also, I would have expected unsupervised learning to play a greater role.\\r\\nThe character recognition model at AT&T was more than a simple classifier, but a complete pipeline. Can you tell more about the implementation problems your team faced?\\r\\nWe had to implement our own program language and write our own compiler to build this. Leon Bottou and I had written a neural net simulator called SN, back in 1987/1988. It was a Lisp interpreter with a numerical library (multidimensional arrays, neural net graphs…). We used this at Bell Labs to develop the first convnets.\\r\\n\\r\\nThen in the early 90’s, we wanted to use our code in products. Initially, we hired a team of developers to convert our Lisp code to C/C++. But the resulting system could not be improved easily (it wasn’t a good platform for R&D). So Leon, Patrice Simard and I wrote a compiler for SN, which we used to develop the next generation OCR engine.\\r\\n\\r\\nThat system integrated a segmenter, a convnet, and a graphical model on top. The whole thing was trained end to end.\\r\\n\\r\\nThe graphical model was called a “graph transformer network”. It was conceptually similar to what we now call a conditional random field, or a structured perceptron (which it predates), but it allowed for non-linear scoring function (CRF and structured perceptrons can only have linear scoring functions).\\r\\n\\r\\nThe whole infrastructure was written in SN and compiled. This is the system that was deployed in ATM machines and check reading machines in 1996 and was reading 10 to 20% of all the checks in the US by the late 90’s.\\r\\n\\r\\n[caption id=\"attachment_4729\" align=\"aligncenter\" width=\"320\"] An animation showing LeNet 5 in action. From \"Invariance and Multiple Characters with SDNN (Multiple Characters Demo)\".[/caption]\\r\\nIn comparison with other methods, training convnets is pretty slow. How do you deal with the trade-off between experimentation and increased model training times? What does a typical development iteration look like?\\r\\nIn my experience, the best large-scale learning systems always take 2 or 3 weeks to train, regardless of the task, the method, the hardware, or the data.\\r\\n\\r\\nI don’t know if convnets are “pretty slow”. Compared to what? They may be slow to train, but the alternative to “slow learning” is months of engineering efforts which doesn’t work as well in the end. Also, convnets are actually pretty fast to run (after training).\\r\\n\\r\\nIn a real application, no one really cares how long it takes to train. But people care a lot about how long it takes to run.\\r\\nWhich recent papers on convolutional nets are you most excited about? Any papers or ideas we should look out for?\\r\\nThere are lots and lots of ideas surrounding convnets and deep learning that have lived in relative obscurity for the last 20 years or so. No ones cared about it, and getting papers published was always a struggle. So, lots of ideas were never properly tried, never published, or were tried and published but soundly ignored and quickly forgotten. Who remembers that the first learning-based face detector that actually worked was a convolutional net (back in 1993, eight years before Viola-Jones)?\\r\\n\\r\\n[caption id=\"attachment_4730\" align=\"aligncenter\" width=\"300\"] A figure with predictions from Vaillant R., Monrocq C., LeCun Y. (1993) \"An original approach for the localisation of objects in images\".[/caption]\\r\\n\\r\\nToday, it’s really amazing to see so many young and bright people devoting so much creative energy to the topic and coming up with new ideas and new applications. The hardware / software infrastructure is getting better, and it’s becoming possible to train large networks in a few hours or a few days. So people can explore many more ideas that in the past.\\r\\n\\r\\nOne thing I’m excited about is the idea of “spectral convolutional net”. This was a paper at ICLR 2014 by folks from my NYU lab about a generalization of convolutional nets that can be applied to any graphs (regular convnets can be applied to 1D, 2D or 3D arrays that can be seen as regular grids in terms of graph). There are practical issues, but it opens the door to many more applications of convnets to unstructured data.\\r\\n\\r\\n[caption id=\"attachment_4731\" align=\"aligncenter\" width=\"438\"] MNIST digits on a sphere. From Bruna J., Zaremba W., Szlam A., LeCun Y. (2013) \"Spectral Networks and Deep Locally Connected Networks on Graphs\".[/caption]\\r\\n\\r\\nI’m very excited about the application of convnets (and recurrent nets) to natural language understanding (following the seminal work of Collobert and Weston).\\r\\nSince the error rate of a human is estimated to be around 6%, and Dr. Graham showed results of 4.47%, do you consider CIFAR-10 to be a solved problem?\\r\\nIt’s a solved problem in the same sense as MNIST is a solved problem. But frankly, people are more interested in ImageNet than in CIFAR-10 nowadays. In that sense, CIFAR-10 is not a “real” problem. But it’s not a bad benchmark for a new algorithm.\\r\\nWhat would it take for convnets to see a much wider adoption in the industry? Will training convnets and the software to set them up become less challenging?\\r\\nWhat are you talking about? Convnets are absolutely everywhere now (or about to be everywhere) in industry: Facebook, Google, Microsoft, IBM, Baidu, NEC, Twitter, Yahoo!….\\r\\n\\r\\nThat said, it’s true that all of these companies have significant R&D resources and that training convnets can still be challenging for smaller companies or companies that are less technically advanced.\\r\\n\\r\\nIt still requires quite of bit of experience and time investment to train a convnet if you don’t have prior training. Soon however, there will be several simple to use open source packages with efficient back-ends for that.\\r\\nAre we close to the limit for convnets? Or could CIFAR-100 be \"solved\" next?\\r\\nI don’t think it’s a good test. ImageNet is a much better test.\\r\\nShallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional architectures. Do deep nets really need to be deep?\\r\\nYes, deep nets need to be deep. Try to train a shallow net to emulate a deep convnet trained on ImageNet. Come back when you have done that. In theory, a deep net can be approximated by a shallow one. But on complex tasks, the shallow net will have to be be ridiculously large.\\r\\nMost of your academic work is highly practical in nature. Is this something you purposefully aim for, or is this an artefact of being employed by companies? Can you tell about the distinction between theory and practice?\\r\\nHey, I’ve been in academia since 2003, and I’m still a part-time professor at NYU. I do theory when it helps me understand things. Theory often help us understand what’s possible and what’s not possible. It helps suggest proper ways to do things.\\r\\n\\r\\nBut sometimes theory restricts our thinking. Some people will not work with some models because the theory about them is too difficult. But often, a technique works well before the reasons for it working well are fully understood theoretically.\\r\\n\\r\\nBy restricting yourself to work on stuff you fully understand theoretically, you are condemned to using conceptually simple methods.\\r\\n\\r\\nAlso, sometimes theory blinds us. For example, some people were dazzled by kernel methods because of the cute math that goes with it. But, as I’ve said in the past, in the end, kernel machines are shallow networks that perform “glorified template matching”. There is nothing wrong with that (SVM is a great method), but it has dire limitations that we should all be aware of.\\r\\n\\r\\n[caption id=\"attachment_4732\" align=\"aligncenter\" width=\"500\"] A slide from LeCun Y. (2013) Learning Hierarchies from Invariant Features[/caption]\\r\\nWhat is your opinion on a well-performing convnet without any theoretical justifications for why it should work so well? Do you generally favor performance over theory? Where do you place the balance?\\r\\nI don’t think their is a choice to make between performance and theory. If there is performance, there will be theory to explain it.\\r\\n\\r\\nAlso, what kind of theory are we talking about? Is it a generalization bound? Convnets have a finite VC dimension, hence they are consistent and admit the classical VC bounds. What more do you want? Do you want a tighter bound, like what you get for SVMs? No theoretical bound that I know of is tight enough to be useful in practice. So I really don’t understand the point. Sure, generic VC bounds are atrociously non tight, but non-generic bounds (like for SVMs) are only slightly less atrociously non tight.\\r\\n\\r\\nIf what you desire are convergence proofs (or guarantees), that’s a little more complicated. The loss function of multi-layer nets is non convex, so the easy proofs that assume convexity are out the window. But we all know that in practice, a convnet will almost always converge to the same level of performance, regardless of the starting point (if the initialization is done properly). There is theoretical evidence that there are lots and lots of equivalent local minima and a very small number of “bad” local minima. Hence convergence is rarely a problem.\\r\\nWhat is your opinion on AI hype. Which practices do you think are detrimental to the field (of AI in general and specifically convnets)?\\r\\nAI hype is extremely dangerous. It killed various approaches to AI at least 4 times in the past. I keep calling out hype whenever I see it, whether it’s from the press, from startups looking for investors, from large companies looking for PR, or from academics looking for grants.\\r\\n\\r\\nThere is certainly quite a bit of hype around deep learning at the moment. I don’t see a particularly high level of hype around convnets specifically. There is more hype around “cortical this”, “spiking that”, and “neuromorphic blah”. Unlike many of these things, convnets actually yield good results on useful tasks and are widely deployed in industrial applications.\\r\\nAny interesting projects at Facebook involving convnets that you could talk a little more about? Some basic stats about the size?\\nDeepFace: a convnet for face recognition. There are also convnets for image tagging. They are big.\\r\\n\\r\\n[caption id=\"attachment_4733\" align=\"aligncenter\" width=\"600\"] A figure describing the architecture from the presentation \"Taigman Y., Yang M., Ranzato M., Wolf L. (2014) DeepFace for Unconstrained Face Recognition\".[/caption]\\r\\nRecently you posted about 4 types of serious researchers. How would you label yourself?\\r\\nI’m a 3, with a bit of 1 and 4.\\r\\n\\n\"People who want to explain/understand learning (and perhaps intelligence) at the fundamental/theoretical level.\\nPeople who want to solve practical problems and have no interest in neuroscience.\\nPeople who want to understand intelligence, build intelligent machines, and have a side interest in understanding how the brain works.\\nPeople whose primary interest is to understand how the brain works, but feel they need to build computer models that actually work in order to do so.\"\\n\\nAnything you wish to say to the top contestants in the CIFAR-10 challenge? Anything you wish to say to (hobbyist) researchers studying convnets? Anything in general you wish to say about the CIFAR dataset/problem?\\r\\nI’m impressed by how much creativity and engineering knack went into this. It’s nice that people have pushed the technique as far as it will go on this dataset.\\r\\n\\r\\nBut it’s going to get easier and easier for independent researchers and hobbyist to play with these things and apply them to larger datasets. I think the successor to CIFAR-10 should be ImageNet-1K-128x128. This would be a version of the 1000 category ImageNet classification task where the images have been normalized to 128x128. I see several advantages:\\r\\n\\nthe networks are small enough to be trainable in a reasonable amount of time on a high-end gamer rig;\\nthe network you get at the end can actually be used for useful application (like robot vision);\\nthe network can be run in real time on embedded platforms, like smart phones or the NVIDIA Jetson TK1.\\n\\r\\n[caption id=\"attachment_4734\" align=\"aligncenter\" width=\"600\"] Predictions on ImageNet. From \"Krizhevsky A., Sutskever I., Hinton. G.E. (2012) ImageNet Classification with Deep Convolutional Neural Networks\".[/caption]\\r\\nThe need to have large amounts of labeled data can be a problem. What is your opinion on nets trained on unlabeled data, or the automatic labeling of data through image search engines?\\r\\nThere are tasks like video understanding and natural language understanding where we are going to have to use unsupervised learning. But these modalities have a temporal dimension that changes how we can approach the problem.\\r\\n\\r\\nClearly, we need to devise algorithms that can learn the structure of the perceptual world without being told the name of everything. Many of us have been working on this for years (if not decades), but none of us has a perfect solution.\\r\\nWhat is your latest research focusing on?\\r\\nThere are two answers to this question:\\r\\n\\nProjects I’m personally involved in (enough that I would be co-author on the papers);\\nprojects that I set the stage for, encourage other work on, and advise at the conceptual level, but in which I am not involved enough to be co-author on a paper.\\n\\r\\nA lot of (1) is at NYU and a lot of (2) is at Facebook.\\r\\n\\r\\nThe general areas are:\\r\\n\\r\\nunsupervised learning that discovers “invariant” features, the marriage of deep learning and structured prediction, the unification of supervised and unsupervised learning, solving the problem of learning long-term dependencies, building learning systems with short-term/scratchpad memory, learning plans and sequences of actions, different ways to optimize functions than to follow the gradient, the integration of representation learning with reasoning (read Leon Bottou’s excellent position paper “from machine learning to machine reasoning”), the use of learning to perform inference efficiently, and many other topics.\\r\\n\\r\\n\\r\\n\\r\\n\\xa0\\r\\n\\r\\nRead an interview with the\\xa0CIFAR-10 - Object Recognition in Images competition\\'s first place winner, Dr. Ben Graham. Also includes interviews with top contenders,\\xa0Phil Culliton and\\xa0Zygmunt Zając.\\r\\n\\r\\n\\xa0\\r\\n\\r\\n\\xa0', 'Dr. Ben Graham\\nDr. Ben Graham is an Assistant Professor in Statistics and Complexity at the University of Warwick. With a categorization accuracy of 0.95530 he ranked first place.\\r\\nCongratulations on winning the CIFAR-10 competition! How do you feel about your victory?\\r\\nThank you! I am very pleased to have won, and quite frankly pretty amazed at just how competitive the competition was.\\r\\n\\r\\nWhen I first saw the competition, I did not think the test error would go below about 8%. I assumed 32x32 pixels just wasn\\'t enough information to identify objects very reliably. As it turned out, everyone in the top 10 got below 7%, which is roughly on a par with human performance.\\r\\nCan you tell us about the setup of the network? How many layers?\\r\\nIt is a deep convolutional network trained using SparseConvNet with architecture:\\r\\ninput=(3x126x126) -\\r\\n320C2 - 320C2 - MP2 -\\r\\n640C2 - 10% dropout - 640C2 - 10% dropout - MP2 -\\r\\n960C2 - 20% dropout - 960C2 - 20% dropout - MP2 -\\r\\n1280C2 - 30% dropout - 1280C2 - 30% dropout - MP2 -\\r\\n1600C2 - 40% dropout - 1600C2 - 40% dropout - MP2 -\\r\\n1920C2 - 50% dropout - 1920C1 - 50% dropout - 10C1 - Softmax output\\r\\n\\r\\nIt was trained taking advantage of:\\r\\n\\nspatial-sparsity in the 126x126 input layer,\\nbatchwise dropout,\\n(very) leaky rectified linear units, and\\naffine spatial and color-space training data augmentation.\\n\\r\\nThe same architecture produces a test error of 20.68% for CIFAR-100.\\r\\n\\r\\n[caption id=\"attachment_4741\" align=\"aligncenter\" width=\"512\"] These cats evaded the DeepCNet solution by looking a lot like a fighter jet and a car.[/caption]\\r\\nCan you tell us a little about the hardware used to train the nets? How long did it take to train? What was the development cycle like?\\r\\nThe network took about 90 hours to train on an NVIDIA GeForce GTX 780 graphics card. I had already written a convolutional neural network for spatially-sparse inputs to learn to recognise online Chinese handwriting.\\r\\n\\r\\nOver the course of the competition I upgraded the program to allow dropout to be applied batchwise, and cleaned up some kernels that were accessing memory inefficiently. That made it feasible to train pretty large networks.\\r\\n\\r\\nWhich papers/approaches authored by other scientists did contribute the most to your top score?\\r\\n\\r\\nThe network architecture is the result of borrowing ideas from a number of recent papers\\r\\n\\nMulti-column deep neural networks for image classification; Ciresan, Meier and Schmidhuber\\nNetwork In Network; Lin, Chen and Yan\\nVery Deep Convolutional Networks for Large-Scale Image Recognition; Simonyan and Zisserman.\\n\\r\\nReading each of those papers was jaw-dropping as the ideas would not have occurred to me.\\r\\n\\r\\n[caption id=\"attachment_4742\" align=\"aligncenter\" width=\"592\"] These images were all correctly classified. To the net they look the most like their respective classes. From DeepCNet\\'s extremes.[/caption]\\r\\nWhere do you see convnets in the future? Anything in particular that you are excited about?\\r\\nI am very interested in the idea of spatially-sparse 3d convolutional networks. For example, given a length of string, you might be able to pull both ends to produce a straight line. Alternatively, the string might contain a knot which you cannot get rid of no matter how hard you pull. That is an idea that is obvious for humans, but hard to solve by computer as there are so many different kinds on knots.\\r\\n\\r\\nHopefully 3d convolutional networks can develop some of the physical intuition humans take for granted.\\r\\n\\r\\nBesides convnets, I am very interested in machine learning techniques for time-series data, such as recurrent neural networks.\\r\\nThank you very much for sharing your code on the forums. What is your opinion on sharing code?\\r\\nMy pleasure; it was nice to see a couple of the other teams in the top 10 (\"Jiki\" and \"Phil & Triskelion & Kazanova\") use the code. Another Kaggler, Nagadomi, also made his code available during the competition. It was fascinating to see him implement some of the ideas to come out of the ILSVRC2014 competition such as \"C3-C3-MP2\" layers and Inception layers.\\r\\nDo you think your convnet could be improved even more on this task, or do you feel it is close to its limit?\\r\\nAfter the competition, I re-ran my top network on the 10,000 images from the original CIFAR-10 test set, resulting in 446 errors.\\r\\n\\r\\nHere is a confusion matrix for showing where the 446 errors come from:\\r\\n         airplane automobile  bird   cat deer  dog frog horse ship truck\\r\\nairplane        0          3    10     2    2    0    2     0   16     3\\r\\nautomobile      1          0     1     0    0    0    0     0    3    12\\r\\nbird            8          1     0    14   19    8    9     5    2     0\\r\\ncat             4          1     8     0    9   57   20     2    5     2\\r\\ndeer            3          1    12     7    0    5    4     8    0     0\\r\\ndog             4          1     7    39   10    0    1     7    1     1\\r\\nfrog            4          0     7     7    3    1    0     1    0     1\\r\\nhorse           6          0     3     4    7    8    0     0    0     0\\r\\nship            2          3     2     0    0    1    0     0    0     3\\r\\ntruck           3         20     0     2    0    0    1     0    7     0\\r\\n\\r\\nLooking at some of the 446 misclassified images, it seems that there is plenty of room for improvement in accuracy. I am sure there is also scope for improving the efficiency of the network.\\r\\nWhich machine learning scientist inspires you?\\r\\nLots of them: Alan Turing, Yann LeCun, Geoffrey Hinton, Andrew Ng, Jürgen Schmidhuber, Yoshua Bengio, Rob Fergus, Alex Krizhevsky, Ilya Sutskever, ...\\r\\nAnything of note on the competition and/or dataset that you found surprising? An approach that worked unexpectedly well, or perhaps did not work for you?\\r\\nI was very surprised how much of a difference fine-tuning (finishing off training the network using a small number of training epochs with a low learning rate and without data augmentation) made.\\r\\nAgain, thank you very much for sharing your code. Our team would not have beaten the estimated human error rate without it!\\r\\nMy pleasure. Academia can be a bit antisocial, so it is lovely to see so much enthusiasm going into Kaggle competitions.\\r\\nPhil Culliton\\nPhil Culliton is a game developer and Senior Researcher at an NLP startup. With a score of 0.94120 his team scored 6th place.\\r\\nCan you tell about the architecture of the net? Number of layers etc.?\\r\\nOur 6th place submission used multiple iterations (with varying epoch counts) of a single network architecture.\\r\\n\\r\\nWe also used a \"trick\" suggested by Dr. Graham which incorporated a small number of epochs that used no affine transformations.\\r\\n\\r\\nThe network architecture in question was Dr. Graham\\'s spatially sparse CNN. It used 12 LeNet layers and a final softmax layer - it looked roughly like this (this is modified output from Dr. Graham\\'s code):\\r\\nLeNetLayer 128 neurons, VeryLeakyReLU\\r\\nLeNetLayer 128 neurons, VeryLeakyReLU MP2\\r\\nLeNetLayer 384 neurons, Dropout 0.0833333 VeryLeakyReLU\\r\\nLeNetLayer 384 neurons, VeryLeakyReLU MP2\\r\\nLeNetLayer 768 neurons, Dropout 0.208333 VeryLeakyReLU\\r\\nLeNetLayer 768 neurons, VeryLeakyReLU MP2\\r\\nLeNetLayer 1280 neurons, Dropout 0.3 VeryLeakyReLU\\r\\nLeNetLayer 1280 neurons, VeryLeakyReLU MP2\\r\\nLeNetLayer 1920 neurons, Dropout 0.4 VeryLeakyReLU\\r\\nLeNetLayer 1920 neurons, VeryLeakyReLU MP2\\r\\nLeNetLayer 2688 neurons, Dropout 0.5 VeryLeakyReLU\\r\\nLeNetLayer 2688 neurons, VeryLeakyReLU\\r\\nLeNetLayer 10 neurons, Softmax Classification\\r\\n\\r\\nThe \"MP\" entries above denote max pooling, and \"VeryLeakyReLU\" denotes a \"leaky\" ReLU with a fairly large (alpha was 0.33) non-zero gradient.\\r\\n\\r\\nDropOut was implemented in a straightforward manner. I considered adding DropConnect into the mix but ran out of time to test it.\\r\\n\\r\\nInput images were distorted using a semi-random system of stretching and flipping - I played around with this but also ran out of time to properly validate it.\\r\\n\\r\\nEarlier in the competition I did attempt to ensemble multiple network architectures but none of them outperformed the top contender.\\r\\n\\r\\n[caption id=\"attachment_4743\" align=\"aligncenter\" width=\"512\"] The cat on the left looks most like a frog. The cats on the right trick the net into thinking it is a boat. From DeepCNet\\'s extreme errors.[/caption]\\r\\nWhat were the technical challenges to overcome to produce submissions for this challenge?\\r\\nI mention this again later, but getting CUDA installed and running properly on various machines turned out to be a much bigger task than I thought it would be - it was difficult and time-consuming. I\\'m an old hand at getting cranky C code to compile - like, say, porting Windows codebases to OSX - so when I say I saw some weird stuff in trying to get CUDA-based libraries to run, I mean it.\\r\\n\\r\\nAlso - in a normal Kaggle competition I try to make use of all of the submissions available to me, even if it\\'s just to try oddball approaches that may or may not work. However, for CIFAR-10, coming up with the machine time was an issue. I farmed the work out over AWS GPU servers as well as multiple local servers, but AWS quickly became expensive and eventually I had to stop using it.\\r\\n\\r\\nFinding the right ratio of network size / sample batch size / speed for each server also took some care. I discovered that sample batch size (the number of samples sent to the GPU at a time) actually had an effect on final results, although I haven\\'t yet quantified it. I\\'d be interested in exploring that further.\\r\\nWhich libraries did you use? Can you give some of the pros and cons?\\r\\nFor the top submission\\'s neural networks we used Dr. Graham\\'s reference code in CUDA / C++, with variations in parameters and some extremely minor changes.\\r\\n\\r\\nThe biggest pro was speed - we were training simply enormous networks and it could only have worked using GPGPUs. The cons - complexity of setup and installation. Each machine\\'s CUDA install was a new mini-adventure, some of which didn\\'t turn out so well. I hadn\\'t played with CUDA much before, and frankly I\\'m not too enamored with it. Getting it working properly - and compiling OpenCV with it! - on OSX was ridiculously hard. Eventually I switched over to all-Linux CUDA servers, where the task was marginally easier.\\r\\n\\r\\nLuckily Dr. Graham\\'s code was very adaptable and didn\\'t have any strange library requirements - several of the other libraries we attempted to use required very specific / old versions of CUDA and would only work if you had a particular compiler, etc., or weren\\'t amenable to running on one platform or another.\\r\\nDid you try anything else besides convolutional nets?\\r\\nI also tried simple neural networks using H2O in R, kNN with scikit-learn, and Vowpal Wabbit. I\\'m a pretty heavy user of the latter two, but H2O was new to me. All produced interesting results, but none ground-breaking.\\r\\n\\r\\nI did really like H2O\\'s deep learning implementation in R, though - the interface was great, the back end extremely easy to understand, and it was scaleable and flexible. Definitely a tool I\\'ll be going back to.\\r\\nDid you read any papers for this competition?\\r\\nSeveral. DropOut, DropConnect, and network architecture papers were heavily featured. I had just been doing some NN work in my day job so I got some dual-purpose reading done.\\r\\n\\r\\nI heartily recommend Dr. Graham\\'s preprint about the architecture we used - you can find it on his website.\\r\\n\\r\\nI spent a fair bit of time on fastml.com as well - their articles on CIFAR-10 were beyond useful.\\r\\n\\r\\n[caption id=\"attachment_4744\" align=\"aligncenter\" width=\"435\"] The DeepCNet architecture from \"Graham B. (2014) Spatially-sparse convolutional neural networks\"[/caption]\\r\\nWhat did you learn from this competition? First time using convnets for a Kaggle competition? Do you think you can apply any knowledge to future competitions?\\r\\nThis was my first time using convnets for anything! I was impressed with their power and accuracy. I was also impressed at the number of GPU hours (and expense) it took to run a decent-sized network. It certainly isn\\'t for the impatient or faint of heart.\\r\\n\\r\\nI strongly suspect that deep learning / NNs will bubble toward the top of my toolbox for some problems. Definitely on anything remotely similar to CIFAR I\\'ll be headed to the code from this competition first - probably with an email to Dr. Graham shortly thereafter.\\r\\nI heard you approached Dr. Ben Graham midway during the competition and he released code. Can you tell a little about how this came to be?\\r\\nSure! I noticed that Dr. Graham was consistently on the top of the leaderboard and clicked through his Kaggle profile to find out if he was working for an ML company or using a particular product. There wasn\\'t anything on his profile except for a link that was only partially visible, so I hopped on Google and dug around a bit.\\r\\n\\r\\nIt was a slightly convoluted process, but I eventually made my way to his website and noted that he had several sets of sample / reference code for dealing with CIFAR-10 that were freely available and accompanied by (rather excellent) write-ups. I grabbed a set and started trying to work with it, had some problems getting it going, and sent him a question. I figured I wouldn\\'t hear back from him - frankly I wasn\\'t sure whether he\\'d be willing to help his competition.\\r\\n\\r\\nHowever, within a few hours he\\'d sent me a version of the code with all the issues ironed out and some friendly comments! Shortly thereafter he shared that same code on the forums, which was great as that got even more people using it.\\r\\n\\r\\nWe kept in touch during the competition, whenever he updated the code on the forums he\\'d send me an email letting me know, and encouraging me to keep trying (although by the end of the competition it was pretty clear to me that he was going to win).\\r\\n\\r\\nHe was a great sport, a tremendous help and I\\'m looking forward to seeing more of his work in the future.\\r\\nZygmunt Zając\\nZygmunt Zając is the author of FastML and a Machine Learning Researcher. He used DropConnect to improve his accuracy to 0.90660, good for 18th place.\\r\\nCan you tell about the architecture of the net? Number of layers etc.?\\r\\nI have used models trained by Li Wan, the author of DropConnect. The details are outlined in the paper: Regularization of Neural Networks using DropConnect.\\r\\n\\r\\n[caption id=\"attachment_4745\" align=\"aligncenter\" width=\"585\"] Figure from the paper: \"Wan L., Zeiler M., Zhang S., LeCun Y., Fergus R. (2013) Regularization of Neural Networks using DropConnect[/caption]\\r\\nWhat were the technical challenges to overcome to produce submissions for this challenge?\\r\\nThe challenges were getting the data in and the predictions out, as usual. In this case it meant converting raw images into cuda-convnet format and learning how to get the predictions from the library.\\r\\n\\r\\nOn top of that, getting DropConnect code to work was a bit tricky. You can read about the journey here:\\r\\n\\nObject recognition in images with cuda-convnet\\nRegularizing neural networks with dropout and with dropconnect\\n\\nWhich libraries did you use? Can you give some of the pros and cons?\\r\\nI used Alex Krizhevsky’s cuda-convnet extended with Li Wan\\'s code. Cuda-convnet struck me as a very well designed and implemented library.\\r\\nDid you try anything else besides convolutional nets?\\r\\nNo.\\r\\nDid you read any papers for this competition?\\r\\nMainly Hinton\\'s et al. dropout paper and Li Wan\\'s et al. DropConnect paper. There are other references in the FastML articles mentioned above.\\r\\nWhat did you learn from this competition? Did any knowledge from previous competitions (cats vs. dogs) transfer? Do you think you can apply any knowledge to future competitions?\\r\\nIt was my first brush with convolutional networks, I gained a general idea of how they work. Also that it isn\\'t as easy to overfit as I thought. ;)\\r\\n\\r\\nAbout DropConnect, it seems to offer results similiar to dropout. State of the art scores reported in the paper come from model ensembling.\\r\\n\\r\\nI went into the cats and dogs competition after CIFAR-10, exposure to convnets certainly helped. Generally the knowledge can be directly applied to contests dealing with images.\\r\\n\\r\\n[caption id=\"attachment_4746\" align=\"aligncenter\" width=\"512\"] The dog on the left looks most like a horse. The dog on the right looks most like a cat. From DeepCNet\\'s extreme errors.[/caption]\\r\\nI saw you being mentioned on the DropConnect page. Can you tell a little more about how this came to be?\\r\\nI exchanged a few emails with Li Wan after I asked him for help with getting his code to work. I mentioned the article I was writing and he saw it fit to post a link.\\r\\nAbout the interviews\\r\\nThese interviews were conducted over email. I would like to thank everyone for taking part in these interviews, and I hope the resulting article may serve as a resource for convolutional nets, the CIFAR-10 dataset and the Kaggle competition.\\r\\n\\r\\nRead our interview with a founding father of convolutional nets, Yann LeCun, here >>', 'On December 3rd, Stanford Data Mining & Analysis course (STATS202) wrapped up a heated Kaggle InClass competition, \"Getting a Handel on Data Science\". Beating out 92 other teams, \"TowerProperty\" came in first place. Below, TowerProperty outlines the competition and their journey to the top of the leaderboard.\\xa0Kaggle InClass is provided free of charge to\\xa0academics as a statistical and data mining learning tool for students. Instructors from any course dealing with data analysis may get involved!\\n\\n\\r\\n\\r\\n[caption id=\"attachment_4755\" align=\"aligncenter\" width=\"500\"] Screen shot of the interactive map we made for exploratory analysis[/caption]\\r\\nTeam TowerProperty\\r\\nThe three of us met as students enrolled in the Data Mining & Analysis graduate course at Stanford University. We’re all working professionals and took the course for credit through Stanford’s Center for Professional Development.\\r\\nWhat was the Stanford Stats 202 InClass challenge?\\r\\nWe were given a database of several thousand accounts with anonymized information about orchestra patrons and their history of subscriptions and single ticket purchases. The challenge was to predict who would subscribe to the 2014-2015 season and at what subscription level. We were given the output variable (total subscriptions) for a subset of the accounts in order to train our model.\\r\\nWhat processing methods did you use?\\r\\nFrom the very beginning, our top priority was to develop useful features. Knowing that we would learn more powerful statistical learning methods as our Stanford course progressed, we made sure that we had the features ready so we would be able apply various models to them quickly and easily. Broadly, the features we developed are grouped as follows:\\r\\n\\r\\nArtistic preferences – We parsed the historical concerts data for the conductor and composer and artists performing at a given time, as well as the composer who’s work was being performed. We then compared this to the number of subscriptions bought that year by account. Our goal was to see if we could classify accounts as Bach, Haydn or Vivaldi-lovers and use that for predictions (perhaps Bach lovers will be more likely to buy subscriptions this year as well because there is another Bach concert in the performance schedule).\\r\\n\\r\\nSpatial features –\\xa0We computed geodesic distances from each account to the orchestra’s venue locations. Even though we were not given street addresses for each account, we able to approximate distance by assigning latitude and longitude coordinates to each account according to the centroid of its billing zip code.\\r\\n\\r\\nDerivative features – Using Hadley Wickam’s tidy data methods and R packages, we prepared a large set of predictors based on price levels, number of tickets bought by that account during that year, etc. We also smoothed some features to avoid over-fitting. For most of the competition, we used predictors from the last few years. But in the winning model, we actually used only the last two years and average values over the last five years.\\r\\n\\r\\nWhen we later applied the boosted decision trees model, we derived additional predictors that expressed the variance in the number of subscriptions bought – theorizing that the decision tree would be more easily able to separate “stable” accounts from “unstable” ones.\\r\\n\\r\\nWe created 277 features in the end, which we applied in different combinations. Surprisingly, our final model used only 18 of them.\\r\\nWhat supervised learning methods did you use?\\r\\nMost importantly – and from the very beginning – we used 10-fold cross validation error as the metric to compare different learning methods and for optimization within models.\\r\\n\\r\\nWe started with multiple linear regression models. These simple models helped us become familiar with the data while we concentrated our initial efforts on preparing features for later use.\\r\\nWe then applied and tested the following methods:\\n\\nsupport vector machines (SVM),\\nBayesian Additive Regression Trees (BART),\\nK-Nearest Neighbors (KNN), and\\nboosted regression trees\\n\\r\\nWe didn’t have much luck with SVM, BART and KNN. Perhaps we did not put enough effort into that, but since we already had very good results from using boosted trees, the bar was already quite high. Our biggest effort soon turned to tuning the boosted regression tree model parameters.\\r\\n\\r\\nUsing cross validation error, we tuned the following parameters: number of trees, bagfrac, shrinkage, and depth. We then tuned the minobinsnode parameter – we saw significant improvements when adjusting the parameter downwards from its default setting.\\r\\n\\r\\nOur tuning process was both manual and automated. We wrote R scripts that randomly changed the parameters and set of predictors as then computed the 10-fold cross-validation error on each permutation. But these scripts were usually used only as a guide for approaches that we then further investigated manually. We used this as a kind of modified forward selection process.\\r\\n\\r\\n\\r\\n\\r\\nIn the end, model that gave us the lowest 10-fold CV error and won the competition, was surprisingly simple and small. It had only 18 predictors:\\r\\n\\r\\n\\nHow did you choose your final submission?\\r\\nThe rule we used to choose our final two submissions was simple: select the one that produced the lowest public Kaggle score, and the one that gave us the lowest 10-fold cross validation error.\\r\\n\\r\\nThis actually turned out to be somewhat of a tough decision because we had other submissions with better public Kaggle scores. Nevertheless, we decided to trust our 10-fold cross-validation and it turned out to be the right choice. (It was actually a pretty tough decision to choose the model that did not use all of the additional predictors, which we had spent so much time creating!)\\r\\n\\r\\nIn the end, the more complicated model – the one which resulted in our best public score – was also beaten by the simpler one.\\r\\nWhat was your most important insight into the data?\\r\\nProbably our most important insight about the data was that there was a pattern to the order of the data we received. In other words, even though the data was anonymized, it still retained useful information from its record sequence.\\r\\n\\r\\nWe noticed this by analyzing the accounts that were buying subscriptions and changing the level of subscriptions over the years, as well as those that bought last year. We observed that the distribution of these values in the file provided was far from random. In fact, the distribution seemed to have a temporal pattern.\\r\\n\\r\\nThe non-random nature of the data file was also manifested in the “missingness” of some spatial features, which can be seen in this plot:\\r\\n\\r\\n\\r\\n\\r\\nWe felt that decision trees have a natural property of finding the right splits, which can be useful to separate some accounts from the others. So, based on that, we started including “account.num” as a predictor and we discovered that it did in fact make our cross-validation scores significantly better. Actually, in the winning model, this simple predictor turned out to be the fourth most important!\\r\\nWere you surprised by any of your insights?\\r\\nAs the competition progressed, we realized that we were probably using too many predictors. This was somewhat of a surprise to us newbies. We discovered that it was often beneficial to drop a predictor that had high importance in the splits!\\r\\n\\r\\nWe first noticed this phenomenon with our spatial predictors like “billing.city” and “distance-from-venue.” They were always getting chosen as very influential … and yet they were consistently increasing the final CV error. Once we detected this phenomenon, we grew more and more suspicious of any predictor that we were using.\\r\\n\\r\\nSimilarly, it turned out that the more aggressive we were in eliminating older predictors, the smaller the error. In our final model, we used only subscriptions for the 2013-14 and 2012-13 seasons and values averaged over last 5 years.\\r\\n\\r\\nIt turned out that simplifying the model (by removing predictors) actually resulted in substantial improvements.\\r\\nWhat have you taken away from this competition?\\r\\nKeep it simple and build a proper cross validation approach into your work.\\r\\nWhat tools did you use?\\r\\nWe used GitHub to host the code and collaborate (you can take a look at it here). This was especially important as our team members lived in distant cities. Our code was written entirely in R (although we sometimes used Excel to derive new predictors).\\r\\n\\r\\n\\r\\n\\r\\nAbout team “TowerProperty”:\\r\\n\\r\\nMichal Lewowski lives in the Seattle-area and works on\\xa0bing.com\\xa0search engines at Microsoft.\\r\\n\\r\\nMatt Schumwinger lives in Milwaukee, Wisconsin and is a data consultant at Big Lake Data.\\r\\n\\r\\nKartik Trasi lives in the Seattle-area and works on big data analytic platforms for Telecommunications Industry at IBM.', 'In 2014 Kaggle completed two seizure predictions challenges, one co-organized by UPenn,\\xa0Mayo Clinic and one by the American Epilepsy Society.\\r\\n\\r\\nAccurate and fast seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives:\\r\\n\\nSeizures that are quickly detected can be aborted earlier by using a responsive neurostimulation device.\\nLarger amounts of EEG data can be analyzed by doctors.\\nPatients can better plan activities when they are notified of an impending seizure.\\n\\r\\n[caption id=\"attachment_4771\" align=\"aligncenter\" width=\"300\"] Positron emission tomography of the brain[/caption]\\r\\n\\r\\n\\xa0\\r\\n\\r\\nIn this blog post, we talk with the top three teams from the American Epilepsy Society Seizure Prediction Challenge.\\r\\n\\r\\n\"[The winning team\\'s results] blew the top off previous efforts. Accurate seizure detection and prediction are key to building effective devices to treat epilepsy.\"\\n— Brian Litt, Professor of neurology and bioengineering at the University of Pennsylvania in \\'A Crowd Of Scientists Finds A Better Way To Predict Seizures\\'\\n\"Seizure detection and seizure prediction are two fundamental problems in the field that are poised to take significant advantage of large data computation algorithms and benefit from the concept of sharing data and generating reproducible results.\"\\n— Dr. Walter J. Koroshetz, director at the NINDS in \\'Predicting epileptic seizures with 82 percent accuracy\\'\\n\"Working in different countries, we exchanged ideas via e-mail, and agreed on how to best use our submissions during the final days of the contest.\"\\n— 1st place team, QMSDP\\n\"My observation is that with the open source tools and learning experience in Kaggle competition, a person can tackle most of the machine learning problems.\"\\n— 2nd place team, Jialun\\n\"All of us hold a PhD in our respective areas, and we are forming a new multidisciplinary research group in data science, that is a field where we have shared interests.\"\\n— 3rd place team, ESAI CEU-UCH\\n\\n1st place team, QMSDP\\nBiographies\\nDr. Quang Tieng is a Senior Research Officer at the Centre for Advanced Imaging (CAI) at the University of Queensland. One of his research projects is super-resolution in MRI.\\r\\n\\r\\nDr. Min Chen is a Postdoctoral Research Fellow at the CAI at the at the University of Queensland. Her research projects focus on temporal lobe epilepsy.\\r\\n\\r\\nDr. Simone Bosshard is a Postdoctoral Research Fellow at the CAI at the University of Queensland. One of her research projects involves studying the structural network responsible for generating epileptic discharges.\\r\\n\\r\\nDrew Abbot is a software engineer at AiLive in California. This company has worked closely together with Nintendo to create software for the Wii video game console.\\r\\n\\r\\nPhillip Adkins is a mathematician and works at AiLive in California. AiLive uses machine learning to facilitate the development of motion recognition packages.\\r\\nBackground prior to entering challenge\\r\\nQuang, Min, and Simone all work at The University of Queensland in Australia.\\r\\n\\r\\nPhillip and Drew work together at AiLive in CA, USA.\\r\\nSolution Summary (as told by the team)\\r\\nTo begin, note that our team merged together after working the contest independently and combined different approaches and ideas to achieve the final result.\\r\\n\\r\\nOur winning submission was a weighted average of three separate models: a Generalized Linear Model regression with Lasso or elastic net regularization (via MATLAB\\'s lassoglm function), a Random Forest (via MATLAB\\'s TreeBagger implementation), and a bagged set of linear Support Vector Machines (via Python\\'s scikit-learn toolkit).\\r\\nFeature selection\\r\\nFor the Lasso GLM model, the features were as follows:\\r\\n\\nSpectrum and Shannon\\'s entropy at six frequency bands: delta (0.1-4Hz), theta (4-8Hz), alpha (8-12Hz), beta (12-30Hz), low-gamma (30-70Hz) and high gamma (70-180Hz).\\nSpectral edge power of 50% power up to 40Hz.\\nShannon\\'s entropy at dyadic frequency bands.\\nSpectrum correlation across channels at dyadic frequency bands.\\nTime-series correlation matrix and its eigenvalues.\\nFractal dimensions.\\nHjorth parameters: activity, mobility and complexity.\\nStatistical moments: skewness and kurtosis.\\n\\r\\nFor the bagged SVM model, the features involved a kernel PCA decomposition of the below\\xa0features.\\r\\n\\r\\nThe features for the Random Forest model were also a combination of time- and frequency-domain information, and were chosen as:\\r\\n\\nSums of FFT power over hand-picked bands spanning frequencies: f0 (fundamental frequency of FFT), 1Hz, 4Hz, 8Hz, 16Hz, 32Hz, 64Hz, 128Hz and Nyquist. DC was also included, yielding 9 bands per channel.\\nTime-series correlation matrix.\\nTime-series variance.\\n\\nTools used\\r\\nMATLAB and Python with Scikit-learn.\\r\\nContinued advancement\\r\\nOnce the contest was over, we realized that using 10 windows (or, simply the 1-minute window) for all subjects actually yielded a better private LB score than the 12- and 150-window choice for dogs and humans, respectively.\\r\\n\\r\\nWe decided that interpolating the signal by a factor of K before taking the final p-norm was worth trying, and indeed, marginal public LB improvements were achieved after doing so (using cubic spline interpolation). In the end, we decided to use Random Forest models trained on 31/32 overlapped preictal and interictal features to classify 63/64 overlapped test features (yielding 4732 and 4737 samples for each 10-minute segment), and interpolate and p-norm those scores for our final Random Forest model.\\r\\n\\r\\nInterestingly, as overlap and interpolation increased, the optimal p used in the p-norm seemed to increase as well, and our final choices for K and p ended up being 8 and 23, respectively.\\r\\nFurther reading on the winning solution\\r\\nTo see a detailed description of the solution, together with code, look at this Github Repo.\\r\\n2nd place team, Jialun He\\nBiography\\nJialun He\\xa0received a Ph.D from MIT in 2003 and is currently a senior algorithm engineer at Hemedex, Inc.\\r\\nWhat was your background prior to entering this challenge?\\r\\nI am a senior algorithm engineer at Hemedex, Inc in the past ten years, where I work on the development of a monitoring device used to measure real time tissue blood flow level. The device is used in neurosurgery and neurointensive care, as well as organ transplant, reconstructive surgery and oncology. I have an interdiscipline background in mechanical and biomedical engineering where I got my Ph.D degree from MIT in 2003. In recent years my focus is on extracting useful information from patient data recoded by our device. My interests are in big data application, especially in healthcare and wearable device\\r\\nWhat made you decide to enter?\\r\\nSeveral years ago I worked on a project for seizure detection using cerebral blood flow (CBF) rate recorded by our devices. Seizure is generally detected with EEG data recorded at a frequency ranging from 100 to 1000Hz. Our monitor records CBF at 1Hz. When patient data labelled with seizure came in, I found that the patient is also experienced high fluctuation in CBF. Seizure CBF chart is very similar to seizure EEG chart, at different time scale. The seizure CBF waveform can also be decomposed into various frequency bands similar to EEG’s wave bands. Anyway, the seizure detection project was successful and it has been implemented in an automatic system for analyzing incoming patient data. So when I found out that Kaggle hosted a competition for seizure detection, I would like to see what I could do with EEG data\\r\\n\\r\\n\\nWhat preprocessing and supervised learning methods did you use?\\r\\nThis competition is all about feature engineering. The core features are the power spectral band. Other candidates of features are signal correlation between EEG channels and eigenvalue of the correlation matrix. Both in frequency domain and time domain.\\r\\nSeveral common classifiers in scikit-learn package have been tested, such as random forest, gradient tree boosting, support vector machine. Most of them had really good CV score for individual subject. But did not get good score in LB. The gaps between CV score and leaderboard (LB) score were very big. One of the reason is that LB score is across all subject. Other possible reason is due to overfitting. My best submissions according to the LB score were based on support vector machine with RBF kernel, which produced better results because of more control in balancing bias and variance\\r\\nWhat was your most important insight into the data?\\r\\nDue to very limited amount of training cases, for example, each patient data only has 3 independent seizure occurrences, it is very important to keep a delicacy balance between bias and variance. With this in mind, I added additional signal processing procedures in feature extraction. I resample the signal from 400Hz in dog and 5000Hz in patient to 100Hz. I split the data into longer window of 50 seconds. I also resample the frequency band of power spectral. Those signal processing procedures all helped reducing overfitting\\r\\nAnother challenge of the competition is that the evaluation matrix is based on AUC cross all subjects. I have tried a cross subject classifier but the score is not good compared to classifiers built on individual subject. My best submission is based on individual classifier. Additional calibration is needed to align predict across subjects.\\r\\nWere you surprised by any of your insights?\\r\\nI was surprised by the final shake up of the leader board when the final scores were revealed. Many competitors’ final scores were reduced dramatically due to overfitting. It was not a surprise for me that I was among the group of competitors that had least amount of overfitting. I had to admitted that luck was also a factor in determine who could be the final prize winner.\\r\\nWhich tools did you use?\\r\\nI use Python and standard packages of numpy, scipy, scikit-learn and matplotlib. I also have a homemade neural network system\\r\\nWhat have you taken away from this competition?\\r\\nStart early is my advice for anyone who wants to enter a Kaggle competition. I started a month before the end of the competition. I was in a rush every day. At the end of the competition I still have some ideals that have not been implemented. My guess is that I need at least two months to explore all the possible ideals and have a chance for good ensemble.\\r\\n\\r\\nGood old school technique in signal processing helps me a lot in this competition. Domain knowledge in patient monitoring also help me understand the nature of the problem. However, what impressed me most is that folks with limited amount of domain knowledge also did pretty well in the competition. My observation is that with the open source tools and learning experience in Kaggle competition, a person can tackle most of the machine learning problems.\\r\\n3rd place team, ESAI-CEU-UCH\\nBiographies\\nJavier Muñoz-Almaraz: PhD in Mathematics with a dissertation about numerical continuation of periodic orbits. Now, he is interested in optimization problems related with data analysis, dynamical systems in mechanics and neuronal dynamics.\\r\\n\\r\\nFrancisco Zamora-Martínez: PhD in Computational linguistics, application of artificial neural networks to language modeling for handwriting recognition, spoken language understanding and machine translation. He is interested in machine learning, energy efficiency, pattern recognition and data science problems.\\r\\n\\r\\nJuan Pardo: PhD in Computer Science Engineering. He has been working in several European research projects in different fields. He is director of the department of Physics, Mathematics and Computing at university. Interested in data science. Volunteer at ISACA and PMI organizations.\\r\\n\\r\\nPaloma Botella-Rocamora: PhD in Mathematics, specialist in Statistics. She has been working in Health research projects a long time. She was visiting researcher last year at Bio-statistics Dpt. at University of Minnesota. Interested in Bayesian statistics in data science.\\r\\n\\r\\n\\nWhat was your background prior to entering this challenge?\\r\\nWe are a multidisciplinary research group (ESAI) composed by lecturers at Universidad CEU Cardenal Herrera, in Valencia (Spain).\\r\\n\\r\\nPaloma Botella-Rocamora and Javi Muñoz-Almaraz are mathematicians, Paloma more focused on statistics and Javi on optimization and dynamical systems. Juan Pardo and Francisco Zamora-Martínez are informatics, Juan more focused on computer engineering and Francisco in computer science.\\r\\n\\r\\nAll of us hold a PhD in our respective areas, and we are forming a new multidisciplinary research group in data science, that is a field where we have shared interests.\\r\\n\\r\\nWe are interested in the application of Bayesian methods, deep learning and optimization methods to solve challenging problems like the proposed in this competition.\\r\\nWhat made you decide to enter?\\r\\nFrom a technical point of view, we wanted to test the team skills and show that it is possible to produce a competitive system combining ideas from different (but related) research areas. Additionally, we tried to apply deep learning techniques to the challenge, whose benefit to this task remains unclear after the competition results analysis.\\r\\n\\r\\nOn the other hand, the competition has been important to let us to work in a common problem and find the way to speak the same language (members work in different areas). And of course the money, as our budget for research is very very limited due to the economical crisis there is in Spain.\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nWe tried different preprocessing techniques. First, we started with Fast Fourier Transform(FFT) of the data over 50% overlapped sliding windows with 60 seconds length.\\r\\n\\r\\nThis transformation produced a very large number of features, and a filter bank with 6 filters has been applied to avoid dimensionality problems. This preprocessing was insufficient to achieve the high AUC results of top10 teams.\\r\\n\\r\\nWe played with eigen values of correlation matrices computed over the same sliding windows as FFT. The combination of both features in the same model was also important to improve the system results, but not enough to be competitive.\\r\\n\\r\\nThe high correlation between windows and filters suggests that our models can be improved by removing these correlations in the data, so we decided to apply Principal Component Analysis (PCA) and Independent Component Analysis (ICA) to the FFT output. Both transformations showed similar performance, and the system achieved the top20 of public test leaderboard.\\r\\n\\r\\nJust to improve a little bit the results, we decided to compute different bunch of statistics over the whole input data, without windowing, and finally combined different models and preprocessing techniques in an ensemble.\\r\\n\\r\\nRegarding to supervised learning methods, we start trying logistic regression models, expecting that linear models wouldn\\'t overfit, and they could be used as a nice baseline. However, our surprise was that this simple logistic regression models achieved so high AUC scores in cross-validation (0.93), but in public test data the AUC dropped to very low values (approx. 0.60). We were very confused because of this result, and discussed during the whole competition about why it happened, but we couldn\\'t realize any clear explanation.\\r\\n\\r\\nFollowing logistic regression, we tried K-nearest-neighbors (KNNs), but computing class probabilities instead of distances. The drop between our cross-validation AUC and the public test AUC was reduced by using KNNs, but not so much. Finally, we trained Artificial Neural Networks (ANNs) with different number of layers, using dropout to avoid overfitting and ReLU activation functions. After a hard manual optimization of these ANN models, we obtained our best single model result.\\r\\n\\r\\nBesides the exploration stated above, the combination of KNNs, ANNs using FFT, PCA, correlations, and other statistics, in an ensemble optimized following Bayesian Model Combination (BMC) was our ticket to be in the top15 in public leaderboard, but 4th place in private leaderboard, and 3rd prize after the winner rejected its first prize.\\r\\n\\r\\nWe found that the ensemble of different knowledge sources was a nice way to ensure good stability between public and private AUC. (See the code in the Github Repo)\\r\\nWhat was your most important insight into the data?\\r\\nWe found that all the channels of the EEG were very correlated, and this correlation could harm the supervised statistical learning. The use of PCA or ICA to reduce this correlation is a way to ensure better performance. However, another ways to exploit channels similarity, and to reduce their dimensionality, could be explored.\\r\\n\\r\\nAs it was discussed in the forum, a global model, able to learn from all the available subjects, would be a very important step forward, but this exploration remains in the future work of this task, at least for us.\\r\\nWere you surprised by any of your insights?\\r\\nWe were surprised about the logistic regression behavior using our features, the large drop between cross-validation and public test AUC was very disturbing. It has complicated the internal comparison between our different approaches.\\r\\nWhich tools did you use?\\r\\nWe used two main tools, R for statistical preprocessing and APRIL-ANN for FFT and supervised learning. This last tool is a brand new development where members of the research team are involved.\\r\\nWhat have you taken away from this competition?\\r\\nWe realize that it is very important to stabilize the system results by using ensembles, and that ensembles of different preprocessing pipelines can be even better. Following this methodology, it is easy to share knowledge and skills in multidisciplinary teams, and it resulted in a way to improve the system results to be in the top10.\\r\\nFurther Reading & Resources\\n\\nCongratulations to the Winners!\\nFeatures for Seizure Detection\\nHowbert JJ, Patterson EE, Stead SM, Brinkmann B, Vasoli V, Crepeau D, Vite CH, Sturges B, Ruedebusch V, Mavoori J, Leyde K, Sheffield WD, Litt B, Worrell GA (2014) Forecasting seizures in dogs with naturally occurring epilepsy. PLoS One 9(1):e81920.\\nCook MJ, O\\'Brien TJ, Berkovic SF, Murphy M, Morokoff A, Fabinyi G, D\\'Souza W, Yerra R, Archer J, Litewka L, Hosking S, Lightfoot P, Ruedebusch V, Sheffield WD, Snyder D, Leyde K, Himes D (2013) Prediction of seizure likelihood with a long-term, implanted seizure advisory system in patients with drug-resistant epilepsy: a first-in-man study. LANCET NEUROL 12:563-571.\\nPark Y, Luo L, Parhi KK, Netoff T (2011) Seizure prediction with spectral power of EEG using cost-sensitive support vector machines. Epilepsia 52:1761-1770.\\nDavis KA, Sturges BK, Vite CH, Ruedebusch V, Worrell G, Gardner AB, Leyde K, Sheffield WD, Litt B (2011) A novel implanted device to wirelessly record and analyze continuous intracranial canine EEG. Epilepsy Res 96:116-122.\\nAndrzejak RG, Chicharro D, Elger CE, Mormann F (2009) Seizure prediction: Any better than chance? Clin Neurophysiol.\\nSnyder DE, Echauz J, Grimes DB, Litt B (2008) The statistics of a practical seizure warning system. J Neural Eng 5: 392-401.\\nMormann F, Andrzejak RG, Elger CE, Lehnertz K (2007) Seizure prediction: the long and winding road. Brain 130: 314-333.\\nHaut S, Shinnar S, Moshe SL, O\\'Dell C, Legatt AD. (1999) The association between seizure clustering and status epilepticus in patients with intractable complex partial seizures. Epilepsia 40:1832-1834.\\n', '\\nThis past December, the defending champions of Kaggle\\'s annual holiday competition swept all three\\xa0prizes in the\\xa0Helping Santa\\'s Helpers\\xa0optimization challenge and claimed $20,000. In their own words,\\xa0Marcin Mucha and Marek Cygan\\xa0of team Master Exploder walk us through their winning approach.\\r\\n[caption id=\"attachment_4775\" align=\"aligncenter\" width=\"300\"] Santa brought team Master Exploder \"the power of the lower bounds\" for Christmas.[/caption]\\r\\n\\r\\nWhat was your background prior to entering this challenge?\\r\\n\\r\\nWe are both active researchers in the field of algorithmics. We are particularly interested in ways of dealing with computational hardness: mainly approximation algorithms, and in Marek’s case parametrized complexity. We both work at the University of Warsaw.\\r\\n\\r\\nWe also both have a long history of competing in all kinds of programming contest. Quick Topcoder style contest, ACM ICPC, marathons, 24 hour challenges - we have done all of these many times.\\r\\n\\r\\nWhat made you decide to enter this competition?\\r\\n\\r\\nConsidering our fixation with programming contest it is not very surprising that we wanted to try our luck with Kaggle’s Christmas Santa challenges. In fact, these contests seem perfect for us. Not only the problems are computationally hard, but also the long duration gives these contest a bit of a research flavor.\\r\\n\\r\\nThe first one we have entered was last year’s “Packing Santa’s Sleigh”. We have had tons of fun competing and in the end we managed to grab the top spot. After that experience, we have been eagerly awaiting the next challenge and it did not disappoint.\\r\\n\\r\\nWhat algorithmic approaches have you used?\\r\\n\\r\\nThe data in this challenge was rather large - we had to schedule 10 million toys for 900 elves. This made it really hard to directly optimize the total production time. Instead, based on investigating the specific features of the problem and the data, we designed a high level structure that our solution would have, and optimized pieces of this structure separately. To solve these subproblems, we used Integer Linear Programs (ILP), as much as we could. When we were not able to find a reasonable ILP formulation, or when the problem seemed too simple to use one, we resorted to local search/simulated annealing.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nInvestigating the data was key to designing a good high level solution structure. However, most of the observations we made were rather straightforward. In contrast to the ML contest, there are no magical features here, no interesting patterns either. The key observation was probably the following:\\r\\n\\r\\nThere was a huge number of large toys and producing those, even at the highest speed rating, would take many years. Since all toys arrive during the first year, this makes arrival times almost irrelevant (except for the first year, which needs to be processed separately). This observation significantly simplifies the problem and gives it a “bin packing flavor”.\\r\\n\\r\\nWere you surprised by any of your findings?\\r\\n\\r\\nFor a long time we used elves with maximum speed rating to produce the largest toys. Then we saw the trailer for the new Marion Cotillard movie “Two days, one night” and thought: “Wow, this is what our elves should do! Get rid of their fellow elves to get salary bonuses!”. Well, not exactly. This might have been a good idea, but the actual idea that we had was that they should work for two days and one night straight. That is 34 hours, including 20 working hours and 14 \\xa0non-working hours. As it turns out, this leads to the speed rating only dropping from 4.0 to about 1.3 and is much better than just producing a very large toy.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nWe started by analyzing the data using R. Then, for non-ILP parts of the solution we \\xa0used \\xa0GCC C++ compiler, and for solving the ILPs we used the interactive version of FICO Xpress Optimizer. We were considering using the C++ solver API, but abandoned that idea - for fast development it is usually better to stick with simplicity. We also used some other tools to analyze the logs generated by our programs and to automate many tasks - mainly standard unix utilities like grep, sort, uniq etc., but also occasionally awk or python.\\r\\n\\r\\nHow did your experience help you succeed in this competition?\\r\\n\\r\\nWe already mentioned in an earlier answer how these Christmas optimization contests are perfect for us because of our research experience. This year’s contest’s problem was particularly suitable for us as it was amenable to ILP approaches. As we do ILP modeling regularly in our research and have a lot of experience with related techniques and tricks, this might have given us an edge over the field.\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\n\\r\\n$20,000. Just kidding, of course we are going to have to pay the taxes. But more seriously:\\r\\n\\r\\nWhy use ILPs instead of, say, local search algorithms? It is not really about the quality of solution, or how fast they are found. The key advantage of using ILPs is that they do not only give you a solution, but also a lower bound. This makes it so much easier to decide when to stop the solver, it also gives you hints as to where your solution might be improved. In principle we knew that already, but this contest let us really feel the power of the lower bounds.', 'The Brain-Computer Interface (BCI) Challenge\\xa0used EEG data captured from study participants who were trying to \"spell\" a word using visual stimuli. As humans think, we produce brain waves that can be mapped to actual intentions. In this competition, Kagglers were given the brain wave data of people with the goal of spelling a word by only paying attention to visual stimuli. This competition was proposed as part of the\\xa0IEEE Neural Engineering Conference (NER2015).\\r\\n[caption id=\"\" align=\"aligncenter\" width=\"355\"] Participant spelling performance is highly dependent upon the subject’s attentional effort towards the target item and his/her simultaneous effort to ignore the flashes of the irrelevant items.[/caption]\\r\\n\\r\\nIn this blog, fourth place finisher, Dr. Duncan Barrack, shares his approach and some key strategies that can be applied across Kaggle competitions.\\r\\nBiography\\r\\nDr. Duncan Barrack received his PhD in applied maths from the University of Nottingham in the UK in 2010 and is currently a research fellow at the Horizon Digital Economy Research Institute at the University of Nottingham.\\r\\nWhat was your background prior to entering this challenge?\\r\\nMy PhD work involved modelling the signalling mechanism which was thought to be responsible for increasing proliferation rates, as well promoting cell cycle synchrony, in clusters of radial glial cells (a type of brain cell). This involved using tools from non-linear dynamical systems theory to study systems of ordinary differential equations. Since 2011, I have been working as a research fellow at the Horizon Digital Economy Research Institute, at the University of Nottingham where I apply statistical and machine learning techniques to solve problems in industry and healthcare.\\r\\nHow did you get started competing on Kaggle?\\r\\nAlthough I had dabbled with the Titanic and Digit Recognizer 101 competitions a while ago, I really got into Kaggle as part of a big data workshop held at Nottingham University where a number of colleagues and I entered the American epilepsy society seizure prediction challenge.\\r\\nWhat made you decide to enter this competition?\\r\\nI had really enjoyed the\\xa0 American epilepsy society seizure prediction challenge. The BCI challenge started shortly after the epilepsy challenge had finished and as it also involved analysing EEG data it seemed natural to enter. Also, I found the notion that it is possible to use brain signals to communicate with a machine (a concept new to me) extremely interesting.\\r\\n\\r\\n[caption id=\"attachment_4798\" align=\"aligncenter\" width=\"282\"] 56 passive EEG sensors captured brain wave data of study participants[/caption]\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nThis competition was all about finding the right features. Because of this I spent a good deal of time reading the BCI literature to find out about the kind of features used to solve similar problems. The best features I found were based on simply taking the mean of the EEG signal in each channel over windows of various lengths and lags as well as features based on template matching.\\r\\n\\r\\nI threw a lot of machine learning methods at the problem including logistic regression with elastic net regularisation, tree based methods and SVMs.\\xa0 In the end my best performing model was a weighted averaged of two SVMs with linear kernels and different feature sets, although the average of two logistic regression models did almost as well.\\nWhat was your most important insight into the data?\\r\\nThe data used to calculate the public leaderboard score came from two subjects only.\\xa0 With such a small number of subjects it was clear to me and, going by the posts in the forums many others as well, that the public leaderboard score was likely a poor estimator of the private score. For this reason, I took care when it came to my cross validation (CV) procedure as I knew I would be relying on it when choosing my final model.\\xa0 The training data came from 16 subjects and, for my CV procedure, I split it into 4 ‘subject wise’ folds. I then calculated the AUC score (the evaluation metric used in the competition) for the four subjects in the test fold. I repeated this CV procedure 5 times with different splits and took the average of the 20 AUC scores produced (5 repetitions × 4 folds). The CV score of my best model (~0.75) was very close to to the public leaderboard score (~0.77). This model was also the most stable (the CV score variance was the lowest of all my models) which I saw as a desirable property given that the number of subjects in the test set was also relatively small.\\r\\nWere you surprised by any of your findings?\\r\\nBecause I had tried to be careful with my cross validation procedure, I wasn\\'t too surprised by my final leaderboard score. However, I was surprised (and also very impressed) with how much higher the score of the overfitting avengers team, who finished in top spot on leaderboard, was. Reading about their approach in the forums really opened my eyes to what was possible. I\\'m just glad they decided not to accept the prize!\\r\\nWhich tools did you use?\\r\\nFor the feature extraction I used Matlab. I used Python with scikit-learn for the modelling.\\r\\nWhat have you taken away from this competition?\\r\\nDespite the fact that simple models like logistic regression have been around for ages they can still be extremely effective. This is especially true in completions like this one where it\\'s important not to overfit because results must generalise across data from different subjects.\\r\\nDo you have any advice for those just getting started in data science?\\nI think sometimes there is a temptation when you\\'re getting in to data science to use the biggest and baddest model you can as soon as you can when simple models may be more effective. Also, it\\'s really important to carry out some exploratory data analysis first. This may help spark some ideas on what features may be useful .', 'We recently wrapped up our second annual March Machine Learning Mania competition with an industry insider finishing at the top of the leaderboard.\\r\\n\\r\\nFirst place finisher, Zach Bradshaw,\\xa0is a Sports Analytics Specialist at ESPN. Prior to joining ESPN, he worked in the basketball analytics departments of the Phoenix Suns and Charlotte Bobcats (now renamed the\\xa0Charlotte Hornets). Zach received a Masters of Science in Statistics from Brigham Young University in 2014.\\r\\n\\r\\n[caption id=\"attachment_4832\" align=\"aligncenter\" width=\"660\"] March Machine Learning Mania 2015 had 341 teams and 405 players competing to correctly predict winning percentages for the likelihood of each possible matchup, not just the traditional bracket.[/caption]\\r\\n\\r\\n\\xa0\\r\\nHow did you get started in data science and sports analytics?\\r\\nFrom a young age, I was passionate about sports and enjoyed solving interesting problems. However, it was not until later in college that I had an unanticipated opportunity to intern with an NBA team. Thanks to good timing, hard work, and my previous basketball research, I was fortunate enough to get an internship doing what I love, applying data science in basketball.\\r\\nWhat made you enter the March Mania competition on Kaggle?\\r\\nAs soon as I heard about the March Mania competition, I wanted to participate due to my interest in basketball and predictive modeling. I had previously done some predictive modeling for NBA games and this competition was the perfect opportunity to apply similar techniques to college basketball.\\r\\nHow did your industry knowledge help you create your dataset and model?\\r\\nMy previous experience modeling NBA games guided my approach in the competition. Both the dataset and modeling techniques closely resembled my previous work.\\r\\nDid you use any intuition or industry knowledge to manually tweak your output probabilities?\\r\\nUsing a Bayesian framework allowed for the incorporation of prior knowledge or intuition that was not accounted for in the data. However, in hindsight this hurt my predictions slightly more than it helped, at least in the 2015 tournament. There were no tweaks to the output probabilities of my first entry. However, with my winning entry, I manually tweaked the prediction for the Baylor vs. Georgia State game. With a series of unlikely events at the end of that game, I successfully “predicted” the upset.\\r\\n\\r\\n[caption id=\"attachment_4831\" align=\"aligncenter\" width=\"300\"] 14th-seeded Georgia State beat 3rd-seeded Baylor with a 3 point buzzer beater.[/caption]\\r\\nHow did your experience in sports and sports analytics help you succeed in this competition?\\r\\nMy experience in sports analytics saved a lot of time in the exploratory phase as I already had a sense of what data and techniques might make a good model. My experience with basketball had a small impact on how I modeled a few nuances of the game. Although my experience in sports analytics was helpful in succeeding in the competition, the gains were marginal and I also needed some good luck to succeed.\\r\\nWhich tools did you use?\\r\\nR and SQL\\r\\nDo you have any advice for those just getting started in data science and sports analytics?\\r\\nAll models are wrong but some are useful, don’t get too caught up in trying to create the perfect model. Taking some time to better understand the problem at hand and its underlying structure is an important and oft overlooked step in the modeling process. For those specifically interested in sports analytics, I think the best way to get started is doing your own research. Like any other industry, connections are important and having some of your own research is the first important step in developing relationships with others in the industry.', 'Team Driving It took second place in the hugely popular AXA Driver Telematics competition. This blog shares their experience working as a team to build a \"telematic fingerprint\" for drivers, making it possible to distinguish if a given driver was behind the wheel.\\r\\n\"Our final telematics model consisted of an ensemble of 6 different models. We mainly used Random Forests, in combination with Gradient Boosting and Logistic Regression.\"\\n\"Collaborating in a team is a great experience, we finished 2nd place because we joined forces. I definitely recommend it.\"\\xa0\\r\\n\\xa0\\r\\n\\r\\n[caption id=\"attachment_4839\" align=\"aligncenter\" width=\"300\"] 1,528 teams and 1,865 players competed to create a \"telematic fingerprint\" for drivers.[/caption]\\r\\nWhat was your background prior to entering this challenge?\\nScott: I work at an aerospace company as a mechanical engineer. I’ve done a fair amount of coding in Fortran, but have only been using python for a few months.\\r\\n\\r\\nJanto: I\\'m a Cognitive Science MSc student. I’m interested in Computational Neuroscience and Machine Learning. My work involves modelling behavioural and brain data as well as applying machine learning. While I mostly work in MATLAB during the day I started coding in Python about 1.5 years ago and it has become my first choice for machine learning tasks (and nearly everything else). Currently, I’m working on my thesis at Oxford University.\\r\\n\\r\\nAndrei: I work as a cloud architect at Sparked (http://sparked.nl), mainly focusing on bringing data analytics and machine learning to mainstream. In parallel, I work on finishing up my PhD thesis as a result of 4 years being a part of the Software Languages Team at the University of Koblenz-Landau in Germany.\\r\\nHow did you get started competing on Kaggle?\\nScott:\\xa0I got started in the Christmas Challenge, helping Santa’s helper’s. I wanted to use it as a method of learning python. I picked that challenge because it was more algorithm based than data analysis based.\\r\\n\\r\\nJanto: This was my first competition on Kaggle. I had played with the Titanic and the MNIST data set to benchmark a few algorithms. My main motivation for joining Kaggle was to test how statistics and machine learning algorithms would perform on real datasets and learn about the strengths and weaknesses of different approaches, hands-on.\\r\\n\\r\\nAndrei: Actually, this was my first competition. I like the automotive and related domains and decided to join.\\r\\nWhat made you decide to enter this competition?\\nJanto: I liked the fact that the data was unlabeled which I found challenging. With only the raw GPS data given the competition left room for a lot of creative work in feature engineering, local testing and trip matching which became evident in the diversity of ideas on the forums. I expected the challenge to be quite different from the \"supervised learning toolbox-battles\".\\r\\n\\r\\nScott:\\xa0I chose to do this analysis because it appeared to me that path matching would be a dominant method of ranking the routes. Since there is no out of the box path matching tools that I know of, it would be much more algorithm based and somewhere I could add value. I contrast this with data analysis techniques like feature generation, random forests, gradient boosted trees, etc. Which at the time I was not familiar with.\\r\\n\\r\\nAndrei: The motivation was twofold. First, I enjoy things happening in the automotive and related fields. Second, I was planning to refresh my data science toolkit, that I used to apply in very special technical domains in the past. For this reason I was looking to join the team, since this is the best way to combine and distill the best approaches out there.\\r\\nWhat preprocessing and supervised learning methods did you use?\\nJanto: I focused on the telematics modelling part and spent a great amount of time on feature engineering and data exploration at the beginning of the challenge. First I tried Self-Organizing Maps which didn\\'t work well at the early stage when I only had few features.\\r\\n\\r\\nFor me, the crucial step was to turn the task into a supervised problem and the effort we put into ensembling telematics and trip matching.\\r\\n\\r\\nWe tackled the problem by combining the results of the trip matching that Scott had done and our driver signature models. The idea was to identify frequently taken trips as they were likely to be trips from the respective driver and apply supervised models to telematic features for unmatched trips.\\r\\n\\r\\nOur final telematics model consisted of an ensemble of 6 different models. We mainly used Random Forests, in combination with Gradient Boosting and Logistic Regression. Some coarse grid searching I had done earlier in the competition yielded the model parameters and we didn’t tune a lot in the end. We trained models on different partially independent feature sets each of us had extracted and combinations of those feature sets.\\r\\n\\r\\nRandom Forests worked pretty well and extremely fast: after decreasing the time spent for input/output routines by converting the data to .npy files, my first ensemble (a simple Random Forest combined with Logistic Regression) scored 0.86 on the leaderboard in about 30 minutes.\\r\\n\\r\\nScott:\\xa0Like many people in the competition, we used a combination of path matching and a telematics approach. My focus was on path matching, and only switched to telematics at the end of the competition.\\r\\n\\r\\nFor the path matching, the most important preprocessing method was the RDP algorithm, which I discovered relatively early in my investigation phase when I stumbled upon this Stack Overflow post\\r\\n\\r\\nhttp://stackoverflow.com/questions/14631776/calculate-turning-points-pivot-points-in-trajectory-path\\r\\n\\r\\nThe RDP algorithm effectively works by drawing a straight line between the starting and ending points of the path, and finding the point on that route that is the maximum distance from that line segment and comparing it against a provided tolerance. If the largest point is outside that tolerance, is remembered as an RDP point, and the path is split and the RDP point serves as a new starting and ending point for the line segment.\\r\\n\\r\\nOnce I had those RDP points, I assigned every adjacent set of 3 points, and every set of 3 RDP points with one skipped between them, an angle and the distance of the legs of the triangle. I then matched every path against every other path and looked for at least 3 matching angles / leg distances that could be rotated to be in the same locations.\\r\\n\\r\\nI was very surprised at the end of the competition to see that many teams utilized the RDP algorithm, since it never broke on the forums during the competition, although many other valuable insights did.\\r\\n\\r\\nAndrei: Gradient boosting proved the reputation of performing well on a small training dataset. There is a perfect in-depth tutorial which I would recommend http://topepo.github.io/caret/index.html especially about model training and parameters tuning.\\r\\n\\r\\n[caption id=\"attachment_4835\" align=\"aligncenter\" width=\"636\"] Schematic illustration of our modelling process. We extracted 3 feature sets and trained different classifiers on them (RF: Random Forest, GBM: Gradient Boosting Machine, LogReg: Logistic Regression). Each trip was assigned a probability (of being a true trip). In each classifier output we globally sorted the trips according to those probabilities and assigned ranks (across drivers). These were used as input for the final ensemble. Our final telematics model consisted of a weighted sum ensemble of the 6 different models.[/caption]\\r\\nWhat was your most important insight into the data?\\nScott:\\xa0For me, it was the power of ensembling. Early on I thought that the competition could be won by path matching. However it quickly became apparent that the best would be combination of path matching and telematics. Since I was not familiar with the telematics approach, I found teammates who could do this. As soon as the team formed, combining my ~120th ranked path matching with my teammates ~120th ranked telematics, we immediately jumped up to 25th place.\\r\\n\\r\\nEnsembling helped us at the end too. We had 6 different telematics solutions scoring between .83 - .89, and combined they scored .91. Basically, anytime we rolled in a independently scored telematics solution we were able to get at least some score boost.\\r\\n\\r\\nJanto: I agree with Scott, the ensembling was key. Early in the competition it seemed there were two types of competitors: the \"telematic modelling guys\" and the \"trip matchers\", both obtained good results in the 0.9 ballpark. I soon realized that trip matching would be important to perform well in this task, so we got together.\\r\\n\\r\\nFor me, the main insight was the fact that the two distinct components of the task - geometry matching and driver modelling - could be best combined by using \"probability ranks\" for the trips. The probability distributions of the different models often looked quite different and the trip matching basically produced counts as output. So, we had to translate the probabilistic outputs and counts into one common metric.\\r\\n\\r\\nAndrei: If we get to the origin of the challenge, namely identifying the driver fingerprint, the path matching part originally sounded like actually cracking the competition. I still think it does not contribute the fingerprint, but that was probably the only way to handle the fact, that real coordinates were anonymized and trimmed down. Having some kind of the “driving landscape”, i.e. types of driving areas, would be more useful feature in real life.\\r\\n\\r\\n[caption id=\"attachment_4836\" align=\"aligncenter\" width=\"642\"] Trip matching. The trips are downsampled using the Ramer-Douglas-Peucker (RDP) algorithm. Subsequently, our algorithm searched exhaustively for triples of GPS points within a specified distance that matched in angle and could thereby match routes but also road segments. Varying the search distance, i.e. the sensitivity of the algorithm, and integrating the results we could significantly improve our trip matching.[/caption]\\r\\n\\r\\n\\xa0\\r\\nWere you surprised by any of your findings?\\nScott:\\xa0I was surprised at the number of junk runs there were. Approximately 3% of the data was just garbage that never left 100 meters from the starting point. We assumed that these was accidentally turned on GPS or similar. We ended up getting a little bit of AUC value out of them by simply counting the number of junk runs per each driver, and if a driver had a lot of junk runs, moving each of those runs higher in the rankings. (Some of the drivers had 30, 40 or more junk runs!)\\r\\n\\r\\nJanto: My SOMs performed surprisingly bad.\\r\\n\\r\\nAndrei: The “real life” nature of the data was indeed the most “uncomfortable” thing, that automatically gets you out of the comfort zone of working with the reference samples. There were many concerns about this in the forums as well.\\r\\nWhich tools did you use?\\nScott:\\xa0For the path matching I used mostly python, with numpy and matplotlib. The path matching was nice because the paths could be plotted for immediate feedback on if the algorithm was working or not. With the path matching, I perennially had a problem with run time. For most of the competition, my path matching algorithm took 2 days to get through all 2736 drivers on my 4 core computer. I spent substantial time improving the code to make it run faster, but that was quickly eaten up by making it more complicated to perform better. Eventually, I wrote some of the more simple, but computationally long parts of the algorithm in Fortran, which processes For loops approximately 100 times faster than python, and used F2Py to tie them into the python.\\r\\n\\r\\nAt the end of the competition, we needed more computing power, so I launched 7 two node virtual machines on google compute, as well as another on Azure for a total of 16 nodes running for 14 days. (The cost of this would have been on the order or ~ $100, however Google has a $300 free credit which I utilized.)\\r\\n\\r\\nFor the telematics, we utilized Random Forests and Gradient Based Trees from the Python SciKit Learn toolkit.\\r\\n\\r\\nJanto: I ran all my analyses in Python, using the great numpy module and scikit learn toolkit for the modelling. For visualizations I used matplotlib. We collaborated via GitHub. As our repository was overflowing with scripts combining those in batch scripts was also quite helpful.\\r\\n\\r\\nAndrei: I mainly used R with the caret package (http://topepo.github.io/caret/index.html) Azure cluster was used with R Enterprise from http://www.revolutionanalytics.com/revolution-r-enterprise and doParallel backend http://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf\\nWhat have you taken away from this competition?\\nJanto: Collaborating in a team is a great experience, we finished 2nd place because we joined forces. I definitely recommend it. Also, we organized our code on Github which I had never used before and it’s a great resource when working together. The competition taught me the power of ensembles and how knowing the evaluation metric can really help to make informed choices in the ensembling process.\\r\\n\\r\\nScott:\\xa0The value in utilizing Github when working with teammates. The basics of using the sci-kit learn toolkit, especially random forests and gradient boosted trees for data analysis. The value of teams, since I would certainly not have scored as highly working on my own.\\r\\n\\r\\nAndrei: One does not need to be a domain expert to already perform above average. I do not think we really innovated in the feature development, neither we did in the methods. As it was indicated above, many competitors tried the same. But mainly focusing on different machine learning techniques and organized them together helped us to boost the performance. During last week the process was pretty much “wait 4 days to complete” to get 0.0005 performance.\\r\\nDo you have any advice for those just getting started in data science?\\nJanto:\\xa0In my opinion, one important point is to be able to test ideas as efficient and reproducible as possible. I like to build pipelines for every necessary task, like \"preprocessing\",\"modelling\", \"validation\" and \"submission\" from the beginning. These modules can be easily scaled up at later stages and you don\\'t have to care anymore about the code that produces your predictions if you change your model.\\r\\n\\r\\nIt helps to have one script in which you only specify the model or the ensemble of models, one that loads the data, one that preprocesses the data, etc. If you now wake up in the middle of the night and think \"I should really change my logistic regression to an ensemble of random forests and a gradient boosting machine and change the window length of my moving average preprocessing to 10s\" you just change two lines in the model and your preprocessing script and type \"python validate.py\" in your shell. This helps to keep an overview over your code and avoids errors in testing or output routines. And you can test a new ensemble when you come home from a party at 4 am before you go to sleep.\\r\\n\\r\\nValidate your models and keep a log. In this competition it turned out that public and private scores correlated nicely but you can’t rely on that. Keep track of every cross-validation and the corresponding LB score.\\r\\n\\r\\nIn this competition it was a bit tricky to validate model performance since the data was unlabeled. That\\'s why I transformed the problem into a supervised one and recreated the task, that is, detecting outliers, with known labels: I took the 200 trips of one driver as \"real\" trips and injected k (usually around 5-10) artificial false trips from another driver into the dataset. Now I had labels and could run a classification scheme. Doing this for around 100 drivers and computing the global AUC score yielded a good indicator of performance.\\r\\n\\r\\nWe observed that the public scores were really close to the local CV scores. This observation justified that we trusted the leaderboard score for some time consuming complex analyses that took a couple of days to compute at later stages of the challenge.\\r\\n\\r\\nScott:\\xa0Watch the forums in your competition. There are a ton of insights to be had in there. Our team got a lot of the features that we ended up using for the telematics competition based on posts from the forums. And our basic method of path matching combined with telematics was all over the forums\\r\\n\\r\\nReally get to know what the scoring criteria is. The AUC criteria used for this competition essentially only cares about the order the results are ranked in, not the actual scores. We got a large score boost ( ~.005 ) by making sure that no two results were scored the same by sorting our path matching results by the telematics scores and doing a few other tricks to make sure that each of the scores would be unique.\\r\\n\\r\\nAlso, don’t bother with running super computationally expensive runs early on. You can always crank up the number of trees in your random forest for the final few submissions, early on it is valuable to be able to iterate quickly. If I were to do this competition again I would have spent less time running the full 2736 driver data set in January / February and run more iterations of just 10 or 100 drivers that I could check and improve quickly.\\r\\n\\r\\nAndrei: Use things like google scholar to find few relevant research papers. Especially if you are not a domain expert. Some domains have a better coverage of different machine learning techniques then the others. It is not worth inventing things until the proven methods have not been tried.\\r\\nThe Team\\nAndrei Varanovich\\xa0With a diploma in System Engineering and Masters in Computer Science, Andrei is currently finishing a PhD thesis as a result of 4 years spent as\\xa0part of the Software Languages Team at the University of Koblenz-Landau in Germany. In his role as cloud architect at Sparked, he mainly focuses on data analytics and machine learning in the Microsoft ecosystem. He is a Microsoft Most Valuable professional since 2008.\\r\\n\\r\\nJanto Oellrich is a Cognitive Science MSc student specializing in Neuroscience and Machine Learning. Currently, he is working as a research assistant at the Department of Experimental Psychology at the University of Oxford. His research interests include perceptual decision making and feature representation in humans, computational modelling of behaviour and machine learning.\\r\\n\\r\\nScott Hartshorn is an aerospace engineer and a graduate of the University of Michigan. His professional work has been in structural engineering. He is currently working to develop an expertise in Machine Learning and Data Analytics.', 'There are Kagglers, there are Master Kagglers, and then there are top 10 Kagglers. Who are these people who consistently win Kaggle competitions? In this series we try to find out how they got to the top of the leaderboards.\\r\\n\\r\\nFirst up is KazAnova -- Marios Michailidis -- the current number 2 out of nearly 300,000 data scientists. Marios is a PhD student in machine learning at UCL and a senior data scientist at dunnhumby (organizer of the Kaggle competitions \\'Shopper Challenge\\' and \\'Product Launch Challenge\\').\\r\\n\\r\\n\\nMarios Michailidis Q&A\\nHow did you start with Kaggle competitions?\\r\\nI wanted a new challenge in the field and learn from the Grand Masters.\\r\\n\\r\\nI was doing software development about machine learning algorithms, which also led to creating a GUI for credit scoring/analytics by the name KazAnova- a nick name I frequently use in Kaggle to keep reminding myself the passion I have for the field and how it started, but I could only go so far by myself.\\r\\n\\r\\nKaggle seemed the right place to learn from the experts.\\r\\nWhat is your first plan of action when working on a new competition?\\n\\nFirst of all to understand the problem and the metric we are tested on- this is key.\\nTo as-soon-as possible create a reliable cross-validation process that best would resemble the leaderboard or the test set in general as this will allow me to explore many different algorithms and approaches, knowing the impact they could yield.\\nUnderstand the importance of different algorithmic families, to see when and where to maximize the intensity (is it a linear or non-linear type of problem?)\\nTry many different approaches/techniques on a the given problem and seize it from all possible angles in terms of algorithms \\'selection, hyper parameter optimization, feature engineering, missing values\\' treatment- I treat all these elements as hyper parameters of the final solution.\\n\\nWhat does your iteration cycle look like?\\n\\nSacrifice a couple of submissions in the beginning of the contest to understand the importance of the different algorithms -- save energy for last 100 meters.\\nDo the following process for multiple models\\r\\n\\nSelect a model and do a recursive loop with the following steps:\\r\\n\\nTransform data (scaling, log(x+1) values, treat missing values, PCA or none)\\nOptimize hyper parameters of the model\\nDo feature engineering for that model (as in generate new features)\\nDo features\\' selection for that model (as in reducing them)\\nRedo previous steps as optimum parameters are likely to have changed slightly\\n\\n\\nSave hold-out predictions to be used later (meta-modelling)\\nCheck consistency of CV scores with leaderboard. If problematic, re-assess cross-validation process and re-do steps\\n\\n\\nCreate partnerships. Ideally you look for people that are likely to have taken different approaches than you have. Historically (in contrast) I was looking for friends; people I can learn from and people I can have fun with - not so much winning.\\nFind a good way to ensemble\\n\\r\\n[caption id=\"attachment_4846\" align=\"aligncenter\" width=\"500\"] Screen from the KazAnova Analytics GUI.[/caption]\\r\\n\\r\\n\\xa0\\r\\nWhat are your favorite machine learning algorithms?\\r\\nI like Gradient Boosting and Tree methods in general:\\r\\n\\nScalable\\nNon-linear and can capture deep interactions\\nLess prone to outliers\\n\\nWhat are your favorite machine learning libraries?\\n\\nScikit for forests.\\nXGBoost for GBM.\\nLibLinear for linear models.\\nWeka for all.\\nEncog for neural nets.\\nLasagne for nets, although I learnt it very recently.\\nRankLib for functions like NDCG.\\n\\r\\n[caption id=\"attachment_4847\" align=\"aligncenter\" width=\"500\"] Image from slides \\'Introduction to Boosted Trees\\' by Tianqi Chen (XGBoost)[/caption]\\r\\n\\r\\n\\xa0\\r\\nWhat is your approach to hyper-tuning parameters?\\r\\nI do this very manually.\\r\\n\\r\\nI have only tried once to use something like Gridsearch. I feel I learn more about the algorithms and why they work the way they do by doing this manually. At the same time I feel that \"I do something, it\\'s not only the machine!\".\\r\\n\\r\\nAfter 40+ competitions I\\'ve found that I can get to the top 90% of the best hyper parameters with the first try, so the manual approach has paid off!\\r\\nWhat is your approach to solid CV/final submission selection and LB fit?\\r\\nIn regards to CV, I try to best resemble what I am being tested on.\\r\\n\\r\\nIn many situations a random split would not work. For example: In the Acquire valued shoppers\\' challenge we were mainly tested on different products (offers) than these available on the train set. I made my CV to always try to predict 1 offer using the rest of the offers as this could resemble the test leaderboard better than a random split.\\r\\n\\r\\nAbout final selection, I normally go for best Leaderboard submission and best CV submission. In the case of a happy collision, I select something as different as possible with respectable CV result just in case I am lucky!\\r\\n\\r\\n(A prayer to the god of overfitting is my secret 3rd submission)\\nIn a few words: What wins competitions?\\n\\nUnderstand the problem well\\nDiscipline ; To have a well-thorough and documented approach that you follow religiously and defines all the modelling process/framework from how you cross-validate, select models, avoids over fitting (which requires a lot of ...discipline).\\nAllow room to try problem-specific things or new approaches within that framework\\nThe hours you put in\\nHave access to the right tools\\nMake key partnerships\\nEnsembling\\n\\nWhat is your favourite Kaggle competition and why?\\r\\nThe Acquire valued shoppers\\' challenge, not only because I won and it is relevant to what my team does, but I also had the honour to collaborate with Gert Jacobusse.\\r\\n\\r\\n[caption id=\"attachment_4848\" align=\"aligncenter\" width=\"500\"] The final private leaderboard for the Valued Shopper\\'s Challenge[/caption]\\r\\nWhat was your least favourite Kaggle competition experience?\\nDecMeg, BCI and such channel-wave type of competitions. They have big data and are very domain specific.\\r\\n\\r\\nI found it hard to even make my CV working properly, plus I did quite bad. Hopefully I will improve.\\r\\nWhat field in machine learning are you most excited about?\\r\\nI like recommender systems if it can be considered a separate field.\\r\\n\\r\\nThere is a broad spectrum of techniques you can use (which are field specific) and to be able to understand what the customer likes is very challenging and rewarding.\\r\\nWhich machine learning researchers do you study?\\r\\nI study: Steffen Rendle, Leo Breiman, Alexander Karatzoglou, Michael Jahrer & Andreas Töscher, the Machine Learning and Data Mining Group at National Taiwan University, and Jun Wang & Philip Treleaven.\\r\\nCan you tell us something about the last algorithm you hand-coded?\\r\\nIt was LibFM for Avazu competition as I believed it could work well in that particular problem. I could not make it work as well as LibFFM apparently.\\r\\nHow important is domain expertise for you when solving data science problems?\\r\\nFor some competitions it is really important.\\r\\n\\r\\nThe fact that I am employed in the recommendation science field and the kind of work that we do within my team, has helped me win the Acquire valued Shoppers challenge.\\r\\n\\r\\nHowever I think you can go a long way by following standard approaches even if you don\\'t know the field well, which is also the beauty of machine learning and the fact that some algorithms do a significant job for you.\\r\\n\\r\\n\\nWhat do you consider your most creative trick/find/approach?\\r\\nI do multiple ensemble meta-stacking if I can use the term.\\r\\n\\r\\nDuring the course of my univariate model tuning I save all the models\\' outputs. Then I make meta-models with the univariate models selections and most of the times I end up with different ensembles of Meta models.\\r\\n\\r\\nSometimes I go to third Meta model with Meta models as inputs (Meta-Meta model).\\r\\nHow are you currently using data science at work and does competing on Kaggle help with this?\\nClassified!\\r\\n\\r\\nKaggle does help in optimizing my methods, learning new skills, meet nice people with same passion, be up to date with new tools and generally stay in touch with what\\'s going on in the field.\\r\\nWhat is your opinion on the trade-off between high model complexity and training/test runtime?\\r\\nThat is a big discussion in principle.\\r\\n\\r\\nI guess there is a trade-off, but there needs to be an understanding that better models are not necessarily the most interpretable ones and that a more complex model (that is likely to score better) is not necessarily less stable/more dangerous.\\r\\n\\r\\nI guess the optimum solution should be somewhere in the middle (e.g. not an ensemble of 100 models nor Naive Bayes)\\r\\n\\r\\n[caption id=\"attachment_4850\" align=\"aligncenter\" width=\"500\"] The best 8 finishes for KazAnova.[/caption]\\r\\nHow did you get better at Kaggle competitions?\\r\\nDid I ?! :D\\r\\n\\r\\nI guess what has helped a lot is:\\r\\n\\nSeeing previous solutions and end-of-competition threads\\nParticipate in Kaggle forums\\nLearn the tools\\nRead papers, websites, machine learning tutorials\\nOptimize processes (use sparse matrices, cut unnecessary steps, write more efficient code)\\nSave everything I\\'ve done and reuse (and improve). E.g. I keep a separate folder for each competition I\\'ve completed.\\nDedicate time (had to reduce video games)\\nCollaborate with others\\n\\r\\nI have found the following resources useful:\\r\\n\\nThis benchmark by Paul Duan in the Amazon competition (my first ever attempt with Python) for a general modelling framework.\\nFrom the same competition : Python code to achieve 0.90 AUC with logistic regression from Miroslaw Horbal to create pairwise interactions with cross-validation.\\nFor text analysis, this benchmark from Abhishek: Beating the benchmark in StumbleUpon Evergreen Challenge\\nXGBoost benchmark in Higgs Boson competition by Bing Xu\\nTinrtgu\\'s FTRL Logistic model in Avazu: Beat the benchmark with less than 1MB of memory\\nData science Bowl tutorial for image classification: IPython Notebook Tutorial.\\nH2O (R) deep learning benchmark from Arno Candel in Africa Soil competition\\nLasagne and nolearn tutorial for Otto competition (by the admin) :\\nAndrew Ng\\'s Coursera course in Machine learning\\nUniversity of Utah Machine learning slides.\\nWikipedia and Google\\n\\nAre partnerships important in achieving good results?\\r\\nVery.\\r\\n\\r\\nSometimes you cannot measure the impact from one competition only as what you learn from the others may be applicable in the future too. I\\'ve been very lucky to have made good and fun collaborations so far and I have learnt from all, especially:\\r\\n\\nFrom Gert I\\'ve learnt model ensembling, feature engineering and Fourier transforms.\\nTriskelion and Phil Culliton: Vowpal Wabbit\\nTVS to avoid over-fitting\\nBluefool (or Domcastro) 1st derivatives and BART\\nMike, Python!\\nGiulio, competition management and Lasagne.\\n\\nBio\\n\\nMarios Michailidis is Senior Data Scientist in dunnhumby and part-time PhD in machine learning at University College London (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK Market and has led many analytics projects with various themes including: Acquisition, Retention, Uplift, fraud detection, portfolio optimization and more. In his spare time he has created KazAnova, a GUI for credit scoring 100% made in Java.\\r\\nMarios loves competing on Kaggle and learning new machine learning tricks. He told us he will create something good for the ML community soon...', 'Marios & Gert, in a team of the same name, took 2nd place in the Microsoft Malware Classification Challenge. The two are regular teammates and previously won the\\xa0Acquire Valued Shoppers Challenge\\xa0together.\\r\\n\\r\\nThis blog outlines their approach to the Malware competition and also gives us a closer look at how successful teams come together and collaborate on Kaggle.\\r\\n\\r\\n\"[...]meta features in the asm files turned out to be even more useful than the alphanumeric contents - especially the number of lines in each section, and interpunction characters in each section.\"\\n\"Text classification techniques work really well in this problem. Neither I nor Gert had any prior knowledge of the field, yet finding the \\'predictive words\\' in the document files was more than enough to score well.\"\\r\\n\\r\\n[caption id=\"attachment_4871\" align=\"aligncenter\" width=\"300\"] 377 teams and 481 players fought back against the malware industry.[/caption]\\r\\nWhat was your background prior to entering this challenge?\\nGert: I work as an independent researcher at my own company Rogatio. I have worked on statistical models in epidemiology, fraud detection and product recommendations. My education was in psychometry (Leiden University), focused on traditional statistics and significance tests. During about 12 years of data analysis, I added programming skills and cross validation to my toolbox, because they become more important as the nature of data analysis problems changes.\\r\\n\\r\\n[caption id=\"attachment_4865\" align=\"aligncenter\" width=\"300\"] Gert\\'s profile on Kaggle[/caption]\\r\\n\\r\\nMarios: I am a data scientist! I have a bachelor\\'s degree in economics, Msc in Risk management and currently do my PhD (Part-Time) in Machine Learning and recommender systems at UCL. At the same time I am senior data scientist in dunnhumby and I have worked as analyst in many roles either in credit or marketing. About 4 years ago (2011), I decided I wanted to learn more about programming and machine learning hence I started learning Java. I put together my efforts into an ML\\xa0package called KazAnova. Since then I have picked up a lot of stuff from Kaggle and other means.\\r\\n\\r\\n[caption id=\"attachment_4864\" align=\"aligncenter\" width=\"300\"] Marios\\' (Μαριος Μιχαηλιδης KazAnova) profile on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\nGert: No malware specific knowledge, I still don’t know what the different malware types mean or even in what way they are different. I think our skills to handle large datasets were most important for our\\xa0success: knowing what bits of feature extraction and modeling to invest our time in, and when to move on to the next idea.\\r\\n\\r\\nMarios: No. Only that I do not like Viruses (of any kind) and I will do anything it takes to stop them from continuing to mess up my computer, forcing me re-format every now and then! I treated the problem as text classification though where I have learnt a couple of things from previous (Kaggle) competitions.\\r\\nHow did you get started competing on Kaggle?\\nGert: About four years ago, one of my colleagues pointed me to the Kaggle website. Since then, I am addicted. For me, competing in a Kaggle challenge is at least as much fun as gaming. With one important difference: at the end of the day you haven’t wasted your time!\\r\\n\\r\\nMarios: A friend of mine pointed it out to me too (about 2 years ago). I remember I was doing a logistic regression model for a client and that friend of mine told me, “Why don’t you use Random Forest instead, everybody seems to say its better…” Then I asked, “Who’s everybody?” Then he pointed to an interview of Jeremy Howard which mentions just that.\\r\\nHow did your team start competing together?\\nMarios: I guess we were both in a quest to get the Master\\'s badge and we were getting almost there... but at the last moment we were always being stripped out of the top 10. In the Allstate competition Gert finished 11th out of 1550+ teams and I finished 17th. I remember we were hoping there will be cheaters (shame on us!) ahead of us so that we would\\xa0finally make it, but no luck! Gert was quite vocal about that and this forum post was the start of a 2 future-Kaggle-wins combo! Putting that aside, I would see Gert many times ahead of me in competitions, and saw his comments in forums (particularly in the\\xa0Yelp\\xa0competition that he was leading for most of phase 1), so I thought it would be a good chance to learn from him. The Acquire Valued Shoppers Challenge (1st place finish) was our first collaboration...\\r\\n\\r\\nGert: I can confirm that this is how we started teaming up... and we are still teaming up because we motivate each other a lot to get the best out of the data!\\r\\nWhat made you decide to enter this competition?\\nGert: It was Marios who convinced me to enter this competition. I actually thought that the dataset was too large (400 GB). I am not such a patient person and I knew that I would have to sit and wait a significant amount of time.\\r\\n\\r\\nMarios: My curiosity, eagerness to compete and learn, and last but not least, my lust for Kaggle ranking points!\\r\\nWhat preprocessing and supervised learning methods did you use?\\nGert: For preprocessing, we wrote our own scripts to go through all the asm files and recognize specific parts, based on keywords (dll extension, function calls) and regular expressions, together with some scripts to compress files or parts of the bytes files. My part of the learning was mainly focused on distinguishing useful features, so I stuck to the same mix of Gradient Boosting and Extremely Randomized Trees from the beginning.\\r\\n\\r\\n[caption id=\"attachment_4858\" align=\"aligncenter\" width=\"608\"] The performance of various subsets of features.[/caption]\\r\\nMarios: For preprocessing I used raw code that basically counts how many times single bytes, 2-gram bytes and 4-gram bytes appear in each file (document). I did the same for full-line bytes.\\r\\n\\r\\n\\r\\n[caption id=\"attachment_4859\" align=\"aligncenter\" width=\"610\"] Different Byte’s considered as ngrams, sample line from 0A32eTdBKayjCWhZqDOQ.bytes[/caption]\\r\\n\\r\\nIn terms of modelling I found XGboost and ExtraTreesClassifier (from Scikit) to combine very well together. Our modelling had 2 layers:\\r\\n\\nCombine different datasets (e.g. bytes 2-grams + bytes 1-gram) and run a combo of the above models and save holdout predictions.\\nDo meta-modelling. Run again a combo of XGBoost and ExtraTrees with the previous models as inputs.\\n\\r\\n[caption id=\"attachment_4861\" align=\"aligncenter\" width=\"610\"] Different models\\' performance.[/caption]\\r\\nWhat was your most important insight into the data?\\n\\xa0Gert: With regard to feature extraction, meta features in the asm files turned out to be even more useful than the alphanumeric contents - especially the number of lines in each section, and interpunction characters in each section.\\r\\n\\r\\n\\r\\n[caption id=\"attachment_4862\" align=\"aligncenter\" width=\"610\"] This plot shows that, as an example, the file property ‘compression rate of bytes file’ is quite strongly related to class membership (dots show 5%, 25%, 50%, 75% and 95% percentiles).[/caption]\\r\\n\\r\\n[caption id=\"attachment_4863\" align=\"aligncenter\" width=\"610\"] Shows the learning curve for adding features to the Gradient Boosting (Python sklearn) part of the ‘untuned’ model, with just over 300 features - based on the reduced asm features. Almost the same performance can be achieved with less than 50 features - selected using feature importances. In contrast, the performance of XGBoost is clearly better when the full 1600 (unreduced) asm features are used.[/caption]\\r\\n\\r\\nMarios: Text classification techniques work really well in this problem. Neither I nor Gert had any prior knowledge of the field, yet finding the “predictive words” in the document files was more than enough to score well.\\r\\nWere you surprised by any of your findings?\\nMarios: I was surprised that we did better than the same guys whose papers we were reading\\xa0to get ideas about the problem. I guess this puts heat to the discussion about what is better, domain knowledge or traditional ML\\xa0approaches? On first sight this problem seemed very domain specific…\\r\\nWhich tools did you use?\\nGert: I did everything in Python, with sklearn for modeling.\\r\\n\\r\\nMarios: Python, XGboost, ExtraTreesClassifier (and luck).\\r\\nHow did your team work together and how did this help you succeed?\\nGert: After Marios beat me using my own feature sets, I spent most of the time to extract more features from the asm files, and see if they improved the cross validation score in a model that was kept unchanged throughout the competition. After that I sent the good features to Marios, who created his own (XGBoost) models on each dataset and combined them into a very clever Meta model.\\r\\n\\r\\nMarios: Initially I worked alone and I started creating my own features from the bytes’ file. At some point I had created 65K features and my models where not as good as Gert’s that had less than 300 features with simpler models :( . Then I maximized the intensity to prove I can still be of some value via focusing on modelling... happily it worked out. I got some luck from feature generation too later on. Generally I combine well with Gert because he thinks very unconventionally about the problems and comes up with breaking (the-local-minima) ideas.\\r\\nWhat have you taken away from this competition?\\nGert: A good friend and 1500 dollars.\\r\\n\\r\\nMarios: Likewise. Plus bragging rights (and a ton of automated code I can re-use for other competitions)!\\r\\nDo you have any advice for those just getting started in data science?\\nGert: You learn most if you do things (instead of read about them), if you apply them to many different problems, and if you compete with and team up with others. So start to play on Kaggle!\\r\\n\\r\\nMarios: Knowledge is having the right answer. Intelligence is asking the right question. To improve in this sport (and data modelling in general) it may be good to dedicate time, keep trying new things, learn the tools (in other words level up!), automate a lot, play/collaborate with others and have fun with many competitions to get a gist of different problems.', 'The\\xa0TAB Food Investments (TFI) Restaurant Revenue Prediction \\xa0competition\\xa0was the second most popular public competition in Kaggle\\'s history to date. 2,257 teams built models to predict the annual revenue\\xa0of TFI\\'s regional quick service restaurants.\\r\\n\\r\\nThe winning model was a\\xa0\"single gradient boosting model with simple parameters\". Wei Yang, known as \"Arsenal\" on Kaggle, took first place ahead of 2,458 other data scientists. In this blog, he shares what\\xa0got him to the top of the private leaderboard and what he\\'s learned from competing.\\r\\n\\r\\n\"To me, this competition was mainly about feature engineering.\"\\n\"Most of my current machine learning knowledge is theoretical; participating in Kaggle competition is the best way to exercise and reinforce it.\"\\r\\n\\r\\n[caption id=\"attachment_4887\" align=\"aligncenter\" width=\"615\"] With over 1,200 quick service restaurants across the globe, TFI represents some of the world\\'s most well-known brands.[/caption]\\r\\n\\r\\nWei created the below interactive geomap\\xa0in scripts during his data exploration. Hover over the map to see the average annual revenue in\\xa0different regions.\\r\\n\\r\\n\\nWhat was your background prior to entering this challenge?\\r\\nI earned my master’s degree from the statistics department at Stanford. I have 2 years of working experience as a data scientist in a data consulting company, where I have the opportunities to work on different data sets for different clients.\\r\\n\\r\\n[caption id=\"attachment_4888\" align=\"aligncenter\" width=\"300\"] Wei (aka Arsenal) on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nNot in the food restaurant industry. Nevertheless I have been working for client on a similar small data set with age information as well, although that one was a classification problem.\\r\\nHow did you get started competing on Kaggle?\\r\\nThe Stanford stat 202 (data mining) InClass competition on the Titanic problem.\\r\\nWhat made you decide to enter this competition?\\r\\nI want to strengthen my machine learning skills. Most of my current machine learning knowledge is theoretical; participating in Kaggle competition is the best way to exercise and reinforce it. What I am truly pursuing is to stabilize my rankings within top 10% for those Kaggle competitions I get involved in. But life is full of surprise.\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nTo me, this competition was mainly about feature engineering. I applied square root transformation to most of the obfuscated P variables (with maximum value >= 10) to make them into the same scale, as well as the target variable “revenue”. I randomly assigned values to the uncommon city levels in both training and test set. This, I believe, has diversified the geo location information contained in the city variable and in some of the obfuscated P variables.\\r\\n\\r\\nI created one missing value indicator for multiple P variables which, I believe, to some degree helped differentiate synthetic and real test data. Time / Age related information was also extracted. After creating all these new variables I treated zeroes as missing values and used the mice imputation. I read from the forum that dealing with outliers properly could improve scores, although I did not try it out myself. The winning model is just a single gradient boosting model with simple parameters.\\r\\n\\r\\n[caption id=\"attachment_4886\" align=\"aligncenter\" width=\"600\"] Histogram plot for 1-37 variables script by another TAB competitor, piby4[/caption]\\r\\nWhat was your most important insight into the data?\\r\\nThe missing data mechanism. P14 to P18, P24 to P27, P30 to P37 are all zeroes at the same time for 88 out of 137 rows in the training data. Based on this I created an indicator, but I believe this could be utilized more.\\r\\nWere you surprised by any of your findings?\\r\\nNot really. I just followed my intuition with adjustment from the feedback of two statistics, i.e. training error and training error with outliers removed, as well as the public leaderboard score to choose my final winning model. Since I didn’t over-fit the training data too much, training error was indicative. I tested it post-deadline for several models and 1) training error 2) training error with outliers removed 3) public LB score and 4) private LB score aligned pretty well. I did not use the cv score for this problem since the data set is relatively small. I believe a simple and logical model will be robust.\\r\\nWhich tools did you use?\\r\\nI used R and package mice for imputation, lubridate for extracting date related features and caret for modelling (with gbm).\\r\\nWhat have you taken away from this competition?\\r\\nAs I mentioned, for small data set, one of the important aspects is to choose the right model. Simple and logical was my criteria. I was not doing well on version control of my R codes. I will definitely use GitHub next time.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nSince I have a mathematical background, I personally prefer understanding the theory thoroughly before the implementation. You can certainly achieve something without deep understanding but your hands are tied. For Kaggle competitions, follow the corresponding forum and you will get great insight about the data.\\r\\nBio\\nWei Yang earned his master\\'s degree from the statistics department at Stanford University in 2013 and received his double Bachelor degrees in Mathematics and Finance from Nankai University, China, in 2011. Wei started his career at Saama Technologies, a data consulting company, which provides data science service for clients, as an associate data scientist in 2013. Besides participating in Kaggle competitions, Wei’s interests lie in theory of computation and mathematical philosophy. Personal Website: http://www.wyang.org/', 'Last week we organized a 6th meetup around the Otto Group Product Classification Challenge. The event was hosted at La Paillasse, a community lab based in the center of Paris that brings together people from various backgrounds and nationalities to work on inspiring projects in the field of Bioscience and related tech fields.\\r\\n\\r\\n\\r\\n[caption id=\"attachment_4788\" align=\"aligncenter\" width=\"200\"] The meetup took place on May 5, 2015. 25 Kagglers attended.[/caption]\\r\\nThis was our first hands-on meetup. While our previous meetups concentrated on talks of competition winners and presentation of new ideas, tricks and methods, this time we were working together on an open competition and exchanging ideas in a collaborative format.\\nThe attending Kagglers presented themselves and the motivation behind their participation in the competition. With 25 total attendants, five had a ranking in the top 100 of the leader board. While in previous meetups many of the attendants were newcomers to the field, in this meetup most participants held roles as data scientists or similar in the industry or the academy.\\nDelphine Lê and Bruno Seznec started the meetup with a presentation of the Otto challenge and the datasets: classification of products into their correct category (slides).\\r\\n\\r\\n\\r\\n[caption id=\"attachment_4906\" align=\"aligncenter\" width=\"600\"] T-sne projection for train data as posted by piotrek in the competition forum[/caption]\\r\\nAlso, an overview of various methods from the competition’s forum were presented and discussed:\\n\\n\\n\\nMethod\\nScore\\nLanguage\\nForum\\n\\n\\nRandom Forest\\n0.54\\nR\\nlink\\n\\n\\nXGBoost\\n0.51\\nR\\nlink\\n\\n\\nDeep Learning (Keras)\\n0.48\\nPython\\nlink\\n\\n\\n\\r\\nWe also had a discussion around the main open source tools we use and especially new tools that we started using in the last few months: XGBoost, Lasagne, Keras,\\xa0GraphLab. This competition was a great opportunity to discover and try them.\\r\\n\\r\\nAfter the presentation, we had split into smaller work groups around specific topics:\\r\\n\\nEnsembles and model fusion methods\\nFeature Engineering\\nDeep Learning and Neural Networks\\nParameter selection\\n\\r\\nOther participants discussed specific tools and a few used the meetup to have first try on these tools. Experience level varied among participants and a few participants got to do their first submissions on Kaggle during the meetup.\\r\\n\\r\\nWe also had a presentation of the parameter selection methods GridSearchCV and RandomizedSearchCV from Christophe Bourguignat and the hyperopt library\\xa0by Amine Benhalloum (code example can be found here).\\r\\n\\r\\nThe evening was a great opportunity for us to meet again, learn new tricks and improve our methods in the Otto competition. We will surely organize more hands-on meetups in the near future!\\r\\n\\r\\nWe are planning to organize a summer meetup in June. If your company is located in Paris and is interested to host or to sponsor the next meetup please don’t hesitate to contact the organizing team. ☺\\r\\n\\r\\n\\nThis article was adapted by\\xa0Koby Karp from a report written by Frédéric Bardolle\\xa0that first appeared\\xa0here\\xa0(in French).\\nThe Kaggle Paris Meetup was founded by Koby Karp and Kenji Lefevre in March 2014 and in the past year six meetups were hosted by Equancy, Dataiku, OCTO and the tech institute Télécom ParisTech, all of which have data scientists that participate regularly in Kaggle competitions.', 'Team \"say NOOOOO to overfittttting\" did just that and took first place in the Microsoft Malware Classification Challenge.\\r\\n\\r\\nSponsored by the\\xa0WWW 2015 / BIG 2015\\xa0conferences, competitors were given nearly half a terabyte of data (when uncompressed!) and tasked with classifying variants of malware into their respective families. This blog outlines their winning approach and includes key visuals from their analysis.\\r\\n\\r\\n[caption id=\"attachment_4921\" align=\"aligncenter\" width=\"450\"] 377 teams and 481 players competed to correctly classify malware into 9 families.[/caption]\\r\\nHow did you get started competing on Kaggle?\\nLittle Boat: I was learning Python by doing the Harvard CS109 online, and was trying to find some practical projects to do. Then Google suggested Kaggle (not surprised!). Learned a lot since then.\\r\\n\\r\\n[caption id=\"attachment_4922\" align=\"aligncenter\" width=\"300\"] Little Boat\\'s Kaggle profile[/caption]\\r\\n\\r\\nrcarson: I was taking the coursera course of Andrew Ng. And I noticed that someone mentioned Kaggle in the course forum. I started competing last April, in the repeat shopper challenge.\\r\\n\\r\\n[caption id=\"attachment_4923\" align=\"aligncenter\" width=\"300\"] rcarson\\'s Kaggle profile[/caption]\\r\\n\\r\\nXueer Chen: rcarson introduced Kaggle to me. We had no machine learning background at that time and we teamed up to compete and learn from scratch.\\r\\n\\r\\n[caption id=\"attachment_4924\" align=\"aligncenter\" width=\"300\"] Xueer Chen\\'s Kaggle profile[/caption]\\r\\nWhat made you decide to enter this competition?\\nLittle Boat: Just thought it was fun to play with 400 GB data, although it turned out to be pretty small after feature extraction.\\r\\n\\r\\nrcarson: The data size. My general impression is that the large data size often come together with rich feature space and stable cross validation performance. I really prefer this kind of contests since my skills are very limited in the opposite type of contests.\\r\\n\\r\\nXueer Chen: Its similarity to biological virus. Also the chance to go to Italy.\\r\\n\\r\\n[caption id=\"attachment_4911\" align=\"aligncenter\" width=\"600\"] An overview of the team\\'s approach.[/caption]\\r\\nWhat preprocessing and supervised learning methods did you use?\\nLittle Boat: Extracting good features is the key to winning this competition. And XGBoost gives the power to validate how useful the new features are. So thinking and trying new features on XGBoost is basically what we did.\\r\\n\\r\\n\\nrcarson: In detail, we extracted three kinds of features: opcode N-gram count, segment line count and asm file pixel intensity features. We wrote our own feature extraction code in plain python which is online and can be accelerated by pypy. For supervised modeling, we use Xgboost and ensemble. Please find details in our slides and papers.\\r\\n\\r\\nXueer Chen: I also tried some image processing techniques such as segmentation and gabor filtering in Matlab but they didn’t contribute to our final feature set.\\r\\n\\r\\n[caption id=\"attachment_4920\" align=\"aligncenter\" width=\"640\"] Random Forest Feature Importance [1][/caption][caption id=\"attachment_4919\" align=\"aligncenter\" width=\"640\"] Random Forest Feature Importance [2][/caption]\\r\\nWhat was your most important insight into the data?\\nLittle Boat: There are only 10K sample points for training and the model accuracy is very high. Including the predicted labels of test data will double your training data, and the signal noise ratio won’t decrease much.\\r\\n\\r\\nrcarson: The features are sparse and non-linear. It is necessary to select feature first to make it denser otherwise the performance of XGBoost will be degraded and be slowed down. Please find the detailed analysis in the slides.\\r\\n\\r\\nXueer Chen: There are very fine texture patterns in images generated by bytes file or assembly file. However using image features alone will not give very good performance. The interactions of image features and the opcode N-gram features is most useful.\\r\\n\\r\\n\\nWere you surprised by any of your findings?\\nLittle Boat: I was always surprised by any improvement we had from including some new features. But Rcarson found a good explanation for them every time! Such a great teammate!\\r\\n\\r\\nrcarson: I’m very impressed by the asm file pixel density features discovered by Little Boat. It is so powerful yet simple and I have never seen it mentioned anywhere.\\r\\n\\r\\nXueer Chen: Most of my findings doesn’t improve the performance of the model. I’m very impressed by my teammates’ work.\\r\\n\\r\\n\\nWhich tools did you use?\\nLittle Boat: Code everything with Python. Run feature extraction with Pypy and model them with XGBoost.\\r\\n\\r\\nrcarson: The feature engineering code is in pure python, ever without numpy and pandas package. The benefit is that it is very memory efficient, one line at a time, and can be accelerated by pypy.\\r\\n\\r\\nXueer Chen: I use matlab to do all the image processing.\\r\\nWhat have you taken away from this competition?\\nLittle Boat: Everything can happen if you have a good team.\\r\\n\\r\\nrcarson: Cross validation is more trustworthy than domain knowledge.\\r\\n\\r\\nXueer Chen: Simple features, such as pixel intensity, can be more useful than high-level features, such as grey level co-occurrence matrix.\\r\\nHow did your team form?\\nLittle Boat: We teamed up in the Avazu competition. So it is pretty easy to send them an invitation.\\r\\n\\r\\nrcarson: We teamed up before and we found each other close on the learderboard this time.\\r\\n\\r\\nXueer Chen: We merged with Little Boat in the early part of the contest. We had great teamwork before.\\r\\nHow did competing on a team help you succeed?\\nLittle Boat: I am always very excited when I\\xa0join some new competition. And then the excitement slowly goes away after trying\\xa0a couple of models, a couple feature engineering tricks, and a couple of blendings. And most of the time I would just give up in the middle of the competition and move on to another one. But with a good team, they bring in new ideas, and it kind of pushes you to the end. That excitement just follows you along the journey and finds a way better spot on the Leaderboard for you.\\nrcarson: Teamwork changes everything. We motivate and inspire each other along the way. Xiaozhou Wang’s discovery of pixel intensity features of asm file and semi-supervised learning trick is the most impressive modeling skills I have ever seen.\\r\\n\\r\\nXueer Chen: Competing in a team really allows me to focus on something that I like and I am good at. It is a pity that I didn’t find high level image features and bio-inspired models work in this challenge but it may work in future.\\r\\n\\r\\n\\xa0\\r\\n\\r\\n\\r\\n\\r\\nFor more details on the team\\'s approach, don\\'t miss their video presentation from the BIG 2015 conference:\\r\\n\\r\\n\\n', 'Kagglers Sudalai (aka\\xa0SRK) and Marios (aka\\xa0Kazanova) came together to form team \"No Rain No Gain!\" and\\xa0take second place\\xa0in the\\xa0How Much Did it Rain? competition. Sudalai had two goals in competing: to earn a Master\\'s badge and to finish in the top 100. In the blog below, Sudalai shares how he managed to accomplish both (and get\\xa0a new friend) by being part of a great team.\\r\\n\\r\\n[caption id=\"attachment_4967\" align=\"aligncenter\" width=\"300\"] 351 players on 321 teams built models to\\xa0predict probabilistic distributions of hourly rainfall[/caption]\\r\\nThe Basics\\r\\n[caption id=\"attachment_4966\" align=\"aligncenter\" width=\"300\"] Sudalai\\'s Kaggle profile[/caption]\\r\\nWhat was your background prior to entering this challenge?\\r\\nI am a data scientist from India with five years of experience. I have a certification in Business Analytics from IIM-Banglore after earning my undergraduate degree. Also I have participated in quite a few Kaggle competitions.\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nNope. This is an absolutely new domain to me.\\r\\nHow did you get started competing on Kaggle?\\r\\nMy Kaggle journey started two years ago. I was looking for an opportunity to learn data science and machine learning concepts through hands-on experimentation. That is when I stumbled up on the “StumbleUpon” Kaggle competition and started competing with the help of Abhishek’s benchmark code.\\r\\nWhat made you decide to enter this competition?\\r\\nThis competition looked quite interesting in various aspects. Understanding the objective itself was quite challenging at the first place. Then the format of the input data was quite complex and different from the other competitions. Also, one could potentially try both regression and classification methodologies in this competition. All these challenges pushed me to take an attempt.\\r\\nLet\\'s Get Technical\\n\\nWhat preprocessing and supervised learning methods did you use?\\r\\nI think feature extraction is also an important step in this competition along with machine learning. We have spent good amount of time coding up the features. We computed a number of features including mean value of the radarwise means, mean value of the good quality radar, mean value of the radar that scans the longest time, percentage of missing values, interaction between the mean values and time of scan and others.\\r\\n\\r\\n[caption id=\"attachment_4976\" align=\"aligncenter\" width=\"612\"] Cumulative rainfall percentages for RR1 percentile bins. See the python code & output on Kaggle scripts[/caption]\\r\\n\\r\\nWe have tried different techniques such as Extra trees regressor, linear regression with regularization, Gradient Boosting classifier and Random Forest Classifier. Our Final model is an ensemble of extra-trees regressor and gradient boosting classifier (50-50). The main intuition behind the selection of these 2 models was the gap between cv and LB. The first model (Extra trees regressor) was tuned based on the leaderboard feedback. The second was tuned at initial stages based on validation sample (70-30 split). We think part of the reason we advanced from public to private was the fact that we had a flavour of “unoverfitted” substance in our models!\\r\\nWere you surprised by any of your findings?\\r\\nWe were surprised to see the improvement when we removed the rainfall values greater than 70mm during training. As one could see from the rainfall plot, there are quite a few high values (rainfall greater than 70mm) in the training set. We thought that these rainfall values are mostly errors in rain gauge measurements and so we decided to remove them. It improved our rank all of a sudden. But we realized this only during the last week of the competition and so we couldn\\'t capitalize on it much after that!\\r\\n\\r\\n[caption id=\"attachment_4963\" align=\"aligncenter\" width=\"614\"] See the\\xa0python code for this histogram on Kaggle scripts[/caption]\\r\\nWhich tools did you use?\\r\\nPython all the way. Coded up the feature extraction part on our own and used scikit-learn for modeling algorithms along with xgboost.\\r\\nHow did you spend your time on this competition?\\r\\nIn this competition, I spent about 60 to 70% of the time on feature engineering (creation, selection etc) and the rest on machine learning.\\r\\nTeamwork\\nHow did your team form?\\r\\nIn the past, I have seen people forming teams at the later stages of a competition and improving their results. I have not tried that successfully so far. Also previously once I have asked Marios for teaming up in another competition. But unfortunately there we could not form a team. So towards the end of this competition (when I was almost out of all my tricks) I reached out to Marios again and this time we successfully formed a team.\\r\\nHow did competing on a team help you succeed?\\r\\nSince we formed a team at the very later stage of this competition, both of us already have our own set of variables and models. So ensembling two entirely different approaches helped a lot.\\r\\n\\r\\nAlso since the number of submissions is limited to 1 per day in this competition, we used to brainstorm a lot before making each submission and also in the process I learned a lot from Marios.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nI was trying to become a “Master Kaggler” and to get a place in top 100. With this competition, both of them came true. Apart from this, I got a very good friend (Marios) and some bragging rights!\\r\\n\\r\\nOn the data science side, I learnt about the importance of outlier removal, bagging, extra-trees classifier and ensembling tricks. (Thanks, Marios!)\\r\\nBio\\nSudalai Rajkumar earned his certification in Business Analytics and Intelligence from IIM-Bangalore and his bachelors in engineering from PSG College of Technology, India. He is currently working as a Data Scientist at Tiger Analytics, a data science consulting company. He is interested in solving real world data science problems, machine learning and Kaggling.', 'The Otto Group Product Classification Challenge\\xa0made Kaggle history as our most popular competition ever. Alexander Guschin\\xa0finished in 2nd place ahead of 3,845 other data scientists. In this blog, Alexander shares\\xa0his stacking centered approach and explains why you should never underestimate the nearest neighbours algorithm.\\r\\n\\r\\n[caption id=\"attachment_4980\" align=\"aligncenter\" width=\"300\"] 3,848 players on 3,514 teams competed to classify items across Otto Group\\'s product lines[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI have some theoretical understanding of machine learning thanks to my base institute (Moscow Institute of Physics and Technology) and our professor Konstantin Vorontsov, one of the top Russian machine learning specialists. As for my acquaintance with practical problems, another great Russian data scientist who once was Top-1 on Kaggle, Alexander D’yakonov, used to teach a course on practical machine learning every autumn which gave me very good basis.\\xa0Kagglers may know this course as PZAD.\\r\\n\\r\\n[caption id=\"attachment_4982\" align=\"aligncenter\" width=\"300\"] Alexander\\'s profile on Kaggle[/caption]\\r\\n\\nHow did you get started competing on Kaggle?\\r\\nI got started in 2014’s autumn in “Forest Cover Type Prediction”. At that time I had no experience in solving machine learning problems. I found excellent benchmarks in “Titanic: Machine Learning from Disaster” which helped me a lot. After that I understand that machine learning is extremely interesting for me and just tried to participate in every competition I could.\\r\\nWhat made you decide to enter this competition?\\r\\nI wanted to check some ideas for my bachelor work. I liked that Otto competition has quite reliable dataset. You can check everything on cross-validation and changes on CV were close enough to leaderboard. Also, the spirit of competition is quite appropriate for checking ideas.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\n[caption id=\"attachment_4970\" align=\"aligncenter\" width=\"612\"] My solution’s stacking schema[/caption]\\r\\n\\r\\nThe main idea of my solution is stacking. Stacking helps you to combine different methods’ predictions of Y (or labels when it comes to multiclass problems) as “metafeatures”. Basically, to obtain metafeature for train, you split your data into K folds, training K models on K-1 parts while making prediction for 1 part that was left aside for each K-1 group. To obtain metafeature for test, you can average predictions from these K models or make single prediction based on all train data. After that you train metaclassifier on features & metafeatures and average predictions if you have several metaclassifiers.\\r\\n\\r\\nIn the beginning of working on the competition I found useful to split data in two groups : (1) train & test, (2) TF-IDF(train) & TF-IDF(test). Many parts of my solution use these two groups in parallel.\\r\\n\\r\\nTalking about supervised methods, I’ve found that Xgboost and neural networks both give good results on data. Thus I decided to use them as metaclassifiers in my ensemble.\\r\\n\\r\\nNevertheless, KNN usually gives predictions that are very different from decision trees or neural networks, so I include them on the first level of ensemble as metafeatures. Random forest and xgboost also happened to be useful as metafeatures.\\r\\nWhat was your most important insight into the data?\\r\\nProbably the main insight was that KNN is capable of making very good metafeatures. Never underestimate nearest neighbours algorithm.\\r\\n\\r\\nVery important were to combine NN and XGB predictions on the second level. While my final second- level NN and XGB separately scored around .391 on private LB, the combination of them achieved .386, which is very significant improvement. Bagging on the second level helped a lot too.\\r\\n\\r\\n[caption id=\"attachment_4973\" align=\"aligncenter\" width=\"612\"] TSNE in 2 dimensions[/caption]\\r\\n\\r\\nBeside this, TSNE in 2 dimensions looks very interesting. We can see on the plot that we have some examples which most likely will be misclassified by our algorithm. It does mean that it won’t be easy to find a way to post-process our predictions to improve logloss.\\r\\n\\r\\nAlso, it seemed interesting that some classes were related closer than others, for example class 1 and class 2. It’s worth trying to distinguish these classes specially.\\r\\n\\r\\n[caption id=\"attachment_4974\" align=\"aligncenter\" width=\"612\"] Final model’s predictions for holdout’[/caption]\\r\\nWere you surprised by any of your findings?\\r\\nUnfortunately, it appears that you won’t necessarily improve your model if you will make your metafeatures better. And when it comes to ensembling, all that you can count on is your understanding of algorithms (basically, the more diverse metafeatures you have, the better) and effort to try as many metafeatures as possible.\\r\\n\\r\\n[caption id=\"attachment_4972\" align=\"aligncenter\" width=\"612\"] The more diverse metafeatures you have, the better. Metafeature by Extratrees vs metafeature by Neural Network.[/caption]\\r\\nWhich tools did you use?\\r\\nI only used sklearn, xgboost, lasagne. These are perfect machine learning libraries and I would recommend them to anyone who is starting to compete on Kaggle. Relying on my past experience they are sufficient to try different methods and achieve great results in most Kaggle competitions.\\r\\nWords of Wisdom\\nDo you have any advice for those just getting started in data science?\\r\\nI think that the most useful advice here is try not to stuck trying to fine-tune parameters or stuck using the same approaches every competition. Read through forums, understand winning solutions of past competitions and all of this will give you significant boost whatever your level is. In another words, my point is that reading past solutions is as important as solving competitions.\\r\\n\\r\\nAlso, when you first starting to work on machine learning problems you could make some nasty mistakes which will cost you a lot of time and efforts. Thus it is great if you can work in a team with someone and ask him to check you code or try the same methods on his own. Besides always compare your performance with people on forums. When you see that you algorithm performs much worse than people report on forum, go and check benchmarks for this and other recent competitions and try to figure out the mistake.\\nBio\\nAlexander Guschin is 4th year student in Moscow Institute of Physics and Technology. Currently, Alexander is finishing his bachelor diploma work about ensembling methods.', 'Facebook\\'s fourth recruiting competition, Humans or Robots?, wrapped up on June 8 as the most popular recruiting competition in Kaggle\\'s history. A record number of 985 teams competed for a chance to interview for a machine learning software engineering role at the world\\'s most iconic social media company.\\r\\n\\r\\nThe competition challenged participants to identify human vs. robot bidders in data from\\xa0on a fictional online auction site. In this blog, second place winner Kiri Nichol (aka small yellow duck) outlines\\xa0the approach that led to a final RandomForestClassifier ensemble\\xa0and discusses the value of Kaggle\\'s\\xa0competition format and forums.\\r\\n\\r\\n[caption id=\"attachment_4995\" align=\"aligncenter\" width=\"300\"] 985 data scientists competed to identify humans vs. robots[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI have a PhD in physics - my thesis was on applying statistical mechanics to granular materials. After that, I worked for a bit as an imaging researcher in the Radiotherapy department of the Dutch Cancer Institute (NKI). Most of my formal background in computer science is from high school and a really excellent course\\xa0I took as an undergrad on scientific programming in Fortran - matrix operations, optimization problems, parallelization, that sort of thing.\\r\\n\\r\\n[caption id=\"attachment_4991\" align=\"aligncenter\" width=\"300\"] Kaggle profile for Kiri (aka small yellow duck)[/caption]\\r\\n\\r\\nOn the way to the PhD I ended up using pretty much every data analysis tool out there - Mathematica, Maple, IDL and Matlab - and I also implemented a lot of toy models in Fortran. My machine learning knowledge was mostly acquired tackling problems on Kaggle and following Andrew Ng\\'s Introduction to Machine Learning class on Coursera. And I\\'ve had some help from friends from university - one coached me through Amazon Web Services and another taught me a lot about natural language processing.\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\n<laughs> I swear I\\'ve never sniped an auction!\\r\\nHow did you get started competing on Kaggle?\\r\\nJust before I left the Netherlands to move back to Canada a friend showed me Kaggle and I thought \"WOW! All these problems sound really, really interesting!\"\\xa0 I didn\\'t have a job lined up when I moved, so I decided to throw myself into learning about machine learning and just work on whatever happened to spark my interest. I did the Titanic problem to learn about sklearn and then I started tackling competitions.\\r\\nWhat made you decide to enter this competition?\\r\\nI\\'d been working on another project for a while and I was starting to get stuck, so I decided I needed a break from it. There were three weeks left in the Bot vs Human competition and that seemed like the right amount of time for me get somewhere with the data.\\r\\nLet\\'s Get Technical\\nWhat supervised learning methods did you use?\\r\\nI tried out pretty much all the sklearn classifiers that generate probabilities -\\xa0 RandomForestClassifier performed the best. My final submission was a simple average of the probabilities predicted by five instances of RandomForestClassifier (each initialized with a different random number).\\r\\nWhat was your most important insight into the data?\\n\\nRobots bid a lot and they bid fast. Just two features - the mean number of bids per auction and the median time between subsequent bids by each user got me to a score of 0.89ish.\\r\\nWere you surprised by any of your findings?\\r\\nWhen I made a histogram of bids per unit time was really surprised that once a day there was a sharp peak in bidding activity by humans. It seemed weird that auctions all over the world would end at the same time, but I couldn\\'t think of any other explanation for the peak.\\r\\n\\r\\n\\r\\n\\r\\nI also made a histogram of the time between each bid and the last bid recorded for the auction - there were two clusters in this plot, which suggested that some of the auctions actually went on for more than two weeks. I naively used the median time from each bid until the last bid placed in an auction as a feature - it ended up being a much more useful feature than I\\'d expected, which puzzled me. I also couldn\\'t figure out why there were hardly any bids placed by robots between 11 and 14 days before the end of the auction. I wondered if this behaviour might have helped to explain why we only got to see three days out of every two weeks in the data....\\r\\n\\r\\n\\nWhich tools did you use?\\r\\nI\\'m a Python fan. I used sklearn for the classification and pandas for manipulating the bidding data. This competition was great for forcing me to be clever about how I\\xa0did operations on dataframes.\\r\\nHow did you spend your time on this competition?\\r\\nI spent about three days poking around in the data, bringing my pandas skills up to speed, defining some very simple features (time between bids, mean bids per auction) and trying out various classifiers in the sklearn arsenal. Once I\\'d settled on RandomForestClassifier, most of the remaining time went into refining features, generating more features and getting more clever about calculating the length of a day.\\r\\n\\r\\nOnce I realized that I had a shot at the top ten, I invested some time trying to squeeze value out of the bidder address and payment_account and using clustering to group bidders with overlapping IPs. I also spent a fair bit of time trying to blend different models\\xa0 with feature-weighted linear stacking, as well as trying to reduce overfitting using recursive feature elimination, but neither effort improved my cross-validation scores.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nExecuting the script for training and predicting only took about three minutes on my modest little laptop. The time-intensive part of the development process was cross-validation because it was necessary to repeatedly test the model on 100+ different train/valid splits in order to get a reasonable estimate for what sort of score I could expect.\\r\\nWords of Wisdom\\nWhat have you taken away from your participation in Kaggle competitions?\\r\\nThis competition helped me to be smarter about manipulating data with pandas. The other moral that always bears repeating is that it pays to look at sample chunks of data - both the raw data and plots. No matter how many smart things I think I\\'ve done, whenever I make a plot I think \"why did I wait so long to look at this?\"\\r\\n\\r\\nFor me, the main benefit of participating in Kaggle is motivational: I am a person who is driven to learn when there is an interesting problem that I would like to solve. I\\'ve also learned a lot from the clever folks who\\'ve shared their strategies and insights on the message board.\\r\\n\\r\\n[caption id=\"attachment_4997\" align=\"aligncenter\" width=\"500\"] Kagglers regularly share their insights and approaches in the forums[/caption]\\r\\n\\r\\nI think that Kagglefication presents a really valuable way of doing research, especially medical or engineering research of the nature \"we introduce technique X for performing task Y\". Usually these sorts of papers are phenomenally boring to read and they also make it difficult to compare different techniques for performing the same task. Kagglefication has a couple of advantages: first, it forces people to get together and identify a problem whose solution is actually really valuable. Second, Kagglefication obliges people to cooperate to create a decent-sized data set (which in medicine is not always such an easy feat). Third, having everyone work on the same data set makes it much more straightforward to compare the effectiveness of different approaches. Fourth, I think that people learn better from others\\' solutions when they have invested effort in trying to figure out how to solve a problem themselves. Finally, Kagglefication provides a metric other than \"number of publications\" and \"poshness of university\" to assess researchers.\\r\\nJust for Fun\\nWhy would you like to work at Facebook as a machine learning engineer?\\r\\nIn the past year I\\'ve gotten really excited about using neural networks to do image segmentation, so I\\'m pretty curious about the \"friend tagger\". I\\'ve been collecting \"friend tagger\" failures and I have some ideas about how to generate passive feedback about whether the friend tagger has tagged somebody correctly. I\\'d also be interested in working on sock-puppet detection and account fraud. And I\\'m curious about how people with limited computer skills or internet access use Facebook. I had the opportunity to travel a bit in Brazil last year and I found out that EVERYBODY has Facebook, even if they don\\'t have electricity or plumbing. Facebook is in a position to help people learn how to get around on the internet.\\r\\nWhat\\'s your dream job?\\r\\nI\\'m pretty open. Something where I have to answer worthwhile questions and solve interesting problems - something which is a tough technical challenge and requires being creative. Having nice colleagues is important too.\\r\\n\\r\\nI\\'d love to work more in medicine. But medicine can be very frustrating. It\\'s very hard for hospitals and research organizations and businesses to share data - and there aren\\'t really any incentives to do so. Even in Canada and the Netherlands, where medical care is socialized, hospitals just don\\'t have the digital infrastructure to move patient records around, to make records available for study. If a patient is treated in Amsterdam and then has complications five years later and has further treatment in Groningen, the hospital in Amsterdam doesn\\'t necessarily find this out. It\\'s the same situation in Canada.\\r\\n\\r\\nWe\\'re throwing away opportunities to answer questions about what makes for effective (and cost-effective) care. In the US there is one organization that I\\'m aware of\\xa0that is trying to initiate a patient-driven system which would allow individuals to make their data available in exchange for treatment advice. But it\\'s very hard to collect complete data this way. One of my dreams would be to have a box on your organ-donor card that says \"Donate My Data To Science\".\\r\\n\\r\\n[caption id=\"attachment_4998\" align=\"aligncenter\" width=\"350\"] Cancer Commons is a nonprofit that unites patients, oncologists and scientists to make sure critical information gets shared with the patients who need it.[/caption]\\r\\n\\r\\nAnother challenge with doing medical research is that there is a separation between hospitals (which have data) and businesses developing drugs, apparatus and software (which have money and technical expertise). Fifty years ago, medical research could be done in hospitals, but it\\'s reached the point where developing drugs, developing tools for genetic analysis and developing software for delivering radiation has become so sophisticated that it\\'s really hard for a hospital or a single lab to do this sort of development. I\\'m not sure if there is a better solution to this problem than the status quo, but maybe we need to start having a conversation about it.\\r\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nOoooh - I\\'d love to have a competition to do automatic segmentation of organs and tissues in CT or MRI scans. The problem is basically to build a Facebook friend tagger for internal organs. Right now, automatic segmentation routines for medical images just aren\\'t good enough, so technicians still delineate all the relevant tissues in a CT scan before a cancer patient can have radiotherapy. But radiotherapy could be improved if there were tools available for rapid, reliable tissue delineation.', 'Next up in our series on top Kagglers is the #1: Owen Zhang (Zhonghua Zhang). Owen comes from an engineering background and currently works as the Chief Product Officer at DataRobot.\\r\\n\\r\\n\\nOwen Q&A\\nHow did you start with Kaggle competitions?\\r\\nBack in 2011 I had just switched to analytics as a full time job (after several years working in IT), and was eager to improve my skills and to “prove myself”.\\r\\n\\r\\nSo it was fortuitous that Kaggle came along with the first Allstate competition.\\r\\n\\r\\nBeing in the same industry I felt I had some advantage as well. I was lucky and got 2nd place. As a consequence I became a Kaggle addict.\\r\\nWhat is your first plan of action when working on a new competition?\\r\\nSubmit a “reasonable” model as soon as possible -- to validate my understanding of the problem and the data.\\r\\nWhat does your iteration cycle look like?\\r\\nIt depends on the competition and I usually go\\xa0through a few stages.\\r\\n\\r\\nAt the beginning I focus on data exploration and try some basic approaches so I iterate pretty quickly.\\r\\n\\r\\nOnce the obvious ideas are\\xa0exhausted I usually slow down and do some research into the domain -- reading papers, forum post, etc. If I get an idea I would then implement it\\xa0and submit it to the public LB.\\r\\n\\r\\nMy iteration cycle usually is short -- I rarely work on feature engineering that requires more than a few hours of coding for a particular feature.\\r\\n\\r\\nMy personal experience is that very complicated features usually do not work well -- possibly because of my buggy code.\\r\\n\\nTips on Modeling Competitions on Kaggle (2015) Owen Zhang, NYC Data Science Academy\\nWhat are your favorite machine learning algorithms?\\r\\nAgain it depends on the problem, but if I have to pick one, then it is GBM (its XGBoost flavor). It is amazing how well it works in many different problems.\\r\\n\\r\\nAlso I am a big fan of online SGD, factorization, neural networks, each for particular types of problems.\\r\\nWhat are your favorite machine learning libraries?\\r\\nI used to be a very focus R user until about 1 year ago. But now I am more of a Python user who use whichever library that’s needed, including scikit-learn, XGBoost, VW, cuda-convnet2 and others.\\r\\nWhat is your approach to hyper-tuning parameters?\\r\\nI mostly tune by hand -- starting with reasonable values and doing trial and error based hill climbing. This fits my style -- I like to tinker with the problem in an ad-hoc fashion.\\r\\n\\r\\nSometimes I think I should eventually be more disciplined and systematic, using some Bayesian optimizer. However that would take some fun away from the process, I think.\\r\\nWhat is your approach to solid CV/final submission selection and LB fit?\\r\\nThis really depends on if there is a public LB data leak.\\r\\n\\r\\nFor time-sensitive models, it is very common that the public LB gives us “hints” about the private LB. In that case I use almost exclusively the public LB results as validation. If that is the case, I would usually give public LB more weight.\\r\\n\\r\\nIn the case the data is small/noise, I use a combination of CV and public LB.\\r\\n\\r\\nI also usually choose 2 different models as final results -- one favors public LB and one favors CV.\\r\\nIn a few words: What wins competitions?\\r\\nLuck + hard work + experience\\r\\n\\r\\n[caption id=\"attachment_5006\" align=\"aligncenter\" width=\"415\"] Owen\\'s top 8 finishes[/caption]\\r\\nWhat is your favourite Kaggle competition and why?\\r\\nEvery competition is different and it is hard to pick a single favorite, so I will pick a few:\\r\\n\\nThe Amazon access challenge was good because\\r\\n\\nI got good result (2nd place),\\nalmost all the different things I tried in the competition worked,\\nI did better than many of my long time nemeses, such as Leustagos and Gxav (Xavier Conort).\\n\\n\\nThe Dogs vs. Cats was very interesting because the problem was interesting. Also this is the first time I actually started learning CNN and it was really fascinating.\\nThe recent Otto competition -- I did quite badly and it is not because of data noise. From methodology perspective I think I learned a lot from others in this one.\\n\\nWhat was your least favourite Kaggle competition experience?\\r\\nUsually I avoid competitions with too little data or too much noise. It is just a lottery.\\r\\n\\r\\nFor example one recent competition had only 130 data points in training, so the results are just random. I don’t participate in those so it is not “experience” per se, but they do make the overall Kaggle experience less perfect than it should be.\\r\\nWhat field in machine learning are you most excited about?\\r\\nDeep learning is really popular and I am very excited about it, like everyone else.\\r\\n\\r\\nHowever I would be really excited if there is a “feature engineering” field. So far it is still mostly ad hoc.\\r\\nWhich machine learning researchers do you study?\\r\\nI learned quite a bit by reading the book “Elements of Statistical Learning”, and I also dabble in all the deep learning publications.\\r\\n\\r\\nHowever I learned most from my fellow Kagglers.\\r\\nCan you tell us something about the last algorithm you hand-coded?\\r\\nI usually use other people’s code. The last “algorithm” I wrote was just generalized linear models with elastic net penalty.\\r\\n\\r\\nWhat I found is that it is usually not “efficient” (from time budget) perspective to write my own algorithm for a competition.\\r\\n\\r\\nAlmost always I can find open source code for what I want to do, and my time is much better spent doing research and feature engineering.\\r\\n\\nOpen Source Tools and Data Science Competitions (Owen Zhang) - 2015 Boston ODSC\\nHow important is domain expertise for you when solving data science problems?\\r\\nIt is quite important because often that is an important source of feature engineering. In addition it is not easy or efficient to re-invent the wheels.\\r\\n\\r\\nThe domain can be very broad. For example, I worked in insurance companies, which make me comfortable with not only insurance problems, but business oriented problems in general. On the other hand I can see physicists doing better with more mathematical or scientific problems.\\r\\n\\r\\nAlso it is worth noting that domain expertise has different meaning in different context. To a degree, knowing “overfitting is bad” is also domain expertise.\\r\\nWhat do you consider your most creative trick/find/approach?\\r\\nI don’t think anything I do is particularly “creative”.\\r\\n\\r\\nIf I have to name one, in the Allstate purchase option competition (with 7 correlated target variables), I built sequential conditional models, from the most “independent” target to the least “independent” target. However I am pretty sure I was reinventing the wheel.\\r\\nHow are you currently using data science at work and does competing on Kaggle help with this?\\r\\nOur company DataRobot’s product is built to help data scientists build generalizable models, and what I learned from Kaggle is very helpful.\\r\\n\\r\\nFor example, if I find a particular approach is effective in certain modeling context, I would make a suggestions to the team to implement that as a feature in our product.\\r\\n\\r\\n\\nWhat is your opinion on the trade-off between high model complexity and training/test runtime?\\r\\nI assume this is about the trade-off between complexity vs prediction performance. In Kaggle competitions there is no trade-off, we only care about accuracy.\\r\\n\\r\\nHowever in reality we rarely implement the most complicated model, for at least 2 reasons:\\r\\n\\nIt takes too much effort to train/execute.\\nMore complicated models tend to be more brittle.\\n\\r\\nOn the other hand, even if we don’t implement the best performing/most complicated model, such a model still has inherent value:\\r\\n\\nIt can provide a benchmark to evaluate simplified models against,\\nit gives us insights that will help us make simplified models better,\\nit helps us detect potential data problems.\\n\\nWhat is your view on automating Machine Learning?\\r\\nWe are in this business so certainly I believe it is doable (and valuable) to a degree.\\r\\n\\r\\nThe mechanical part of machine learning (like CV, hyper parameter tuning, model algorithm selection) will be almost completely automated, so that data scientists can focus on problem selection, model definition, and feature engineering.\\r\\nHow did you get better at Kaggle competitions?\\r\\nTo be honest I don’t know for sure. I am not even sure that I “got better” or even that I “am good”.\\r\\n\\r\\nI was definitely lucky. Learning from others on Kaggle certainly improves my skills over time, but the same is true for everyone.\\r\\nWhat did you learn from doing Kaggle competitions?\\r\\nToo many things to list, but here are a few that come to mind:\\r\\n\\nNothing is certain until the private leaderboard is revealed.\\nThere are always more things that I don’t know than I do.\\nRelying on the public LB is dangerous.\\nThere is usually a simple elegant approach that does equally well.\\n\\nBio\\nOwen Zhang currently works as a data scientist at DataRobot a Boston based startup company.\\xa0His education background is in engineering, with a master’s degree from U of Toronto, Canada, and bachelor’s from U of Science and Technology of China.\\xa0Before joining DataRobot, he spent more a decade in several U.S. based property and casualty insurance companies, last one being AIG\\xa0in New York.', 'Peter Best (aka fakeplastictrees) took 1st place in\\xa0Human or Robot?,\\xa0our fourth Facebook recruiting competition. Finishing ahead of 984 other data scientists, Peter ignored early results from the public leaderboard and stuck to\\xa0his own methodology (which involved removing select\\xa0bots from the training set). In this blog, he shares what led to this winning approach and how the competition helped him grow as a data scientist.\\r\\n\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nAfter studying chemistry for my first degree, I worked for twenty years in quantitative asset management culminating in being a partner in my own firm. After this, I went back to university to obtain a second degree, in maths this time. Recently, I have been looking for the right project to commit to. My programming experience extends all the way back to a ZX81. Whilst I would never describe myself as a professional programmer, I seem to be able to program.\\r\\n\\r\\n[caption id=\"attachment_5014\" align=\"aligncenter\" width=\"378\"] Kaggle profile for Peter Best (aka fakeplastictrees)[/caption]\\r\\nHow did you get started competing on Kaggle?\\r\\nWith all my experience, I decided my area of greatest interest lay in analysing complex data. I also quickly realised that my coding skills were from the previous century and thus I opted to learn python. Perhaps the best way to learn a programming language is to actually do something in it and this led me to Kaggle.\\r\\n\\r\\n[caption id=\"attachment_5017\" align=\"aligncenter\" width=\"525\"] Peter\\'s top competition\\xa0finishes[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nNot really. Financial markets are somewhat like auctions and the people who work in financial markets are somewhat like bots (occasionally!), but I don’t think I used any specialist knowledge in this competition. Many examples of data analysis have similarities despite relating to completely different fields.\\r\\nWhat made you decide to enter this competition?\\r\\nThe key to this competition looked like it was going to be feature engineering, something I regard as a strength. In addition, the size of the data set was small enough to manage easily on my Mac with short run times.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nMost features were composed by manipulating the data in the bids database then transferring some aggregate measure into a smaller database uniquely indexed by bidder. This smaller database was then used as input to a boosted trees model.\\r\\nWhat was your most important insight into the data?\\r\\nAs soon as I started doing some cross-validation on an early version of my model, I found that the statistical error on the estimate of the Area Under the ROC Curve (the evaluation metric used in the competition) was going to be quite high. This led me to undertake extensive resampling in my cross-validation to ensure my estimate of the AUC was accurate enough to base decisions upon. In addition, it suggested that the public leaderboard score was likely to be a poor guide to the eventual private leaderboard score and it was going to be much better to trust my own cross-validation. As a consequence of this I only actually submitted three entries into the competition.\\r\\nWere you surprised by any of your findings?\\r\\nOne of the most straightforward features to envisage was simply the number of bids made by each bidder. Ex ante, one could suppose that a bot could place far more bids than the average human. When a histogram of the number of bids placed by each bot was considered, however, there was a striking anomaly. Five bots were identified as having only one bid each in the data set, which looks unusual in the distribution, as shown.\\r\\n\\r\\n\\r\\n\\r\\nHaving only one bid per bot suggests that either there are some links with other bots in the data or that they have been labelled as bots using data which we don’t know. Investigation yielded no obvious links to other bots and the latter point implies that the data from these bots won’t generalise well to a different data set. Thus, the algorithms that I used may be negatively affected by trying to learn from these bot examples. I decided then to create a solution that removed these five observations from the training data set and submitted this as one of my two selected entries. It was this entry that won the competition.\\r\\n\\r\\nAlso, note that this picture was on the home page of the competition. There are five bots! Was it a clue?\\r\\n\\r\\n\\nWhich tools did you use?\\r\\nI did everything in python, using the excellent Scikit-learn package for the boosted trees algorithm.\\r\\nHow did you spend your time on this competition?\\r\\nI tend to flit to and fro between the two [feature engineering & machine learning] as I’m going along. I tend to start with some feature engineering, then perform some gross parameter tuning for the machine learning algorithm, then return to some more feature engineering and so on. In this competition the feature engineering took up a lot more time than the machine learning. I spent quite a lot of time lovingly hand-crafting features that didn’t improve the model in cross-validation, but this is normal for this sort of competition. Just because you think a feature will be useful doesn’t mean it will be.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nJust a few minutes, though each cross-validation run took over an hour.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nEnhanced self-belief in persevering with what you know to be the correct methodology and not getting distracted by things that don’t really matter (in this competition, that meant the public leaderboard!).\\r\\n\\r\\nAlso, a realisation that even though I think my methods gave me the best chance of doing well, actually winning the competition required the cards to fall my way this time.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nFirstly, look through the Kaggle website, especially the forums, to pick up a vast amount of good advice on how to improve your data science.\\r\\n\\r\\nMostly though, my advice is to just have a go, make mistakes and learn from these.\\r\\nBio\\nPeter Best is a graduate in both chemistry and mathematics. He worked for many years in the quantitative asset management industry where he performed extensive data analysis. He is currently competing in Kaggle competitions and trying to decide on his next project.', 'An early insight into the importance of splitting the data on the number of radar scans in each row helped\\xa0Devin Anzelmo take\\xa0first place in the How Much Did It Rain? competition. In this blog, he gets into details on his approach and shares key visualizations (with code!) from his analysis.\\r\\n\\r\\n[caption id=\"attachment_5030\" align=\"aligncenter\" width=\"300\"] 351 players on 321 teams built models to predict probabilistic distributions of hourly rainfall[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nMy background is primarily in cognitive science and biology, but I dabbled in many different areas while in school. My particular interests are in human learning and behavior and how we can use human activity traces to learn to shape future actions.\\r\\n\\r\\n[caption id=\"attachment_5028\" align=\"aligncenter\" width=\"340\"] Devin\\'s profile on Kaggle[/caption]\\r\\n\\r\\nMy interest in gaming as a means of teaching and competitive nature has made Kaggle a great fit for my learning style. I started competing seriously on Kaggle in October 2014. I did not have much experience with programming or applied machine learning, and thought entering a competition would provide a structured introduction. Once I started competing I found I had difficult time stopping.\\r\\nWhat made you decide to enter this competition?\\r\\nI thought there was a decent chance I could get into the top five in the competition and this drove me to enter. After finishing the BCI competition I had to decide between Otto group product challenge and this one. I chose How Much Did it Rain because the dataset was difficult to process, and it wasn\\'t obvious how to approach the problem. These factors favored my skills. I didn\\'t feel like I could compete in Otto where the determining factor was going to primarily rely on ensembling skills.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nMost of the preprocessing was just feature generation. Like most other competitors I used descriptive statistics and counts of the different error codes. These made up the bulk of my features and turned out to be enough to get first place. We were given QC\\'d reflectivity data, but instead of using this information to limit the data used in feature generation I included it as a feature and let learning algorithm (Gradient Boosted Decision Trees) use it as needed.\\r\\n\\r\\nThe most important decision with regard to supervised learning was how to model the output probability distribution. I decided to model it by transforming the problem into a multi-class classification problem with soft output. Since there was not enough data to perform classification with the full 70 classes the problem had to be reduced further. It turned out there were many different ways that people solved this problem, and I highly recommend reading the end of competition thread for some other approaches.\\r\\n\\r\\n[caption id=\"attachment_5024\" align=\"aligncenter\" width=\"612\"] See the code on scripts[/caption]\\r\\n\\r\\nI ended up using a simple method in which basic component probability distributions were combined using the output of a classification algorithm. For classes that had enough data a step function was used for a CDF. When there was less data the several labels were combined and replaced by a single value. In this case an estimation of the empirical distribution for that class was used as a component CDF. This method worked well and I used it for most of the competition. I did try regression and classification just on the data from the minority classes but it never performed quite as well as just using the empirical distribution.\\r\\nWhat was your most important insight into the data?\\r\\nEarly in the competition I discovered that it was helpful to split the data based on the number of radar scans in each row. Each row has data spanning the hour previous to the rain gauge reading. In some cases there was only one radar scan in others there was more then 50. There are over one hundred thousand rows in the training set with more then 17 radar scans. For this data I wanted to create features which take into account the changing of weather conditions over time. In doing this I realized it was not possible to make these features for the rows that had only 1 or 2 radar scans. This was the initial reason for splitting the dataset. When I started looking for places to split it I found that there was also a strong positive correlation between the number of radar scans and the average rain amount. Those rows with 1 scan had 95% 0mm of rain, while the subset with 17 or more scans only 48% of the data had 0mm of rain. Interestingly for the data with few radar scans many of the most important features were the counts of the error codes.\\r\\n\\r\\n[caption id=\"attachment_5025\" align=\"aligncenter\" width=\"612\"] See the code on scripts[/caption]\\r\\n\\r\\nIn contrast the most important features in the data with many scans were derived from Reflectivity and HybridScan which have a physical relationship to rain amount. Splitting the data allowed me to use many more features for the higher scan data which gave a large boost to the score. Over 65% of the error came from the data with more then 7 scans. The data with low scans contributed to very small amount of the final score and I was able to spend less time modeling these subsets.\\r\\nWere you surprised by any of your findings?\\r\\nThe most mysterious aspect of the competition was the 5000 rows in the training data that had Expected rain amount over 70mm. The requirements of the competition only asked us to model up to 69mm of rain in an hour but the evaluation metric punished large classification errors so severely that I felt compelled to figure out how to predict these large values. A quick calculation showed that of the 1.1 million rows in the training set these 5000 large values, if mis-predicted, would account for half of my error.\\r\\n\\r\\nIt turned out that many of the samples with labels above 70mm did not have reflectivity values indicating heavy rain. I was still able to improve my local validation score by treating the large rain amount samples as their own class and using an all zero CDF in generating the final prediction. Unfortunately this also worsened my public leaderboard score by a large amount.\\r\\n\\r\\n[caption id=\"attachment_5026\" align=\"aligncenter\" width=\"547\"] See the code on scripts[/caption]\\r\\n\\r\\nThrough leaderboard feedback I was able to determine that there were differences in the distribution of these large values in the 2013 training set and the 2014 test set. Removing the rows with large values from the training set turned out to be the best course of action.\\r\\n\\r\\nMy hypothesis about the large values is that they were generated by specific rain gauges, which the learning algorithm was able to detect using features based on DistanceToRadar and the -99903 error code. The -99903 error code can correspond to physical blockage of a radar beam by mountains or other physical objects. Both of these features can help identify specific rain gauges which would lead to overfitting the train set if there were fixes to the malfunction before the start of 2014. As I don\\'t have access to the 2014 labels this will remain speculation for now.\\r\\nWhich tools did you use?\\r\\nI used python for this competition relying heavily on pandas for data exploration and direct numpy implementations when I needed things to be fast. This was my first competition using Xgboost, and I was very pleased with the ease of use and speed.\\r\\nHow did you spend your time on this competition?\\r\\nI probably spent 50% percent of my time coding, and then having to refactor when I realized my implementation was not flexible enough to incorporate my new ideas. I also tried several crazy things that required substantial programming time that I didn\\'t end up using.\\r\\n\\r\\nThe other 50% percent was split pretty equally between feature engineering, data exploration and tweaking my classification framework.\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nI spent many hours coding and refactoring in this competition. Since I had to do nearly the same thing on five different datasets having manually code everything made it difficult to try new ideas. Having a flexible framework to try out many ideas is critical and this is one of the things I spent time learning how to do in this competition. The effort has already payed off in other competitions.\\r\\n\\r\\nWith only one submission a day it was important to try out things in a systematic way. What worked best was changing one aspect of my method and seeing whether it improved my score. I needed to keep records of everything I did or it was possible waste time redoing things I already tried. Having the discipline to keep on track and and not try too many things at once is critical for doing well and this competition put me to the test on this.\\r\\nDo you have any advice for those just getting started competing on Kaggle?\\r\\nRead the Kaggle blog post profiling KazAnova for a great high level perspective on competing. I read this about two weeks before the end of the competition and I started saving my models and predictions, and automating more of my process which allowed for some late improvements.\\r\\n\\r\\nOther then this I think its very helpful to read the forums and follow up on hints given by those at the top of the leaderboard. Very often people will give small hints, and I have gotten in habit of following up on even the smallest clues. This has taught me many new things and helped me find critical insights into problems.\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nWith the introduction of Kaggle scripts it seems it will now be possible to have solution code evaluated remotely instead of requiring competitors to submit a CSV submission file. I think having this functionality opens up the possibility of solving new types of problems that were not feasible in the past.\\r\\n\\r\\nWith this in mind I would like to run a problem that favors reinforcement learning based solutions. As a simple example we could teach an agent to explore mazes. The training set would consist of several different mazes (perhaps it would be good generate their own training data) and the test set could be another set of unseen mazes hosted on Kaggle. All the training code would be required to run directly on scripts making transition to an evaluation server easy. I don\\'t think this type of problem would have worked without scripts, and I think it would be fun to see if it is possible to turn agent learning problems into Kaggle competitions.\\r\\n\\r\\nAnother possibility with remote execution of solutions would be a Rock Paper Scissors programming tournament. There are already some RPS tournaments available online. Perhaps hosting a variant as a knowledge competition would be possible as these types of competitions are really fun.\\r\\nWhat is your dream job?\\r\\nIdeally I would like to work with neural and behavioural data to help improve human performance and alleviate problems related to mental illness. There are many very challenging problems in this area. Unfortunately most of the current classification frameworks for mental illness are deeply flawed. My dream job would allow for the application of diverse descriptions, methods, and sensors, without the need to push a product out immediately.\\r\\n\\r\\nMy sense is that the amount of theoretical upheaval needed is holding back research in academia, and the ineffectiveness of most current techniques is hampering the development of new businesses (plus the legal issues of the health industry). I would be interested in any project that is making progress through this mire ;)\\r\\n\\r\\nBeyond these interests what motivates me is the ability to explore complex problems and the freedom to try new solutions. Any job in which I can tackle a difficult problems with the ability to actually try to solve it is fundamentally interesting to me.\\r\\nBio\\r\\nDevin Anzelmo is a recent graduate from University of California Merced where he received a B.S in Cognitive Science and a minor in Applied Mathematics. His research interests include machine learning, neuroscience, and animal learning and behavior. He enjoys immersion in complex problems and puzzles. He currently spends most of his time competing on Kaggle but is also interested in employment opportunities in the San Luis Obispo area.', 'The goal of the CrowdFlower Search Results Relevance competition was to come up with a machine learning algorithm that can automatically evaluate the quality of the search engine of an e-commerce site. Given a query (e.g. ‘tennis shoes’) and an ensuing result (‘adidas running shoes’), the goal is to score the result on relevance, from 1 (least relevant) to 4 (most relevant). To train the algorithm, teams had access to a set of 10,000 queries/result pairs that were manually labeled by CrowdFlower using the aforementioned classes.\\r\\n\\r\\n[caption id=\"attachment_5061\" align=\"aligncenter\" width=\"300\"] Team \"Quartet\" finished 3rd out of 1,424 data scientists on 1,326 teams. The competition closed on July 6, 2015.[/caption]\\r\\nThe Basics\\n\"Do not try to climb Everest all by yourself - doing it alongside a team is more fun and will probably win you more money in the end.\" - Team Quartet\\nTell us about your team\\r\\nOur team ‘Quartet’ was formed by Maher Harb, Roman Khomenko, Sergio Gamez, and Alejandro Simkievich. This was a very multi-faceted and multi-cultural team indeed, with folks ranging from physicists to computer scientists to business majors and nationalities from Canada, Ukraine, Spain, and Brazil. While diverse, we are guilty of being an all-male team and yet we hope we can fix that in the next competition (ladies, join our team next time!).\\r\\nLet\\'s Get Technical\\nWhat was your overall approach to the challenge?\\r\\nOur approach consisted of implementing the following steps:\\r\\n\\nCleaning the original datasets (e.g. removing stopwords, removing special characters, converting characters to lowercase, stemming, etc.) and applying the TF-IDF (term frequency, inverse document frequency) algorithm to a concatenation of the query, product title, and product description.\\n\\n\\nApplying dimensionality reduction to the resulting datasets to work with roughly 250 instead of thousands of features.\\n\\n\\nAugmenting the resulting datasets with custom engineered features.\\n\\n\\nBuilding three different model families, including support vector machines (SVM), artificial neural networks (ANN), and gradient boosting machine (GBM).\\n\\n\\nRounding the regression predictions to integers using specially developed thresholds that optimize the kappa score.\\n\\n\\nEnsembling the individual models and building second-layer models. The winning model was an ensemble of a single-layer ANN, a single-layer SVM, and a second-layer ANN (with features derived from over 20 single-layer models).\\n\\nWhat processing methods did you use?\\r\\nThe original dataset was raw text. A number of tasks, discussed extensively at the Kaggle forum and that most competing teams carried out, were performed to convert the raw data to one on which machine learning algorithms can be trained. First, the datasets were cleaned by applying steps that included:\\r\\n\\nRemoving html tags and other non-text elements. These elements were present since, presumably, the datasets were generated programmatically by parsing the web.\\n\\n\\nRemoving stop-words such as common articles, prepositions, etc. (e.g. ‘the’, ‘a’, ‘with’,’on’).\\n\\n\\nUsing word-stemmers so that similar terms such as ‘root’ and ‘roots’, or ‘game’ and ‘gaming’ are converted to the same base word.\\n\\r\\nBasic features were generated by applying the TF-IDF operation on the resulting text. TF-IDF is an algorithm that assigns a value between 0 and 1 to every word in a given document, which in turn is part of an existing corpus. The more frequent a term is in the document and less frequent in the corpus, the higher the value for such term. For more details on TF-IDF, please check this wikipedia article.\\r\\n\\r\\nBecause the TF-IDF creates a feature for every token in the corpus, we used dimensionality reduction to end up with a more workable dataset of 225-250 features, depending on the model. The particular algorithm we used for dimensionality reduction is truncated singular value decomposition (SVD). More information on SVD is available here.\\r\\nWere there any innovative aspects to your approach?\\r\\nAs it turned out, the quality of the models could be enhanced significantly by creating ad-hoc features that allow all machine learning models alike to make better decisions. Therefore, feature engineering was an important factor in this competition. The following set of features were added to the training and test datasets:\\r\\n\\r\\nN-gram similarity features - A total of 14 features were added through the evaluation of similarity between unigrams, bigrams, and trigrams extracted from the query and the product title & description strings. Some of those features are boolean (e.g. 1 if unigram or bigram is found in search string, 0 if it is not) while others are similarity scores between the n-gram and the search string. To fully understand these features, we recommend taking a look at the R code used to generate the features. However, as a representative example of the general idea, the following demonstrates how one of the 14 features was created:\\r\\nQuery: “led christmas lights”\\nTitle: “Set of 10 Battery Operated Multi LED Train Christmas Lights - Clear Wire”\\r\\n6 possible n-grams can be generated from the query string. Those are [“led christmas lights”, “led christmas”, “christmas lights”, “led”, “christmas”, “lights”]. Searching the product title for exact match of the query n-grams yields [0, 0, 1, 1, 1, 1]. The sum of the result (4) normalized by the maximum number of possible matches (6) becomes a feature (0.67).\\r\\n\\r\\nQuery-product name similarity features - While checking the cross-validation results, we noticed that our solution performed particularly bad whenever the main noun in the query was present in the product title, but it was not the actual product (i.e. it was an adjective in the title rather than the main noun). An example would be: query = “night gown” and product title = “night gown necklace”. Thus, we developed an algorithm to extract the main noun from the product title and performed a similarity measure between the query and the extracted noun. This was accomplished by implementing a set of rule-based regular expressions that were carefully crafted to remove all unnecessary descriptions, prefixes, suffixes, etc. For example any phrase following the words “for”, “by”, and “with” was removed, sizes & colors were removed,and so on. Finally, the last two words in the cleaned title were regarded as the main product. The following is a good example of what is achieved through this algorithm:\\r\\nQuery: “reusable straws”\\nTitle: “Itzy Ritzy Snack Happens Reusable Snack Bag with Thermos Straw Bottle, Jungle”\\nCleaned title: “Itzy Ritzy Snack Happens Reusable Snack Bag”\\nProduct name: “Snack bag”\\nCode:\\xa0For additional details refer to the R code used for generating this feature.\\nAlternative query similarity features - For each of the 261 unique queries, we created a corpus by combining all titles associated with that query, extracted the top trigram, and used it as an alternative query. This basically allowed us to potentially double our engineered features. The idea behind this approach was to capture the correct product implicated when the query did not have similarity with the product title. This is especially an issue when the search query is a brand name. As an example, figure 1 below is a word cloud visualization of the trigrams in the title corpus associated with query = ‘samsonite’. The top trigram is ‘piece luggage set’. Because class 4 is the most common class (accounting for ~60% of the all labels), a reasonable assumption was made that the intended product is indeed a ‘piece luggage set’. Thus, we considered ‘piece luggage set’ as an alternative query and subsequently generated similarity features between it and the product title/description. For additional details refer to the R code used for generating this feature.\\r\\n\\r\\n[caption id=\"attachment_5058\" align=\"aligncenter\" width=\"612\"] Word cloud of trigrams extracted from the titles corpus corresponding to query ‘samsonite’. See the full code on scripts.[/caption]\\r\\n\\r\\nIntra-title similarity features -  The idea behind this feature is to use the group of titles with the same relevance and within the same query as reference point and measure similarity of all individual titles within that query to that reference point. So, for instance, if a certain query has 40 labeled titles (10 per relevance class), then various similarity measures between each title (both labeled and unlabeled) and the 4 groups are constructed. In practice, this is carried out by measuring similarity between the title and each title within the group, then aggregating the result. Three different similarity measures were used: cosine similarity, count of same words, count of same words after extracting the product name, and two different aggregation functions were applied (mean and max), yielding a total of 4✕3✕2= 24 features. For additional details refer to the python code used for generating this feature.\\r\\n\\r\\nAntonym feature - Noticing that some queries performed particularly bad when compared to hand-labeled predictions in some particular cases, we developed some additional rules to address this issue. One such case was when an antonym of a noun in the query was in the product title or description (e.g. query: “men shoes”, product title: “beautiful women shoes”). To solve for that, we created a boolean variable indicating whether an antonym is present (1) or not (0). This was done only for the most common antonyms.\\r\\n\\r\\n[caption id=\"attachment_5059\" align=\"aligncenter\" width=\"612\"] Bar plot of the correlation between selected engineered features and label.[/caption]\\r\\nWhat supervised learning methods did you use?\\r\\nWe used a variety of models that included support vector machines, artificial neural networks, gradient boosted regressors, k-nearest neighbor classifiers, and random forest classifiers. For some of the models, we created both classifications and regression versions (e.g. svm classifiers and svm regressors). We also used alternative implementations of the same model (e.g. using both XGBOOST package and H2O to build gradient boosted regressors). As a result we had a wealth of different models to be used in ensembling and building of second-layer models (in specific, the second-layer ANN was based on features from over 20 different models).\\r\\n\\r\\nImplementation wise, SVM was built using the excellent machine learning library of scikit learn (hyperlink). The artificial neural networks were built using keras (which in turn outsources some tasks to theano) on python. Whereas GBM models were built using the XGBOOST and H2O R packages.\\r\\nHow did you round the regression outputs to yield classes?\\r\\nBy running experiments on the cross validation folds, we determined the optimal rounding thresholds for rounding predictions obtained through regression. In the end, we found that the following thresholds produced the best kappa score: 1.9, 2.7, 3.5. (i.e. if prediction is lower than 1.9 then label 1 is assigned, if bigger than 1.9 but lower than 2.7, then label 2 is assigned, and so on).\\r\\nDescribe your winning ensemble\\r\\nOur final model (depicted in the diagram below) is a 2-step ensemble, which can be described as follows:\\r\\n\\r\\nStep 1 - Ensemble of first-layer ANN (30% weight) and second-layer ANN (70% weight). The averaging is applied on the unrounded predictions and the optimal rounding thresholds were applied after averaging.\\r\\n\\r\\nStep 2 - Ensemble of the predictions obtained from step 1 and SVM classifier, according to the following expression: floor(0.8*step1 + 0.35*SVM). The expression looks rather arbitrary, but when carefully examining the possible outcomes of combining the two models according to this formula, we noticed that this simply applies a slight correction to predictions of step 1. Most notably, when step 1 predicts class 2 and the SVM predicts class 1, class 1 is chosen. Our guess for why this operation boosts the overall score is due to the SVM being better at classifying class1 than the step1 ensemble.\\r\\n\\r\\n[caption id=\"attachment_5060\" align=\"aligncenter\" width=\"677\"] Block diagram depicting the overall solution[/caption]\\r\\nWord of Wisdom\\nWhat have you taken away from this competition?\\r\\nAs can be seen from this blog, many different ideas were applied. Those ideas can be grouped into a number of initiatives such as feature engineering, testing different models, testing different machine learning tasks such as regression vs. classification, and trying different ensembling and rounding strategies. Not only does it take a lot of time to test so many different ideas, but a lot of ingenuity needs to be put into idea generation. While different team members contributed differently to different initiatives, everyone was actively looking for ways to improve the overall solution and the competition score. This leads us to one issue that is often discussed in machine learning and management overall: the power of teams and teamwork.\\r\\n\\r\\n[caption id=\"attachment_5063\" align=\"aligncenter\" width=\"622\"] Kaggle profiles for Sergio, Roman (aka Dowakin), Maher (aka madcap), & Alejandro.[/caption]\\r\\n\\r\\nIn machine learning, it is well known that the product of model ensembling is usually better than any individual model. The simple reason is that every model suffers from errors due to variance, bias, and irreducible errors. Ensembling models allows for correcting of those errors, at least in part, and as long as the models are independent. Subsequently, a team of data scientists with different approaches to the same problem can create a diversity of models more than an individual, and thus can ensemble more models. So, all things being equal, the product of those ensembles will be better the larger the number of data scientists engaged in creating models.\\r\\n\\r\\nBut that is not all. Idea generation is a tiresome process, and at different points in time, different team members can suffer from burnout. When that happened among us, someone was always willing and ready to take the lead. Some of us are 100% convinced that in a real-life setting, where companies compete aggressively for their customers’ dollars, individuals cannot compete with teams, no matter how good the individual. This is a lesson to all of us.\\r\\n\\r\\nWhile exceptional data scientists do win competitions by themselves - and this particular competition is one of those - more often than not the top 10 in Kaggle competitions is comprised of teams. When you move to real life, where serious money is involved, you can expect individuals to team up to stand a higher chance to win in the marketplace. Do not try to climb Everest all by yourself - doing it alongside a team is more fun and will probably win you more money in the end.\\r\\nBios\\nMaher Harb - Physicist and data scientist. Starting sept 2015 will take on a new position as Assistant Professor at the Dept. of Physics at Drexel University (Philadelphia).\\r\\n\\r\\nRoman Khomenko - Senior software developer and security researcher from Ukraine.\\r\\n\\r\\nSergio Gámez - Computer vision researcher from Spain.\\r\\n\\r\\nAlejandro Simkievich - Technology entrepreneur and CEO at Statec, a machine learning consulting company in Sao Paulo, Brazil\\r\\n\\r\\n\\r\\n\\r\\nRead other blogs on the CrowdFlower Search Results Relevance\\xa0competition by clicking the tag below.', 'This spring, Kaggle hosted\\xa0two competitions with the ECML PKDD conference in Porto, Portugal. The competitions shared a dataset but focused on different problems. Taxi Trajectory asked participants to predict where a taxi would drop off a customer given partial information on their journey, while Taxi Trip Time\\'s goal was to predict the amount of time a journey would take given the same dataset.\\xa0You can read a write-up from the first place team in Taxi Trajectory here.\\r\\n\\r\\n[caption id=\"attachment_5073\" align=\"aligncenter\" width=\"322\"] 418 players on 345 teams competed to predict the time a taxi journey would take.[/caption]\\r\\n\\r\\nTeam BlueTaxi finished 3rd in Taxi Trip Time and 7th in Taxi Trajectory. This blog outlines how their team of data scientists from five different countries came together, and their winning approach to the Taxi Trip Time competition.\\r\\nThe Basics\\nThe BlueTaxi Team\\r\\nThe BlueTaxi team is very multicultural, we are Ernesto (El Salvador), Lam (Vietnam), Alessandra (Italy), Bei (China), and Yiannis (Greece). We had great fun participating and winning the third place in this ECML/PKDD Discovery Challenge\\xa0which was\\xa0organized as the Kaggle, Taxi Trip Time, competition. In this post we would like to share with you how we did it.\\r\\nWhat made you decide to enter this competition?\\nErnesto: The Discovery Challenges, organized annually by ECML/PKDD, are always very interesting and this year was not the exception, especially this one organized on top of Kaggle. After a small chat with Lam, we decided to go for it and enter as the BlueTaxi team for both tracks of the challenge.\\r\\n\\r\\nI took the lead for the trip time prediction and Lam led the destination prediction.\\r\\n\\r\\nWe invited Alessandra, Bei, and Yiannis to consolidate the final team of five.\\r\\n\\r\\nLam: I decided to enter because the competition was hosted by Kaggle and the ECML/PKDD 2015 conference will be a great a opportunity for researchers to benefit from exchanging ideas and their experience during the workshop session being held in conjunction with the main conference in Porto next September.\\r\\n\\r\\nAlessandra: Lam and Ernesto presented the challenge to me and I thought it was very interesting so I decided to join.\\r\\n\\r\\nBei: For me that was also the case.\\r\\n\\r\\nYiannis: The problem of predicting the destination and trip time for a taxi route seemed very challenging and that\\'s why I decided to participate and join the BlueTaxi team.\\r\\nWhat was your background prior to entering this challenge?\\nLam: I did my PhD in pattern mining for data streams at Technische Universiteit Eindhoven (TU/e) and joined IBM Research Lab in Ireland about a year and a half ago. My research interests include mining big and fast data on big data platforms with applications in telcos, transportation under the smarter city project.\\r\\n\\r\\nAlessandra: My background is in transportation analytics. Specifically I work on estimation and prediction of traffic and urban traffic control systems. I hold a PhD degree in Information Technology from Politecnico di Milano and currently I am a Research Scientist at IBM Research - Ireland (also known as the IBM Smarter Cities Technology Centre in Dublin, Ireland.)\\r\\n\\r\\nBei: I am a statistician with primary interests in time series analysis, forecasting, resampling/subsampling methods for dependent data and financial econometrics. My recent work focuses on statistical methods in urban applications. I received my PhD in Statistics from the University of Waterloo, Canada. I joined IBM Research, Ireland, in late 2012.\\r\\n\\r\\nYiannis: I am a Research Software Engineer at Smarter Cities Technologies Center, IBM Research - Ireland. I hold a Masters Degree from Athens University of Economics and Business in Computer Science. Lately, I have been working with spatio-temporal data on various projects focusing on data curation, efficient storing and analysis. Moreover, I have experience with visualisation of similar type of data, helping data scientists to gain insights.\\r\\n\\r\\nErnesto: I hold a PhD in Computer Science from the L3S Research Center in the University of Hannover, Germany. My background is in supervised machine learning applied to Web Science, Social Media Analytics, and Recommender Systems. I joined IBM Research, Ireland, early 2014.\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\nLam: In the IBM Research lab I have been working on several projects with similar data, e.g. with GPS traces from buses used for prediction of bus arrival time at bus stop, e.g., see our related paper regarding this topic (Flexible Sliding Window for Kernel Regression Based Bus Arrival Time Prediction) in the industry track at ECML/PKDD 2015.\\r\\n\\r\\nErnesto: I did not have any particular domain knowledge in transportation systems, but my experience in machine learning, data science and analytics were of course valuable for the competition.\\r\\n\\r\\nAlessandra: Yes, my knowledge in the transportation field helped me in the challenge.\\r\\n\\r\\nYiannis: My experience with spatio-temporal data helped me in the competition.\\r\\n\\r\\nBei: I had some experience analyzing transportation data, which helped me in this challenge.\\r\\nHow did you get started competing on Kaggle?\\nErnesto: I joined Kaggle a couple of years ago during my PhD. I did enter in some competitions before. The datasets available in Kaggle are usually very interesting and some of them were very useful for my research. But to be honest, I never got good traction in a competition until this time.\\r\\n\\r\\nLam: I also joined Kaggle 4 years ago when I was a PhD student. But I haven\\'t tried to compete since then.\\r\\n\\r\\nBei: I joined Kaggle in 2012, and this was my second competition since then.\\r\\n\\r\\nAlesandra and Yiannis: For us this was the first time that we had entered a Kaggle competition :)\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nFirst we created our local training set by selecting the cut-off times the same as the five snapshots on the test set (the same week-date as well). We also observe that 14th of August is the day before a big holiday in Portugal and 21th of December is the last Sunday before Christmas, both very particular days.\\r\\n\\r\\nWe created a bunch of features as follows:\\r\\n\\r\\n1. Features from 10-NN. For every test trip we find 10 nearest neighbours w.r.t the Euclidean distance and consider the durations of those trips as predictors.\\r\\n\\r\\n2. Features from Kernel Regression. Similar to 10-NN, kernel regression was used to predict the duration. Remember that kernel regression is a smooth version of kNN and these features yield very good results.\\r\\n\\r\\n3. Some features from the partial trips: travelled distance, number of GPS updates, last GPS coordinates, average speed at different part of the trips and accelerations at different part of the trips.\\r\\n\\r\\nWhen matching a test trip with the training trips, we only consider to match the last 100, 200, 300, 400, 500, 1000 meters and the full trips as well. This is because the later part of the trip is more important in some cases. Our model show that the last 500 meters of the trip is very important.\\r\\n\\r\\n[caption id=\"attachment_5066\" align=\"aligncenter\" width=\"612\"] Two trips with different starting points but with the same destination (Porto Airport). The later part of the trajectories are very close to each other. Therefore via trip matching we can guess destination of the other trip if we can guess the destination of trips with similar route.[/caption]\\r\\n\\r\\nWe also considered contextual matching (match only trips with same taxi id, the same week date, the same call id, etc.) because we observed different distributions of destination for these contexts. The kernel regression on taxi id context produced the best results.\\r\\n\\r\\nWhen modelling, we did not predict the duration of the whole trip but instead predict the additional delta travel time with respect to the cut-off timestamp. Since the evaluation metric was RMSLE, we log-transformed the time target labels, i.e., the log of the additional delta time.\\r\\n\\r\\nOutlier handling: we found that trips with missing values (identified at speed limits 160, 140, 100 Km/h) are more difficult to predict, we try to recover this information on the test set by looking at the gap between the cut-off timestamp and the start timestamp. Unfortunately, this information is not reliable so we decided to remove outliers based on the number of GPS updates based on an absolute deviation from the median of 3.5.\\r\\n\\r\\nThe test dataset for this competition was very small (320 instances), which makes it very prone to overfitting. Our final solution was a robust Ensemble of several models that included: Random Forests, Gradient Boosted Trees, and Extremely Randomized Trees.\\r\\n\\r\\nBlueTaxi the overall winner approach ;) We expected some variation in the top ranking positions given the small test set, but were quite surprised on all the shuffling we observed from the public to the final private leaderboard. Note that if we average the performance in the 100% of the test set (public and private scores), the BlueTaxi approach is actually the most robust and with the best overall RSMLE score ;)\\r\\nWhat was your most important insight into the data?\\r\\nThere are many interesting patterns found during data exploration, e.g., some trips with the same call id are very regular, trips to the airport are very easy to predict as well.\\r\\n\\r\\nWe used some open source journey planners to predict travel time but the results, using for example the destination prediction results from the other track of the challenge, but we found that the results were not very good for the taxi trips. This tells us that journey planners should include the real-time traffic information to improve its estimation of travel time.\\r\\n\\r\\n[caption id=\"attachment_5067\" align=\"aligncenter\" width=\"612\"] Heatmap of destinations of 155 trips with call id 59708. There are only 8 major destinations. Using call id information one can narrow down the possible destinations of the given taxi.[/caption]\\r\\nWhich tools did you use?\\r\\nR, Python (Numpy, Scipy, Sklearn and Pandas), Matlab, and Javacript and Leaftlet for visualisation.\\r\\nHow did you spend your time on this competition?\\r\\nA rough estimate would be 80% for feature engineering, 10% in data exploration, and 10% in modelling.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nMost features were based on kNN search, which was very time consuming in general, but we implemented a novel indexing technique to solve the efficiency issues, the running time was reduced significantly with the index.\\r\\n\\r\\nFor the modelling part, the most time consuming step is always the model parameter selection, which we conducted using cross validation. The final training is relatively fast. We included in our ensemble Extremely Randomized Trees, which given their nature are much faster to train with respect to Random Forest or Gradient Boosted Trees, and their individual results were quite competitive in our validation set.\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\r\\nOur data-driven and machine learning solution in the end led to significant better results than other models based solely on transportation domain knowledge.\\r\\n\\r\\nWe learnt that data exploration, feature extraction and engineering are very important. Unfortunately, this task still requires a lot of human effort. Modelling is much easier now with many open-source off-the-shelf machine learning tools.\\r\\n\\r\\nThis fact encourages us to continue working on research directions that try to find a generic solution to automate or at least semi-automate this process, i.e. make ML and AI closer to each other.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nBe always curious and open your mind to what the data has to tell you.\\r\\n\\r\\nMake data science your passion!\\r\\n\\r\\nIn Kaggle competitions, what it really matters is your passion to learn and discover new things and not what you already know.\\r\\nTeamwork\\nHow did competing on a team help you succeed?\\r\\nWe think that competing as a team was the fundamental key for success. Remember that we were participating in both tracks of the ECML/PKDD challenge. Basically two Kaggle competitions at the same time and this was our side project and hobby.\\r\\n\\r\\nIn the end, our BlueTaxi finished 3rd in the taxi trip time prediction and 7th in the destination prediction track. We would not be able to achieve such high rankings if we were not competing as a team.\\r\\nBio\\nErnesto: Ernesto is passionate about Web Science, Big Data analytics and the power of Machine Learning to support decision making processes. His research focus is to contribute novel and intelligent filtering approaches that leverage social interactions, multidimensional relationships, and other ubiquitous data in the Social Web in order to relieve people\\'s information overload. Ernesto\\'s research has advanced the state-of-the-art in personalized ranking and recommender systems and has demonstrated to be valuable for industrial practitioners as well. Ernesto holds a PhD from the L3S Research Center in the University of Hannover, Germany, and currently is a Research Scientist in Machine Learning and Data Science at IBM Research - Ireland.\\r\\n\\r\\nLam: Lam did his PhD in pattern mining in data stream at Technische Universiteit Eindhoven in December 2012. He joined IBM research lab in Dublin Ireland right after that and has been working on different research projects, proposing novel machine learning, statistical modelling approaches to solve real-world problems to improve city life.\\r\\n\\r\\nAlessandra: Alessandra Pascale received the B.Sc. degree (with honors) from Politecnico di Bari in 2006 and the M.Sc. degree (mark 110/110) from Politecnico di Milano in 2009, both in Telecommunication Engineering, and the Ph.D. degree in Information Technology from Politecnico di Milano in 2013. She was also enrolled in the ASP (Alta Scuola Politecnica) program, a school of excellence between Politecnico di Milano and Politecnico di Torino. She was visiting researcher at University of California, Berkeley, in the TOPL group, from February to July 2012. In 2013, she worked for 6 months in the framework of the European research project DIWINE, \"Dense cooperative wireless cloud network\".\\r\\n\\r\\nAt the present time she works as Research Scientist at the IBM Smarter Cities Technology Centre in Dublin, Ireland. Her research interests are in the field of signal processing, in particular statistical estimation and prediction of vehicular traffic, distributed/cooperative estimation approaches and urban traffic control systems.\\r\\n\\r\\nBei: Bei Chen received her BMath in Statistics and Actuarial Science (with Dean\\'s Honour\\'s list and President\\'s Award), MMath and PhD in Statistics (with Outstanding Achievement in Graduate Studies Award) from the University of Waterloo, Canada. She is the recipient of the 2011 Pierre Robillard Award of the Statistical Society of Canada, which recognizes the best thesis in probability and statistics defended at a Canadian university.\\r\\n\\r\\nPrior to joining IBM, Bei Chen was an tenure-track Assistant Professor (September 2011 - October 2012) in the Department of Mathematics and Statistics at McMaster University, Canada. Her research was funded by various grants, including NSERC, ECR and MITACS.\\r\\n\\r\\nYiannis: Yiannis Gkoufas holds a Masters Degree from Athens University of Economics and Business in Computer Science. He is working as a Research Software Engineer on the High-Performance Systems Group in Dublin,Ireland since 2012. His main focus is building domain-specific applications and architectures on top of distributed computing frameworks. Moreover, he is responsible for developing the middleware supporting web-based applications. He has been involved in a vast variety of funded projects evolving around Energy and Demand Forecast Analytics, Transportation Monitoring and Planning and Event Detection from Social Media.\\r\\nDisclaimer\\nDisclaimer from the authors: This post is our own and doesn\\'t necessarily represent IBM\\'s positions, strategies or opinions.\\n\\r\\n\\r\\nRead other blogs on the Taxi Trip Time\\xa0& Trajectory competitions by clicking the tags below.', 'The Crowdflower Search Results Relevance competition asked Kagglers to\\xa0evaluate the accuracy of e-commerce search engines on a scale of 1-4 using a dataset of queries & results. Chenglong Chen finished ahead of 1,423 other data scientists to take first place. He shares his approach\\xa0with us from his home in\\xa0Guangzhou, Guangdong, China. (To compare winning methodologies, you can read a write-up from the third place team here.)\\r\\n\\r\\n[caption id=\"attachment_5082\" align=\"aligncenter\" width=\"300\"] The competition ran from May 11-July 6, 2015.[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI was a Ph.D. student in Sun Yat-sen University, Guangzhou, China, and my research mainly focused on passive digital image forensics. I have applied various machine learning methods, e.g., SVM and deep learning, to detect whether a digital image has been edited/doctored, or how much has the image under investigation been resized/rotated.\\r\\n\\r\\n[caption id=\"attachment_5076\" align=\"aligncenter\" width=\"380\"] Chenglong\\'s profile on Kaggle[/caption]\\r\\n\\r\\nI am very interested in machine learning and have read quite a lot of\\xa0related papers. I also love to compete on Kaggle to test out what I have learnt and also to improve my coding skill. Kaggle is a great place for data scientists, and it offers real world problems and data from various domains.\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nI have a background of image proecssing and have limited knowledge about NLP except BOW/TF-IDF kinda of things. During the competition, I frequently refered to the book Python Text Processing with NLTK 2.0 Cookbook or Google for how to clean text or create features from text.\\r\\n\\r\\nI did read the paper about ensemble selection (which is the ensembling method I used in this competition) a long time ago, but I haven\\'t have the opportunity to try it out myself in real word problem. I previously only tried simple (weighted) averaging or majority voting. This is the first time I got so serious about the model ensembling part.\\r\\nHow did you get started competing on Kaggle?\\r\\nIt dates back a year and a half ago. At that time, I was taking Prof. Hsuan-Tien Lin\\'s Machine Learning Foundations course on Coursera. He encouraged us to compete on Kaggle to apply what we have learnt to real world problems. From then on, I have occasionally participated in competitions I find\\xa0interesting. And to be honest, most of my programming skills about Python and R are learnt during Kaggling.\\r\\nWhat made you decide to enter this competition?\\r\\nAfter I passed my Ph.D. dissertation defense early in May, I have had some spare time before starting my job at an Internet company. I decided that I should learn something new and mostly get prepared for my job. Since my job will be about advertising and mostly NLP related, I thought this challenge would be a great opportunity to familiarize myself with some basic or advanced NLP concepts. This is the main reason that drove me to enter.\\r\\n\\r\\nAnother reason was that this dataset is not very large, which is ideal for practicing ensemble skills. While I have read papers about ensembling methods, I haven\\'t got very serious about ensembling in previous competitions. Usually, I would try very simple (weighted) averaging. I thought this is a good chance to try some of the methods I have read, e.g., stacking generalization and ensemble selection.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nThe documentation and code for my approach are available here. Below is a high level overview of my method.\\r\\n\\r\\n[caption id=\"attachment_5077\" align=\"aligncenter\" width=\"640\"] Figure 1. Flowchart of my method[/caption]\\r\\n\\r\\nFor preprocessing, I mainly performed HTML tags dropping, word replacement, and stemming. For a supervised learning method, I used ensemble selection to generate an ensemble from a model library. The model library was built with models trained using various algorithms, various parameter settings, and various feature sets. I have used Hyperopt (usually used in parameter tuning) to choose parameter setting from a pre-defined parameter space for training different models.\\r\\n\\r\\nI have tried various objectives, e.g., MSE, softmax, and pairwise ranking. MSE turned out to be the best with an appropriate decoding method. The following is the decoding method I used for MSE (i.e., regression):\\r\\n\\nCalculate the pdf/cdf of each median relevance level, 1 is about 7.6%, 1 + 2 is about 22%, 1 + 2 + 3 is about 40%, and 1 + 2 + 3 + 4 is 100%.\\nRank the raw prediction in an ascending order.\\nSet the first 7.6% to 1, 7.6% - 22% to 2, 22% - 40% to 3, and the rest to 4.\\n\\r\\nIn CV, the pdf/cdf is calculated using the training fold only, and in the final model training, it is computed using the whole training data.\\r\\n\\r\\nFigure 2 shows some histograms from my reproduced best single model for one run of CV (only one validation fold is used). Specifically, I plotted histograms of 1) raw prediction, 2) rounding decoding, 3) ceiling decoding, and 4) the above cdf decoding, grouped by the true relevance. It\\'s most obvious that both rounding and ceiling decoding methods have difficulty in predicting relevance 4.\\r\\n\\r\\n[caption id=\"attachment_5078\" align=\"aligncenter\" width=\"640\"] Figure 2. Histograms of raw prediction and predictions using various decoding methods grouped by true relevance. (The code generated this figure is available here.)[/caption]\\r\\n\\r\\nFollowing are the kappa scores for each decoding method (using all 3 runs and 3 folds CV). The above cdf decoding method exhibits the best performance among the three methods we considered.\\r\\n\\n\\n\\nMethod\\nCV Mean\\nCV Std\\n\\n\\nRounding\\n0.404277\\n0.005069\\n\\n\\nCeiling\\n0.513138\\n0.006485\\n\\n\\nCDF\\n0.681876\\n0.005259\\n\\n\\n\\nWhat was your most important insight into the data?\\r\\nI have found that the most important features for predicting the search results relevance is the correlation or distance between query and product title/description. In my solution, I have features like interset word counting features, Jaccard coefficients, Dice distance, and cooccurencen word TF-IDF features, etc. Also, it’s important to perform some word replacements/alignments, e.g., spelling correction and synonym replacement, to align those words with the same or similar meaning.\\r\\n\\r\\nWhile I didn\\'t have much time exploring word embedding methods, they are very promising for this problem. During the competition, I came across a paper entitled “From word embeddings to document distances”. The authors of this paper used Word Mover’s Distance (WMD) metric together with word2vec embeddings to measure the distance between text documents. This metric is shown to have superior performance than BOW and TF-IDF features.\\r\\nWere you surprised by any of your findings?\\r\\nI have tried optimizing kappa directly uisng XGBoost (see below), but it performed a bit worse than plain regression. This might have something to do with the hessian, which I couldn\\'t get to work unless I used some scaling and change it to its absolute value (see here).\\r\\nWhich tools did you use?\\r\\nI used Python for this competition. For feature engineering part, I heavily relied on pandas and Numpy for data manipulation, TfidfVectorizer and SVD in Sklearn for extracting text features. For model training part, I mostly used XGBoost, Sklearn, keras and rgf.\\r\\n\\r\\nI would like to say a few more words about XGBoost, which I have been using very often. It is great, accurate, fast and easy of use. Most importantly, it supports customized objective. To use this functionality, you have to provide the gradient and hessian of your objective. This is quite helpful in my case. During the competition, I have tried to optimize quadratic weighted kappa directly using XGBoost. Also, I have implemented two ordinal regression algorithms within XGBoost framework (both by specifying the customized objective.) These models contribute to the final winning submission too.\\r\\nHow did you spend your time on this competition?\\r\\nWhere I\\xa0spent my time on the competition changed during the competition.\\r\\n\\nIn the early stage, I mostly focused on data preprocessing. I have spent quite a lot\\xa0of time on researching and coding down the methods to perform text cleaning. I have to mention that quite a lot of\\xa0effort was\\xa0spent on exploring the data (e.g., figuring out misspellings and synonyms etc.)\\nThen, I spent most of my time on feature extraction and trying to figure out what features would\\xa0be useful for this task. The time was split pretty equally between researching and coding.\\nIn the same period, I decided to build a model using ensemble selection and realized my implementation was not flexible enough to that goal. So, I spent most of the time refactoring my implementation.\\nAfter that, most of my\\xa0time was\\xa0spent on coding down the training and prediction parts of various models. I didn\\'t spend much time on tuning each model\\'s performance. I utilized Hyperopt for parameter tuning and model library building.\\nWith the pipeline for ensemble selection being built, most of my time was spent on figuring out new features and exploring the provided data.\\n\\r\\nIn short, I would say I have done a lot of researching and coding during this competition.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nSince the dataset is kinda of a small size and kappa is not very stable, I utilized bagged ensemble selection from a model library containing hundreds or thousands of models to combat overfitting and stabilize my results. I don\\'t have an exact number of the hours or days, but it should take quite a large amount\\xa0of time to train and make prediction. Furthermore, this also depends on the trade-off between the size of the model library (computation burden) and the performance.\\r\\n\\r\\nThat being said, you should be able to train the best single model (i.e., XGBoost with linear booster) in a few hours. It will give you a model of kappa score about 0.708 (Private LB), which should be enough for a top15 place. For this model, feature extraction occupied most of the time. The training part (using the best parameters I have found) should be a few minutes using multi-threads (e.g., 8).\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\n\\nEnsembling of a bunch of diverse models helps a lot. Figure 3 shows the CV mean, Public LB, and Private LB scores of my 35 best Public LB submissions generated using ensemble selection. As time went by, I have trained more and more different models, which turned out to be helpful for ensemble selection in both CV and Private LB.\\nDo not ever underestimate the power of linear models. They can be much better than tree-based models or SVR with RBF/poly kernels when using raw TF-IDF features. They can be even better if you introduce appropriate nonlinearities.\\nHyperopt is very useful for parameter tuning, and can be used to build model library for ensemble selection.\\nKeep your implementation flexible and scaleable. I was lucky to refactor my implementation early on. This allowed me to add new models to the model library very easily.\\n\\r\\n[caption id=\"attachment_5079\" align=\"aligncenter\" width=\"612\"] Figure 3. CV mean, Public LB, and Private LB scores of my 35 best Public LB submissions. One standard deviation of the CV score is plotted via error bar. (The code generated this figure is available here.)[/caption]\\r\\nDo you have any advice for those just getting started in data science?\\n\\nUse things like Google to find a few relevant research papers. Especially if you are not a domain expert.\\nRead the winning solutions for previous competitions. They contain lots of insights and tricks, which are quite inspired and useful.\\nPractice makes perfect. Choose one competition that you are interested in on Kaggle and start Kaggling today (and every day)!\\n\\nBio\\nChenglong Chen\\xa0 is a recent graduate from Sun Yat-sen University (SYSU), Guangzhou, China, where he received a\\xa0B.S. degree in Physics in 2010 and recently got a\\xa0Ph.D. degree in Communication and Information Systems. As a Ph.D. student, his research interests included image processing, multimedia security, pattern recognition, and in particular digital image forensics. He will be starting his job career at Tencent this August, working on advertising. Chenglong can be reached at:\\xa0c.chenglong@gmail.com\\r\\n\\r\\n\\r\\n\\r\\nRead other blogs on the CrowdFlower Search Results Relevance\\xa0competition by clicking the tag below.', 'Taxi Trajectory Prediction was the first of two competitions that we\\xa0hosted for the 2015\\xa0ECML PKDD conference\\xa0on machine learning. Team\\xa0?\\xa0took first place using deep learning tools developed at\\xa0the MILA lab where they currently study. In this post, they share more about the competition and their winning approach.\\r\\n\\r\\nFor more write-ups from our 2015 ECML PKDD taxi competitions, be sure to read the story of BlueTaxi, third place team in Taxi Trip Time Prediction,\\xa0here.\\r\\n\\r\\n[caption id=\"attachment_5092\" align=\"aligncenter\" width=\"427\"] 459 data scientists on 381 teams competed to best predict where taxi riders would be dropped off.[/caption]\\r\\nThe Basics\\r\\nThe task was particularly simple: we had to predict the destination of a taxi given the beginning of its trajectory (GPS points) and a few pieces of meta information (date, time, taxi id, client information).\\r\\n\\r\\nThe training data was composed of all the taxi rides that took place in 2013/2014 in the city of Porto, Portugal. This represents around 1.7 million taxi rides, run by 442 taxis. You can find the competition homepage on Kaggle here.\\r\\n\\r\\nBefore going further, here are two videos showing the predictions of our model as two taxis run:\\r\\n\\r\\nPredicted destinations by our best model as the taxis run towards their destination. For the first video, we can clearly see that our model has learned the position of the airport.\\n\\n\\nWhat were the main difficulties you faced in\\xa0the dataset?\\r\\nThere are several difficulties in the dataset, such as:\\r\\n\\nThe taxi trajectories are variable-length sequences, the shortest being of length 0 (i.e. when the data is missing) and the longest being of length around 5000 GPS points (i.e a 20 hour ride!).\\nThe features are very heterogeneous: in particular, certain metadata is\\xa0discrete (taxi and client ids), others are highly structured (date, time), and the GPS trajectories are sequences of continuous coordinates.\\nThe testing dataset is unbelievably small: 320 taxi rides only, whereas the training dataset is composed of more than 1.7 million rides. To win the competition with confidence, the only way was to design a method that would surpass those of our competitors by far.\\nAs a real-world dataset, there are several inconsistencies in the data, such as taxi rides that last more than 16 hours or a taxi driver who went to Iran :). The datapoint in Iran is certainly due to a badly calibrated GPS, but in general, training trajectories can be quite noisy due to the imprecision of the GPS.\\n\\nWhat was your background prior to entering this challenge?\\r\\nWe are three students at the Montréal Institute for Learning Algorithms (MILA, formerly known as LISA), a research lab specialised in (deep) neural networks and led by Professor Yoshua Bengio. We are two interns (Alex and Étienne) with backgrounds in computer science and one first-year PhD student (Alexandre) with more background in machine learning (ML). The ML techniques, skills and tools used during this competition are those of our every-day work at the MILA lab: (exotic) (deep) neural networks implemented with programming frameworks developed by our lab (Theano, Blocks).\\r\\nDid you have any domain knowledge that helped you succeed?\\r\\nNo, we did not have any prior domain knowledge about Porto and its taxis!\\r\\nHow did you get started competing on Kaggle?\\r\\nFor the three of us, this was our first (serious) Kaggle competition. As students of the MILA lab, we were interested in trying out Deep Learning (DL) techniques on less mainstream applications than those in which DL already rocks (such as computer vision, NLP, speech).\\r\\nLet\\'s Get Technical\\nWhat was your general approach?\\r\\nWe wanted to design a full machine-learning method with as little\\xa0hand-engineering as possible: ideally no pre-processing/feature engineering, no post-processing and no model ensembling.\\r\\n\\r\\nWe tried several models, all neural network based. It turned out that our more sophisticated models did not perform as well as our simpler models, strangely enough!\\r\\nCan\\xa0you describe your winning model?\\r\\n[caption id=\"attachment_5084\" align=\"aligncenter\" width=\"640\"] Figure 1: Architecture of our winning model. We tried replacing the MLP by a recurrent neural network, but it did not work as well. Deeper networks also did not improve the results.[/caption]\\r\\n\\r\\nOur winning architecture is based on a multi-layer perceptron (MLP), one of the simplest neural network architectures. More precisely, a MLP is composed of an input layer that takes a fixed-size representation of the input, one or several hidden layers, which compute intermediate representations of the input, and an output layer that gives a prediction.\\r\\n\\r\\nOf course, we had to adapt a few things to make the MLP work with the task:\\r\\n\\nThe known part of the trajectory is of varying length, whereas an MLP takes a fixed-length input. We tackle this by taking only the 5 first and 5 last points of the trajectory as an input for the MLP. It turns out that these are indeed the most important parts of the trajectory, what happens in the middle matters less.\\nTo consider the metadata (time and client information), we use an approach commonly used in natural language models in which word embeddings are trained. Instead of words, we learn embedding vectors for each of the metadata values (quarter hour of the day, day of the week, week of the year, client ID, etc.) and use those embeddings as supplementary inputs to the MLP.\\nWe initially tried to predict the output position x, y directly, but we actually obtain significantly better results with another approach that includes a bit of pre-processing. More precisely, we first used a mean-shift clustering algorithm on the destinations of all the training trajectories to obtain around 3,392 popular destination points. The penultimate layer of our MLP is a softmax that predicts the probabilities of each of those 3,392 points to be the destination of the taxi. As the task requires to predict a single destination point, we then calculate the mean of all our 3,392 targets, each weighted by the probability returned by the softmax layer.\\n\\r\\nOur model is then trained to minimize directly the error distances between our predictions and the targets. We use stochastic gradient descent and momentum for that.\\r\\nWhat was your most important insight into the data?\\r\\n[caption id=\"attachment_5085\" align=\"aligncenter\" width=\"640\"] Figure 2: Heatmap of all the GPS points of all the training rides. Note that there is no map printed here, only GPS points, which reveal the road network of Porto.[/caption]\\r\\n\\r\\nThis challenge was quite interesting from a visualization point of view. At the beginning of the competition, we computed a heatmap of the most visited zones of Porto by the taxis (Figure 2). As you can see, we can clearly identify the main highways, the airport, the train station, the city centre and zones with low urban concentration. These insights made us think that we should provide our model with a prior that corresponds to the distribution of the data. And this led us to consider clusters for the destinations.\\r\\nWere you surprised by any of your findings?\\r\\nJust out of curiosity, we used t-SNE to project the embeddings that we learnt for each metadata into 2D dimensional spaces (our original embedding spaces have 10 dimensions each) and this gave us a nice visual idea of the way each metadata value influences the prediction (if one wants to quantitatively assess the importance of a single metadata, one could train a model with only this specific metadata as input). In particular, Figures 3 and 4 show the projections of, respectively, the embedding of the quarter of hour through the day and the embedding of the week of the year. These visual patterns confirm that these two metadata are important in the prediction. We did not find such obvious patterns in the other metadata t-SNE visualizations.\\r\\n\\r\\n[caption id=\"attachment_5086\" align=\"aligncenter\" width=\"640\"] Figure 3: t-SNE 2D projection of the embeddings learnt for the quarter of hour at which the taxi departed (there are 96 quarters in one day, so 96 points, each one representing a particular quarter). This suggests that each quarter is pretty important on its own.[/caption]\\r\\n\\r\\n[caption id=\"attachment_5087\" align=\"aligncenter\" width=\"640\"] Figure 4: t-SNE 2D projection of the embeddings learnt for week of the year during which the taxi departed (there are 52 weeks in one year, so 52 points, each one representing a particular quarter).[/caption]\\r\\nHow did you process the data?\\r\\nThe training dataset is composed of complete trajectories, whereas the test set is composed of only partial trajectories. This means that the GPS sequences had to be cut before being fed to our networks. The ideal solution would have been to cut all the trajectories at all different times, which would have resulted in 100.000.000 training examples and shuffle them so that we could do better stochastic gradient descent but this would have been too expensive. Instead we randomly iterate through the 1.700.000 full training examples, and dynamically generate 100 random cuts for each of them.\\r\\nWhich tools did you use?\\r\\nWe used the libraries developed at the MILA lab, namely Theano, a numpy-like library for GPU accelerated computation, and Blocks, a fairly recent deep learning framework built on top of Theano. We would like to thank all their developers!\\r\\n\\r\\nOur code and instructions to reproduce our results are available on our github repo here.\\r\\nHow did you spend your time on this competition?\\r\\nAs described previously, our approach uses only a little feature engineering. We did spend a bit of time trying to understand the data by visualization, but most of our time was spent on thinking about neural network architectures and implementing them with Blocks.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nOur model was trained by stochastic gradient descent, meaning that it would consider all the examples in the dataset one after another - or, more precisely, by batches of 200 examples - several times. One epoch, i.e. one pass through all the data, takes varying time depending on the complexity of the model. It takes a few hours on a GTX 680 to do a single epoch with our best model, and around half a day to train it.\\r\\nWhat are the other models that you tried?\\r\\n[caption id=\"attachment_5091\" align=\"aligncenter\" width=\"640\"] Figure 5: Memory-like network. The RELU layers could be replaced by recurrent neural networks. In our experiments, the candidates were extracted randomly but a hand-coded similarity function could be used.[/caption]\\r\\n\\r\\nWe tried more sophisticated neural net architectures, such as recurrent neural networks and something that relates to memory networks. Surprisingly, recurrent architectures, that take the full trajectory prefix as input, did not improve our results. Our memory network architecture (Figure 5) is based on learning a similarity function, which weights candidate trajectories extracted from the training dataset.\\r\\nBios\\nAlexandre de Brébisson\\xa0\\xa0is a PhD student at the MILA lab in Montréal under the supervision of Professors Pascal Vincent and Yoshua Bengio.\\r\\n\\r\\nÉtienne Simon\\xa0is a computer science student at ENS Cachan, and currently doing an internship at the MILA lab in Montréal under the supervision of Professor Yoshua Bengio.\\r\\n\\r\\nAlex Auvolat\\xa0is a computer science student at ENS Paris, and currently doing an internship at the MILA lab in Montréal under the supervision of Professors Pascal Vincent and Yoshua Bengio.\\r\\n\\r\\n\\r\\n\\r\\nRead other blogs on the Taxi Trip Time\\xa0& Trajectory competitions by clicking the tags below.', '\\r\\n\\r\\nThe past almost four months I have been competing in a Kaggle competition about diabetic retinopathy grading based on high-resolution eye images. In this post I try to reconstruct my progression through the competition; the challenges I had, the things I tried, what worked and what didn\\'t. This is not meant as a complete documentation but, nevertheless, some more concrete examples can be found at the end and certainly in the code. In the end I finished fifth of the almost 700 competing teams.\\r\\n\\r\\nUpdate 02/08/2015:\\xa0Code and models (with parameters) added.\\nTable of Contents\\n\\nIntroduction\\nOverview / TL;DR\\nThe opening (processing and augmenting, kappa metric and first models)\\nThe middlegame (basic architecture, visual attention)\\nThe endgame (camera artifacts, pseudo-labeling, decoding, error distribution, ensembling)\\nOther (not) tried approaches and papers\\nConclusion\\nCode, models and example activations\\n\\n\\nIntroduction\\nDiabetic retinopathy (DR) is the leading cause of blindness in the working-age population of the developed world and is estimated to affect over 93 million people. (From the competition description where some more background information can be found.)\\r\\n\\r\\nThe grading process consists of recognising very fine details, such as microaneurysms, to some bigger features, such as exudates, and sometimes their position relative to each other on images of the eye. (This is not an exhaustive list, you can look at, for example, the long list of criteria used in the UK to grade DR.)\\r\\n\\r\\nSome annotated examples from the literature to get an idea of what this really looks like (the medical details/terminology are not very important for the rest of this post):\\r\\n\\r\\n[caption id=\"attachment_5099\" align=\"aligncenter\" width=\"446\"] Example of non-proliferative diabetic retinopathy (NPDR): Thin arrows: hard exudates; Thick arrow: blot intra-retinal hemorrhage; Triangle: microaneurysm. (Source)[/caption]\\r\\n\\r\\n[caption id=\"attachment_5100\" align=\"aligncenter\" width=\"512\"] Some (sub)types of diabetic retinopathy. The competition grouped some together to get 5 ordered classes. (Source)[/caption]\\r\\n\\r\\nNow let\\'s look at it as someone who simply wants to try to model this problem.\\r\\n\\r\\nYou have 35,126 images in the training set that look like this...\\r\\n\\r\\n[caption id=\"attachment_5101\" align=\"aligncenter\" width=\"612\"] Some pseudorandom samples from the training set. Notice the black borders and different aspect ratios.[/caption]\\r\\n\\r\\n...annotated by a patient id and \"left\" or \"right\" (each patient has two images, one per eye) and divided into 5 fairly unbalanced classes\\xa0(per eye/image, not per patient)!\\r\\n\\r\\n[table]\\r\\nClass,Name,Number of images,Percentage\\r\\n0,Normal,25810,73.48%\\r\\n1,Mild NPDR,2443,6.96%\\r\\n2,Moderate NPDR,5292,15.07%\\r\\n3,Severe NPDR,873,2.48%\\r\\n4,PDR,708,2.01%\\r\\n[/table]\\r\\n\\r\\nYou are asked to predict the class (thus, one of the 5 numbers) for each of the 53,576 test images and your predictions are scored on the quadratic weighted kappa metric. For the public leaderboard that was updated during the competition, 20 percent of the test data was used. The other 80 percent are used to calculate the final ranking.\\r\\n\\r\\nNot surprisingly, all my models were convolutional networks (convnets) adapted for this task. I recommend reading the well-written blogpost from the ≋ Deep Sea ≋ team that won the Kaggle National Data Science Bowl competition since there are a lot of similarities in our approaches and they provide some more/better explanation.\\r\\n\\r\\nA small but important note: notice that 20 percent of the test images is roughly 11k images and the training set is only 35k images. Hence, the leaderboard score can actually provide some more stable scoring than only using, for example, a 90%/10% split on the training set which would result in only about a third of the size of the public leaderboard dataset used for validation. Normally I don\\'t like to make many submissions during a competition but because training and evaluating these models is quite time-consuming and the public leaderboard could provide some interesting and somewhat reliable information, I did try to make use of this at the end.\\r\\n\\r\\nAlso note that competing in a competition such as this requires you -- unless you have a huge amount of resources and time -- to interpret a stream of fairly complex and noisy data as quickly and efficiently as possible and thus, the things I have \"learned\" are not really tested rigorously and might even be wrong. In the same mindset, a lot of the code is written very quickly and even obvious optimisations might be postponed (or never done!) because the trade-off is not worth it, there are more immediate priorities, you don\\'t want to risk breaking some things, I don\\'t know how (to do it quickly), etc. I started working on this competition somewhere in the beginning of April because before that I was in the process of moving to London.\\r\\ntop\\n\\nOverview / TL;DR\\r\\nThis post is written in an approximate chronological order, which might be somewhat confusing to read through at first. Therefore, I\\'ll try to sketch in a very concise fashion what my eventual models looked like. (It\\'s not completely independent of the rest of the text, which does try to provide some more explanation.)\\r\\n\\r\\nMy best models used a convolutional network with the following relatively basic architecture (listing the output size of each layer)\\r\\n\\r\\n[table]\\r\\nNr,Name,batch,channels,width,height,filter/pool\\r\\n0,Input,64,3,512,512,\\r\\n1,Conv,64,32,256,256,7//2\\r\\n2,Max pool,64,32,127,127,3//2\\r\\n3,Conv,64,32,127,127,3//1\\r\\n4,Conv,64,32,127,127,3//1\\r\\n5,Max pool,64,32,63,63,3//2\\r\\n6,Conv,64,64,63,63,3//1\\r\\n7,Conv,64,64,63,63,3//1\\r\\n8,Max pool,64,64,31,31,3//2\\r\\n9,Conv,64,128,31,31,3//1\\r\\n10,Conv,64,128,31,31,3//1\\r\\n11,Conv,64,128,31,31,3//1\\r\\n12,Conv,64,128,31,31,3//1\\r\\n13,Max pool,64,128,15,15,3//2\\r\\n14,Conv,64,256,15,15,3//1\\r\\n15,Conv,64,256,15,14,3//1\\r\\n16,Conv,64,256,15,15,3//1\\r\\n17,Conv,64,256,15,15,3//1\\r\\n18,Max pool,64,256,7,7,3//2\\r\\n19,Dropout,64,256,7,7,\\r\\n20,Maxout (2-pool),64,512, , ,\\r\\n21,Concat with image dim,64,514, , ,\\r\\n22, Reshape (merge eyes),32,1028, , ,\\r\\n23,Dropout,32,1028, , ,\\r\\n24,Maxout (2-pool),32,512, , ,\\r\\n25,Dropout,32,512, , ,\\r\\n26,Dense (linear),32,10, , ,\\r\\n27, Reshape (back to one eye),64,5, , ,\\r\\n28,Apply softmax,64,5, , ,\\r\\n[/table]\\r\\nWhere a//b\\xa0in the last column denotes pool or filter size a x a\\xa0with stride b x b.\\r\\nwhere the reshape was done to combine the representations of the two eyes belonging to the same patient. All layers were initialised with the SVD variant of the orthogonal initialisation (based on Saxe et al.) and the non-linear layers used leaky rectify units max(alpha*x, x) with alpha=0.5.\\r\\n\\r\\nThe inputs were 512x512 images which were augmented in real-time by\\r\\n\\r\\n1. Cropping with certain probability\\r\\n2. Color balance adjustment\\r\\n3. Brightness adjustment\\r\\n4. Contrast adjustment\\r\\n5. Flipping images with 50% chance\\r\\n6. Rotating images by x degrees, with x an integer in [0, 360]\\r\\n7. Zooming (equal cropping on x and y dimensions)\\r\\n\\r\\ntogether with their original image dimensions. Because of the great class imbalance, some classes were oversampled to get a more uniform distribution of classes in batches. Somewhere in the middle of training this oversampling stopped and images were sampled from the true training set distribution to\\r\\n\\nTry to control the overfitting, which is particularly difficult when the network sees some images almost ten times more often than others.\\r\\nHave the network fine-tune the predictions on the true class distribution.\\nTraining was done with Stochastic Gradient Descent (SGD) and Nesterov momentum for almost 100k iterations on a loss which was a combination of a continuous kappa loss together with the cross-entropy (or log) loss:\\n\\n kappalogclipped = cont_kappa + 0.5 * T.clip(log_loss, log_cutoff, 10**3)\\r\\n\\r\\n\\r\\nAn important part was converting the softmax probabilities for each label to a discrete prediction. Using the label with the highest probability (i.e. argmax) did quite well but is unstable and a significant improvement comes from converting these probabilities to one continuous value (by weighted sum), ranking these values and then using some boundaries to assign labels (e.g., first 10% is label 0, next 20% is label 1, etc.).\\r\\n\\r\\nDoing all this gets you to somewhere around +0.835 with a single model.\\r\\n\\r\\nA final good improvement then came from ensembling a few models using the mean of their log probabilities for each class, converting these to normal probabilities in [0, 1] again and using\\r\\n weighted_probs = probs[:, 1] + probs[:, 2] * 2 + probs[:, 3] * 3 + probs[:, 4] * 4\\r\\n\\r\\n\\r\\nto get one vector of predictions on which we can apply the ranking procedure from the previous paragraph to assign labels. A few candidate boundaries were determined using scipy\\'s minimize function on the kappa score of some ensembles.\\r\\ntop\\n\\nThe opening\\r\\nThe patients are always split into a training set (90%) and a validation set (10%) by stratified sampling based on the maximum label of the two eyes per patient.\\r\\nProcessing and augmenting\\r\\nFirst of all, since the original images are fairly large (say, 3000x2000 pixels on average) and most contained a fairly significant black border, I started by downscaling all the images by a factor of five (without interpolation) and trying to remove most of these black borders.\\r\\n\\r\\n[caption id=\"attachment_5102\" align=\"aligncenter\" width=\"612\"] The same images from before but now the black borders are cropped.[/caption]\\r\\n\\r\\nThis made it computationally much more feasible to do many augmentations: we augment the training set with seemingly similar images to increase the number of training examples.\\r\\n\\r\\nThese augmentations (transformations) were:\\r\\n\\r\\n1. Cropping with certain probability\\r\\n2. Color balance adjustment\\r\\n3. Brightness adjustment\\r\\n4. Contrast adjustment\\r\\n5. Flipping images with 50% chance\\r\\n6. Rotating images by x degrees, with x an integer in [0, 360[\\r\\n7. Zooming (equal cropping on x and y dimensions)\\r\\n\\r\\nMost of these were implemented from the start. The specific ranges/parameters depend on the model (some examples can be found at the end). During training random samples are picked from the training set and transformed before queueing them for input to the network. The augmentations were done by spawning different threads on the CPU such that there was almost no delay in waiting for another batch of samples. This worked very well for most of the competition, which is why I did not spend much time trying to optimise this.\\r\\n\\r\\nResizing was done after the cropping and before the other transformations, because it makes some of other operations computationally way too intensive, and can be done in two ways:\\r\\n\\r\\n1. Rescale while keeping the aspect ratio and doing a center crop on the resulting image\\r\\n2. Normal bilinear rescaling, destroying the original aspect ratio\\r\\n\\r\\nThe method chosen also depends on the model. Method 1 was used a lot in the beginning, then 2 in the middle and in the end I revisited the choice again. Eventually I stuck with method 2 since my best performing models in the end used that and I did not want to have another hyperparameter to take into account, and I was afraid of losing too much information with the center crops from the first method.\\r\\n\\r\\n[caption id=\"attachment_5103\" align=\"aligncenter\" width=\"612\"] An older batch of augmented 512x512 samples as input to a network. The augmenting process went through some subtle changes since then. You can also see that the images are paired by patient but with independent augmentations (more about that later).[/caption]\\r\\n\\r\\nDuring the training the input is normalised by subtracting the total mean and dividing by the total standard deviation estimated on a few hundred samples before training (a pixel based normalisation was also used for a lot of models, at the moment of writing I have not figured out if there is a big difference).\\r\\n\\r\\nThe original image dimensions (before they are rescaled to a square input) are also always concatenated as features with the other dense representations (I never removed this or tested if it was of any use).\\r\\nThe kappa metric\\r\\nThe first few nets were initially trained with SGD and Nesterov momentum on the cross-entropy (or log) loss function. Thus, as a classification problem. However, the competition used the (quadratic weighted) kappa metric to score submissions and not the log loss or accuracy. The (quadratic weighted) kappa score of a class prediction y\\xa0for a ground truth t can be given by the Python code (not optimised for edge cases and assuming one-hot encoded):\\r\\n import numpy as np\\r\\n \\r\\ndef quadratic_kappa(y, t, eps=1e-15):\\r\\n # Assuming y and t are one-hot encoded!\\r\\n num_scored_items = y.shape[0]\\r\\n num_ratings = y.shape[1]\\r\\n ratings_mat = np.tile(np.arange(0, num_ratings)[:, None],\\r\\n reps=(1, num_ratings))\\r\\n ratings_squared = (ratings_mat - ratings_mat.T) &amp;amp;amp;lt;b&amp;amp;amp;gt; 2\\r\\n weights = ratings_squared / (float(num_ratings) - 1) &amp;amp;amp;lt;b&amp;amp;amp;gt; 2\\n# We norm for consistency with other variations.\\r\\n y_norm = y / (eps + y.sum(axis=1)[:, None])\\n# The histograms of the raters.\\r\\n hist_rater_a = y_norm.sum(axis=0)\\r\\n hist_rater_b = t.sum(axis=0)\\n# The confusion matrix.\\r\\n conf_mat = np.dot(y_norm.T, t)\\n# The nominator.\\r\\n nom = np.sum(weights * conf_mat)\\r\\n expected_probs = np.dot(hist_rater_a[:, None],\\r\\n hist_rater_b[None, :])\\r\\n # The denominator.\\r\\n denom = np.sum(weights * expected_probs / num_scored_items)\\nreturn 1 - nom / denom\\r\\nAlternatively, for some more information about the metric, see the evaluation page of the competition. The kappa metric tries to represent the agreement between two different raters, where the score typically varies from 0 (random agreement) to 1 (complete agreement). The downside of this metric is that it is discrete\\xa0-- forcing you to pick only one class of the five per sample instead of allowing you to use the probabilities for each class -- which necessarily gives you some more variance when evaluating models. There already is a decent amount of noise in the dataset (very questionable ground truth classifications, noisy/broken images, etc.) so there certainly was a noticeable amount of variance throughout the competition to take into account.\\r\\n\\r\\nI only trained one or two models on the cross-entropy loss since the competition was scored on the kappa metric and I felt it was important to quickly change to a loss function closely based on this metric. Several variations of a continuous kappa loss\\xa0-- you simply use your softmax prediction matrix y\\xa0as input to the quadratic_kappa function above -- were tried but using only a kappa loss seemed too unstable in the beginning of training to learn well. Combining the continuous kappa with the cross-entropy loss seemed to fix this. Eventually this \"kappalogclipped\" loss was used\\r\\nkappalogclipped = cont_kappa + 0.5 * T.clip(log_loss, log_cutoff, 10**3)\\r\\nwhere there was an additional y_pow\\xa0parameter which determined the power to which to raise the predictions before calculating the continuous kappa (squashing lower probabilities, making the predictions more discrete). This y_pow\\xa0was 2 for a lot of the later experiments and has a significant influence on the model. By using y_pow=2\\xa0the continuous kappa approximates the discrete kappa very well, which will give you some more variance in the updates. The log_cutoff\\xa0determined from which point to start ignoring the log loss, this was 0.80 for most of the experiments. The scaling by 0.5 is something left behind by many other experiments with losses.\\r\\n\\r\\nThere has been a lot of discussion about optimising the kappa metric, partly because of the recent CrowdFlower Search Results Relevance competition\\xa0which also used this metric to score the submissions. Most of the competitors in that competition used regression on the Mean Squared Error (MSE) objective together with some decoding strategy\\xa0to convert the continuous predictions to discrete ones. The first place winner has an excellent write-up\\xa0where he compared many different methods to optimise the kappa metric and concludes that MSE together with a ranking based discretisation worked best. I also considered such an approach but since training my models took quite a while and I did not want to lose too much time testing all these different methods -- and\\xa0it already worked quite well! -- I stuck with the kappalogclipped\\xa0loss. These other losses also don\\'t take the distribution of the predictions into account, which I thought might be important (even though you could optimise for that after training). The kappalogclipped\\xa0loss had the benefit of allowing me to easily monitor a relatively reliable kappa score during the training by using the label with the highest probability (the same argmax strategy was used for almost all the models but is revisited at the end). Also note that the fact that these labels are ordered\\xa0is implicitly defined by the kappa loss itself.\\r\\n\\r\\nI did test the MSE objective very briefly at the end and got somewhat similar performance.\\r\\n\\r\\nThis was still trained with SGD with Nesterov momentum using some learning rate schedule (decreasing the learning rate 3-5 times). Most of the time some L2 regularisation on the network parameters, or weight decay, was added.\\r\\nFirst models\\r\\nMy first models used 120x120 rescaled input\\xa0and I stayed with that for a decent amount of time in the beginning (first 3-4 weeks). A week or so later my first real model had an architecture that looked like this (listing the output size of each layer)\\r\\n[table]\\r\\nNr,Name,batch,channels,width,height,filter/pool\\r\\n0,Input,32,3,120,120,\\r\\n1,Cyclic slice,128,3,120,120,\\r\\n2,Conv,128,32,120,120,3//1\\r\\n3,Conv,128,16,120,120,3//1\\r\\n4, Max pool,128,16,59,59,3//2\\r\\n5,Conv roll,128,64,59,59,\\r\\n6,Conv,128,64,59,59,3//1\\r\\n7,Conv,128,32,59,59,3//1\\r\\n8,Max pool,128,32,29,29,3//2\\r\\n9,Conv roll,128,128,29,29,\\r\\n10,Conv,128,128,29,29,3//1\\r\\n11,Conv,128,128,29,29,3//1\\r\\n12,Conv,128,128,29,29,3//1\\r\\n13,Conv,128,64,29,29,3//1\\r\\n14,Max pool,128,64,14,14,3//2\\r\\n15,Conv roll,128,256,14,14,\\r\\n16,Conv,128,256,14,14,3//1\\r\\n17,Conv,128,256,14,14,3//1\\r\\n18,Conv,128,256,14,14,3//1\\r\\n19,Conv,128,128,14,14,3//1\\r\\n20,Max pool,128,128,6,6,3//2\\r\\n21,Dropout,128,128,6,6,\\r\\n22,Maxout (2-pool),128,512, , ,\\r\\n23,Cyclic pool,32,512, , ,\\r\\n24,Concat with image dim,32,514, , ,\\r\\n25,Dropout,32,514, , ,\\r\\n26,Maxout (2-pool),32,512, , ,\\r\\n27,Dropout,32,512, , ,\\r\\n28,Softmax,32,5, , ,\\r\\n[/table]\\nWhere a//b\\xa0in the last column denotes pool or filter size a x a\\xa0with stride b x b.\\r\\nwhich used the cyclic layers from the ≋ Deep Sea ≋ team. As nonlinearity I used the leaky rectify function, max(alpha*x, x), with alpha=0.3. Layers were almost always initialised with the SVD variant of the orthogonal initialisation (based on Saxe et al.). This gave me around 0.70 kappa. However, I quickly realised that, given the grading criteria for the different classes (think of the microaneurysms which are pretty much impossible to detect on 120x120 images), I would have to use bigger input images to get anywhere near a decent model.\\r\\n\\r\\nSomething else that I had already started testing in models somewhat, which seemed to be quite critical for decent performance, was oversampling the smaller classes. I.e., you make samples of certain classes more likely than others to be picked as input to your network. This resulted in more stable updates and better, quicker training in general (especially since I was using small batch sizes of 32 or 64 samples because of GPU memory restrictions).\\r\\ntop\\n\\nThe middlegame\\r\\nFirst I wanted to take into account the fact that for each patient\\xa0we get two retina images: the left and right eye. By combining the dense representations of the two eyes\\xa0before the last two dense layers (one of which being a softmax layer) I could use both images to classify each image. Intuitively you can expect some pairs of labels to be more probable than others and since you always get two images per patient, this seems like a good thing to do.\\r\\n\\r\\nThis gave me the basic architecture for 512x512 rescaled input\\xa0which was used pretty much until the end (except for some experiments):\\r\\n\\r\\n[table]\\r\\nNr,Name,batch,channels,width,height,filter/pool\\r\\n0,Input,64,3,512,512,\\r\\n1,Conv,64,32,256,256,7//2\\r\\n2,Max pool,64,32,127,127,3//2\\r\\n3,Conv,64,32,127,127,3//1\\r\\n4,Conv,64,32,127,127,3//1\\r\\n5,Max pool,64,32,63,63,3//2\\r\\n6,Conv,64,64,63,63,3//1\\r\\n7,Conv,64,64,63,63,3//1\\r\\n8,Max pool,64,64,31,31,3//2\\r\\n9,Conv,64,128,31,31,3//1\\r\\n10,Conv,64,128,31,31,3//1\\r\\n11,Conv,64,128,31,31,3//1\\r\\n12,Conv,64,128,31,31,3//1\\r\\n13,Max pool,64,128,15,15,3//2\\r\\n14,Conv,64,256,15,15,3//1\\r\\n15,Conv,64,256,15,14,3//1\\r\\n16,Conv,64,256,15,15,3//1\\r\\n17,Conv,64,256,15,15,3//1\\r\\n18,Max pool,64,256,7,7,3//2\\r\\n19,Dropout,64,256,7,7,\\r\\n20,Maxout (2-pool),64,512,,,\\r\\n21,Concat with image dim,64,514,,,\\r\\n22,Reshape\\xa0(merge eyes),32,1028,,,\\r\\n23,Dropout,32,1028,,,\\r\\n24,Maxout (2-pool),32,512,,,\\r\\n25,Dropout,32,512,,,\\r\\n26,Dense (linear),32,10,,,\\r\\n27,Reshape\\xa0(back to one eye),64,5,,,\\r\\n28,Apply softmax,64,5,,,\\r\\n[/table]\\r\\nWhere a//b\\xa0in the last column denotes pool or filter size a x a\\xa0with stride b x b.\\r\\nSome things that had also been changed:\\r\\n\\nUsing higher leakiness on the leaky rectify units, max(alpha*x, x), made a big difference on performance. I started using alpha=0.5 which worked very well. In the small tests I did, using alpha=0.3 or lower gave significantly lower scores.\\nInstead of doing the initial downscale with a factor five before processing images, I only downscaled by a factor two. It is unlikely to make a big difference but I was able to handle it computationally so there was not much reason not to.\\nThe oversampling of smaller classes was now done with a resulting uniform distribution of the classes. But now it also switched back somewhere during the training to the original training set distribution. This was done because initially I noticed the distribution of the predicted classes to be quite different from the training set distribution. However, this is not necessarily because of the oversampling (although you would expect it to have a significant effect!) and it appeared to be mostly because of the specific kappa loss optimisation (which takes into account the distributions of the predictions and the ground truth). It is also much more prone to overfitting when training for a long time on some samples which are 10 times more likely than others.\\nMaxout worked slightly better or at least as well as normal dense layers (but it had fewer parameters).\\n\\nVisual attention\\r\\nThroughout I also kept trying to find a way to work better with the high resolution input. I tried splitting the image into four (or sixteen) non-overlapping (or only slightly overlapping) squares, passing these through a smaller convnet in parallel and then combining these representations (by stacking them or pooling over them) but this did not seem to work. Something I was a little more hopeful about was using the spatial transformation layers from the Spatial Transformer Networks paper\\xa0from DeepMind. The intention was to use some coarse input to make the ST-modules direct their attention to some parts of the image in higher resolution (for example, potential microaneurysms!) and hopefully they would be able to detect those finer details.\\r\\n\\r\\nHowever, training this total architecture end-to-end, without initialising with a pre-trained net, was incredibly difficult at first. Even with norm constraints, lowered learning rates for certain components, smaller initialisation, etc., it was quite difficult, for one, to not have it diverge to some limiting values for the transformation parameters (which sort of makes sense). However, this might be partly because of the specific problem and/or my implementation. In the paper they also seem to work with pre-trained nets and this does seem like the way to go. Unfortunately, when I first tried this, it started overfitting quite significantly on the training set. I wish I had more time to explore this but since my more basic networks already worked so well and seemed to still allow for a decent amount of improvement, I had to prioritise those.\\r\\n\\r\\nSome small things I have learned:\\r\\n\\nIn general I found it is hard to have different \"networks\" competing with each other in one big architecture. One tends to smother the other. Even when both were initialised with some trained network. This is possibly partly because of my L2 regularisation on the network weights. I had to keep track of the activations of certain layers during training to make sure this didn\\'t happen.\\nIt wasn\\'t really mentioned anywhere in the paper but I used a sort of normalisation of my transformation parameters for the ST-modules using some sigmoid.\\nI think some parts of the network might still need to be fixed (i.e., fixing the parameters) during training to get it working.\\n\\r\\nIn general, I think these layers are very interesting but they certainly didn\\'t seem that straightforward to me. However, this might be partially because of this specific problem.\\r\\ntop\\n\\nThe endgame\\r\\nIn the last two to three weeks I was trying to wrap up any (very) experimental approaches and started focusing on looking more closely at the models (and their predictions) to see where I could maybe still improve. The \"basic architecture\" from the previous section barely changed for more than a month and most of my improvements came from optimising the learning process. My best (leaderboard) score (of about 0.828, which was top 3 for a few weeks) three weeks before the end of the competition came from a simple log mean ensemble from 2-3 models scoring individually around 0.82.\\r\\nCamera artifacts\\r\\nWhen looking at the two images of certain patients next to each other I noticed something which is harder to see when looking at single images: camera artifacts.\\r\\n\\r\\n[caption id=\"attachment_5104\" align=\"aligncenter\" width=\"612\"] Sample camera artifacts: tiny black dots on the outer left center and bigger black spots and stripes on the outer right.[/caption]\\r\\n\\r\\nSometimes they can even resemble microaneurysms (well, they can be tiny black dots, the size of a microaneurysm) and it can be very hard to distinguish between the two unless you have the other image. Even more, these artifacts seemed to be fairly common as well! The problem is: it is very unlikely my models at the time would be able to figure this out because\\r\\n\\nAlthough they take both eyes into account by merging the dense representations, these are very high level representations (fully connected layers get rid of a lot of the spatial information, see e.g. Dosovitskiy and Brox).\\nAugmentations were done on each eye separately, independent of patient. I did have a \"paired transformations\" option but at the time I did not see any big improvements using that.\\n\\r\\nOne thing I tried to take this into account was to merge the outputs of the first convolutional or pooling layer for each of the two eyes (or maybe even the input layer). Then theoretically the convolutional layer could be able to detect similar patterns in the left and\\xa0right eye. However, then I felt I was reducing the input space way too much after the merging (which was -- and had to be -- done in the first few layers) and thus, I instead took the outputs a\\xa0and b from the output of some layer (for the two eyes of a patient) and replaced them with ab and ba by stacking\\xa0them on the channel dimension (instead of simply replacing them both by ab). This way the net still had access to the other low level representations but I was not losing half the input space.\\r\\n\\r\\nUnfortunately this did not seem to help that much and I was running out of time such that I put this aside.\\r\\nPseudo-labeling\\r\\nNearer to the end of the competition I also started testing the clever pseudo-labeling idea from the ≋ Deep Sea ≋ team which uses the predictions from other (ensembles of) models on the test set to help guide\\xa0or regularise new models. Essentially, during training I added some test images to the batches, such that on average roughly 25% of the batch was comprised of images from the test set, together with the softmax predictions for those images from my best ensemble. This probably helped to push me to about 0.83\\xa0for a single model.\\r\\nBetter decoding\\r\\nFor a long time I simply used the class with the highest probability from my softmax output as my prediction (i.e., argmax). Even though this allowed me to get this far, it was obvious that this method is quite unstable since it doesn\\'t take the magnitude of the (other) probabilities into account, only their size relative to each other. To get around that, I used a similar ranking decoding\\xa0strategy as was used by some people in the CrowdFlower Search Results Relevance competition: first we convert the probabilities from the softmax output to one value by weighing each probability by the class label {0, 1, 2, 3, 4}\\r\\nweighted_probs = probs[:, 1] + probs[:, 2] * 2 + probs[:, 3] * 3 + probs[:, 4] * 4\\r\\n\\r\\n\\r\\nNext we rank the weighted probabilities from low to high and try to find the most optimal boundaries [x_0, x_1, x_2, x_3] to assign labels. I.e., the images with a weighted probability in [0, x_0] we assign the label 0, the images with a weighted probability in [x_0, x_1] we assign the label 1, etc. I used scipy\\'s minimize\\xa0function to quickly find some boundaries that optimise the kappa score on the train set of some ensemble. This probably could have been optimised better but I did not want to overtune and have the risk of badly overfitting.\\r\\n\\r\\nThis helped quite a bit, pushing some of my single models from 0.83 to 0.835. However, I was kind of surprised that changing the prediction distribution quite significantly with different boundaries did not result in bigger differences in scores.\\r\\nError distribution\\r\\nSomething that stood out was the fact that the models were highly variable on the validation sets. Although this was mostly because of the discrete metric, the noise in the dataset itself and my small validation set, I still wanted to try to find out more. If you look at the quadratic_kappa metric/loss above you can see that it is determined by two matrices: nom and denom. The kappa score is then given by 1 - sum(nom) / sum(denom). Hence, you want to minimise the nom sum and maximise the denom sum. The denominator denom is fairly stable and captures the distributions of the predictions and the targets. The nominator nom is the one which will give us the most insight into our model\\'s predictions. Let\\'s look at the nom and denom for some random well-performing model (+0.82 kappa) when I was using the simple argmax decoding:\\r\\n\\r\\n[caption id=\"attachment_5105\" align=\"aligncenter\" width=\"475\"] Normalised nom (left) and denom (right) for some well performing model with highest probability decoding (+0.82 kappa). Position (i, j) = predicted j for true label i.[/caption]\\r\\n\\r\\nWhereby the matrices are normalised and show the percentage of the total sum located in that position. You immediately notice something: the error is almost dominated by the errors of predicting class 0 when it was really class 2\\xa0(i.e., you predict a normal eye but it is really a moderate NPR eye). Initially this gives you some hope, since you think you should be able to bring down some misclassifications which are 2 classes apart and they would have an important impact on your score. However, the problem with this, and in general, was that the ground truth was not sure at all (since I\\'m not an ophthalmologist, not even a doctor!) and some really questionable classifications had already come to light.\\r\\n\\r\\nBecause the other ranking decoding strategy is only applied after training\\xa0and was done quite late in the competition, I don\\'t have any error distribution pictures for those yet. But I do remember the nom behaving quite similarly and most of the error (30-40%) coming from predicting class 0 when it was really class 2.\\r\\nEnsembling\\r\\nA good improvement then came from ensembling a few models using the mean of their log probabilities for each class, converting these to normal probabilities in [0, 1] again and using the ranking decoding\\xa0strategy from one of the previous paragraphs to assign labels to the images. A few candidate boundaries were determined using scipy\\'s minimize function on the kappa score of some ensembles. Doing this on the best models at the time pushed my models to +0.84.\\r\\ntop\\n\\nOther (not) tried approaches and papers\\r\\nSome other things that did not seem to work\\xa0(well enough):\\r\\n\\nBatch Normalisation: although it allowed for higher learning rates, it did not seem to speed up training all that much. (But I do believe the training of convnets should be able to be much faster than it is now.)\\nOther types of updates (e.g., ADAM): especially with the special loss based on the kappa metric I was not able to get it working well.\\nRemove pooling layers: replacing all\\xa0the 3//2 (pool size 3x3 and stride 2x2) max pooling layers with a 3//2 convolutional layer\\xa0made the training quite a bit slower but more importantly seemed to have pretty bad performance. (I was more hopeful about this!)\\nPReLU: tried it only briefly but it seemed to only worsen performance / result in (more) overfitting.\\nUsing 2//2 max pooling instead of 3//2 in the hope of being able to distinguish finer details better (since, for example, microaneurysms are most of the time located closely to some vessel).\\nSeveral architecture changes: more convolutional layers after the first, second or third pooling layer, bigger filter size in the first layer, replace the first 7//2 convolutional and 3//2 pooling layer by two 5//2 convolutional layers, etc. But it did not seem to help that much and I was struggling with the variance of the models.\\nSecond level model: at the very, very end I tried using second level models (such as ridge regression, Lasso, etc.) on the softmax outputs of many different models (instead of just using a log mean ensemble). Unfortunately, in my case, it was too messy to do this at the last minute with many models with small and different validation sets. But I would expect it to potentially work really well if you are able to regularise it enough.\\n\\r\\nSome things I haven\\'t tried:\\r\\n\\nSpecialist networks makes everything much more complicated and there was still the problem of some classes being very small. I also wasn\\'t sure at all that it would have helped.\\nHigher resolution input 512x512 was about the limit for networks that could train in 1-2 days on a GTX 980. One of the bigger bottlenecks then actually became the augmentation. I estimated from looking at some images (and hoped) that 512x512 would be able to capture most of the information.\\nImplementing one of the many more manual approaches there were many papers reviewing the different approaches tried in the literature for detecting diabetic retinopathy on eye images and most of them were quite manual (constructing several filters and image processing pipelines to extract features). I believe(d) quite firmly that a convnet should be able to do all of that (and more) but supplementing the network with some of these manual approaches might have been helpful since you could simply run it only once on higher resolution input (maybe helping you detect, for example, microaneurysms). However, I couldn\\'t bring myself to do all this manual image processing and wanted a more elegant solution.\\nUnsupervised training it would have been a bit of a gamble since I haven\\'t read that many recent experiences where unsupervised training was very helpful and it would have taken a lot of time. I certainly wanted to use the test set and was more than happy with the pseudo-labeling approach.\\nMultiple streams / scales / crops I felt I had more immediate problems to overcome and this would have made the model too big (for my resources).\\n\\r\\nI have read quite a few papers (and skimmed a lot more) and even though some of them were very interesting, I was quite limited in time and resources to try them all out. Some I have read and found interesting (not necessarily directly related to this competition):\\r\\n\\nEfficient Multiple Instance Convolutional Neural Networks for Gigapixel Resolution Image Classification: in the paper they iteratively select discriminative patches, patches whose hidden label equals the true label of the image, from a gigapixel resolution image and train a convnet on them. I decided the retina images did not need to be that high res to capture almost all the low level information to have to do something like this (I would guess maximum 1024x1024, probably smaller, which is still very big but more doable). It also felt a little unsuitable for these eye images that have so many different types of low level information (not the best explanation).\\nMultiple Object Recognition with Visual Attention\\xa0and Spatial Transformer Networks: these are two different methods to have visual attention on images: the first uses recurrent nets, the second normal, end-to-end convnets. This was all when I was searching for a way to be able to work with higher resolution images.\\nInverting Convolutional Networks with Convolutional Networks, Visualizing and Understanding Convolutional Networks\\xa0and Object Detectors Emerge in Deep Scene CNNs: all of these papers try to deduce what kind of information each layer holds, how well you can reconstruct the original image just from this information and some try to see how invariant this information is under certain transformations of the original image. If you\\'ve heard or read about DeepDream\\xa0(kind of hard not to have), there are some related things going on in the first paper but more theoretical. The more direct inspiration for DeepDream was the paper Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps which is also very interesting!\\n\\ntop\\n\\nConclusion\\r\\nThe actual process was quite a bit more lengthy and chaotic (especially at the end) but hopefully this captures the most important elements of my competition experience. All in all, a relatively basic architecture was able to achieve top scores in this competition. Nevertheless, the predictions always felt quite \"weak\" and my feeling is that there is still quite a bit of room for improvement. Without a doubt the biggest difficulty for me was dealing with the large amount of variance resulting from\\r\\n\\nthe noisy labels\\nthe extremely small classes (the two highest gradings together represent less than 5% of the samples!)\\nthe discrete metric which then very heavily punishes extreme misclassifications (and this in combination with the previous point!)\\n\\r\\nMaybe I should have worked harder on taking 2. into account, although, when checking the misclassifications of my models, I still felt that 1. was a very significant problem as well. It would be very interesting (and important!) to get some scores of other (human) expert raters.\\r\\n\\r\\nIn hindsight, optimising learning on the special kappa metric seemed to be much more important than optimising the architecture\\xa0and I did lose a lot of time trying to work with some more special architectures because I thought they might be needed to be able to finish high enough. It is also possible that the MSE objective was the better choice for this competition. I tested it briefly at the end and the performance seemed somewhat similar but I would expect it to be more stable than using my kappalogclipped\\xa0loss. I should also have explored different train/validation splits instead of always using 90%/10% splits. This possibly could have made evaluating the models more stable.\\r\\n\\r\\nPersonally, I very much enjoyed the competition. I learned a lot, am left with a whole bunch of ideas to work on and many pages of ugly code to rework. Hopefully next time I can compete together with other smart, motivated people since I miss having those interactions and doing everything on my own while also working a full time job was quite challenging and exhausting! Congratulations to the winners and all the people who contributed!\\r\\n\\r\\nAlso, when comparing results from this competition to other approaches or software, take into account that it does not necessarily make any sense because they may be training and/or evaluating on different datasets!\\r\\ntop\\n\\nCode, models and example activations\\r\\nEverything was trained on a NVIDIA GTX 980 in the beginning; this was the GPU for the desktop I was also working on, which wasn\\'t ideal. Therefore, later I also tried using the GRID K520 on AWS (even though it was at least two times slower). The code\\xa0is based on the code from the ≋ Deep Sea ≋ team that won the Kaggle National Data Science Bowl competition and uses mostly Lasagne (which uses Theano). Models (including static and learned parameters and data from the training) were dumped via cPickle and a quickly written script was used to produce images for each of these dumps summarising the model and its performance. For example:\\r\\n\\r\\n[caption id=\"attachment_5106\" align=\"aligncenter\" width=\"640\"] Example of a model image. It isn\\'t the prettiest but it has almost all the information I need. This model got about 0.824 on the public leaderboard. The long training time is mostly because of AWS. (Click to enlarge.)[/caption]\\r\\n\\r\\nThis way it was much easier to compare models and keep track of them. (This is just an example to give you an idea of one method I used to compare models. If you are curious about the specifics, you can find all the information in the code itself.)\\r\\n\\r\\nExample activations for one of the better models with the basic 512x512 architecture (each vertical block represents the output of one channel of the layer):\\r\\n\\r\\n[caption id=\"attachment_5107\" align=\"aligncenter\" width=\"640\"] Input layer without augmentations and normal rescaling. Labels (from left to right, up to down): 0, 0, 0, 2, 1, 0, 4, 0 -- 1, 0, 0, 2, 2, 0, 4, 0 -- 0, 0, 0, 0, 0, 2, 2, 0 -- 0, 0, 0, 0, 0, 2, 3, 0.[/caption]\\r\\n\\r\\n[caption id=\"attachment_5108\" align=\"aligncenter\" width=\"242\"] First layer, 7//2 convolutional layer activations. Larger image.[/caption]\\r\\n\\r\\n[caption id=\"attachment_5109\" align=\"aligncenter\" width=\"242\"] Second layer, 3//2 pooling layer activations. Larger image.[/caption]\\r\\n\\r\\n[caption id=\"attachment_5110\" align=\"aligncenter\" width=\"242\"] Third layer, 3//1 convolutional layer activations. Larger image.[/caption]\\r\\n\\r\\n[caption id=\"attachment_5111\" align=\"aligncenter\" width=\"242\"] Fourth layer, 3//1 convolutional layer activations. Larger image.[/caption]\\r\\n\\r\\n[caption id=\"attachment_5112\" align=\"aligncenter\" width=\"242\"] Fifth layer, 3//2 pooling layer activations. Larger image.[/caption]\\r\\ntop\\n\\nThanks\\r\\nI would like to thank Kaggle, California Healthcare Foundation and EyePACS for organising and/or sponsoring this challenging competition. Also many thanks to the wonderful developers and contributors of Theano and Lasagne\\xa0for their continuous effort on these libraries.\\r\\n\\r\\n\\n\\r\\n\\r\\nThis blog was originally published here on Jeffrey De Fauw\\'s site.\\r\\n\\r\\nRead other posts on the Diabetic Retinopathy competition by clicking the tag below.', 'The Diabetic Retinopathy (DR) competition asked participants to identify different stages of the eye disease in color fundus photographs\\xa0of the retina. The competition ran from February through July 2015 and the results were outstanding. By automating the early detection of DR, many more individuals will have access to diagnostic tools and treatment. Early detection of DR is key to slowing the disease\\'s progression to blindness.\\r\\n\\r\\nFourth place finishers, Julian De Wit and Daniel Hammack, share their approach here (including a simple recipe for using ConvNets on a noisy dataset).\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\nJulian De Wit:\\xa0I studied software engineering at the Technical University of Delft in the Netherlands.\\xa0I\\'ve always loved to implement complex (machine learing) algorithms.\\xa0Nowadays I work as a freelancer and mainly do machine learning projects.\\xa0I use Kaggle to battle-test ideas and try new algorithms and frameworks.\\r\\n\\r\\n[caption id=\"attachment_5129\" align=\"aligncenter\" width=\"382\"] Julian\\'s Kaggle profile[/caption]\\r\\n\\r\\n Daniel Hammack:\\xa0I have been involved in the machine learning field for a few years, starting with science fair in High School. I thought machine learning was pretty neat and mostly taught myself using the great resources online these days. Andrew Ng, Geoff Hinton, Steven Boyd, and Michael Collins all deserve a shoutout for having excellent lectures available online for free.\\r\\n\\r\\n[caption id=\"attachment_5130\" align=\"aligncenter\" width=\"378\"] Daniel\\'s Kaggle profile[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\n Daniel:\\xa0I had never done any image processing before, and also not worked with any medical data, so this competition was a great learning experience for me. I have been keeping up with the research on deep learning for computer vision, but I wanted the chance to try that knowledge out on some real data.\\r\\n\\r\\n Julian:\\xa0I have always been following advances in Neural networks and biologically inspired computing. Since the big breakthroughs with convnets I\\'ve been trying to solve practical problems with this technology. I was just building a feature trainer/localizer for a customer and used/tested this software to find DR features. However... in the end this was only a small part of the solution.\\r\\nWhat made you decide to enter this competition?\\n Daniel: In the last 3-5 years there has been incredible progress in computer vision mainly through the application of deep convolutional neural networks. I have been excitedly following this research for a few years and lately decided that I\\'d like to try it out. I was only taking one class during the summer and I knew I\\'d have some extra time so I decided to give the competition a go as a fun side project.\\r\\n\\r\\n Julian: I was working on a project that required me to find small objects in big images. This sounded very similar to the DR competition. I thought that competing on the challenge would give me insights for my project and the other way around. I started out making a classifier that localized and counted DR symptoms. However, in the end, an end-to-end convnet dominated the solution.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nLike the other top performing solutions, we used deep convolutional neural networks (CNNs) trained on consumer GPUs. CNNs have been shattering state-of-the-art records on a wide variety of computer vision datasets recently, and they do so without much domain knowledge required (which is great!).\\r\\n\\r\\n[caption id=\"attachment_5124\" align=\"aligncenter\" width=\"612\"] Note the differences in image color, brightness, contrast, etc. in some of the random training images.[/caption]\\r\\n\\r\\nFor preprocessing we both had similar approaches. There was a lot of noise in the data, so our normalization pipeline was designed to combat this.\\r\\n\\r\\nWe ended up first centering the eye, cropping out the extra blank space, downsizing, then applying brightness and contrast normalization. We trained models with varying input size -\\xa0Daniel used 256x256 for the duration of the competition and Julian experimented more with different sizes (larger and smaller). We found larger input sizes to work better, but at the cost of longer time to learn and the usage of more precious GPU memory (the biggest constraint to performance).\\r\\n\\r\\n[caption id=\"attachment_5125\" align=\"aligncenter\" width=\"600\"] Some examples of preprocessed images. Note that color, size, eye location, brightness, and contrast are now more uniform.[/caption]\\r\\nWhat was your most important insight into the data?\\n Daniel: The importance of symmetry. Given that the eye is a sphere, there are several classes of transformations we can apply that should not affect the label of the image. The major ones we used were mirroring and random rotations.\\r\\n\\r\\n[caption id=\"attachment_5126\" align=\"aligncenter\" width=\"600\"] An example of how rotations of the eye should not affect the diagnosis.[/caption]\\r\\n\\r\\n Julian: Again I realized just how good convnets work. I am really convinced that having a convnet as an extra pair of eyes in medical diagnosis would really be useful if not plain mandatory. With a tool I built here, you can compare the model predictions to the official diagnoses yourself. When the doctor and the model disagree, often it looks like the model is correct!\\r\\nWere you surprised by any of your findings?\\n Daniel: Yes! I was surprised to find that the ADAM learning rule seemingly made overfitting much easier (but also better training set performance). I was also very impressed by the performance of Batch Normalization. In the models where I used Batch Normalization, I observed much quicker convergence and was able to remove dropout.\\r\\n\\r\\n Julian: I used a 2nd level classifier over the output of the convnet(s) and some extra features. I thought this was a unique approach but somehow this was a necessary step to come to the the 80+ kappa scores.\\r\\nWhich tools did you use?\\n Julian: I started out with my own convnet but halfway the competition I switched to CXXNet with the CuDNN library. In the last week I tried SparseConvnet by Ben Graham. I really like his ideas and his implementation but, due to my inexperience with his software, I could not get a good score with it at such short notice.\\r\\n\\r\\n Daniel: I started with Pylearn2 and switched to Keras with about a month left in the competition. Keras is a great library - the code is very easy to understand and customize. I ended up doing quite a bit of customization during the competition, so having that ability was very important.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nOur solution was an ensemble of several models, of course, but the training time for a strong model was typically about 36 hours. We used SGD + momentum which worked pretty well. To generate predictions it\\'s about .02 seconds per image. We found test-time augmentation to be beneficial, so we actually ended up generating predictions on several different views of the same image (from 4-16).\\r\\nWords of Wisdom\\nWhat are your thoughts on teaming up?\\r\\nTeaming up with another Kaggler is a great learning experience. Not only do you get the benefit from ensembling your solutions, but often you can share complementary approaches to the same problem.\\r\\nWhat have you taken away from this competition?\\n Daniel: One interesting thing about this competition was the quick drop-off in performance on the leaderboard. This is probably because deep convnets are still relatively niche, but they are getting much closer to becoming useful without an intimate knowledge of deep learning. With that said, here is my convnet recipe. It\\'s heavily inspired from OxfordNet\\xa0and the results\\xa0of the Deep Sea team in the NDSB competition:\\r\\n\\r\\n1. Normalize your data. This means removing irrelevant attributes of the input data. If brightness and contrast are not important, then normalize them out.\\r\\n\\r\\n2. Set up your network. Start by alternating layers of 3x3 convolutions with ReLU activations and 2x2 stride 2 pooling. Each time you pool, increase the number of convolutional filters. I like to double the number of filters, others increase it linearly. Keep alternating these layers until the result is small enough to deal with in fully connected layers. Domain knowledge comes into play here, you need to know how far apart two pixels need to be before you can ignore their interaction. Stack a few fully connected ReLU or MaxOut layers on top.\\r\\n\\r\\n3. Initialize your network with a tested (theoretically sound) method. I like sqrt(2) scaled orthogonal initialization, but Julian had good results with the Xavier method\\xa0so I think either is fine. Good initialization is extremely important.\\r\\n\\r\\n4. Train with SGD + momentum. Exploit any label-preserving transformations to artificially enlarge your dataset. Use dropout, weight decay, and weight norm penalties if necessary.\\r\\n\\r\\nA single network following this scheme should have ended up in the top 10% on the leaderboard. Improving the result then takes some work, but the major things to try are: more convolutional layers, different activations, different numbers of filters, different pooling, different preprocessing, and other recent research (e.g. Batch Normalization). Intuition plus trial and error worked well for me in this competition.\\r\\nBios\\nJulian de Wit is a freelance software engineer. His main interest is to implement theoretical machine learning ideas into practical applications.\\r\\n\\r\\nDaniel Hammack is a researcher at Voloridge Investment Management and a student at the University of Central Florida. He is interested in unsupervised learning, natural language processing, computer vision, and recurrent neural networks.\\r\\n\\r\\n\\r\\n\\r\\nRead other posts on the Diabetic Retinopathy competition by clicking the tag below.', 'The Avito Context Ad Click competition asked Kagglers to predict if\\xa0users of Russia\\'s largest general classified website would click on context ads while\\xa0they browsed the site. The competition provided a truly robust dataset with\\xa0eight comprehensive relational tables of data on historical user browsing and search behavior, location, and more. Changsheng Gu (aka Gzs_iceberg) finished in second place by\\xa0using a combination of custom and public tools. You can read about Owen Zhang\\'s first place approach here.\\r\\n\\r\\n[caption id=\"attachment_5145\" align=\"aligncenter\" width=\"200\"] 456 data scientists on 414 teams competed to predict if users would click context ads[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI’m working as a software engineer at Bytedance, a Chinese company focusing on news recommendation. My main work is on advertising. Before that, I worked at Amazon doing optimization about warehouse capacity planning.\\r\\n\\r\\n[caption id=\"attachment_5146\" align=\"aligncenter\" width=\"300\"] Changsheng Gu\\'s Kaggle profile[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nMy work helps me, but past competitions about CTR prediction also make me learn a lot. The book “Pattern Recognition And Machine Learning” is also very useful.\\r\\nHow did you get started competing on Kaggle?\\r\\nTwo years ago, I was looking for a place to practice what I learned, then I found Kaggle in some online course.\\r\\nWhat made you decide to enter this competition?\\r\\nThe reason is very simple, I lost in Search Results Relevance competition. Seriously, I’m very interested in CTR prediction.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nFor preprocessing, hash tricks and negative down sampling.\\r\\n\\r\\nFor learning methods, FFM, FM and XGBoost.\\r\\nWhich tools did you use?\\r\\nI like to reinvent the wheel, if there’s enough time, I tend to write the tools by myself. But I also used XGBoost, pypy and lasagne in this competition.\\r\\nHow did you spend your time on this competition?\\r\\nIn the early stage, I focused on designing the data pipeline and validation set.\\r\\n\\r\\nThen, I spent a lot of time doing feature extraction.\\r\\n\\r\\nAfter that, I tried to tune the hyperparameters, I used hyperopt but didn\\'t see much\\xa0improvement. Then I decided to train different models by using different feature sets for model ensembling.\\r\\n\\r\\nFinally, tried to find a good way to do model ensembling.\\r\\nWords of wisdom\\nDo you have any advice for those just getting started in data science?\\r\\nI believe the devil is in the detail, I often found same methods but somebody always did better. So I encourage people reinventing the tools you used if you have enough time, it helps you understand why and how it works.\\r\\nBio\\r\\nChangsheng Gu earned his bachelor in Xidian University, China, in 2013. He started his career at Amazon, working on optimization about warehouse. Now he\\'s working as a software engineer in Bytedance, a Chinese company focusing on news recommendation. He’s interested in optimization and prediction problem.\\r\\n\\r\\n\\r\\n\\r\\nRead other posts on the Avito Context Ad Click\\xa0Prediction competition by clicking the tag below.', 'It was no surprise to see Owen Zhang, currently ranked #1\\xa0on Kaggle, take first place in the Avito Context Ad Click competition. Owen used previous competition experience, domain knowledge, and a fondness for XGBoost to finish ahead of 455 other data scientists. The competition gave participants plenty of\\xa0data\\xa0to explore, with eight comprehensive relational tables on historical user browsing and search behavior, location, and more.\\r\\n\\r\\n[caption id=\"attachment_5151\" align=\"aligncenter\" width=\"300\"] The competition ran June through July 2015[/caption]\\r\\n\\r\\nIn this blog, Owen shares what surprised him, what gave him an edge, and some words of wisdom for all expert and aspiring data scientists. You can read about Changsheng Gu\\'s\\xa0second place approach here.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI am a data scientist and probably should be considered a veteran Kaggler even before\\xa0joining this challenge\\r\\n\\r\\n[caption id=\"attachment_5148\" align=\"aligncenter\" width=\"300\"] Owen\\'s profile on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in\\xa0this competition?\\r\\nYes, the two previous CTR challenges (Criteo and Avazu) certainly help. Also, experience gained from some recent competitions helped.\\r\\n\\r\\n[caption id=\"attachment_5150\" align=\"aligncenter\" width=\"500\"] Owen\\'s finishes in the Avazu and Criteo competitions[/caption]\\r\\nHow did you get started competing on Kaggle?\\r\\nI started to learn more about predictive modeling in 2011. The community has been great and I learned so much since then.\\r\\nWhat made you decide to enter this competition?\\r\\nI like competition with lots of data (so less leaderboard shakeup), in a domain that I understand, with an\\xa0interesting structure. This competition is a perfect fit in those aspects.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nI did quite a bit of manual feature engineering and my models are entirely based on xgboost. Feature engineering is a combination of “brutal force” (trying different transformations, etc that I know) and “heuristic” (trying to think about drivers of the target in real world settings). One thing I learned recently was entropy based features, which were useful in my model.\\r\\nWhat was your most important insight into the data?\\r\\nThere was some “soft leakage”, such as how many other ads are displayed for a given query. Those features are always very powerful, but provides only limited impact in real world applications.\\r\\n\\r\\nUnlike Criteo and Avazu, where FFM and VW outperformed GBM, in this competition GBM (xgboost) easily outperformed FFM and VW.\\r\\nWere you surprised by any of your findings?\\r\\nYes, I thought FFM and VW were required for click through rate, but apparently this is not the case.\\r\\nWhich tools did you use?\\r\\nMy solution was entirely written in R. I used packages including data.table, tau, irlba, and xgboost.\\r\\nHow did you spend your time on this competition?\\r\\nAbout ⅔ of time on feature engineering and ⅓ of time on model tuning.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nIt takes about 20 hours.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\n- With a good computer, R can process “big data” too\\r\\n- Always write data processing code with scalability in mind\\r\\n- When in doubt, use xgboost\\r\\nDo you have any advice for those just getting started in data science?\\r\\n- Don’t be afraid to try things and ask questions\\r\\n- Get the fastest computer you can afford\\r\\n- Try to understand the problem/domain, don’t build models “blindly” unless you have to\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other\\xa0Kagglers?\\r\\nIt would be fun to predict future performance of Kagglers.\\r\\n\\r\\nWe could\\xa0make past performance available and then predict ranking for the next N competitions. The only down side of this set up is that we have to wait for several competitions to start and finish to evaluate the results. But I am sure it would be fun.\\r\\n\\r\\nAlso some recruiting related comp might be very interesting as well. For example, we can try to predict which job posts on Kaggle generate most interest.\\r\\nBio\\nOwen Zhang currently works as a data scientist at DataRobot a Boston based startup company.\\xa0His education background is in engineering, with a master’s degree from U of Toronto, Canada, and bachelor’s from U of Science and Technology of China.\\xa0Before joining DataRobot, he spent more a decade in several U.S. based property and casualty insurance companies, last one being AIG\\xa0in New York.\\r\\n\\r\\n\\r\\n\\r\\nRead other posts on the Avito Context Ad Click\\xa0Prediction competition by clicking the tag below.', 'Ben Graham finished at the top of the leaderboard in the high-profile Diabetic Retinopathy competition. In this blog, he shares his approach on a high-level with key takeaways. Ben finished 3rd in the National Data Science Bowl, a competition that helped develop many of the approaches used to compete in this challenge.\\r\\n\\r\\n[caption id=\"attachment_5174\" align=\"aligncenter\" width=\"300\"] Ben\\'s Kaggle profile[/caption]\\r\\nThe Basics\\nWhat made you decide to enter this competition?\\r\\nI wanted to experiment with training CNNs with larger images to see what kind of architectures would work well. Medical images can in some ways be more challenging than classifying regular photos as the important features can be very small.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nFor preprocessing, I first scaled the images to a given radius. I then subtracted local average color to reduce differences in lighting.\\r\\n\\r\\n\\r\\n\\r\\nFor supervised learning, I experimented with convolutional neural network architectures. To map the network predictions to the integer labels needed for the competition, I used a random forest so that I could combine the data from the two eyes to make each prediction.\\r\\n\\r\\n\\nWere you surprised by any of your findings?\\r\\nI was surprised by a couple of things. First, that increasing the scale of the images beyond radius=270 pixels did not seem to help. I was expecting the existence of very small features, only visible at higher resolutions, to tip the balance in favor of larger images. Perhaps the increase in processing times for larger images was too great.\\r\\n\\r\\nI was also surprised by the fact that ensembling (taking multiple views of each image, and combining the results of different networks) did very little to improve accuracy. This is rather different to the case of normal photographs, where ensembling can make a huge difference.\\r\\nWhich tools did you use?\\r\\nPython and OpenCV for preprocessing. SparseConvNet for processing. I was curious to see if I could sparsify the images during preprocessing; however, due to time constraints I didn\\'t get that working. SparseConvNet implements fractional max-pooling, which allowed me to experiment with different types of spatial data aggregation.\\r\\nBio\\nBen Graham is an Assistant Professor at the University of Warwick, UK. His research interests are probabilistic spatial models such as percolation, and machine learning.\\r\\n\\r\\n\\r\\n\\r\\nRead other posts on the Diabetic Retinopathy competition by clicking the tag below.\\r\\n\\r\\n\\xa0', 'This summer, the\\xa0ICDM 2015\\xa0conference\\xa0sponsored\\xa0a competition focused\\xa0on making\\xa0individual user connections across multiple digital devices. Top teams were invited to submit\\xa0a paper for\\xa0presentation at an ICDM workshop.\\r\\n\\r\\nRoberto Diaz, competing as team \"CookieMonster\", took 3rd place. In this blog, he shares how he became a Kaggle addict, what\\xa0he values in a competition, and most importantly, details on his approach to this unique dataset. Congrats to Roberto for achieving his goal of becoming a top 100 Kaggle user!\\r\\n\\r\\n[caption id=\"attachment_5179\" align=\"aligncenter\" width=\"500\"] 407 players on\\xa0340 teams competed\\xa0in ICDM 2015: Drawbridge Cross-Device Connections[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nIn addition to being a Kaggle addict, I am a researcher at\\xa0Treelogic working in the machine learning area. In parallel I work on my PhD thesis at the University Carlos III de Madrid focused on the parallelization of Kernel Methods.\\r\\n\\r\\n[caption id=\"attachment_5183\" align=\"aligncenter\" width=\"342\"] Roberto\\'s Kaggle profile[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nI didn\\'t have any knowledge about this domain. The\\xa0topic is quite new and I couldn\\'t find any papers related to\\xa0this problem, most probably because there are not public datasets.\\r\\nHow did you get started competing on Kaggle?\\r\\nI started on the first Facebook competition a long time ago. A friend of mine was taking part in the challenge and he encouraged me to compete. That caught my initial curiosity so I accessed the challenge\\'s forum and I read a post with a solution that scored quite well\\xa0on the leaderboard and I thought \"I think I can do better than that\". At the end I scored 9th on the leaderboard.\\r\\n\\r\\nFor my second challenge (EMC Israel Data science challenge) I was on a team with my PhD mates. We finished 3rd receiving a prize.\\r\\n\\r\\nAfter that it was too late for me, I had become an addict.\\r\\nWhat made you decide to enter this competition?\\r\\nThe things I value most in a challenge are:\\r\\n\\nA conference associated to the challenge: It is a good opportunity to publish your results. For example, my solution in the Higgs Boson Machine Learning Challenge:\\n\\nDÌaz-Morales, R., & Navia-V·zquez, A. (2015, September). Optimization of AMS using Weighted AUC optimized models. In *JMLR: Workshop and Conference Proceedings*, Vol. 42, pp. 109-127.\\n\\nA domain unknown to me: It is the best way to learn about how to work with a different kind of data.\\nThe need to preprocess and extract the features from raw data to build the dataset: It gives you the chance to use your intuition and imagination.\\n\\r\\nThis challenge looked very interesting to me because all the conditions were met.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nIn this challenge we had a list of devices and a list of cookies and we had to tell what cookies belonged to\\xa0the person using the device.\\r\\n\\r\\nThe most important part was the feature extraction procedure, they had to contain information about the relation between devices and cookies (for example, the number of IP addresses visited by each one and by both of them).\\r\\n\\r\\nOnce I had the features I tried simple supervised machine learning algorithms and complex ones (my winning methodology was Semi-Supervised learning procedure using Gradient Boosting + Bagging) and the score just grew up from 0.865 to 0.88.\\r\\nWhat was your most important insight into the data?\\r\\nA key part of the solution was the initial selection of candidates and the post processing:\\r\\n\\nInitial selection: It was not possible to create a training set containing every combination of devices and cookies due to the high number of them. In order to reduce the initial complexity of the problem and to create an affordable dataset, some basic rules were created to obtain an initial reduced set of candidate cookies for every device. The rules are based on the IP addresses that both device and cookie have in common and how frequent they are in other devices and cookies.\\nSupervised Learning: Every pattern in the training and test set represents a device/candidate cookie pair obtained by the previous step and contains information about the device (Operating System (OS), Country, ...), the cookie (Cookie Browser Version, Cookie Computer OS,...) and the relation between them (number of IP addresses shared by both device and cookie, number of other cookies with the same handle than the cookie,...).\\nPost Processing: If the initial selection of candidates did not find a candidate with enough likelihood (logistic output of the classifier) we choose a new set of candidate cookies selecting every cookie that shares an IP address with the device and we score them using the classifier.\\n\\r\\nThe initial selection of candidates reduces the complexity of the problem and the post processing step find out most of the device/cookie pairs lost by that initial selection strategy.\\r\\nWere you surprised by any of your findings?\\r\\nYes. When I sorted the scores obtained by the classifier for every candidate I saw that if the first score is high and the second is very low, is extremely likely that the first cookie belongs to the device. I made use of this information to create semi-supervised learning procedure updating some features in the training set and retraining the algorithm again with this new information to improve the results.\\r\\n\\r\\nThis picture shows the F05 score and the percentage of devices that fulfill the condition when we match devices and the first cookies candidate when the second candidate scores less than a threshold:\\r\\n\\r\\n\\nWhich tools did you use?\\r\\nThis solution has been implemented in python and uses the external software XGBoost.\\r\\n\\r\\nThe libraries of python used were:\\r\\n\\nNumpy\\nScipy\\nSklearn\\n\\nHow did you spend your time on this competition?\\r\\nI spent about 20% of the time in feature engineering, 10% in the supervised learning part and 70% eagerly awaiting for the results.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nToo much, the training procedure takes around 9 hours using 12 cores.\\r\\n\\r\\nThe prediction procedure takes around 30 minutes, it is necessary to extract some features from the relational database.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nI was trying to reach a place in top 100 of the users global ranking and I finally got it.\\r\\n\\r\\nRegarding the challenge:\\r\\n\\nI have learned how useful it is to save intermediate results in order to not repeat the full training procedure only to change the last steps of the algorithm.\\nA paper with my approach to the problem in the next ICDM 2015 workshop dedicated to the challenge.\\n\\nDo you have any advice for those just getting started in data science?\\r\\n\"All hope abandon, ye who enter here\".\\r\\n\\r\\nNo, seriously, at the beginning you may feel frustrated because it is difficult area but you are in the correct place if:\\r\\n\\nYou love statistics more than other software engineers\\nYou love software engineering more than other statisticians.\\n\\nBio\\nRoberto Diaz\\xa0is a researcher in the R&D department of Treelogic, a SME Spanish company focused on Machine Learning, Computer Vision and Big Data that takes part in many EU Research and Innovarions programmes.\\xa0In parallel he works on his PhD thesis in the University Carlos III de Madrid focused on the parallelization of Kernel Methods.', 'The Caterpillar Tube Pricing competition asked teams to use detailed tube, component, and volume data to predict the price a supplier would quote for the manufacturing of different tube assemblies. Team \"Gilberto | Josef | Leustago | Mario\" finished in first place, bringing in new players (with new models) near the team merger deadline to create a strong ensemble. Feature engineering played a key role in developing their individual models, and team discussions in the last week of the competition brought them to the top of the leaderboard.\\r\\n\\r\\n[caption id=\"attachment_5185\" align=\"aligncenter\" width=\"500\"] 1,452 players on 1,323 teams competed from June 19 through August 21, 2015[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\nMario: Since February 2014 I have been taking MOOCs, reading papers, watching lectures about data science/machine learning. Now I work as a freelance data scientist in projects from startups and consulting companies, but I am currently looking for a data scientist position (preferably remote) at a company.\\r\\n\\r\\n[caption id=\"attachment_5189\" align=\"aligncenter\" width=\"300\"] Mario\\'s profile on Kaggle[/caption]\\r\\n\\r\\nJosef: I\\'m working as a Data Scientist for the Otto Group in Hamburg. Additionally, I\\'m currently working on my PhD thesis in Machine Learning at the University of Leipzig.\\r\\n\\r\\n[caption id=\"attachment_5190\" align=\"aligncenter\" width=\"300\"] Josef\\'s profile on Kaggle[/caption]\\r\\n\\r\\nLucas: I work as a Senior Data Scientist at\\xa0Niddel. At Niddel we\\'re building a system to identify security threats based on machine learning. We are building something very exciting and advanced there.\\r\\n\\r\\n[caption id=\"attachment_5191\" align=\"aligncenter\" width=\"300\"] Lucas\\' (aka Leustagos)\\xa0profile on Kaggle[/caption]\\r\\n\\r\\nGilberto: I am an electronic engineer and have experience with software for at least 18 years. Since 2012 I\\'ve participated in Kaggle competitions.\\r\\n\\r\\n[caption id=\"attachment_5192\" align=\"aligncenter\" width=\"300\"] Gilberto\\'s profile on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\nMario: I have worked in companies that sold items that looked like tubes, but nothing really relevant for the competition.\\r\\nJosef: Well, I have a basic understanding of what a tube is.\\r\\nLucas: Not a clue.\\r\\nGilberto: No.\\r\\nHow did you get started competing on Kaggle?\\nMario: In 2013 I entered the Big Data Combine, but I didn’t know what I was doing. In 2014 I participated for the first time, knowing what I was doing, in the Avito competition, and got my first Top 25%.\\r\\nJosef: I joined Kaggle about 3 years ago because I had some theoretical knowledge about machine learning and wanted to apply it to some interesting problems. I\\'ve been quite active since then and I usually learn something new in every competition, which is great fun.\\r\\nLucas: I joined Kaggle in the end of 2011, just after doing the wonderful Andrew Ng Machine Learning course available on Coursera. After completing that\\xa0I have been learning by myself by reading ML forums, FAQs, previous winners\\' posts on Kaggle and such.\\r\\nGilberto: Despite being an engineer I have always been interested in machine learning algorithms. In 2012 I found Kaggle via a Google search.\\r\\nWhat made you decide to enter this competition?\\nMario: It looked like a competition that could benefit from feature engineering and understanding the data. I am trying to get better at this, which I consider one of the core skills of a good data scientist.\\r\\nJosef: I like competitions were feature engineering is a major part of the problem. This competition looked exactly like that, given all the different input files.\\r\\nLucas: I like competitions that have a time series component and this one also looked like it was possible to build a proper validation set to try out ideas.\\r\\nGilberto: I always try to participate in all competitions and test the performance of my algorithms. I usually choose the competitions that I feel more comfortable programming.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\nMario: There were many different files about tubes, so the only preprocessing was joining relevant files together. After this step, I did a lot of feature engineering, and focused on using XGBoost and Regularized Greedy Forests.\\r\\nJosef: I focused on feature engineering, which was very important in this competition. For example, aggregated statistics for the tubes, the supplier or the material proved to be useful.\\r\\nIt was also very important to predict different transformations of the cost, like the logarithm.\\r\\nAs for the supervised learning methods, I achieved the best results with gradient boosted tree models, like the awesome XGBoost. I also spend some time tinkering with deep neural nets, but couldn\\'t get near the best XGBoost model.\\r\\nLucas: For the first part of the competition I focused on feature engineering. I tried to extract the proper relationship between the train and test sets to build a validation and tried to at least beat the famous \\'beat the benchmark\\' script using my own features and code. Finding some soft leaks also get our scores on par with the other teams on lb. For this task I used the pandas python module to check statistics and xgboost to train a simple model and try out ideas fast.\\r\\nGilberto: Preprocessing steps was basically setting up the dataset in a proper way like one-hot encoding categorical variables and calculating some physical characteristics of the tubes based in quantity. The preferred supervised methods were libFM and XGboost.\\r\\nWhat was your most important insight into the data?\\nMario: Calculating the total component weight for a tube, finding that expensive tubes had a smaller “max quantity” when their prices varied with quantity, and the fact that the tube ids were not random, and had predictive power.\\r\\nJosef: Leakage played an important part again. Using the tube IDs as a feature turned out to be crucial.\\r\\nLucas: Finding some soft leaks related to tubes in the same action pool being both on train and test sets. Understanding a bit of the highly non linear relationship between quantity and cost helped too.\\r\\nGilberto: Training over transformed cost function improved a lot the final model performance.\\r\\n\\r\\n[caption id=\"attachment_5186\" align=\"aligncenter\" width=\"614\"] Visualizing Important Variables script by another competition participant, saihttam[/caption]\\r\\nWere you surprised by any of your findings?\\nMario: I have been reading about data leaks for some time, so I was happy about finding the tube id pattern. Besides that, I love single models that do well, and my best single model was an XGBoost that could get the 10th place by itself.\\r\\nGilberto: Yes, the assembly_id feature improved a lot the performance. Also some physical features like volume, area and weight made an important role.\\r\\nWhich tools did you use?\\nMario: XGBoost, Regularized Greedy Forests, scikit-learn and Keras.\\r\\nJosef: That was the first competition for me were I used Python 3.4 for everything and it worked out very well. I especially liked the pipelines and feature unions of sklearn.\\r\\nLucas: Used python3 all the way, and modules like pandas, sklearn, Keras and XGboost.\\r\\nGilberto: Basically R.\\r\\nHow did you spend your time on this competition?\\nMario: When I was on my own I did a lot of feature engineering, when I teamed up, it was all about creating new models and ensembling.\\r\\nJosef: I spend about 1/2 of my time on feature engineering and 1/2 on model selection, fine-tuning and blending.\\r\\nLucas: Half of my time on building diverse models by using different outputs transformations, training parameters, and modeling algorithms. The other half I spent building a proper ensemble stack that blended all models from our team.\\r\\nGilberto: Most of the time I spent testing different algorithms over some prebuilt datasets.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\nMario: Each of my models took, on average, 2h 30m to run. To generate the predictions for stacking and the test set it should take around 20h.\\r\\nJosef: The training of my models took quite some time. Creating all the out-of-fold predictions for the final blending and creating the predictions for the test set took up to 15h.\\r\\nLucas: Depends on the model. We have many. Some take just a few minutes, others can take up to 10 hours. We used\\xa0many combinations of output transformations and parameters jammed together.\\r\\nGilberto: It depends of the model. LibFM model runs fast even bagging it many times. Bagged XGboost takes a little more time, it\\'s about 3~4 hours per model on a 8 core cpu.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\nMario: Teaming up teaches you a lot. I always wanted to team up with more experienced data scientists, and this was a great opportunity. And a good model can take you far in Kaggle, but ensembling makes you win.\\r\\nJosef: Team work and blending seem to get more and more important for winning Kaggle competitions.\\r\\nAll of the Top 7 teams consisted of at least three participants. In some of my previous competitions, you could win by finding all the important features and training some solid models. That was simply not enough for this competition. You also had to build a very strong ensemble of different models.\\r\\nLucas: Team work is playing a very important role in winning Kaggle competitions. The diversity of approaches and ideas usually leads to better models and data processing.\\r\\nGilberto: Not much.\\r\\n\\r\\n[caption id=\"attachment_5187\" align=\"aligncenter\" width=\"562\"] The Top 100 Users With Most Team Memberships script by mlandry shows high ranked Kagglers perform frequently on teams[/caption]\\r\\nDo you have any advice for those just getting started in data science?\\nMario: Take MOOCs, read papers, and watch lectures to understand the theory. But consider Kaggle as a “Projects course”, and learn from experience too.\\r\\nJosef: When I started with Data Science, I simply choose a competition and spent all the time I had on it.\\r\\nI think, there is a strong positive correlation between the time spent for a competition and the final rank: The more time you invest, the better your final rank will be.\\r\\nLucas: Do online courses, read the forums and the previous winners\\' posts. If that isn’t enough, ask on the forums. Some people aren\\'t cheap on advice. Just make sure you are asking a question that doesn\\'t have an\\xa0easy to find answer, because that would be just laziness and not curiosity.\\r\\nGilberto: Take some online training, read the specific forums and start coding. Learning by hits and mistakes is the best way to improve knowledge.\\r\\nHow did competing on a team help you succeed?\\nJosef: I joined our team very late, about 10 days before the end of the competition. Simply adding my models to our ensemble helped us to achieve our final score.\\r\\nWe also merged some of our data sets and trained some new models on our combined features, which helped too.\\r\\nI guess, we wouldn\\'t have been able to win without our collaborative effort.\\r\\nLucas: Competing on a team helped to build distinct approaches for the same problem and thats is very useful when ensembling. We also had an online discussion chat that surely guided each of us to improve even more on our solutions.\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\nJosef: I already did that! You can find the result at the “Otto Group Product Classification Challenge”.\\r\\nLucas: If I had the proper dataset and patient history, I would try to build a model to match cancer patients with the proper medicine to improve their chances.\\r\\nGilberto: I would\\xa0start a competition to find out who\\xa0is going to win a new competition based just on users\\' competitions historical data ;-)\\r\\nWhat is your dream job?\\nGilberto: My dream job has a dash of machine learning, a handful of money, a lot of free time to enjoy my\\xa0family and the opportunity to help save the world.\\r\\nBio\\nMario Filho is a self-taught data scientist. He currently works as a freelance data scientist, doing projects for startups and consulting companies. He is interested in validation techniques, feature engineering and tree ensembles.\\r\\nJosef Feigl is currently working as a Data Scientist for the Otto Group in Hamburg and writing his PhD thesis in Machine Learning at the University of Leipzig. He holds a diploma in Business Mathematics and is interested in recommender systems and neural networks.\\r\\nLucas is a Senior Data Scientist currently working to improve network security. He is also an enthusiastic Kaggler that loves to learn something new and tackle new challenges.\\r\\nGilberto is an electronics engineer with a M.S. in telecommunications. For the past 16 years he\\'s been working as an engineer for big multinationals like Siemens and Nokia and later as an automation engineer for Petrobras Brazil. His main interests are in machine learning and electronics areas.\\r\\n\\r\\n\\r\\n\\r\\nRead an interview with\\xa0the 3rd\\xa0place team\\xa0in the Caterpillar Tube Pricing\\xa0competition by clicking the tag below.', 'The hugely popular Liberty Mutual Group: Property Inspection Prediction competition wrapped up on August 28, 2015 with Qingchen Wang at the top of a crowded\\xa0leaderboard. A total of 2,362 players on 2,236 teams competed to predict how many hazards a property inspector would count during a home\\xa0inspection.\\r\\n\\r\\nThis blog outlines Qingchen\\'s approach, and how a relative newbie to Kaggle competitions learned\\xa0from the community and ultimately took\\xa0first place.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI did my bachelor’s in computer science. After working for a few months at EA Sports as a software engineer I felt the strong need to learn statistics and machine learning as the problems that interested me the most were about predicting things algorithmically. Since then I’ve earned master’s degrees in machine learning and business and I’ve just started a PhD in marketing analytics.\\r\\n\\r\\n[caption id=\"attachment_5196\" align=\"aligncenter\" width=\"300\"] Qingchen\\'s profile on Kaggle[/caption]\\r\\nHow did you get started competing on Kaggle?\\r\\nI had an applied machine learning course during my master’s at UCL and the course project was to compete on the Heritage Health Prize. Although at the time I didn’t really know what I was doing it was still a very enjoyable experience. I’ve competed briefly in other competitions since, but this was the first time I’ve been able to take part in a competition from start to finish and it turned out to have been quite a rewarding experience.\\r\\nWhat made you decide to enter this competition?\\r\\nI was in a period of unemployment so I decided to work on data science competitions full-time until I found something else to do. I actually wanted to do the Caterpillar competition at first but decided to give this one a quick go since the data didn’t require any preprocessing to start. My early submissions were not very good so I became determined to improve and ended up spending the whole time doing this.\\r\\n\\r\\nWhat made this competition so rewarding was how much I learned. As more or less a Kaggle newbie, I spent the whole two months trying and learning new things. I hadn’t known about methods like gradient boosting trees or tricks like stacking/ blending and the variety of ways to handle categorical variables. At the same time, it was probably the intuition that I developed through previous education that set my model apart from some of the other competitors so I was able to validate my existing knowledge as well.\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nI have zero prior experience or domain knowledge for this competition. It’s interesting because during the middle of the competition I hit a wall and a number of the top-10 ranked competitors have worked in the insurance industry so I thought maybe they had some domain knowledge which gave them an advantage. It turned out to not be the case. As far as data science competitions go, I think this one was rather straightforward.\\r\\n\\r\\n[caption id=\"attachment_5198\" align=\"aligncenter\" width=\"614\"] Histogram of all fields in the dataset with labels. Script by competition participant, Rajiv Shah[/caption]\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nI used only XGBoost (tried others but none of them performed well enough to end up in my ensemble). The key to my result was that I also did binary transformation of hazards which turned the regression problem into a set of classification problems. I noticed that some other people also tried this method through the forum thread but it seems that they didn’t go far enough with the binary transformation as that was the best performing part of my ensemble.\\r\\n\\r\\nI also played with different encodings of categorical variables and interactions, nothing sophisticated, just the standard tricks that many others have used.\\r\\nWere you surprised by any of your findings?\\r\\nI’m surprised by how poor our prediction accuracies were. This seemed like a problem that was well suited for data science algorithms and it was both disappointing and exciting to see such high prediction errors. I guess that’s the difference between real life and the toy examples in courses.\\r\\nWhich tools did you use?\\r\\nI only used XGBoost. It’s really been a learning experience for me as I entered this competition having no idea what gradient boosted trees was. After throwing random forests at the problem and getting nowhere near the top of the leaderboard, I installed XGBoost and worked really hard on tuning its parameters.\\r\\n\\r\\n[caption id=\"attachment_5197\" align=\"aligncenter\" width=\"300\"] XGBoost fans or those new to boosting, check out this great blog by Jeremy Kun on the math behind boosting and why it doesn\\'t overfit[/caption]\\r\\nHow did you spend your time on this competition?\\r\\nSince the variables were anonymous there wasn’t much feature engineering to be done. Instead I treated feature engineering as just another parameter to tune and spent all of my time tuning parameters. My final solution was an ensemble of different specifications so there were a lot of parameters to tune.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nThe combination of training and prediction of my winning solution takes about 2 hours on my personal laptop (2.2ghz Intel i7 processor).\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nOne thing that I learned which I’ve always overlooked before is that parameter tuning really goes a long way in performance improvements. While in absolute terms it may not be much, in terms of leaderboard improvement it can be of great value. Of course, without the community and the public scripts I wouldn’t have won and may still not know about gradient boosted trees, so a big thanks to all of the people who shared their ideas and code. I learned so much from both sources so it’s been a worthwhile experience.\\r\\n\\r\\n[caption id=\"attachment_5199\" align=\"aligncenter\" width=\"500\"] Click through to an animated view of the community\\'s leaderboard progression over time, and the influence of benchmark code sharing. Script by competition participant,\\xa0inversion[/caption]\\r\\nDo you have any advice for those just getting started in data science?\\r\\nFor those who don’t already have an established field, I strongly endorse education. All of my data science experience and expertise came from courses taken during my bachelor’s and master’s degrees. I believe that without already having been so well educated in machine learning I wouldn’t have been able to adapt so quickly to the new methods used in practice and the tricks that people have talked about.\\r\\n\\r\\nThere are now a number of very good education programs in data science which I suggest that everyone who wants to start in data science to look into. For those who already have their own established fields and are doing data science on the side, I think their own approaches could be very useful when combined with the standard machine learning methods. It’s always important to think outside the box and it’s all the more rewarding when you bring in your own ideas and get them to work.\\r\\n\\r\\nFinally, don’t be afraid to hit walls and grind through long periods of trying out ideas that don’t work. A failed idea gets you one closer to a successful idea, and having many failed ideas often can result in a string of ideas that work down the road. Throughout this competition I tried every idea I thought of and only a few worked. It was a combination of patience, curiosity, and optimism that got me through these two months. The same applies to learning the technical aspects of machine learning and data science. I still remember the pain that my classmates and I endured in the machine learning courses.\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nI’m a sports junkie so I’d love to see some competitions on sports analytics. It’s a shame that I missed the one on March Madness predictions earlier this year. Maybe one day I’ll really run a competition on this stuff.\\r\\n\\r\\nEditor\\'s note: March Machine Learning Mania is an annual competition so you can catch it again in 2016!\\nWhat is your dream job?\\r\\nMy dream job is to lead a data science team, preferably in an industry that’s full of new and interesting prediction problems. I’d be just as happy as a data scientist though, but it’s always nice to have greater responsibilities.\\r\\nBio\\nQingchen Wang is a PhD student in marketing analytics at the Amsterdam Business School, VU Amsterdam, and ORTEC. His interests are in applications of machine learning methods to complex real world problems in all domains. He has a bachelor’s degree in computer science and biology from the University of British Columbia, a master’s degree in machine learning from University College London, and a master’s degree in business administration from INSEAD. In his free time Qingchen competes in data science competitions and reads about sports.', 'The Grasp-and-Lift EEG Detection competition asked participants to identify when a hand was grasping, lifting, and replacing an object using\\xa0EEG data that was taken from healthy subjects as they performed\\xa0these activities. The competition was sponsored by the WAY Consortium\\xa0(Wearable interfaces for hAnd function recoverY) as part of their work towards developing better prosthetic devices for patients with amputation or neurological disabilities that have lost hand function.\\r\\n\\r\\nTeam daheimao finished in second\\xa0place using recurrent convolutional neural networks (RCNN). In this blog Ming Liang outlines his competition approach, including\\xa0RCNNs\\xa0architecture and inspiration.\\r\\n\\r\\n[caption id=\"attachment_5208\" align=\"aligncenter\" width=\"300\"] 452 players & 379 teams competed[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI am a Ph.D student in the department of computer science at Tsinghua University. My research interests include neural networks and computer vision.\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nNo. My past research projects were all related to vision. Basically I know little about EEG, although I spent several years in the School of Medicine.\\r\\n\\r\\n[caption id=\"attachment_5207\" align=\"aligncenter\" width=\"300\"] Kaggle profile for Ming Liang (aka daheimao)[/caption]\\r\\nHow did you get started competing on Kaggle?\\r\\nThis is my first Kaggle competition. I knew Kaggle from the winners’ blogs. In my view, the winners were cool data magicians. I wanted to be a member of that so I registered for my Kaggle account about 9 months ago. But I did not have enough time for a competition until recently.\\r\\nWhat made you decided to enter this competition?\\r\\nI recently proposed a neural network model for computer vision tasks. The model is called recurrent convolutional neural network (RCNN). I entered this challenge to evaluate the performance of this model in processing time series data, which has very different statistics from image data. If a good score was achieved, it would\\xa0be a good advertisement for my work. :)\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nAs I said, I joined this competition to evaluate the performance of RCNN. So most of the\\xa0models are RCNN, except for some convolutional neural network (CNN) models used as baseline. It seems that RCNN performs well on this dataset. The\\xa0best single model achieves a private LB score of 0.97661. I guess this may be the best single model of this competition?\\r\\n\\r\\nRCNN is a natural integration of CNN and recurrent neural network (RNN). It is composed of a stack of recurrent convolutional layers (RCL), which is obtained by incorporating recurrent connections into a convolution layer. This idea is inspired by the anatomical findings on the visual system, where intra-layer recurrent connections are abundant. In the following figure, the red arrows denote feed-forward connections and the blue the red arrows denote recurrent connections. As a consequence, a feed-forward convolutional layer which is typically feed-forward can be unfolded for several iterations just like a RNN.\\r\\n\\r\\n\\r\\n\\r\\nI used very simple preprocessing steps, just removing the per-channel average for each sample. Low-pass and high-pass filters were not used. This is because I am not familiar with the general processing procedure of EEG signals. So I decided to keep as much information as possible and let the network learn it.\\r\\nWhat was your most important insight into the data?\\r\\nI assume the data is locally correlated along the time axis and this correlation is stationary, so that 1D convolution can be used. But this is a simple assumption and adopted by many other teams.\\r\\nWhich tools did you use?\\r\\nLasagne. It is a wonderful toolkit and fits neatly to Kaggle competitions.\\r\\nHow did you spend your time on this competition?\\r\\nI\\xa0spent about 2/3 time on training single models and the other time on trying model ensembles. The\\xa0single models performed well, but some mistakes happened with model ensembles. As a result, the performance of my\\xa0ensemble stopped improving several days before the deadline.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nMany hours are needed to train a model, depending on the model size. About an hour is needed for a model to make the predictions for all the test data.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nAlthough I\\xa0made some mistakes in the stage of ensemble, I\\xa0earned precious experience in how to combine neural network models. Moreover, I\\xa0got an exciting time in preparing the competition.', 'This is our 3rd place solution to the Grasp-and-Lift EEG Detection Competition on Kaggle. The main aim of the competition was to identify when a hand is grasping, lifting, and replacing an object using EEG data that was taken from healthy subjects as they performed these activities. Better understanding the relationship between EEG signals and hand movements is critical to developing a BCI device that would give patients with neurological disabilities the ability to move through the world with greater autonomy.\\r\\n\\r\\n\"Our final solution was an ensemble of 34 nets.\"\\r\\n\\r\\nWe would like thank the competition sponsor: The WAY Consortium (Wearable interfaces for hAnd function recoverY; FP7-ICT-288551).\\r\\n\\r\\n[caption id=\"attachment_5211\" align=\"aligncenter\" width=\"300\"] Fig. 1 Grasp-and-Lift EEG Detection, 379 teams and 452 players competed.[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\nTim : I had a rather meandering academic career-I started as an applied physics major at Caltech, wandered off and got a masters in nuclear engineering doing fusor work at the University of Illinois and finally got Ph.D. in electrical engineering doing computational electromagnetics, also at Illinois. These days I create tools for linear electronic component characterization (mostly cables), focusing on making inexpensive test equipment do things it wasn\\'t designed for.\\r\\n\\r\\n[caption id=\"attachment_5218\" align=\"aligncenter\" width=\"300\"] Kaggle profile,\\xa0Tim Hochberg[/caption]\\r\\n\\r\\nEsube: I am a Ph.D. student with research areas of assistive (Robot-mediated and VR-based) technology for children with autism and adults with schizophrenia.\\r\\n\\r\\n[caption id=\"attachment_5219\" align=\"aligncenter\" width=\"300\"] Kaggle profile,\\xa0Esube Bekele aka \"Deep\"[/caption]\\r\\n\\r\\nElena: I am a physicist working on data analysis for many years. I have a Ph.D. in physics and I work as data analyst for the Virgo experiment in Italy for the detection of Gravitational Waves. My fields of interest are noise analysis, transient detection and data cleaning. I\\'m rather new to Machine Learning techniques, but have some experience with time series analysis.\\r\\n\\r\\n[caption id=\"attachment_5220\" align=\"aligncenter\" width=\"300\"] Kaggle profile, Elena Cuoco[/caption]\\r\\n\\r\\nJing: I am a Ph.D. student working on socially assistive robotic technology for elderly care.\\r\\n\\r\\n[caption id=\"attachment_5221\" align=\"aligncenter\" width=\"300\"] Kaggle profile, Jing Fan[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\nTim: I have zero domain knowledge, but I have some experience with neural networks gained while participating in Kaggle\\'s Diabetic Retinopathy and Plankton Identification contests. I also have quite a bit of Python experience, which helped working with Lasagne.\\r\\n\\r\\nEsube: My research involves electrophysiological signal processing for affective state prediction. This entails various signal pre-processing, feature extraction, feature level analysis and modeling. I already had good experience in peripheral physiological signals such as EKG, EMG, etc..., which are very similar to EEG. Recently, I also incorporated EEG among the modalities for my virtual reality (VR) based assistive system. So, this experience helped me in exploring the data more and coming up ways to diversify our nets to eventually arrive at stronger ensemble using only simple weighted averaging. Moreover, I also have some experience with convolutional nets gained by participating in the Diabetic Retinopathy and National Data Science\\xa0Bowl (plankton) competitions.\\r\\n\\r\\nElena: I have no knowledge about this specific domain. No idea of EKG, EGG etc., but I have knowledge of time series analysis and transient signals detection. I\\'ve a bit of experience with python and related library for ML, in particular scikit-learn, but very little experience with neural networks.\\r\\n\\r\\nJing: I have some experience analyzing EEG signals recorded from Emotiv EEG headset. The EEG signals were used to build classification models for affective computing.\\r\\nHow did you get started competing on Kaggle?\\nTim: Someone I know through our local Python users group suggested I check out Kaggle if I was interested in data science. So I did.\\r\\n\\r\\nEsube: When I was taking machine learning course, our professor introduced us to Kaggle. However, it took me a couple of years till I decided to join.\\r\\n\\r\\nElena: 2 years ago I decided to follow an online course on Machine Learning at Caltech by Professor Yaser Abu-Mostafa. I was very enthusiastic about the course. In the forum discussion I read about Kaggle site and its competitions, so I decided to join. I started with 2 basic competition: the first on scikit-learn and the second on Titanic. Only after more than one year I decided to participate to featured competitions.\\r\\n\\r\\nJing: The Grasp-and-Lift EEG Detection problem is my first Kaggle competition. Esube told me about this competition. I thought it was a good chance to improve my understanding of EEG signals as well as machine learning strategies.\\r\\nWhat made you decide to enter this competition?\\nTim: It looked like something it might be fun trying out neural nets out on. I hadn\\'t attempted anything with 1D CNN before and it seemed like an interesting challenge.\\r\\n\\r\\nEsube: As I described above in my prior experience, introducing EEG into my research pipeline was a major motivating factor to join this competition. Although I had good experience with peripheral physiological signals, this is my first time predicting from EEG. Also, this is my first time of using conv nets in 1D. Yoshua Bengio published a paper on using deep conv nets for peripheral physiological signals, although the results were not very good. After reading that paper, I was quite interested to apply CNNs to my physiological dataset for quite some time. After realizing there is a shortage of literature of applying CNNs to electrophysiology, I thought this competition would be the best opportunity for me to test if indeed CNNs can perform as well as traditional paradigm.\\r\\n\\r\\nElena: I love time series analysis and I was pretty confident I could have done a good job with these data.\\r\\n\\r\\nJing: Detecting hand and finger movements from EEG signals seems very interesting.\\r\\nLet\\'s Get Technical\\nWhat was your overall approach to the challenge?\\r\\nOur primary approach to the Grasp-and-Lift EEG Detection problem was convolutional neural networks (convnets). We used very little pre-processing of the data, primarily filtering out the very low and high frequencies to reduce noise and wander, and relied on the convnet itself for feature generation and selection. The predictive value of the individual convnets was quite strong and in the end we simply averaged together the results of our better performing nets to achieve our final submission.\\r\\nWhat pre-processing methods did you use?\\r\\nWe used very limited pre-processing. The major pre-processing that we applied were only filtering (several band pass and sometimes low pass filtering to exploit the seemingly important low frequency component in this dataset) and dropping the first two channels after we discovered that these two channels are corrupted by ocular artifacts the most and they have very little to do with predicting motor actions (See the data insights section for more details on this).\\r\\nWhat supervised learning methods did you use?\\r\\nWe used convolutional neural networks (CNNs). The CNNs took sliding, 8 second (4096 point) long slices of the 32 input channels and produced as output the probabilities of each of the 6 possible events. The CNNs were built using a somewhat customized version of nolearn, which is a thin layer on top of the neural networking toolkit Lasagne, which is in turn built on top of Theano.\\r\\n\\r\\nThe specifics of the successful nets varied but they were all similar to net stf7, shown in Fig. 2. Other than using 1D convolutional layers, much of the net is similar to a typical image recognition net, with a section of convolutional / maxpooling layers followed by a section of dense / dropout layers. However there are two regions that are atypical:\\r\\n\\nThe initial, linear convolutional layers.\\r\\n\\nThe first convolutional layers reduces the number of channels from 32 to 6. This gives the net a chance to learn a spatio-temporal filter to reduce the noisiness of the data being fed into the net.\\nThe second convolutional layer with a stride of 16 allowed the net to learn a strategy for down sampling by a factor of 16.\\n\\n\\nThe stride: 8 maxpooling layer and accompanying bypass.\\r\\n\\nThe stride: 8 maxpooling helped reduce overfitting dramatically. However it had the side effect of making the location of the start of events fuzzy to the dense portion of the net.\\nBy duplicating the most recent 8 time points and bypassing the maxpooling, the fuzziness can be greatly reduced while still reducing overfitting.\\n\\n\\n\\r\\n[caption id=\"attachment_5212\" align=\"aligncenter\" width=\"600\"] Fig. 2 Structure of the net stf7[/caption]\\r\\n\\r\\nDuring training, we used two types of validation, which we will refer to as `[3&6]` and `3i`. It is clear from Table I, which shows the validation scores for nets trained using both validation scheme for various structures, that `3i` definitively outperforms `[3&6]`. Unfortunately, this didn\\'t become clear till the end of the competition and the vast majority of the nets in our submitted ensemble used `[3&6]` validation.\\r\\n\\r\\nTable I: accuracy comparisons of various net architectures\\r\\n\\r\\n[table]\\r\\nNet,Validation,Public LB,Private LB\\r\\nnet34,[3&6],0.96555,0.96291\\r\\nnds4,[3&6],0.96534,0.96438\\r\\nstf2,[3&6],0.96821,0.96643\\r\\nstf3,[3&6],0.96558,0.96786\\r\\nstf3,3i,0.97204,0.97109\\r\\nstf7,[3&6],0.97117,0.97088\\r\\nstf7b,[3&6],0.97138,0.96917\\r\\nstf7b,3i,0.97216,0.97147\\r\\nstf7m,[3&6],0.97138,0.96917\\r\\nstf7m,3i,0.97216,0.97158\\r\\n[/table]\\r\\n\\r\\nNote: `[3&6]` means series `3` and `6` were used for validation while the rest of the training series were used for training and `3i` means that only `3%` of the training data were used for validation while the remaining `97%` were used for training.\\nThe Winning Ensemble\\r\\nOur winning ensemble was very simple weighted averaging. Since the individual nets were strong in performance, a simple averaging of diversified nets was sufficient to come up with a strong ensemble. We started on stacking, but, didn\\'t pursue it much due to lack of time during the competition. Fig. 3 shows the pipeline of our solution. We tried to diversify the individual nets by training with different known EEG frequency bands such as delta, theta, alpha, beta, and gamma which were implemented as filter banks and trained separately as shown in Fig. 3. We also dropped the first two channels in some of the nets, i.e. Fp1 and Fp2 to reduce the effect of ocular artifacts. The other method we used to diversify our nets was varying the validation methods.\\r\\n\\r\\n[caption id=\"attachment_5213\" align=\"aligncenter\" width=\"614\"] Fig. 3 The overall solution diagram[/caption]\\r\\n\\r\\n\\xa0\\r\\nWhat was your most important insight into the data?\\r\\nAs described in the ensemble section above, we tried to explore the dataset in an effort to come up with diversified models by varying filter frequencies and changing the validation strategies. However, in the process of that exploration we stumbled up on the idea that the first two channels were corrupted by very large artifacts and baseline wander (Fig. 4), which we associated with ocular artifacts due to the position of the channels. Based on literature, these two channels have little to do with motor imagery, visual evoked and motor related potentials. Therefore, we decided to drop the two channels in training some of the nets. This is in essence sort of channels selection. Although we could not explore the effect of dropping these two channels, proper treatment of these artifacts could have helped.\\r\\n\\r\\n[caption id=\"attachment_5214\" align=\"aligncenter\" width=\"614\"] Fig. 4 Raw EEG from 1st series of 1st subject with large artifacts and baseline wander[/caption]\\r\\n\\r\\n\\xa0\\r\\n\\r\\nAnother data exploration we attempted was to convert the raw EEG channels into common spatial pattern (CSP) space to maximize discriminability. However, the events given in this dataset were overlapping (Fig. 5) and that made the conversion difficult. Therefore, we abandoned this idea due to shortage of time.\\r\\n\\r\\n[caption id=\"attachment_5215\" align=\"aligncenter\" width=\"614\"] Fig. 5 Raw EEG (top) and overlapping events (bottom)[/caption]\\r\\n\\r\\nWe also tried at the very end of the competition to do more relevant channels selection and whitening the signal in an effort to reduce the effect of the artifacts and large baseline wanders (Fig. 6). The whitening had undesirable effect of minimizing the evoked potentials (red circles in Fig. 6). However, due to shortage of time, these ideas were not used in the training of the nets that made it to the final ensemble.\\r\\n\\r\\n[caption id=\"attachment_5216\" align=\"aligncenter\" width=\"614\"] Fig. 6a Effect of whitening on baseline wander[/caption]\\r\\n\\r\\n[caption id=\"attachment_5217\" align=\"aligncenter\" width=\"614\"] Fig. 6b Effect of whitening on ocular artifacts[/caption]\\r\\nWere you surprised by any of your findings?\\r\\nWe were indeed very surprised by how well CNNs were able to perform on this dataset. There is shortage of literature that applied CNNs on electrophysiology in general and EEG in particular. The most prominent paper on electrophysiology is Learning deep physiological models of affect\\xa0by Yoshua Bengio in which he applied CNNs to 2 channels of peripheral physiological signals. The results in this paper and other EEG with CNN papers were not very impressive.\\r\\n\\r\\nThis is the first time to our knowledge CNN performed so well on electrophysiological signals. With single model performance, no traditional feature extraction and modeling comes close. The best reported single traditional model in this competition performed less than 95%. Our single model CNN performance was a little more than 97%. What was even more surprising was the data exploration we were doing in the last week of the competition was so useful that the performance of the ensemble kept improving up until the deadline. We had some nets running that didn\\'t make it by the deadline and we ended up with public/private LB 98.130%/98.015% post deadline once those nets finished. The public LB score would have been #1 while the private LB seemed to preserve our 3rd place, albeit with slightly better score.\\r\\n\\r\\nWe were even more surprised to learn that the 2nd ranked team used recurrent CNNs (RCNN) and their single model performance was more than 97.6%! The surprise is beyond expectation and sparked significant curiosity that we have now formed a team of 5 people (4 of whom are from teams that finished 1st, 2nd, and 3rd in this competition) to continue working on this with a target of publishing at least a conference paper using the larger dataset of which the data for this competition came from and/or another standard EEG motor imagery dataset.\\r\\nWhich tools did you use?\\r\\nWe used customized NoLearn\\xa0which is a wrapper on top of Lasagne which in turn is a library on top of Theano.\\r\\nHow did you spend your time on this competition?\\r\\nAs the CNNs learned end-to-end, meaning extracting features in addition to features to output labels mapping, we spent little time on feature engineering. Originally, we were focused on traditional feature extraction as well. However, once we realized the performance of the CNNs was superior we abandoned the feature extraction completely and focused more on training more CNNs. We spent quite some time exploring the dataset in an effort to diversify the CNNs, however. So, we could say 20% time for pre-processing and feature extraction (again this is just to account for the time spent initially for traditional feature extraction) and 80% model training.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nThe run time for the final 3rd place ensemble was about 4 days using 2x GTX 980 GPUs on a machine with 32GB RAM, 220GB swap space and 6 cores/12 threads Intel Xeon CPU running 4 models in parallel with each GPU training 2 models at the same time.\\r\\nTeamwork\\nHow did your team form?\\nEsube: The three (excluding Jing) of us started alone. As I was working on my dissertation while competing, I knew I needed someone to work with. I encouraged Jing to join me first and asked Elena to join us after she posted her script (which, btw, was script of the week). I saw that she wanted to finish top 10 as I was.\\r\\n\\r\\nAround the same time, I also asked Tim (Again after tinkering with his awesome script) to join us whenever he feels like joining a team. He said he wanted to test most of his ideas alone before joining a team, first. He eventually came along and joined our team two weeks before the end of the competition. After the fact, I wished he joined us earlier as we were making progress until the end.\\r\\n\\r\\n[caption id=\"attachment_5223\" align=\"aligncenter\" width=\"612\"] Top competition scripts, including those by Elena & Tim[/caption]\\r\\n\\r\\nTim: Esube contacted me and asked about teaming up fairly early. At the time I still had several ideas I wanted to try and I thought it would be easier to test them out on my own since I\\'d have more submissions available. Once I\\'d worked through those ideas, which sadly were mostly unsuccessful, I contacted Esube to see if he was still interested in me joining his team.\\r\\n\\r\\nBeing part of team turned out to be very helpful: there were a lot of ideas being batted around and these helped me come up with ideas for new nets. In addition, once we\\'d settled on neural nets as our primary approach, Esube in particular worked tirelessly helping to train variations on our nets to improve our resulting ensemble. In retrospect, we probably could have done better if I\\'d joined up sooner since we were making quite a bit a progress as the competition ended.\\r\\nHow did your team work together?\\r\\nJing was trying traditional feature extraction of linear classifiers, although eventually we decided not to include them in the ensemble due to the poor performance compared to CNNs. Elena started out with linear classifiers and her own implementation of CNN. But, again due to performance, we decided to stick to Tim\\'s CNN implementation. Tim was a crucial member of the team by coming up with CNN structures in fast iterations and training almost half of the nets that eventually made it to the ensemble. Esube trained nets by exploring different parts of the data and trying different pre-processing and validation strategies.\\r\\nHow did competing on a team help you succeed?\\r\\nIf it wasn\\'t for the team, we all are convinced we wouldn\\'t be able to surge like that in the last weeks of the competition and end up placing 3rd. The competition was so fierce and was so close that any additional model to the ensemble was very crucial.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\nTim: First, even simple ensembling helps a lot. In previous competitions I\\'d used single models, and the best finish I\\'d managed was 14th place. In this competition, had we used our best single model we would have finished in, interestingly enough, 14th place! However, using an average of our top models moved us up 11 places. Second, organize your code so that you\\'re ready to submit it if you place. We did not do this and as a result, it took a lot of extra work to get the code ready to submit at the end of the competition.\\r\\n\\r\\nEsube: My biggest take home point is the exceptional performance of CNNs on 1D time-series signals. I have learned quite a lot from my teammates especially Tim on how to quickly prototype CNN architectures. I also learned quite a lot from other competitors in the forums especially from Alexander for EEG processing domain knowledge.\\r\\n\\r\\nElena: I was really surprised of the performance of Neural Networks! I\\'ve learned a lot by the Tim\\'s prototype of CNN applied to these time series. I\\'m only sorry to have had so few time to be more active and help my teammates during the last weeks of competitions.\\r\\n\\r\\nJing: I learned how powerful CNN is. With very little pre-processing and no feature extraction, it performs so well. I also learned from my teammates to try out different methods, do not solely trust on literature.\\r\\nDo you have any advice for those just getting started in data science?\\nTim: Dive in and tackle real problems as soon as possible. Kaggle is great for that since you get a chance to try challenging problems, see how you can do relative to other, more experienced data scientists, and see how the best results were obtained.\\r\\n\\r\\nEsube: I strongly advice newbies like me to participate in teams and learn by doing, meaning there is no substitute (shortcut) for an experience that comes from exercising and trying out different methods.\\r\\n\\r\\nElena: The online course I followed was very useful, but it is also important to have good background on statistics, time series analysis, and data cleaning before going to ML techniques.\\r\\n\\r\\nJing: Work in a team and learn by solving problems.\\r\\nBios\\nTim Hochberg: received a B.S. in applied physics from Caltech in 1989. He went on to earn an M.S. and Ph.D. in Nuclear Engineering and Electrical Engineering respectively from the University of Illinois. His is currently Chief Scientist for atSpeed Technologies where he focuses on coaxing accurate, frequency-domain measurements out of inexpensive, time-domain measurement instruments using a variety of software and hardware techniques.\\r\\n\\r\\nEsube Bekele: received the M.S. degree in electrical engineering, in 2009, from Vanderbilt University, Nashville, TN, USA, where he is currently working towards the Ph.D. degree in electrical engineering. He served as junior faculty in Mekelle Uiversity, Ethiopia, before joining Vanderbilt University. His current research interests include human–machine interaction, robotics, affect recognition, machine learning, and computer vision. He will join the Naval Research Laboratory (NRL), Washington, D.C. as research associate upon completion of his PhD at the end of October. His research at NRL will focus on convolutional neural networks and cognitive architecture for semantic context learning for visual object recognition.\\r\\n\\r\\nElena Cuoco: received a Master degree in Physics in 1993 and a Ph.D in Physics in 1997 from Pisa University, Italy. Working as Post Doc for 2 years at \\'Osservatorio Astronomico di Arcetri\\', Firenze, Italy and for 3 years as Researcher at INFN Firenze. Staff member from 2004 at European Gravitational Observatory in Italy and member of Virgo Collaboration from 1995. Now I\\'m Scientific coordinator for GraWIToN project, EGO referent for Data Analysis and the EPO activities coordinator for EGO and Virgo.\\r\\n\\r\\nJing Fan: received the M.S. degree in electrical engineering, in 2014, from Vanderbilt University, Nashville, TN, USA, where she is currently working toward the Ph.D. degree in electrical engineering. Her research interests include human-robot interaction, robotics, machine learning, and cognitive computing.\\r\\n\\r\\n\\r\\n\\r\\nCode for team HEDJ\\'s solution can be found here.\\r\\n\\r\\nRead other blogs\\xa0on the\\xa0Grasp-and-Lift EEG Detection competition by clicking the tag below.', 'Team Cat & Dog took first place in the Grasp-and-Lift EEG Detection competition ahead of 378 other teams. The pair also comprised 2/3 of the first place team from another recent EEG focused competition on Kaggle,\\xa0BCI Challenge @ NER 2015. Domain knowledge and a strong collaborative relationship have made Alexandre Barachant\\xa0(aka Cat) and Rafał Cycoń (aka Dog) successful in both competitions.\\r\\n\\r\\nIn this blog, they share best practices for working with EEG data, as well as the tools and code that took them to the top of the Grasp-and-Lift EEG Detection leaderboard. They also\\xa0tip their hat to all the Kagglers\\xa0who shared scripts during the competition. The code shared and models developed during this challenge were\\xa0huge contributions\\xa0to the WAY Consortium\\'s work in developing prosthetic devices for patients who have lost hand function due to neurological disabilities or amputation.\\r\\n\\r\\n[caption id=\"attachment_5211\" align=\"aligncenter\" width=\"300\"] The Grasp-and-Lift EEG Detection competition ran from June 29 - August 31, 2015[/caption]\\r\\nThe Basics\\n\\nWhat was your background prior to entering this challenge?\\nCat: I hold a PhD in signal processing and an electrical engineering degree from the Grenoble University, France. I’m currently working as a post-doc fellow at the Burke Medical Research Institute (Cornell University), where I study the effect of transcranial direct current stimulation (tDCS) as a clinical treatment for pediatric hemiplegia. My background is more about signal processing than machine learning. But I caught the virus and now I can’t live without my daily dose of data science.\\r\\n\\r\\nI’m not saying I was a complete noob, and I was already playing with ML during my PhD, but I’ve always been more comfortable with feature engineering than modelisation.\\r\\n\\r\\n[caption id=\"attachment_5238\" align=\"aligncenter\" width=\"300\"] Kaggle profile for Alexandre (aka Cat)[/caption]\\r\\n\\r\\nDog: I studied Mechatronics at AGH UST in Krakow, Poland, which gave me a solid background in signal processing. I was interested in Data Science and Machine Learning since early years of studies, which resulted in basing both Bachelor and Master theses in these topics. Right now I’m running a startup that provides custom Data Science solutions.\\r\\n\\r\\n[caption id=\"attachment_5237\" align=\"aligncenter\" width=\"300\"] Kaggle profile for Rafał (aka Dog\\xa0or blaine)[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\nCat: I won’t lie, this challenge was right in my comfort zone. I work with EEG signal on a daily basis. As a matter of fact, my current jobs mainly consist of recording and analyzing EEG signal to decode hand movement in order to understand the physiology of sensory-motor cortex on children with cerebral palsy. In addition, this challenge was more or less the topic of my PhD, where I built a brain computer interface to decode asynchronous hand movement in order to drive an home automation system (control the light or your TV with\\xa0your thoughts).\\r\\n\\r\\nDomain knowledge is important for EEG. There are multiple ways to deal with the problem, and tons of different features to investigate. However, it can also blind you during the exploratory phase. For example, if it wasn’t for my teammate and the scripts posted by other contestants, I would never have used very low frequency time-domain signal as a feature (they are not reported that useful in the literature), and it turned out to be one of the most important feature.\\r\\n\\r\\nDog: I was always extremely interested in Brain Computer Interfaces and processing brain-related signals, but my knowledge was very limited. Thanks to Alex I learnt a lot during the previous BCI competition. Also of course previous experience with signal processing and machine learning was useful.\\r\\nHow did you get started competing on Kaggle?\\nCat: The first real challenge I took part in was the DecMeg2014 challenge. Back then, I was between two post-doc, and was looking for something to keep me busy. Over my years of research, I developed some very good classification methods for EEG signal. They are working really well for online applications, and seemed to be competitive when compared to state-of-the-art methods. However, the field of BCI lacks a standard benchmark. It was the perfect opportunity to find out how these methods were ranking in a very competitive and standardized environment. After a few submissions, I took first place on the leaderboard, and was definitely hooked.\\r\\n\\r\\nDog: I entered Kaggle with the challenge of getting a Master badge, and thanks to lots of luck I was able to get it quite fast. But tasting the thrill and fun of competing, and realizing the possibilities of self-development that this platform offers made me stay for longer. Zero regrets.\\r\\nWhat made you decide to enter this competition?\\nCat: This challenge is such a match to my background that I had the moral obligation to enter...\\r\\n\\r\\nDog: “Oooh yesss, a competition about processing EEG to control a hand prosthetic! That is so cool !” :-)\\r\\nLet\\'s Get Technical\\nWhat domain-specific features did you use ?\\r\\nThere are two major kinds of event related activity that we can extract from EEG.\\r\\n\\r\\nThe first one is called Event Related Potential (ERP), and is characterized by a phase-locked, time-domain waveform that appears in response to a stimulation. ERP are without any doubt the most studied type of activity in EEG. The BCI challenge and the DecMeg2014 challenge were about classification of ERP. Typical features are time-domain signal, generally averaged across several repetitions of the stimulation in order to increase the signal to noise ratio.\\r\\n\\r\\n[caption id=\"attachment_5232\" align=\"aligncenter\" width=\"608\"] Fig. 1 : Example of Visual Evoked Potential (VEP) present in the dataset. The graph on top represents the individual time course for each electrode, with respect to their position on the scalp. The second figure represents the topographical map of the amplitude of the VEP for different timing. Link to script[/caption]\\r\\n\\r\\nThe other one is called neural oscillation, and is characterized by change in signal power in specific frequency bands. These oscillations appear naturally in ongoing EEG activity, and are representative of a wide range of different cognitive state (e.g sleep stage, meditation, etc.) or can be induced by a specific task, for example a hand movement, or mental calculus. This was the case for the two seizure detection challenge organized last year. Typical features are FFT-based, or more simply log-variance / covariance of the signal after frequential filtering.\\r\\n\\r\\n[caption id=\"attachment_5233\" align=\"aligncenter\" width=\"608\"] Fig. 2 : Example of induced neural oscillation related to hand movement in the grasp-and-lift dataset. During a right Hand movement, the power in the mu (10 Hz) and beta (20 Hz) frequency bands decrease over the left sensory-motor cortex. Link to script[/caption]\\r\\n\\r\\nBoth of these activities have parameters (exact spatial location, latency, shape, frequency band, etc.) specific to each subject, and can be seen as some kind of brain fingerprint. This is the reason why models trained on a subject does not generalize well to other subjects and why optimal parameters of features extraction must be subject-specific.\\r\\n\\r\\nThe Grasp-and-Lift EEG dataset contains both types of activities: induced power related to hand movements, and visual evoked potential elicited by a visual stimulus instructing the subject to start the task. As a results, we used as feature time domain low-pass filtered signal, and covariance matrices estimated in different frequency bands.\\r\\nWhat preprocessing and supervised learning methods did you use?\\r\\nOn the basis of above domain-specific features and a generic feature called a Filter Bank we built a 3-level classification pipeline:\\r\\n\\nLevel1 models are subject-specific, i.e. trained independently on each subject (but with global hyper-parameters). Most of them are also event-specific. Their main goal is to provide support and diversity for level2 models by embedding subject and events specificities using different types of features.\\nLevel2 models are global models (i.e. not subject-specific) that are trained on level1 predictions (metafeatures). Their main goal is to take into account the temporal structure and relationship between events. Also the fact that they are globally significantly helps to calibrate predictions between subjects.\\nLevel3 models ensemble level2 predictions via an algorithm that optimizes level2 models\\' weights to maximize Area under the ROC Curve, which was the target metric in this competition.\\n\\r\\n[caption id=\"attachment_5234\" align=\"aligncenter\" width=\"612\"] Fig. 3 : Classification pipeline of the solution.[/caption]\\r\\nWhat was your most important insight into the data?\\r\\nThere was a clearly defined temporal structure of events (i.e. the 6 target classes), so including a signal time course history of several seconds was beneficial in all models. Especially Recurrent Neural Networks were able to “naturally” catch the pattern.\\r\\nIn addition, ensembling was very powerful in this challenge. The amount of data, and the stability across time (which is something rare in a EEG dataset) was allowing us to build a high level of supervised ensembling, without too much risk of overfitting.\\r\\nWere you surprised by any of your findings?\\r\\nQuite a lot of predictive power was contained in very low frequency content of the data, which is quite unusual for BCI/EEG. We can also observe relatively good performances with a covariance model estimated on the 70-150 Hz frequency range. This frequency range is too high to contain EEG, underlying the possible presence of task-related EMG activity.\\r\\n\\r\\nWe were very surprised by the fact that simply heavily decimating the input to XGBoost served as a much better regularization than many ideas for parameter tuning that we tried. What’s also interesting is that this fact surfaced only due to Alex’s laziness impatience in waiting for XGBoost’s results :-)\\r\\nHow did you spend your time on this competition?\\r\\nWe merged a few days after the competition reset (there was a timing error in initial data). We spent the first weeks exchanging ideas about feature extraction and testing different ensembling strategies. We then spent 2 weeks for a complete code refactoring in order to merge our solutions, cross-checking each other’s code for leakage and rules infringements in the process. This gave us a very powerful and robust codebase, and we spent another few days generating many level 1 models. The last 2 weeks was entirely dedicated to ensembling, with a specific focus on insuring the robustness of our solution against overfitting.\\r\\n\\r\\n[caption id=\"attachment_5235\" align=\"aligncenter\" width=\"612\"] Fig. 4 : Leaderboard progress.[/caption]\\r\\nWhich tools did you use?\\r\\nEverything was done in Python. We used Scipy, MNE and Alex’s pyRiemann packages for signal processing, and scikit-learn, XGBoost, Theano, Lasagne, Nolearn, Keras and hyperopt for machine learning. We would like to thank the authors of these packages for their dedication and excellent work.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nLots of fun, new knowledge and skills, and $5,000.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nPractice, practice, practice. And the best way to do that is through Kaggle. Jump in even if you think you lack knowledge (yet!). Possibility to learn from others, to learn with others, and the competitive spirit will drive you to boost your skills considerably. And it’s for free!...well, almost :-)\\r\\n\\r\\nOne of the most important skills of a data scientist is to validate solutions correctly. Test your work with a bulletproof validation procedure that you are 100% sure is reasonable. You’re going to create features, build models, implement crazy-sounding ideas, but in the end you have to know if you should keep them, throw them away or change them in one way or another. The public leaderboard alone is often very deceptive. We learned that the hard way in the Seizure Prediction challenge, where we dropped from 2nd place on public LB to 26th on private LB. And the reason was a flawed cross-validation procedure. It was used because it gave really well correlated scores with public LB and it happened they were uncorrelated with private LB, which gave us lots of disappointment in the end.\\r\\n\\r\\nFinally, team-up, and do it early to share discussions and insights, combine skills and points of view - it can be extremely valuable.\\r\\nTeamwork\\nHow did your team work together?\\nCat: I’m generally more focused on feature engineering, and Rafal on modeling and ensembling. For this challenge, there was no specific role, except for ensembling where I mainly worked on XGBoost while Rafal was playing with RNN (no GPU for me).\\r\\nWe communicated by email, and shared code through gitlab.\\r\\n\\r\\n[table]\\r\\n,Average per day (41 days as a team),Total\\r\\nemails,13.6,560\\r\\ncommits,7.3,300\\r\\nbeers,0,0 :-(\\r\\n[/table]\\r\\n\\r\\nDog: Here we can observe the joy of intercontinental cooperation… :-)\\r\\nHow did competing on a team help you succeed?\\nDog: Our team was a really good synergy - Alex’s domain expertise and excellent feature engineering skills were leveraged with machine learning stuff that I was able to provide. We both learned a lot. And I think it’s highly unlikely either of us would have won this challenge alone.\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\nDog: More BCI/EEG competitions would be awesome of course :-) Besides that it’d be great to see Kagglers’ knowledge and imagination be put into development of more medical applications, like previous seizure competitions or the recent diabetic retinopathy competition. For example identifying and/or predicting arrhythmia attacks on the basis of ECG signals or perhaps identifying cancer in radiographs.\\r\\n\\r\\nIt would also be fun to try to forecast stock prices of some companies on the basis of lots of various data, like fuel prices, mean salaries for certain jobs, number of positive/negative tweets about a brand, google search trends etc.\\r\\n\\r\\nCat: I have been very surprised of the high level of the solutions (and their performance) developed during this challenge. For a problem that is considered as one of the most difficult in the field (asynchronous BCI), it’s amazing how fast Kagglers were able to beat the state-of-the-art methods. It proves once again that the Kaggle community is able to tackle any problem with bright and imaginative solutions that can potentially unlock or boost research. So I would love to see how Kagglers will crack other challenges in BCI/EEG. If i could host a challenge, it will be a Big Data BCI challenge, with hundreds of subjects and data recorded in a real life context i.e. not in very controlled laboratory conditions. This is where the field has to move.\\r\\n\\nBios\\nAlexandre Barachant (“Cat”) is a French Researcher and expert in Brain computer interfacing and Biosignal analysis. In 2012, he received his Ph.D. degree in signal processing from the Grenoble University, France. During his Ph.D. thesis, he developed a robust and adaptive brain computer interface based on self-paced motor imagery. Between 2012 and 2013, he has been a post-doc fellow at the Centre National de la Recherche Scientiﬁque (CNRS) in the GIPSA Laboratory, Grenoble, where he developed a calibration-less P300 brain computer interface for video-game control and assistive communication. Since November 2014, he joined the Burke Medical Research Institute, New York, to study the effects of non-invasive brain stimulation for rehabilitation. His research interests include statistical signal processing, Machine learning, Riemannian geometry and classification of neurophysiological recordings.\\r\\ntop\\nRafał Cycoń (“Dog”) is co-founder and CTO of a Data Science consultancy startup FORNAX, where he is enjoying solving challenging problems and building intelligent systems. He holds a MSc diploma in Mechatronics received from AGH-UST in Krakow, Poland. During the last 5 years he has completed a variety of research and industrial projects, such as development of image processing pipelines for an intercellular manipulator, detecting early signs of autism in children on the basis of their interaction with tablets during playing mobile games, or development of a software system controlling high-speed cameras for military applications. His scientific interests include Brain-Computer Interfaces and all kinds of stuff related to Data Science and Machine Learning - in particular validation methods, ensembling, (Deep) Neural Networks, Gradient Boosting Machines and signal/image processing techniques. He has just learned that writing about himself in 3rd person is a strange experience.\\r\\ntop\\n\\nRead other posts\\xa0on the\\xa0Grasp-and-Lift EEG Detection competition by clicking the tag below.\\nThe code and documentation for team Cat & Dog\\'s solution can be found on this repo.', 'The Caterpillar Tube Pricing competition challenged Kagglers\\xa0to predict the price a supplier would quote for the manufacturing of different tube assemblies\\xa0using\\xa0detailed tube, component, and volume data. Team Shift Workers finished in 3rd place by combining a diverse set of approaches different members of the team had used before joining\\xa0forces. Like other teams in the competition, they found XGBoost to be particularly powerful on this dataset.\\r\\n\\r\\n[caption id=\"attachment_5258\" align=\"aligncenter\" width=\"251\"] 1,323 teams and 1,451 players competed[/caption]\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\nShize: I am currently a phd student in the department of electrical and computer engineering at University of Virginia, United States. My research interests lie in 1) Large Scale Network Modeling, Analysis, Optimization and Control; 2) Complex Systems; 3) Machine Learning/Data Mining with Application to Big Data. I had already participated in a couple of Kaggle competitions prior to entering this CAT competition.\\r\\n\\r\\n[caption id=\"attachment_5249\" align=\"aligncenter\" width=\"300\"] Shize Su\\'s profile on Kaggle[/caption]\\r\\n\\r\\nMatias: I currently work as a data analyst for Aimia. Previously I was a database admin, and previous to that a software developer. I started doing data science courses last year through EDx and Coursera. Over there I became\\xa0familiar with Python and R, which is my principal tool now for Kaggle.\\r\\n\\r\\n[caption id=\"attachment_5250\" align=\"aligncenter\" width=\"300\"] Matias Thayer\\'s profile on Kaggle[/caption]\\r\\n\\r\\nNaokazu: I took my Ph.D. in mathematics and have been worked on data analysis in several fields.\\r\\n\\r\\n[caption id=\"attachment_5251\" align=\"aligncenter\" width=\"300\"] Naokazu Mizuta\\'s profile on Kaggle[/caption]\\r\\n\\r\\nYannick: I am working in a media company as a data analyst, focusing on turning transactional data into marketing reports, which are\\xa0used for marketing consultant and publication. Since my major of master degree is information intelligence, I am quite familiar with data mining theory, database, data manipulating softwares like R and SAS, and coding skills like Python and Java.\\r\\n\\r\\n[caption id=\"attachment_5252\" align=\"aligncenter\" width=\"300\"] Yannick\\'s profile on Kaggle[/caption]\\r\\n\\r\\nArto: I\\'ve a worked in the Business Intelligence field for some years now. Currently I\\'m Senior Consultant at Affecto. My expertise lies in data modelling, architecture and integrations. Lately I\\'ve been getting more and more into data science and machine learning.\\r\\n\\r\\n[caption id=\"attachment_5253\" align=\"aligncenter\" width=\"300\"] Arto\\'s profile on Kaggle[/caption]\\r\\nHow did you get started competing on Kaggle?\\nShize: I knew Kaggle when I took a graduate Machine Learning course at UVA -- the instructor held\\xa0a Kaggle in Class competition for the course, and luckily, I finished in 1st place\\xa0in the competition. Then, I took the Kaggle 2014 PAKDD Cup as my course project for that Machine Learning course, and, fortunately, I won the 3rd place prize (which was my 1st public Kaggle competition). It was definitely a surprise for me. I was quite encouraged by the\\xa0experience, and from then on, I attended a couple of more Kaggle competitions in my spare time, and I have learned a lot from the Kaggle community. Kaggle is definitely a wonderful place for sharping your skills on\\xa0machine learning and data mining.\\r\\n\\r\\nMatias: My first competition was on October 2014, as part of the “15.071x - The Analytics Edge (Spring 2015)” challenge. I was doing a MIT course through EDx, and this competition was part of the assignments. I really got very engaged during this competition and since then I\\'m an addict to Kaggle.\\r\\n\\r\\nYannick: Well, this is my second Kaggle competition, I still have a long way to go (what good luck to meet with other guys!). I guess the attraction for me is: I can learn a lot of cutting-edge skills and feel happy at the same time.\\r\\n\\r\\nArto: I found Kaggle’s tutorial competitions through Coursera. After I had done few of those, my first real competitions were Otto Group and West Nile Virus this spring. In every competition I’ve learned a lot, achieved slightly better results than previously, and have gotten more hooked on Kaggle.\\r\\nWhat made you decide to enter this competition?\\nShize: The problem is interesting to me, and the size of data is ok. Second, I had\\xa0some spare time during the competition period.\\r\\n\\r\\nMatias: I liked the fact that the amount of data was relatively small which means you can do many experiments on your laptop without having to wait for ages for the processes to finish. Also, I could use libreoffice solver to optimize the different predictions, which is quite easy and quick. In addition, I enjoy\\xa0when the data is not “hidden” like in other competitions, and you can make logical and common sense inferences.\\r\\n\\r\\nNaokazu: The datasets were small enough to explore quickly, diverse enough to try several ideas.\\r\\n\\r\\nYannick: The size of the data is suitable for my Internet condition! Luckily, I have already changed the ISP so it wont be a problem anymore.\\r\\n\\r\\nArto: The summer in Finland was cold and rainy so it was perfect for coding and learning new stuff. Also, the dataset consisted of many relational tables, which meant home ground advantage for me.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\nShize: In the early stage of the competition, I explored a couple of different models, including XGBoost, randomforest, keras,\\xa0nn, knn, etc. But later I found that XGBoost outperformed all other method significatly in this CAT competition, and so\\xa0mostly focused on improving XGBoost models. After teaming up, my role mostly turned to coordinate team work plans, set up cv experiments for ensemble, and suggest different ideas & provides instructions for new directions (e.g., meta-features and 2 level models) to explore for further team improvement.\\r\\n\\r\\nMatias: Early in the competition I realized that XGBoost performed considerably better than other models. Also, I noticed that exactly the same XGBoost model produced considerable different results depending on the seed. In this way I started bagging as many XGBoost models as I could. My strategy was very simple: Just run 100 or more XGBoost models overnight, using different parameters and against different portions of the train data (columns and rows) and then average the predictions. Also I worked a lot on feature engineering in the meantime, so my new models progressively included new features as well. In this way, as more models stacked together, the more the RMSLE score improved (decreased). That was very cool. This is how I managed to be between the 6th and 8th position on the public LB, and then I received an invitation to form part of the “Shift Workers” team, which I accepted happily.\\r\\n\\r\\n[caption id=\"attachment_5256\" align=\"aligncenter\" width=\"640\"] Diagram of final solution. Click image for full\\xa0size.[/caption]\\r\\n\\r\\nThen the approach switched a bit. As a team, we used fold 2 validations, and validated the models locally before submitting to Kaggle, so progressively I abandoned the “bagged” approach and used a weighted blend approach instead. As it was hard to improve further, I started building meta features on a CV-Fold 5 subset. Shize Su gave me very good guidelines to build those meta features – Many thanks to him! Here I used the following techniques: KNN, XGBoost, standard linear regression, CART, and Random forest predictions. The predictions obtained with those 2nd level models didn’t perform better than the previous ones, but they provided some diversity that helped to improve our score.\\r\\n\\r\\nNaokazu: First I tried plain XGBoost model and then tried ensembling. My ensembling method, \"quantity\"-wise modeling, was somewhat peculiar for this competition. For each \"quantity\" I dropped certain records from the training data and built models.\\r\\n\\r\\nYannick: At first I tried to use Adaboost and Randomforest, but lately I\\'ve found that XGBoost has an excellent performance.\\r\\n\\r\\nMatias has talked a lot about the methods to the\\xa0team, and to be honest, I was\\xa0not familiar with these methods at first (like blending, meta-bagging and complex CV mechanism). For myself, all these\\xa0techniques I learned from others are even more important and precious than winning this competition.\\r\\n\\r\\nArto: I used ensemble learning\\xa0from the start of the competition. Feature engineering was done fast with Pentaho’s Kettle. After I had one model with a good set of features, I broke it down into multiple models with different feature sets and parameters.\\r\\n\\r\\nThese models were then added to the ensemble with calculated weights and also dropped from the ensemble according to cross validation scores. Later on I did the same work with the datasets of my team mates. Early ensembles had many different algorithms, but in the end all but one of the models (extratrees) were using XGBoost.\\r\\nWhat was your most important insight into the data?\\nShize: Comprehensive and reliable cv experiment and XGBoosts ensemble were the key for success in this CAT competition. Developing many different variants of the XGBoost model (different parameters, different feature data sets, etc.) and blending based on the cv performs fairly well.\\r\\n\\r\\nMatias: The power of team work and diversity. Also, I was a bit surprised about the bad performance of regressions. Probably because we were trying to predict 2 costs at the same time (set up cost and product cost).\\r\\n\\r\\nNaokazu: I usually do not do lots of parameter tunings on XGBoost, especially row and col sampling since they usually have minor effects on performance. This time I got a drastic performance change for tuning these parameters on this dataset.\\r\\n\\r\\nYannick: I found that the number of components used in a tube may have strong affect on the prediction. We used a method which was quite similar to TFIDF to deal with it, and it worked well.\\r\\n\\r\\nArto: At first the competition was focused on feature engineering and in the end it was all about building an ensemble. So key points to success were cross validation, model diversity, and big team size.\\r\\nWere you surprised by any of your findings?\\nShize: I was a bit surprised that the XGBoost method was so dominant in this CAT competition. Adding other method models into the ensemble only provided quite marginal or no improvement (except in the 2 level model with other method model\\'s predictions as meta features).\\r\\n\\r\\nMatias: I was a bit surprised at how different approaches from my team mates worked together, so our final work was basically a big ensemble of many, many different trials.\\r\\n\\r\\nNaokazu: Same as above.\\r\\n\\r\\nYannick: After the competition, I believe the key point to winning on Kaggle is teamwork. Techniques are important, but collaboration is the decisive fact.\\r\\n\\r\\nArto: I was a bit surprised that the ensemble score could be improved by adding diversity with deliberately made weaker models.\\r\\nWhich tools did you use?\\nShize: R and Python\\r\\n\\r\\nMatias: R and LibreOffice\\r\\n\\r\\nNaokazu: R and PostgreSQL\\r\\n\\r\\nYannick: R and LibreOffice\\r\\n\\r\\nArto: Python and Pentaho\\'s Kettle\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\nShize: First, and most importantly, 4 new good friends^_^ Second, a wonderful experience for me to train to be more experienced in coordinating the work and plans of a team.\\r\\n\\r\\nMatias: That I need to care more about reproducibility from the beginning. At first I started stacking many different XGBoost models and sometimes I forgot to include the seed. Because of that we had to ignore some of those models when doing ensembling after we teamed up.\\r\\n\\r\\nNaokazu: Ensembling diverse models especially built by different people often gives you improvements on some datasets. I realized it is quite important for winning on Kaggle to team up in certain challenges. Most top teams on this competition teamed up and a recent change on Kaggle\\'s ranking metric seems to encourage participants to team up.\\r\\n\\r\\nYannick: One thing is collaboration, another one is knowing lots of new techniques. It will absolutely help me in other competitions.\\r\\n\\r\\nArto: Just like Matias, I got into trouble with reproducibility because I juggled with so many models without a decent source control system. So key learning points for me were better coding practices.\\r\\nTeamwork\\nHow did competing on a team help you succeed?\\r\\nThe close collaboration between 5 teammates was the key for our success. Each teammate had his own ideas, strengths & weaknesses, and our final blending included portions of everyone\\'s brain. We believe that, 1+1>2\\xa0in such\\xa0a successful collaboration.\\r\\n\\r\\n\\r\\n\\r\\nRead an interview with\\xa0the 1st place team\\xa0in the Caterpillar Tube Pricing\\xa0competition by clicking the tag below.', 'Recruit Ponpare is Japan\\'s leading joint coupon site, offering huge discounts on everything from hot yoga, to gourmet sushi, to a summer concert bonanza. The\\xa0Recruit Coupon Purchase Prediction\\xa0challenge asked the community to predict which coupons a customer would buy in a given period of time\\xa0using past purchase and browsing behavior.\\r\\n\\r\\n\\r\\n\\r\\nHalla Yang finished 2nd ahead of 1,191 other data scientists. His experience working with time series data helped him use unsupervised methods effectively in conjunction with gradient boosting. In this blog, Halla walks through his approach and shares key visualizations that helped him better understand and work with the dataset.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI\\'ve worked almost a decade in finance as a quantitative researcher and portfolio manager. I\\'ve also competed in several Kaggle contests, placing first in the Pfizer Volume Prediction Masters competition, sixth in the Merck Molecular Activity Challenge, and ninth in the Diabetic Retinopathy Detection.\\r\\n\\r\\n[caption id=\"attachment_5266\" align=\"aligncenter\" width=\"300\"] Halla Yang\\'s profile on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nPredicting prices for thousands of stocks and predicting purchases by thousands of Japanese internet users are loosely similar problems. You can forecast stock returns by looking at time series data such as past returns and cross-sectional data such as industry averages. You can forecast coupon purchases by looking at time series features based on past purchases and cross-sectional features based on peer group averages.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nFor each (user, coupon) pair, I calculated the probability that the user would purchase that coupon during the test period using a gradient boosting classifier. I sorted the coupons for each user by probability, composing the ten highest probability coupons into my submission.\\r\\n\\r\\nTo train my classifier, I constructed training data for 24 \"train periods\" that simulated the test period. Train period 1 is the week from 2012-01-08 through 2012-01-14, and includes all coupons with a DISPFROM date - the date on which they\\'re supposed to be first displayed - in that week. Train period 2 is the week from 2012-01-15 through 2012-01-21, and includes all coupons with a DISPFROM date in that week. Train period 24 is the week from 2012-06-17 through 2012-06-23, and includes all coupons with a DISPFROM date in that week.\\r\\n\\r\\nFor each of these training periods, I built a set of features for each (user, relevant coupon) pair. This set of features includes user-specific data, e.g. gender, days on site, and age; coupon-specific data, e.g. catalog price, genre, and price rate; as well as user-coupon interaction data, e.g. how often has the user viewed coupons of the same genre. The target for each observation is set to 1 if the user purchased that coupon during the training week, and 0 otherwise.\\r\\n\\r\\nTo calibrate the parameters of my model, I first trained a model on the first twenty-three weeks of data, and estimated my log loss and confusion matrix on the twenty-fourth week. I then trained a model on the full twenty-four weeks of data to generate my competition submission.\\r\\n\\r\\nThe only supervised learning method I used was gradient boosting, as implemented in the excellent xgboost package. I cycled through other algorithms at the start of my analysis to get a feel for their relative performance -\\xa0logistic regressions, random forests, SVMs, as well as deep neural networks - but found that gradient boosting was the single best classifier for my approach.\\r\\nWhat was your most important insight into the data?\\r\\nFirst, many test set and training set coupons were viewed prior to their DISPFROM, the date on which they\\'re supposed to be first displayed, and so one could use direct views as a forecasting variable. The violin plot below shows the distribution of first view times relative to DISPFROM. A negative x-value indicates the coupon was viewed prior to its DISPFROM. Over a quarter of coupons are first viewed more than twelve hours before their DISPFROM, and five percent of coupons are first viewed more than ninety hours before their DISPFROM.\\r\\n\\r\\n\\r\\n\\u200b\\r\\nSimply counting the number of times a user has viewed a test set coupon is tremendously helpful in forecasting test set purchases. As shown in the left panel of the figure below, users are 2.5% likely to buy a coupon if they\\'ve viewed it exactly once prior to its DISPFROM, but that probability rises to 32% if they\\'ve viewed the coupon four or more times.\\r\\n\\r\\n\\r\\n\\r\\nSecond, users tend to buy the same coupons over and over. As shown in the middle panel of the above figure, a user who has purchased a coupon with a given prefecture, genre, and catalog price four or more times has a 38% chance of buying a matched coupon again in the next week if it is offered for sale.\\r\\n\\r\\nThird, peer group averages can help forecast the behavior of users with little or no history. The right panel of the above figure shows that a user\\'s probability of buying a coupon increases from less than 0.1% to above 0.6% if more than ten percent of age, sex, and geography-matched peers have bought a coupon with the same characteristics.\\r\\n\\r\\nFourth, it\\'s important to consider the geographic coverage of each coupon. To be specific, a coupon is relevant for the multiple prefectures listed in coupon_area_train.csv, not just the single prefecture listed for that coupon in coupon_list_train.csv. In the kernel density plots below, I show the purchase intensity for users based in four prefectures: Tokyo, Kanagawa, Osaka, and Aichi, using the geographic data in coupon_list_train.csv. The purchases for Osaka and Aichi users appear strongly bimodal, with an unusually large number of purchases occurring in the Tokyo region.\\r\\n\\r\\n\\r\\n\\r\\nOn the other hand, if we look at all the prefectures that map to a given coupon, we find that Osaka users purchased Tokyo coupons not because they planned to travel to Tokyo, but because these coupons were also local to Osaka. If we plot the geographic intensity of \"nearest-to-user\" prefecture rather than a coupon\\'s primary listing prefecture, we see much more localized purchase behavior.\\r\\n\\r\\n\\nWords of Wisdom\\nDo you have any advice for those just getting started in data science?\\r\\nFocus on understanding the problem. Without understanding the problem, it\\'s impossible to develop a solution.\\r\\n\\r\\nStart with simple approaches and models. A fast development cycle is key to testing out ideas and learning what works. Don\\'t start building computationally expensive ensembles until you have iterated through most of your best ideas.\\r\\nBio\\nHalla Yang has worked as a quantitative researcher, portfolio manager and trader at Goldman Sachs Asset Management, Jump Trading and Arrowstreet Capital. He holds a Ph.D. in Business Economics from Harvard, and a B.A. in Physics, summa cum laude, also from Harvard. He is about to start a new position as data scientist at a management consulting firm.', 'Vicens Gaitan participated in the\\xa0Flavours of Physics: Finding τ → μμμ challenge, finishing near 14th place (final competition results are still being validated). After the competition close, he spent time researching how other participants handled this complex\\xa0challenge.\\nIn this blog, Vicens walks us through a series of scripts he created that share different methods\\xa0competitors used to pass the Agreement and the Correlation / CVM tests while achieving a high overall score. Vicen\\'s background in physics (including time with the\\xa0ALEPH experiment at LEP) has helped him understand and explain certain nuances of the competition data and design. We\\'ll let Vicens take it from here...\\r\\n[caption id=\"attachment_5284\" align=\"aligncenter\" width=\"177\"] 706 data scientists on 673 teams competed[/caption]\\r\\n\\r\\nFlavours of Physics: Finding τ → μμμ was a challenging problem from both a machine learning perspective (due the side constraint of trying to preserve the model\\'s “physical soundness”), as well as from the\\xa0pure physics analysis methodology (of mixing real data and Monte Carlo data for event selection).\\r\\n\\r\\nThe scope of this project probably goes beyond a reasonable setup for an open competition (not only for high energy people) and this justifies the simplification in physics tests made by the organizers.\\r\\n\\r\\nDuring the first weeks of the competition, it was clear that the control signal\\xa0can be separated from the train sample with high efficiency making the agreement test useless. (Meaning, it would not be hard to \"cheat\" and get around the agreement test.) Because of this the organizers of the competition announced they would manually confirm models physical soundness. So the question then was: how can you perform well without discriminating in an explicit way between both samples?\\r\\n\\r\\nThis problem was looming large during the competition, raising doubts about the coherence of the stated problem, but a few days before the competition’s end some submissions with a lot of 9’s stated that something more fundamental had gone\\xa0unnoticed by most of us: The mother particle\\'s mass is a strong feature and boosted the classifier performance. In fact, it can be calculated with the available information. Therefore the question is now: If this is such a strong feature, why isn\\'t the classifier discovering the formula for the tau mass?\\xa0This is the topic covered in the first notebook:\\r\\nWhy is it so hard to \\'learn\\' the tau mass from kinematics?\\r\\n[caption id=\"attachment_5277\" align=\"aligncenter\" width=\"614\"] This plot shows the “Big Trap”: mass separates simultaneously signal vs background and train vs agreement, but prevents you from passing the CVM test. See the full script[/caption]\\r\\n\\r\\nIf we use the tau mass as a variable, then the result will be highly correlated with mass, preventing us from passing the CVM test. (Curiously, nobody complained about this test until the last days of the competition). That makes the question: How can we bypass the CVM test (and the agreement test).\\r\\n\\r\\nThe second workbook shows how to hack both tests with a single trick. (The trick is not mine. I realized it after analyzing what my solution had in common the winning solutions post-competition :) :\\r\\nClipping & Spreading…\\r\\n[caption id=\"attachment_5278\" align=\"aligncenter\" width=\"633\"] Raising the probability to a high power (clipping to 0 small probabilities & spreading high probabilities between 0-1) adds a small amount of noise & the CVM value drops drastically. See the full script[/caption]\\r\\n\\r\\nAfter discovering this exploit, the next question is: what can be done with this data that neither uses the previous hack nor discriminates agreement events from the training ones.\\r\\n\\r\\nThe solution\\xa0is what I call \"doping\":\\r\\n…or Doping?\\r\\nOne promising possibility (outside of the competition rules) is to discourage the classifier from learning “imperfect simulated” features as real discriminating variables by “doping” the training sample with a small set of Monte Carlo events from a different channel. You need to label the events as “background” (so, as real data in this case).\\r\\n\\r\\n[caption id=\"attachment_5281\" align=\"aligncenter\" width=\"630\"] Without doping, we can reach a weighted AUC of ~.98, but we would fail the agreement test with KS because the KS score is larger than .09. See the full script[/caption]\\r\\n\\r\\n[caption id=\"attachment_5282\" align=\"aligncenter\" width=\"628\"] By adding 3,460 doping events to the training set, we can get a weighted AUC score of ~.98 and we will pass the KS test with a KS score smaller than .09. See the full script[/caption]\\r\\n\\r\\nThis procedure obtains a modest 0.98912 on the public leaderboard, but provides a classifier not relying on MC imperfections and is not correlated with the mass. It is probably useful for event selection.\\r\\nHow did your background in physics help you understand the problem and the competition objective?\\r\\nIt was very useful. Probably the features have no meaning to people outside High Energy Physics, and as it has been shown, domain knowledge can provide insights (for example, the role of mass in\\xa0this problem) that very sophisticated classifiers are not able to catch. Deep neural networks probably had an opportunity to build a mass-like relation, (this is an exercise I want to try), but remember that the mass of a tau particle in an experiment with energies of Tev is a tiny effect and is hard to discover if you don’t know where to look at.\\r\\nWhat are your key takeaways from the competition?\\r\\nA lot of them. Probably the most dramatic is that you cannot rely only in the power of machine learning: looking at the data through the glass of a model can expose facts hard to discover by general function approximation. Another one: a nostalgic feeling of similar problems, many years ago, using neural networks for event selection in the ALEPH experiment at LEP. And the most important: the lessons learned from my\\xa0fellow competitors.\\r\\nWhat was your key insight into the data?\\r\\nDon’t trust the Monte Carlo simulations. At least not fully. Monte Carlo calibration is the key step to use machine learning tools for event selection. “Weak” learners are relatively insensitive to this little imperfections in the simulation, but “hard” ones (like Gradient Boosting Machines) will pick discrepancy as a real separating fact.\\r\\nWhy is it so important to calculate the mass of the tau particle?\\r\\nNo, no… ;) The important fact is to not to calculate the tau mass. Not in this competition because the classifier efficiency is fully based on an artifact in the training set, allowing us to separate signal and background only by the mother particle\\'s mass.\\r\\n\\r\\nI’m joking. The only way to perform well in this competition is to realize this\\xa0fact and to be able to bypass the side band test with clever tricks.\\r\\n\\r\\nCongratulation to the winners, they realized all of this before the competition ended. I’m only analyzing their findings post-mortem. It has been very fun!\\r\\nBio\\nVicens Gaitan is \\xa0R&D director in the Grupo AIA innovation area. He studied physics and get a PhD in Machine Learning applied to experimental High Energy Physics in 1993 with the ALEPH collaboration at CERN. Since 1992 he has been working in AIA in complex problem solving and algorithmic development applied to model estimation, simulation, forecasting and optimization, mainly in the energy, banking and telecommunication sectors.\\r\\n\\r\\n\\n\\r\\n\\r\\n\\xa0\\r\\n\\r\\nCheck out more posts on the Flavour of Physics competition by clicking the tag below!\\r\\n\\r\\n\\xa0', 'In\\xa0Dato\\'s Truly Native? competition\\xa0Kagglers were given the\\xa0HTML of\\xa0webpages\\xa0on StumbleUpon\\xa0and challenged\\xa0to identify paid content\\xa0(in the form of native advertising) from unpaid content. Morten Hustveit finished in second place out of 340 data scientists on 274 teams. His previous work researching and building a text classifier program for HTML documents gave him a unique competitive edge.\\r\\n\\r\\n[caption id=\"attachment_5288\" align=\"aligncenter\" width=\"300\"] Kaggle profile for Morten (aka Mortehu)[/caption]\\r\\nBackground & Tools\\r\\nDay-to-day I work for a venture capital firm called e.ventures. Part of my job there is to develop tools for helping our analysts discover companies that would make good investments. In the weeks before the “Truly Native?” competition began, I had been developing a tool for classifying HTML documents based on their contents. We already use this for a variety of purposes internally. The competition used a very similar type of data, so it was only a matter of writing a small Python script for importing the data to get going.\\r\\n\\r\\nMy text classifier program was inspired by CRM114, an open source program used primarily to identify e-mail spam. At first I just wanted to use that program as-is, but it turned out to have scaling issues that could not be easily overcome. It could not finish processing even 10% of our data in two hours, and I wanted to build many models on all the data every day. The CRM114 team participated at several TRECs (Text REtrieval Conferences), most recently in 2007, and documented the performance of their various classifiers. Based on this, I decided building a linear SVM model on skip-gram features would be the best way to go. This means pairs of nearby words are converted into a feature by hashing, and each document is represented as a set of these features.\\r\\n\\r\\nBack in 2006 I was attempting to implement my own web browser from scratch, being dissatisfied with the speed of web browsers at the time. Like some other overly ambitious projects I’ve started, this was put “on hold” after a couple of months. Even so, I was left with a fast HTML parser, which could be reused for this project. In addition to the skip-grams mentioned above, the HTML parser allowed generating features based on simple semantic relationships expressed in the documents. The end result was a setup that could unzip, parse and generate features for all 400,000 HTML documents in less than five minutes. Counting only features occurring in at least two training documents, this resulted in 79 million features.\\r\\n\\r\\nNot all SVM training programs can build a model with 79 million features in a reasonable timeframe. A paper called “A Dual Coordinate Descent Method for Large-scale Linear SVM”, by Cho-Jui Hsieh et al, describes a training algorithm that works well with this data size. A C++ implementation of that algorithm is available in LIBLINEAR, under the name L2R_L2LOSS_SVC_DUAL. However, LIBLINEAR is designed as a user-friendly library, and makes some compromises in how it represents data internally, leading to a high memory overhead. The algorithm requires randomly shuffling the training data, so it’s critical that all of it can fit in main memory. The feature matrix requires more than 25 gigabytes of storage space when using a compact representation, so I decided to reimplement the algorithm to work with minimal memory overhead. The training time for this model was 32 minutes, not counting hyperparameter optimization.\\r\\n\\r\\nOne thing bothering me about many machine learning tools is that they don’t have built-in functionality for hyperparameter optimization. Users are left to build their own system, or to manually edit configuration files. I ensured my text classifier would automatically select the per-class value for C using some basic heuristics and 10-fold cross validation. Integrating hyperparameter search into the training tool itself has the advantage of being able to train all ten folds in parallel using only one in-memory copy of the data, and to use past solutions as the initial state when training with new hyperparameters.\\r\\nCompetitive Approach\\r\\nAs is common on Kaggle, competition stiffened in the last couple of weeks before the deadline, and I realized I couldn’t win using just this simple linear model. On the private leaderboard it would have scored only 0.98519, and on September 28th user Glen had already made a submission scoring 0.98801. I needed a second model.\\r\\n\\r\\nWhen I was hired by e.ventures I did not have any machine learning experience, so they provided me with a crash course by Timo Duchrow, who organized the Otto Group Kaggle competition. He introduced me to the idea of using frequent substrings as features, which has been successfully applied in his old field, bioinformatics. When you first start looking at the problem of counting substring occurrences, you realize that an N character document has N(N+1)/2 unique substrings. Enumerating all the substrings of a gigabyte of text would take longer than the duration of this competition, so what is a programmer to do?\\r\\n\\r\\nThe approach I used involves constructing an alphabetically sorted list of all substrings, called a suffix array, which can be done in O(N) time. From there it’s easy to skip all substrings that only occur once, which usually is the vast majority. The remaining strings can be enumerated and processed in a tiny fraction of the time. This requires a lot of RAM, so I only processed a one gigabyte sample of the training data this way; 512 MB of positive examples, and 512 MB of negative examples. Substrings were counted only once for each document they occurred in, and most redundant substrings were deleted. The running time was about ten minutes.\\r\\n\\r\\nWith the most predictive substrings extracted, I selected the top 30,000 to use in my model. I’m a big fan of automatic feature selection, because this makes the model design process more repeatable and generally applicable. However, I decided to also include one manually crafted feature; 10-digit integers looking like Unix time. Since the competition organizers had largely downloaded the positive and negative training examples on different dates, date related hints inside the documents were very strong features, including references to photographs of Pluto, Donald Trump’s media appearances, and El Chapo’s escape from prison. Had the documents been downloaded in random order, the top score on the leaderboard would probably have been far lower. These numbers were present in about 25% of the training data.\\r\\n\\r\\n\\r\\n\\r\\nUsing the 30,000 substrings and the numeric feature, I trained an XGBoost model with 2000 trees, doing almost no hyperparameter optimization. This took almost 5 hours. This model alone scores 0.98856 on the private leaderboard. My final submission was a simple linear combination of these two models: the predictions from the SVM model multiplied by 6 added to the predictions of the XGBoost model, resulting in a score of 0.99017. The scaling factor was selected, rather arbitrarily, to make the range of each model roughly the same based on a scatterplot of the predictions from the two models:\\r\\n\\r\\n\\r\\n\\r\\nBoth the text classifier and substring counter I used are available on my GitHub page, under the terms of the GNU General Public License.', 'Kaggle has a new #1 data scientist! Gilberto Titericz usurped Owen Zhang\\xa0to take the title of #1 Kaggler\\xa0after his team finished 2nd in the Springleaf Marketing Response\\xa0competition. As part of our series\\xa0Profiling Top Kagglers, we interviewed Gilberto to learn more about his background and how he made his way to the top of the Kaggle community.\\r\\n\\r\\n[caption id=\"attachment_5298\" align=\"aligncenter\" width=\"300\"] Gilberto\\'s profile on Kaggle[/caption]\\r\\nGilberto Titericz Q&A\\nHow did you start with Kaggle competitions?\\r\\nI am an electronic engineer, but I always had interest in machine learning algorithms. In 2012 I attended the Google AI challenge and when it finished I looked for another way of competing online and found Kaggle via a Google search and I loved it. My first competition was\\xa0Global Energy Forecasting Competition 2012 - Wind Forecasting and I luckily placed 3rd using a bag of Matlab neural networks. Since then,\\xa0I\\'ve tried to learn more from each competition.\\r\\nWhat is your first plan of action when working on a new competition?\\r\\nFirst of all it is very important understand the problem, the features, and the evaluation metric. After that I look for a good cross validation strategy. Those\\xa0are the very basic and essential steps. Only after that you can start thinking about feature engineering and training algorithms that match the problem.\\r\\nWhat does your iteration cycle look like?\\r\\nI always prepare the dataset and apply feature engineering as much as I can, then I choose a training algorithm and optimize hyperparameters based on a cross validation score. If a model is good and stable I save the trainset and testset predictions. Then I start all over again using another training algorithm or model. When I have a handful of\\xa0good model predictions, I start ensembling at the second level of training.\\r\\nWhat are your favorite machine learning algorithms?\\nGradient Boosting Machines are the best! It is amazing how GBM can deal with data at a high level of depth. And some details in an algorithm can lead to a very good generalization. GBM is great dealing with linear and non-linear features, also it can handle dense or sparse data. So it\\'s a very good algorithm to understand core parameters and it\\'s always a good choice if you are not sure what algorithm to use. Before I knew of GBM, I was a big fan of neural networks.\\r\\nWhat are your favorite machine learning libraries?\\r\\nI have many favourite libraries, but I will try to rank them according the ones I use most:\\r\\n- XGBoost - Fast and optimized GBM implementation.\\r\\n- Vowpal Wabbit - Very fast with\\xa0lots of parameters and linear algorithms.\\r\\n- LibFM - Factorization Machines.\\r\\n- scikit-learn - Lots of functions and algorithms.\\r\\n- Lasagne - Very good and fast NN implementation (but unfortunately I don\\'t have a GPU).\\r\\n- Matlab Neural Network Toolbox - Classic.\\r\\n- R GBM, randomForest and glm - Classic.\\r\\nWhat is your approach to hyper-tuning parameters?\\r\\nI tried grid search in the past, but I found that it consumes a lot of time. So I often use manual brute force searching based on my previous experience. And I always use cross validation to consistently improve the evaluation scores.\\r\\nWhat is your approach to solid CV/final submission selection and LB fit?\\r\\nI always trust my local cross validation score, but in some competitions when the public testset has\\xa0enough samples and the data is not very noisy I will trust both CV and LB. A good scoring formula I\\'ve used before is:\\r\\n[(LocalCVscore*number_rows_trainset) + (LBscore*number_of_rows_used_to_calculate_LB)] / (sum_of_number_of_rows_in_CV_and_LB )\\r\\nSo for my two final selections I use the formula above if the LB is stable. If not, I usually choose my best reliable\\xa0Local CV model and the best LB submission model.\\r\\nIn a few words: What wins competitions?\\r\\nSome good models and features, consistent cross validation methods, teaming up, ensemble meta models, hard work(~15h/week), and always \"Luck\"!!!\\r\\n\\r\\n[caption id=\"attachment_5297\" align=\"aligncenter\" width=\"413\"] Gilberto\\'s top 8 finishes[/caption]\\r\\nWhat is your favourite Kaggle competition and why?\\nOtto is my favourite. Not only because I won, but in this competition it was possible to apply various methods of machine learning and ensembling techniques. In addition, it was the biggest competition in terms of number of competitors in Kaggle\\'s history.\\r\\n\\r\\n[caption id=\"attachment_5299\" align=\"aligncenter\" width=\"500\"] Final private leaderboard for the Otto Product Classification Challenge[/caption]\\r\\nWhat was your least favourite Kaggle competition experience?\\r\\nCompetitions with\\xa0very small trainset datasets. It makes cross validation and final results unstable. Now\\xa0I just avoid noisy and small datasets, because it\\'s very hard to apply feature engineering and cross validation efficiently.\\r\\nWhat field in machine learning are you most excited about?\\r\\nNowadays I\\'m very interested in deep learning. I think it can still surprise us in the future.\\r\\nWhich machine learning researchers do you study?\\r\\nMost of what I learn comes from Kaggle competitions, forums, open source code, and scripts.\\r\\nCan you tell us something about the last algorithm you hand-coded?\\r\\nI prefer to use open source algorithms and I spend most of my time tuning models. Algorithms I usually hand-code\\xa0are simple, like feature selection or some kind of training automation.\\r\\nHow important is domain expertise for you when solving data science problems?\\r\\nIt depends on the problem. For example, for the BCI Challenge NER 2015, I\\'m sure you had to be an expert in the subject to be well placed. The same for the last two CERN competitions (Flavour of Physics & Higgs Boson ML Challenge). If you knew physics, you had\\xa0a big\\xa0advantage over others.\\r\\n\\r\\nBut I\\'m not an expert in the subject matter of all of my winning competitions. Some examples are: Otto Group, Caterpillar, Fast Iron, AMS 2013-2014 Solar Energy Prediction Contest, Springleaf and GEFCom2012. So I believe for most of the problems you can do well even if you aren\\'t an expert.\\r\\nWhat do you consider your most creative trick/find/approach?\\r\\nI don\\'t have that creative trick, but if I can say what I use that gives me more spots it is a second training level ensemble technique. Some people also call that: \"stacking models\", but it\\'s all the same. That technique depends a lot of correct cross validation and generation of non over fitted meta features. Also it can increase a lot the performance if meta features are generated using different features and training algorithms, because L2 can extract only the best predictions of each meta feature.\\r\\n\\r\\n[caption id=\"attachment_5301\" align=\"aligncenter\" width=\"486\"] Example of stacking taken from the\\xa0MLWave Kaggle Ensembling Guide[/caption]\\r\\nHow are you currently using data science at work and does competing on Kaggle help with this?\\r\\nI work as an automation engineer in a oil refinery. So I don\\'t have much chance to use data science for it. But I can say I\\'m looking foward to change it on next year. And I can say that most of what I know today about data science I owe it to Kaggle.\\r\\nWhat is your opinion on the trade-off between high model complexity and training/test runtime?\\r\\nThat a good question. I confess that I just care about training runtime. Model complexity must be proportional to the CPU power you have and the perfomance that you want. So I always start using simplest dataset as possible and take notes of its training time and performance. After that a good approach in to increase model complexity and note if it increase performance and times. A good tradeoff between complexity and time depends a lot of how much time you have to spend increasing complexity of model and waiting the training time. Usually models that you can build and train in just a few hours are the best, because you will need time to fine tune parameters and it\\'s normal having to train it a lot of time.\\r\\nWhat is your view on automating Machine Learning?\\r\\nAutomation is good for preprocess some dataset conditions, cross validation folds, model selection, feature selection and hyperparameters search.\\r\\nI think all other processes are better if have human interference.\\r\\nHow did you get better at Kaggle competitions?\\r\\n- Joinning a lot of competitions.\\r\\n- Learning from Kaggle forums and scripts.\\r\\n- Trying new algorithms and techniques.\\r\\n- Teaming up with experienced Kagglers.\\r\\n- Study competitions code and methods from winners.\\r\\n- After each competition update your personal helping functions library.\\r\\n- Dedication. Sometimes when coding I share my laptop screen with my two daughters, they love cartoons.\\r\\nWhat did you learn from doing Kaggle competitions?\\r\\nKaggle in an excellent place to learn about Data Science. It offers many kind of datasets covering a vast type of problems. I learned a lot after those three years competing and I can say it is addictive. Specially if you are that type of person that like to see the machine learning magic in action.\\r\\nBio\\nI have a MS in Electronic Engineering. In the past 16 years I\\'ve been working as an Electronic Engineer for big multinationals like Siemens, Nokia, and later as an Automation Engineer for Petrobras Brasil. In 2008 I started learning Data Science by myself, at first to improve personal stock market gains. In 2012 I joined Kaggle.com and started to continually learn more. See Gilberto\\'s\\xa0LinkedIn.', 'The Flavour of Physics competition challenged Kagglers to identify a rare decay phenomenon (τ- → μ+μ-μ-\\xa0or τ → 3μ) to help establish proof of \"new physics\". The competition was an exciting opportunity for the community to collaborate with scientists from the LHCb experiment at CERN. 706 data scientists on 673 teams participated, grappling with the complex subject matter and unusual competition setup. Josef Slavicek finished 3rd with the help of XGBoost (and a lucky typo). Below he explains the competition design, its potential weaknesses, and his wild path\\xa0to the top of\\xa0the leaderboard.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nFor me, it all started with the great Stanford online course \"Introduction to artificial intelligence\" led by Peter Norvig and Sebastian Thrun. (The course is now on Udacity.) I wish to thank both the professors because they really changed my life. Since then, I\\'ve taken several more online courses on Coursera and Udacity. And in 2013, I started to compete on Kaggle.\\r\\n\\r\\n[caption id=\"attachment_5311\" align=\"aligncenter\" width=\"300\"] Josef\\'s profile on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nThe domain of this competition was high energy physics. I studied physics when I was young. Unfortunately, it was not in this century and I\\'ve forgotten almost everything I learned then. One of few things I still remember is picture of ant walking on the surface of apple, which is an image I have seen on the cover of one mysterious big black book.\\r\\n\\r\\n[caption id=\"attachment_5313\" align=\"aligncenter\" width=\"240\"] Edit: I looked now at the book again after all these years and I found that I remembered it wrong: there is no ant, only an apple and a magnifying glass.[/caption]\\r\\n\\r\\nSo, this is present state of my domain knowledge. Not surprisingly, I didn\\'t attempt to use it in the competition.\\r\\nWhat made you decide to enter this competition?\\r\\nThe competition gave me chance to participate in something really great - interpreting data from the LHCb experiment at CERN. To touch the border of human knowledge. For me, there was nothing to decide about. It was clear that I must try it.\\r\\n\\xa0Let\\'s Get Technical\\nWhat made this competition unusual?\\r\\nLet me first briefly describe some unusual properties of this competition. In common classification tasks, we have a dataset containing positive and negative examples and we are asked to train classifiers which can distinguish between them. In the Flavour of Physics competition, the situation was more complex.\\r\\n\\r\\nThe negative examples were actual sensor data from LHCb experiment. The problem is that for positive examples no such real data exists. The goal of the competition was to create a detector of new physics events which\\xa0hadn\\'t\\xa0been\\xa0observed yet. To make this possible, the dataset contained simulated positive examples. An obvious problem with this approach was that if the simulation introduced some easily detectable artifacts into the data, we could\\xa0easily end up with a classifier distinguishing simulatedEvent X realEvent (instead of the desirable interestingEvent X notInterestingEvent).\\r\\n\\r\\nTo prevent this, the organizers introduced a statistical \"Agreement test\" on a separate data set. The agreement dataset contained both simulated and real data for another kind of physical event which was already known to exist and already observed.\\r\\n\\r\\nThe second unusual test in this competition was the so called \"Correlation test\". For some reason, in high energy physics, the mass of a particle is considered to be an unreliable feature to detect the actual type\\xa0of particle. So the Correlation test is designed to prove that the results of our classifiers are not highly\\xa0correlated with particle mass.\\r\\n\\r\\n[caption id=\"attachment_5307\" align=\"aligncenter\" width=\"629\"] An illustration of the correlation test. On the left shows no correlation between prediction and mass while on the right there is correlation.[/caption]\\r\\n\\r\\nThe third unusual property of this competition was the evaluation metric. It was \"weighted area under the ROC curve\", as described here.\\r\\n\\r\\n[caption id=\"attachment_5305\" align=\"aligncenter\" width=\"461\"] The ROC curve is divided into sections based on the True Positive Rate (TPR).[/caption]\\r\\n\\r\\nNote that a perfect score of 1.0 can be achieved even with a far-from-perfect classifier: because of weighting, it doesn\\'t matter what happens above TPR=0.8. In other words, if your classifier can classify the first 80% of positive examples perfectly without a single false positive, it can fail on the rest without penalty.\\r\\nHow did you spend your time on this competition?\\r\\nNow I can tell the crazy story of the development of my classifier. I started with small and simple neural networks: a single hidden layer of 64 units, all units logistic, learned by backpropagation. Performance of this NN was poor, so I downloaded the introductory xgboost example from the competition scripts and tried to average the results of xgboost with the results of my NN. The performance was slightly better than the xgboost alone, scoring around 20th place on LB.\\r\\n\\r\\nThen I spent weeks trying to improve on it with not much success. As the end of competition approached, someone made available a more sophisticated xgboost script. (I believe, but I\\'m not sure, that it was this).\\r\\n\\r\\nWhen I combined it with result of my NN, wonders started to happen. The first breakthrough move I discovered was when I tried to preprocess the xgboost input data with PCA (without dimensionality reduction, only to zero-mu, unit-sigma normalize and decorrelate the data). For some reason, xgboost was able to achieve an almost perfect ROC score when applied to preprocessed data. However, it failed desperately on both agreement and correlation tests. But averaging this result with NN with ratios\\r\\n0.1 * xgboostResult + 0.9 * NNResult\\r\\nproduced predictions that passed the tests and still achieved a\\xa0very good ROC score (at one moment I was at first place on LB). Another small improvement I obtained was when I trained simple mass estimator and added this estimated mass as an additional input (particle mass was known for the training data, but not for the test data).\\r\\n\\r\\nThe second key trick I found really accidentally in the last\\xa0hours of the competition. By a typo error, I applied strange exponentiation to my averaging formula:\\r\\nq * xgboostResult ^ exponent + (1 - q) * NNResult\\r\\nThe result was surprising for me and until the end of competition I didn\\'t understand how it worked internally. I only observed that:\\r\\n\\nwith high values of exponent, it is possible to use much higher values of q and still pass the the agreement and correlation test.\\nwith high values of exponent, the ROC score of the averaged result is closer to the (almost perfect) score of xgboostResult even for smaller values of q.\\n\\r\\nWith the exponent trick, my solution achieved a perfect score on public LB and placed third on the private LB.\\r\\n\\r\\nAfter the end of competition I simplified my solution to the level where it can be run as this Kaggle script.\\r\\n\\r\\nThis simplified version contains no mass estimator and no neural networks. It consists only of\\xa0two gradient boosted trees and produces slightly worse results than my original solution. However, the results are still sufficient to achieve third place on the private LB. And I finally understood how the exponent trick works. (It is explained in the discussion below the script.)\\r\\n\\r\\n[caption id=\"attachment_5310\" align=\"aligncenter\" width=\"614\"] Zoomed histograms of my \"Strong\" and \"Weak\" predictor for test and check_agreement data with peaks cropped[/caption]\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nThe simplified version finishes in 4min on my local computer (1 x Inter Core i7).\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nUnfortunately, instead of doing something useful, my solution just found ways to bypass the tests introduced by organizers and exploited the weakness of the evaluation metric. (See discussion below the script of my simplified solution). I\\'m sad about\\xa0it and I didn\\'t do it intentionally.\\r\\n\\r\\nIt shows that the algorithms we are using are starting to be more clever than we are. When there is a way, they will find it. The Correlation and Agreement tests were carefully designed obstacles to prevent our solutions from using two particular ways of reasoning - and both the obstacles failed.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nCompete on Kaggle, next time you can be in the top three in a\\xa0prestigious competition like this one was. Yes, you can do it. If I could, so can anyone :-) .\\r\\nBio\\nJosef Slavicek: I\\'m not data scientist. I\\'m not even a professional in a field of data science, machine learning, or something similarly exciting. I\\'m an ordinary, plain old software developer at an insurance company. To make my life a little bit more interesting, I recreationally compete in cycling races (in the summer). And similarly, in the winter, I recreationally compete on Kaggle.\\r\\n\\r\\n\\r\\n\\r\\nCheck out more posts on the Flavour of Physics competition by clicking the tag below!\\r\\n\\r\\n\\xa0', 'Vlad Mironov and Alexander Guschin of team Go Polar Bears took first place\\xa0in the\\xa0CERN LHCb experiment\\xa0Flavour of Physics competition. Their model was best able to identify a rare decay phenomenon (τ- → μ+μ-μ-\\xa0or τ → 3μ) to help establish proof of \"new physics\". Below they share the technical highlights of their approach and solution.\\nZero to 1.000000\\r\\nWe decided to be short and to post the most critical and interesting parts of our solution.\\r\\nFeature engineering with mass recreation\\r\\nUsing scholar physics equations we recreated mass which in our case worked out as a golden feature:\\r\\n\\r\\n\\r\\n\\r\\nFirst of all, we should find the projection of the momentum to the z-axis for each small particle. Then summarize all of them for our big particle and get pz. After that we can\\xa0find the full momentum p.\\r\\n\\r\\nFrom the equation time ∗ Velocity = Distance we can\\xa0find Velocity (in our case it’s the ‘speed’ feature). Considering that FlightDistance was calculated untruly and since we knew\\xa0its error, theoretically, we could improve our prediction. But after a few different implementations with no improvement we left\\xa0‘FlightDistance’ as it was.\\r\\n\\r\\nThe same thing for ‘new mass’. From the scholar equation p = mV we get the rest.\\r\\nTriade of the models\\r\\nThis competition depended on three main parts: feature engineering (read as improvement of AUC score), agreement test, and mass correlation. So there should be no surprise that each of our models solves exactly one problem at a time and, like the famous Borromean rings, support each other.\\r\\n\\r\\n\\nSmall forests\\r\\nWe found that Mass correlation error could be significantly decreased using XGBoost with a small number of trees and colsample. For example, here are the parameters for our first XGBoost (XGB1):\\r\\n\\r\\n\\r\\n\\r\\nLater we had some problems with non-deterministic behavior on different machines. We found that this behavior\\'s roots were from this unsolvable problem with C++ rand() function. Here’s our fix.\\r\\nCorrected mass and new features\\r\\nThe main problem with ‘new mass’ that it poorly correlates with real mass and generates both false-positive and false-negative errors near signal/background border:\\r\\n\\r\\n\\r\\n\\r\\nSo we implement model which corrects this behavior. And it’s the heaviest part of the solution. XGBoost with almost three thousand trees and with all features predicts new mass error. Jointly we use multilevel kfold with bagging. In addition at this stage we calculate new mass delta and new mass ratio. This features we’ll use for XBG5 and Neural Networks:\\r\\n\\r\\n\\nNN\\r\\nNeural Network with all new features, bagging. One DenseLayer with 8 neurons. Any other configurations as: more layers, more neurons shows the same or less AUC. Also Dropout leads to poorer results.\\r\\nAll of this could be explained by physical nature of the contest. Every feature has clear and one-way dependence between each other.\\r\\nFinal combination\\n\\nAbout us\\nVlad Mironov, M.S. in CS, Lomonosov Moscow State University (hello@vladmironov.com)\\r\\nRole: feature engineering, small forests, xgboost fix and team spirit :)\\r\\n\\r\\nRight now I’m looking for an interesting job in data science including not only ML, but also a heavy load computing and backend. I’m willing to relocate and work with passion.\\r\\n\\r\\n[caption id=\"attachment_5323\" align=\"aligncenter\" width=\"300\"] Kaggle profile for Vlad (aka littus)[/caption]\\r\\n\\r\\nAlexander Guschin, Moscow Institute of Physics and Technology (1aguschin@gmail.com)\\r\\nRole: NN, mass correction, final mix, Kaggle’s magic.\\r\\n\\r\\n[caption id=\"attachment_5324\" align=\"aligncenter\" width=\"300\"] Kaggle profile for Alexander[/caption]\\r\\nTl;dr\\r\\nRTFM, correct your golden features, small forests could be useful.\\r\\n\\r\\n\\r\\n\\r\\nLooking for more in the Flavour of Physics competition? Click the tag below!\\r\\n\\r\\n\\xa0', 'This contest was organized by Dato (from GraphLab Create).\\r\\n\\r\\nThe task of the Truly Native? contest was:\\r\\n\\r\\nGiven the HTML of ~337k websites served to users of StumbleUpon, identify the paid content disguised as real content.\\n\\nThe Basics\\nWhat was your background prior to entering this challenge?\\nMarios Michailidis: I am a PhD. student (on improving recommender systems) and a sr. personalization data scientist at dunnhumby.\\r\\n\\r\\nMathias Müller: I have a Master\\'s in computer science (focus areas cognitive robotics and AI) and work as a machine learning engineer at FSD.\\r\\n\\r\\nHJ van Veen: I study machine learning and artificial intelligence. I work in R&D for a Dutch healthcare IT startup and write for MLWave.\\r\\nHow did you get started with Kaggle?\\nMarios Michailidis: I wanted a new challenge and learn from the best.\\r\\nMathias Müller: I found Kaggle after doing a TopCoder Marathon Match.\\r\\nHJ van Veen: I was looking for cool challenges and read a post by Rob Renaud.\\r\\n\\nLet\\'s Get Technical\\r\\nDuring the competition we tried a wide variety of models and approaches.\\r\\n\\r\\nOnce the fog of war cleared these essentials remained:\\r\\n\\nA deep XGBoost on text with tokenizer, tfidf-vectorizer, cleaning, stemming and n-grams,\\nA weighted rank average of multi-layer meta-model networks (StackNet).\\n\\r\\nThe XGBoost model got us to top 10. The meta-modelling then got us to the first position.\\r\\nWhat worked\\nMeta-modelling. Stacked generalization in a multi-layered fashion. As described in the Stacked Generalization paper, the output of a stacker model can serve as the input for yet another stacker model. We did this for 3 levels as it kept increasing our AUC score. The first level predictions are also fed to the highest (3rd) level of stacking (together with a subset of raw features).\\r\\n\\r\\nCross-validation. With the large dataset we were able to keep stacking out of fold predictions and improve the AUC score. 5-fold CV for every model was very close (and always a little below) our public leaderboard score. We went up one position on the private leaderboard.\\r\\n\\r\\nCleaning. Basic lowercasing, removing double spaces, removing non-alphanumeric characters, repairing documents where the characters are separated by 3 spaces.\\r\\n\\r\\nNgrams, chargrams, and skipgrams. 1-3-grams in combination with cleaning captures dates, url\\'s, and natural language indicative of native advertising.\\r\\n\\r\\nStemming. Both stemming with the Snowball stemmer (NLTK) and the Mineiro stemmer worked for detecting the root of tokens or to lessen noise.\\r\\n\\r\\nTf-idf. Term frequency can prevent a bias to longer documents. Inverse document frequency can prevent a bias to common tokens. tfidf outperformed tf, which outperformed binary frequency.\\r\\n\\r\\nSmall and large memory. We had access to machines ranging from 2-core 4GB laptops to 30-core 256GB servers. Both types of hardware were able to contribute.\\r\\n\\r\\nNon-alphanumeric features. Building on the forum post by David Shinn we created count features for every document (number of dots, spaces, tabs, URL\\'s, script tags, commas etc.). These counts were fed to the highest levels of stackers as raw features.\\r\\n\\r\\nXGBoost. As the winner of an increasing amount of Kaggle competitions, XGBoost showed us again to be a great all-round algorithm worth having in your toolbox. With a max_depth of over 30 XGBoost was allowed to build deep trees with the text tokens.\\r\\n\\r\\nArtificial Neural Nets. We further dipped our toes in the lake of deep learning with single-layer, dual-layer and three-layer neural nets. The shallow nets, like Perceptron were trained on the raw features. The multi-layer nets (Theano, Lasagne via NoLearn) were used for stacking.\\r\\n\\r\\nExtremely Randomized Trees. ExtraTreesClassifier from Scikit-learn was used in all levels of stacking. Especially in the higher levels it proved to be a good algorithm with the best variance and bias reduction.\\r\\n\\r\\nLibLinear. Regularized Logistic Regression with LibLinear (via Scikit-learn) was used in the first layer of generalization as a \"grunt\" linear algorithm. It did not beat XGBoost in accuracy, but it did so in speed.\\r\\n\\r\\nSub-linear debugging. Parameter tuning, feature extraction and model selection was done with sublinear debugging. For online learning algorithms we looked at progressive validation loss, and for all models we looked at the first round of out-of-fold-guessing.\\r\\n\\r\\nPatience. The larger deep models took days to train. We resisted early ensembling and kept focus on creating out-of-fold predictions.\\r\\n\\nAnimation by Nadja Rößger. Click here to replay\\nWhat more or less worked\\nOnline learning. For online learning we experimented with the tinrtgu\\'s FTRL and SGD scripts and Vowpal Wabbit. The FTRL script is able to get to ~97 using multiple passes/epochs as shown by Subhajit Mandal.\\r\\n\\r\\nAdaBoost. In the second generalization layer we added AdaBoosting with decision tree stumps. Random Forests did a little better.\\r\\n\\r\\nRandom Forests. Also from the second generalization layer, and also slightly beaten by Extremely Randomized Trees.\\r\\n\\r\\nLatent Semantic Analysis. We used TF-IDF followed by truncatedSVD to generate a new feature set.\\r\\n\\r\\nModels trained on external data. We trained models on both the IMDB movie review dataset (from the Kaggle Word2Vec tutorial) and on the 20 newsgroups datasets. These models generated predictions for both the train and test set.\\r\\nWhat did not make it\\n\\nVowpal Wabbit. Models trained before the reset were not updated, as we were further along in the modeling process by then.\\nMatlab NN. We had some troubles with instability/overfit using Matlab NN\\'s for stacking.\\nNormalized Compression Distance. Though promising for creating informative features this technique was left out. (code)\\nApproximate IDF-weighing. Using a count-min sketch we generated approximate inverse document frequency for online learning.\\nNaive Bayes. Using the same count-min sketch we generated approximate Naive Bayes vectors as used in NBSVM. We did not want to add information from the entire train set for out-of-fold predictors.\\nWe never tried LDA, word vectors, POS-tagging or RNN\\'s.\\n\\n\\nDifferent models\\' performance maps\\nWhat did not work\\nFast & simple solution. Our entire solution takes 4 weeks to run on ~55 cores and may take, at peak, around 128GB of memory. Mortehu definitely won, in this regard, with his simpler production-friendly models.\\r\\n\\r\\nHeaders. There was a brief conflict in the beginning of the competition about the \"correct\" way to store and share out-of-fold predictions. This spawned a meme than ran for the entirety of the contest. Let\\'s just say that it doesn\\'t matter how you store these (.npy, .txt, .csv, with/without headers, with/without ID\\'s, ordered by ID or sample submission), you should agree on one, and only one way to do this.\\r\\n  Triskelion: I put the oof\\'s in the svn in .csv format, with \\r\\n              headers (file,prediction), just like you asked\\r\\n Michailidis: ...\\r\\n      Mathias: :)\\r\\n\\nBlacklists. Before the restart simple \"blacklists\" seemed to work a bit. After the reset this effect was gone. All URL\\'s on page were parsed and URL\\'s that appeared only within positively labelled documents were added to the blacklist.\\r\\n\\r\\nRestart. Not all modelling techniques, ideas and vectorizing approaches survived the restart.\\r\\n\\r\\n\\nRanking feature and model interactions with XGBfi. (click to enlarge)\\nFuturistic\\r\\nTeam discussion often ventured beyond standard meta-modelling. Interesting questions and ideas popped up:\\r\\n\\nstacking features: Use the leaf ID\\'s from XGB as features. Also try to use the residual (Y_real-Y_pred). See Jeong-Yoon Lee presentation.\\nmulti-layer stacking: Just how deep can we go for a significant improvement?\\ntransfer learning: Use XGBoost as a feature-interaction extractor for linear models like those from VW. Train a shallow net on multi-class predictions from ensemble.\\nnet architecture: We did a form of human backprop. Can this be automated? Likewise, can we implement dropout?\\nmodel averaging: Practical sharing of insights with geometric mean vs. mean vs. rank averaging.\\nreusable holdout set: Can we fully eliminate the need for leaderboard feedback and rank models on a ladder?\\nreusable trained models: Can we store models trained on different data and use their predictions for a new train and test set as features?\\nreusable training information: Can we store and reuse the knowledge gained during training on data, models, pipelines, problems and generalization?\\nself-guessing:\\n\\nRather one constructs a generalizer from scratch, requiring it to have zero cross-validation error. Instead of coming up with a set of generalizers and then observing their behavior, one takes the more enlightened approach of specifying the desired behavior first, and then solving the inverse problem of calculating the generalizer with that desired behavior. This approach is called \"self-guessing\". -Wolpert (1992) Stacked Generalization\\nWe Learned\\n\\nBasic NLP still works very well for this task.\\nHTML is a messy \"natural\" language.\\nBigger datasets allow for bigger ensembles.\\nBigger ensembles require bigger hardware.\\nMulti-layer stacking can keep squeezing out AUC.\\nThe higher the stacking level, the shallower the models need to be.\\nTeaming up is a good thing to do.\\nDiverse independent teams work well with an agile management style.\\nPatience. Don\\'t get lost in premature optimization with ensembles.\\nUse solid development tools: code/model repositories, issue tracker, communication.\\nYou should always store out of fold predictions with headers and sample ID\\'s.\\n\\nJust For Fun\\nWhat kind of Kaggle competition would you like to see?\\nMarios Michailidis: Predicting (Kaggle) rank from (LinkedIn) profiles.\\r\\nMathias Müller: a ML corewars.\\r\\nHJ van Veen: Predicting pixel values for partly obscured images.\\r\\nMad Professors\\r\\nOur team name is a nod to the Mad Professor -- a creative dub artist twisting hundreds of knobs to mix music -- and pays homage to a real driving force behind (academic) research: all teachers, assistants, and professors.\\r\\n\\r\\nBy name we would like to mention: Abu-Mostafa, Bishop, Caruana, Dean, Graham, Langford, Lecun, Hinton, Huttunen, Ng, Schmidhuber, Vitanyi, Wiering, Wolpert, Zajac.\\r\\n\\r\\nFollowed by a randomly shuffled list, (capped at 99), of people who have, directly or indirectly, contributed brain power to the computation of StackNet.\\r\\nrandom.shuffle(l)\\r\\nprint l[:99]\\r\\n\\nThanks\\r\\nThanks to the competitors for the challenge, Kaggle for hosting, Dato for organizing, and StumbleUpon for providing the data. Thanks to the open source community and the research that makes it all possible.\\r\\n\\r\\n\\n\\nYou know you like ExtraTreesClassifier, when you feed the output of a logistic regression algorithm A trained on a different dataset to an ExtraTreesClassifier B, its output to another ExtraTreesClassifier C, its output to another ExtraTreesClassifier D, its output to a weighted rank average E. Thanks Gilles Louppe and Geurts et al.!\\nBios\\n\\nMarios Michailidis (KazAnova) is Senior Data Scientist in dunnhumby and part-time PhD in machine learning at University College London (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK Market and has led many analytics projects with various themes including: Acquisition, Retention, Uplift, fraud detection, portfolio optimization and more. In his spare time he has created KazAnova, a GUI for credit scoring 100% made in Java.\\r\\n\\r\\n\\nMathias Müller (Faron) is a machine learning engineer for FSD Fahrzeugsystemdaten. He has a Master\\'s in Computer Science from the Humboldt University. His thesis was about \\'Bio-Inspired Visual Navigation of Flying Robots\\'.\\r\\n\\r\\n\\nHJ van Veen (Triskelion) is a data engineer and researcher at Zorgon, a Dutch Healthcare & IT startup. He studies machine learning and artificial intelligence, and uses Kaggle to learn more about predictive modelling. He is also the author of MLWave.com.', 'How Much Did It Rain? II was the second competition (of the same name) that challenged Kagglers to predict hourly rainfall measurements. Luis Andre Dutra e Silva finished in second place, and in doing so, became\\xa0a Kaggle Master (congrats!). In this blog, Luis shares his approach, and why using an LSTM model \"is like reconstructing a melody with some missed notes.\"\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\n[caption id=\"attachment_5377\" align=\"aligncenter\" width=\"300\"] Luis\\' profile on Kaggle[/caption]\\r\\n\\r\\nI have been\\xa0a software developer since 1985 and I developed a predictive analytical software in 2002 for a university foundation. Since then, I have used, not infrequently, time series prediction techniques (which are the basis of this competition) in many other solutions, like in my previous job at the Brazilian National Treasury. I am currently working with predictive models in my current job at the Brazilian Court of Audit.\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nMy experience with the meteorological domain was nothing before this competition. Nevertheless, I have been using neural networks since the early 2000\\'s and I begun to study machine learning more seriously after being introduced to the subject at college. Recently, in 2012, I attended a graduate machine learning class at Harvard Extension School and, in 2014, I finished with a 100% grade the Machine Learning Class from Stanford, offered online by Coursera.\\r\\nHow did you get started competing on Kaggle?\\r\\nI was already involved in spoken language recognition using signal processing techniques and machine learning when my current boss mentioned, in 2015, the existence of a web site dedicated to machine learning competitions. Since then, I am been participating in Kaggle competitions in order to benchmark my knowledge and skills.\\r\\nWhat made you decide to enter this competition?\\r\\nMy interest in recurrent neural networks, especially LSTM, flourished this year and I have been looking for an opportunity to use them in a concrete problem.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nI used Marshall-Palmer transformation of dBZ values and linearization of DB values as preprocessing and added two new features based on data observations. Each sequence of radar snapshots was used to train a LSTM network that would produce a rainfall estimation as output at the end of each hour.\\r\\n\\r\\n[caption id=\"attachment_5383\" align=\"aligncenter\" width=\"614\"] The evolution of the model\\'s CV error based on architecture choice.[/caption]\\r\\nWhat was your most important insight into the data?\\r\\nThe existence of clogged radar measurements were a perfect fit for a LSTM model, since if some observations are not good, this kind of model can fill the gaps and still produce a meaningful rainfall estimate. It is like reconstructing a melody with some missed notes.\\r\\nWere you surprised by any of your findings?\\r\\nI was surprised in the beginning for the fact that less complex models were better with rainfall predictions than a model with many layers and parameters. The final solution consists of only two layers.\\r\\nWhich tools did you use?\\r\\nI used Theano/Keras for neural networks and scikit-learn for cross validation and metrics. I developed a particular 50-fold CV algorithm based on RMSE, covariances, and average MAE that was consistently better with lower mini batches size.\\r\\nHow did you spend your time on this competition?\\r\\nMost of the time I spent adjusting model parameters and waiting for each ensemble to be trained.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nThe training time took about 10 hours in a Geforce Titan X with mini batches of 256 for all of the 50 different models. The prediction time took around\\xa0less than 10 minutes.\\r\\n\\r\\n[caption id=\"attachment_5382\" align=\"aligncenter\" width=\"614\"] The evolution of the model\\'s CV error based on batch size choice.[/caption]\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\r\\nI have learned that Occam\\'s Razor principle was not simply a matter that he didn\\'t use to shave his beard.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nBegin small, progress slowly, target the stars and reach the Moon.\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nI would propose a project similar to another competition about fighting mosquitos, but targeted to aedes aegypti which are causing an epidemic of many diseases in Latin America.\\r\\nWhat is your dream job?\\r\\nMy current job is my dream job. I love my colleagues and the institution I work for.\\r\\nBio\\nLuis Andre Dutra e Silva is a Federal Auditor at Brazilian Court of Audit. He earned a BS in computer science, and has 30 years of experience in software development and engineering.\\r\\n\\r\\n\\r\\n\\r\\nWant to read more on the How Much Did It Rain? competitions? Click the tag below!\\r\\n\\r\\n\\xa0', 'Rossmann operates over 3,000 drug stores in 7 European countries. In their first Kaggle competition, Rossmann Store Sales, this drug store giant challenged Kagglers to forecast\\xa06 weeks of daily sales for 1,115 stores located across Germany. The competition attracted 3,738 data scientists, making it our second most popular competition by participants ever.\\r\\n\\r\\nGert Jacobusse, a professional sales forecast consultant, finished in first place using an ensemble of over 20 XGBoost models. Notably, most of the models individually achieve a very competitive (top 3 leaderboard) score. In this blog, Gert shares some of the tricks he\\'s learned for sales forecasting, as well as wisdom on the why and how of using hold out sets when competing.\\r\\nThe Basics\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nMy hobby and daily job is to work on data analysis problems, and I participate in a lot of Kaggle competitions. With my own company Rogatio\\xa0I deliver tailored sales forecasts for several companies - product specific as well as overall. Therefore I knew how to approach the problem.\\r\\n\\r\\n[caption id=\"attachment_5409\" align=\"aligncenter\" width=\"300\"] Gert\\'s profile on Kaggle[/caption]\\r\\nHow did you get started competing on Kaggle?\\r\\nI don’t remember, somehow it has become a part of my life. I enjoy the competitions so much that it is really addictive for me. But in a good way: it is nice exposure for my skills, I learn a lot of new techniques and applications, I get to know other skilled data scientists and if I am lucky I even get paid!\\r\\nWhat made you decide to enter this competition?\\r\\nA sales forecast is a tool that can help almost any company I can think of. Many companies rely on human forecasts that are not of a constant quality. Other companies use a standard tool that is not flexible enough to suit their needs. As an individual researcher I can create a solution that really improves business. And that is exactly what this competition is about. I am very eager to further develop and show my skills - therefore I did not hesitate a moment to enter this competition.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nThe most important preprocessing was the calculation of averages over different time windows. For each day in the sales history, I calculated averages over the last quarter, last half year, last year and last 2 years. Those averages were split out by important features like day of week and promotions. Second, some time indicators were important: not only month and day of year, but also relative indicators like number of days since the summer holidays started. Like most teams, I used extreme gradient boosting (xgboost) as a learning method.\\r\\n\\r\\n[caption id=\"attachment_5398\" align=\"aligncenter\" width=\"610\"] Figure 1 a/b. Illustration of the task: predict sales six weeks ahead, based on historical sales (only last 3 months of train set shown).[/caption]\\r\\nWhat was your most important insight into the data?\\r\\nThe most important insight was that I could reliably predict performance improvements based on a hold out set within the trainset. Because of this insight, I did not overfit the public test set, so my model worked very well on the public test set as well as the unseen private test set that was four weeks further ahead.\\r\\nDo you always use hold out sets to validate your model in every competition?\\r\\nYes, sometimes using cross-validation (with multiple holdout sets) and sometimes with a single holdout set, like I did in this competition. The advantage of a holdout set is that I can use the public test set as a real test set, not a set that gives me feedback to improve my model. As a consequence, I get reliable feedback about how much I overfitted my own holdout set. Therefore, I do not like competitions where the train/ test split is not-random, while the public/ private split is random: in such competitions, you can build a better model by using feedback from the public leaderboard. I do not like that because I am not aware of any real life problem that would require such an approach. This competition was ideal for me: the train test split was time based, and so was the public/private split!\\r\\nDo you have any recommendations for selecting data for a hold out set and using it most effectively?\\r\\nFor selecting a hold out set, I always try to imitate the way that the train and test set were split. So, if it is a time split, I split my holdout sample time based; if it is a geographical split by city, I split my holdout set by city; and if it is a random split, then my holdout split will be random as well. You can effectively use a holdout set to push the limit towards how much you can learn from the data without overfitting. Don\\'t be afraid to overfit your holdout set, the public leaderboard will tell you if you do so.\\r\\nWere you surprised by any of your findings?\\r\\nYes, I was surprised that a model without the most recent month of data (that I used to predict sales further ahead) did almost as well as a model that did include recent data. This finding is very specific for the Rossmann data, and it means that short term changes are less important than they often are in forecasting.\\r\\n\\r\\n[caption id=\"attachment_5407\" align=\"aligncenter\" width=\"610\"] Figure 2. This picture illustrates the progress we made in this competition. Xgboost predictions without feature engineering (black) were already quite good. The improvements that full feature engineering (red) gave were really about finetuning.[/caption]\\r\\nWhich tools did you use?\\r\\nFor preprocessing I loaded the data into an SQL database. For creating features and applying models, I used Python.\\r\\nHow did you spend your time on this competition?\\r\\nI spent 50% on feature engineering, 40% on feature selection plus model ensembling, and less than 10% on model selection and tuning.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nThe winning solution consists of over 20 xgboost models that each need about two hours to train when running three models in parallel on my laptop. So I think it could be done within 24 hours. Most of the models individually achieve a very competitive (top 3 leaderboard) score.\\r\\n\\r\\n[caption id=\"attachment_5408\" align=\"aligncenter\" width=\"610\"] Figure 3. A time indicator for the time until store refurbishment (last four days on the right of the plot) reveals how the sales are expected to change during the weeks before a refurbishment.[/caption]\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nMore experience in sales forecasts and a very solid proof of my skills. Plus a nice extra turnover of $15,000 dollars that I had not forecasted.\\r\\nDo you have any advice for those just getting started in data science?\\n\\nmake sure that you understand the principles of cross validation, overfitting and leakage\\nspend your time on feature engineering instead of model tuning\\nvisualize your data every now and then\\n\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nYou have proven to be very good at creating competitions, I don’t have an idea to improve on that right now ;) But\\xa0I have the opportunity so let me share one idea for improvement: to create good models and anticipate the kind of error that can be expected, I often miss explicit information on how the train/test and public/private sets are being split. A competition is (even) more fun for me when I don’t have to guess at\\xa0what types of mechanisms impact model performance.\\r\\nWhat is your dream job?\\r\\nWork for a variety of customers - and help them with data challenges that are central to the success of their business. And have enough spare time to participate in Kaggle competitions!', 'Aaron Sim took first place in our recent How Much Did It Rain? II competition. The goal of the challenge was to\\xa0predict a set of hourly rainfall levels from sequences of weather radar measurements. Aaron and his research lab supervisor were in the midst of developing deep learning tools for their own research when the competition was launched. There was sufficient overlap in the statistical tools and datasets to make the competition a great ground for testing their approach on a new dataset. In this blog, Aaron shares his background, competition experience and methodology,\\xa0and biggest takeaways (hint: Kaggle competitions are anything but covert). To read a more detailed technical analysis, take a look at his personal\\xa0blog\\xa0post\\xa0on GitHub.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI am a postdoc researcher in the Theoretical Systems Biology Group at Imperial College London in the UK. My background is in theoretical physics where I worked on the geometry of string theory backgrounds (in other words, no data whatsoever). My current research involves the development of mathematical and statistical models of biological and other complex systems such as protein interaction networks and cities (lots of data!).\\r\\n\\r\\n[caption id=\"attachment_5419\" align=\"aligncenter\" width=\"300\"] Profile for Aaron\\xa0(aka PuPa) on Kaggle[/caption]\\r\\nWhat made you decide to enter this competition?\\r\\nIt was clear to me from the start that this prediction task – hourly rainfall values from variable-length, time-labelled, sequences of weather radar observations – is very nearly the classic type of problem that one would not hesitate to throw a recurrent neural network at (more of this below). Since I was in the midst of applying some deep learning methods in my current research project, I saw it as a good opportunity to validate some of my ideas in a different context. That, at least, is my post hoc justification for the time spent on this competition!\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nI used recurrent neural networks (RNN) exclusively with minimal preprocessing. As alluded to above, the prediction of cumulative values (hourly rainfall) from variable-length sequences of vectors with a time component is highly reminiscent of the so-called Adding Problem in machine learning – a toy sequence regression task that is frequently employed to demonstrate the power of RNNs in learning long-term dependencies (see Le et al., Sec 4.1, for a recent example).\\r\\n\\r\\n[caption id=\"attachment_5415\" align=\"aligncenter\" width=\"614\"] Figure 1: The prediction target of 1.7 is obtained by adding up the numbers in the top row where the corresponding number in the bottom row is equal to one (i.e. the green boxes). The regression task is to infer this generative model from a training set of random sequences of arbitrary lengths and their targets.[/caption]\\r\\n\\r\\nIn the rainfall prediction problem, the situation is somewhat less trivial as there is still the additional step of inferring the rainfall numbers (top row) from radar measurements. Furthermore, instead of binary 0/1 values (bottom row) one has continuous time readings between 0 and 60 minutes that have somewhat different roles. Nevertheless, the underlying structural similarities are compelling enough to suggest that RNNs, even simple vanilla RNNs with off-the-shelf architectures (see below), would be well suited to the problem.\\r\\n\\r\\n[caption id=\"attachment_5416\" align=\"aligncenter\" width=\"614\"] Figure 2. A basic RNN setup. The bottom layer represents a single input sequence of radar measurements within a single hour. Each number is the minutes past the top of the hour of the measurement, which is preserved as a component of the feature vector. The output is the cumulative rainfall in the hour as measured by a rain gauge.[/caption]\\r\\n\\r\\nIf there’s a secret sauce in my approach beyond simply deepening and widening the above RNN architecture, however, it would probably be the implementation of training- and test-time augmentations to the radar sequences. One common way to reduce overfitting is to augment the training set via label-preserving transformations of the data. The canonical examples are found in image classification tasks where images are cropped and perturbed to improve the generalization capabilities of the classifier. Since we have here a regression problem, it is less obvious what such augmentations should be.\\r\\n\\r\\nMy solution was to implement a form of ‘dropin’ augmentation of the datasets where I lengthened the radar measurement sequences to a single fixed length by duplicating the vectors at random time points. This is, loosely speaking, the opposite of performing dropout on the input layer, hence the name. This is illustrated in the figure below:\\r\\n\\r\\n[caption id=\"attachment_5417\" align=\"aligncenter\" width=\"614\"] Figure 3. Lengthening a length-5 sequence to length-8 sequences. Each coloured box represents a vector of radar measurements. Note that the temporal order of the augmented sequence is preserved.[/caption]\\r\\n\\r\\nThe lengths of the sequences in both the training and test sets ranged from one to 19 measurements per hour. Over the competition I experimented with fixed augmented sequence lengths of 19, 21, 24 and 32 timepoints. I found that stretching out the sequence lengths beyond 21 steps was too aggressive as the models began to underfit.\\r\\n\\r\\nMy original intention was to find a way to standardise the sequence lengths to facilitate mini-batch training. However it soon became clear that this simple generalization of a basic padding operation could be a way to train the network to properly factor in the time intervals between observations; specifically, this is achieved by encouraging the network to ignore readings when the intervals are zero, thereby mimicking the input gate in gated variants of RNNs such as LSTM networks. To the best of my knowledge this is a novel, albeit simple, idea.\\r\\n\\r\\nTo predict each rainfall value at test time, I took the mean of 61 separate rain gauge predictions that were generated from different random dropin lengthening of the radar data sequences. Implementing this procedure alone led to a huge improvement (~0.03) in the public leaderboard score, which translates roughly into a jump from 40th position into a top-ten place.\\r\\n\\r\\nI did not perform any data preprocessing beyond replacing missing components in the radar measurement vectors with zeros.\\r\\n\\r\\nThe best architecture I found over the competition is a 5-layer deep bidirectional RNN with 64 to 256 hidden units, with additional single dense layers after each hidden stack and a single linear layer at the bottom of the network to reduce the dimension of the input vectors. At the top of the network the vector at each time position is fed into a dense layer with a single output and a ReLU non-linearity. The final output is obtained by taking the mean of the predictions from the entire top layer. This is summarised in the figure below:\\r\\n\\r\\n[caption id=\"attachment_5418\" align=\"aligncenter\" width=\"614\"] Figure 4. The best performing architecture in the final ensemble. The red numbers on the right indicate the size of each layer. The output from this single model had a public leaderboard score of 23.6971, which should be good enough for 5th place in the competition.[/caption]\\r\\nWhat were some of the challenges thrown up by this particular dataset?\\r\\nSince the physical locations of the rain gauges and the calendar date and hour of their measurements were not provided for this competition, this had the somewhat unusual implication that it was difficult, if not impossible, to separate out from the training data a sufficiently independent holdout subset. This was discussed at some length in the forum.\\r\\n\\r\\nIndeed as the competition progressed I became increasingly suspicious of what my local validation scores were indicating, especially as I was struggling to get my models to overfit (most definitely a data science first-world problem). I began to rely on the public leaderboard submissions to validate my models – yes I was one of those people making the maximum number of submissions every day (*yikes*). This goes completely against the conventional wisdom of building a robust local cross validation setup and holding one’s nerve and trusting it over the public leaderboard scores. I did, however, live in fear of a great leaderboard shakeup, which thankfully for me did not materialise.\\r\\nWere you surprised by any of your findings?\\r\\nMy biggest surprise was that implementing dropout resulted in consistently poorer scores, contrary to what has been reported by many others for RNNs and in other models such as CNNs. I tried many combinations, including varying the dropout percentage and implementing it only at the top or bottom of the network, all without success.\\r\\n\\r\\nAlso, LSTM networks did not appear to work any better than RNNs with standard hidden layers. Perhaps the advantages are only really apparent for much longer sequences with more complex dependencies than the ones here.\\r\\nWhich tools did you use?\\r\\nI used Python with Theano throughout and relied heavily on the Lasagne layer classes to build the RNN architectures. Additionally, I used scikit-learn to implement the cross-validation splits, and pandas and NumPy to process and format the data and submission files. I trained the models on several NVIDIA GPUs in my lab, which include two Tesla K20 and three M2090 cards.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\n\\nIt is impossible to hide your participation in a Kaggle competition from your partner/spouse. (“You’re doing another Kaggle competition, aren’t you?”)\\nThrowing the neural network equivalent of ‘everything but the kitchen sink’ at any and every problem is almost never a bad idea.\\nDon’t bother with feature engineering; the machines have won.\\n\\nDo you have any advice for those just getting started in data science?\\r\\nThe field is moving very fast – one can’t just rely on standard statistics or machine learning textbooks (or even year-old blog posts). Most research papers are freely available on arXiv, often many months before they are properly published, so that is a good place to hang out.\\r\\n\\r\\nAlso, everything becomes a lot easier to understand once you’ve learnt how to build it. So get stuck in early and don’t worry about not understanding everything from the start.\\r\\nBio\\r\\nAaron Sim has a background in theoretical physics and is currently a postdoc researcher in the Theoretical Systems Biology Group at Imperial College London. Read more by Aaron on his github blog.', 'Rossmann operates over 3,000 drug stores in 7 European countries. In their first Kaggle competition, Rossmann Store Sales, this drug store giant challenged Kagglers to forecast\\xa06 weeks of daily sales for 1,115 stores located across Germany. The competition attracted 3,738 data scientists, making it our second most popular competition by participants ever.\\r\\n\\r\\nCheng Guo competed as team Neokami Inc. and\\xa0took third place using a method, \"entity embedding\", that he developed during the course of the competition. In this blog, he shares more about entity embedding, why he chose to use neural networks (instead of the popular xgboost), and how a simplified version of his model still manages to perform quite well.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI work at Neokami, a machine learning startup located in Munich. I built the neural network behind some of our computer vision products such as VisualCortex, which lets you create your own image classifier easily. I hold a PhD in theoretical physics and have developed algorithms to simulate quantum systems while studying at Ludwig Maximilians University in Munich and Chinese Academy of Sciences in Beijing.\\r\\n\\r\\n[caption id=\"attachment_5466\" align=\"aligncenter\" width=\"381\"] Profile for Cheng Guo (aka entron) on Kaggle[/caption]\\r\\nWhat made you decide to enter this competition?\\r\\nWe had a busy year developing our product, but I was not so busy for a few weeks before Christmas. My colleague Felix reminded me that I could join some Kaggle contest. I knew that a very popular German chain store Rossmann had been running a competition on Kaggle for some time, and I thought it would be fun to join it. Our founders Ozel and Andrei liked the idea and supported me, so I joined in the last month of the competition.\\r\\nWhat have you taken away from this competition?\\r\\nFirst, I invented a new method \"entity embedding\" for this competition. It is a general method and can be applied to many other problems.\\r\\n\\r\\nSecond, a special part about this Rossmann competition is that external data is allowed as long as it is shared in the forum. This brought lots of exciting explorations, insights and fun just like in scientific research. There are so many smart and passionate people in Kaggle and there are also many important and difficult questions waiting to be solved. If those problems are carefully divided and well formulated into small and easy to understand subproblems it may be solved collaboratively by the Kaggle community. This is a world changing potential.\\r\\nJust For Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nI recently read a news article about IBM and Microsoft\\'s effort to forecast China\\'s smog. I think it could have a tremendous environmental, social and economical value to run a competition with similar external data policy like Rossmann Sales to forecast smog. This will help to pinpoint what are the most important contributors (factories/sources etc.) to the air pollution.\\r\\nLet\\'s Get Technical\\nWhat supervised learning methods did you use?\\r\\nDeep neural network is very powerful and flexible and it is already the dominant method in many machine learning problems like computer vision and natural language processing. When reading the Rossmann competition forum I was surprised that most top teams used tree based methods like xgboost rather than neural network. As a fan of neural networks I decided to use only neural network and see how it compares with xgboost.\\r\\n\\r\\nTo make a neural network work effectively on this type of problem which has many category features, I proposed a new method Entity Embedding to represent category features in a multi-dimensional space. It is inspired by semantic embedding in the natural language processing domain. With entity embedding, I found that neural networks generate better results than xgboost when using the same set of features.\\r\\n\\r\\nI also used an unusual small dropout 0.02 after the input layer to improve the generalization. The reasoning is that the 0.02 dropout will randomly remove one small feature or a few dimensions of large features and the model should still be able to get similar result based on the remaining features.\\r\\n\\r\\nI have shared our code in the Kaggle forum here.\\r\\nWhat was your most important insight into the data?\\r\\nMy second favorite approach to the Rossmann Sales problem is to use the historical median of the 4 features (store_index, day_of_week, promo, year) as the prediction. Its score on my validation set, which is close to the final score on the leader board, is 0.133. The result is amazingly good considering how simple this approach is. I wanted to include more features to improve it, unfortunately then comes the data sparsity problem. More specifically, after I added another feature \"month\", some combinations of the 5 features don\\'t have any historical record in the dataset leave aside the median. To overcome the data sparsity problem I got the idea to represent the discrete category features in a continuous space in which the distance between different \"category points\" reflects the similarity of the categories. This is the idea behind the entity embedding method. In this way one can interpolate or use nearby data points to approximate missing data points.\\r\\n\\r\\nTo get an intuitive idea about entity embedding, I used t-SNE to map the high dimensional embeddings into 2D figures. First, let\\'s see the German states (Fig. 1).\\r\\n\\r\\n[caption id=\"attachment_5465\" align=\"aligncenter\" width=\"512\"] Fig 1. t-SNE embedding of German states[/caption]\\r\\n\\r\\nThough the algorithm does not know anything about German geography and society, the relative positions on the learnt embedding of German states resemble that on the below map (Fig. 2)\\xa0surprisingly well!\\r\\n\\r\\n[caption id=\"attachment_5458\" align=\"aligncenter\" width=\"443\"] Fig. 2 Map of German States[/caption]\\r\\n\\r\\nThe reason is that the embedding maps states with similar distribution of features, i.e. similar economical and cultural environments, close to each other, while at the same time two geographically neighboring states are likely sharing similar economy and culture. Especially, the three states on the right cluster, namely \"Sachsen\", \"Thueringen\" and \"SachsenAnhalt\" are all from eastern Germany while states in the left cluster are from western Germany. This shows the effectiveness of entity embedding for abductive reasoning.\\r\\n\\r\\nSimilarly, the following are the learnt embeddings (after converted to 2D) of day of week (Fig. 3), month (Fig. 4) and Rossmann stores (Fig. 5).\\r\\n\\r\\n[caption id=\"attachment_5464\" align=\"aligncenter\" width=\"505\"] Fig. 3 Learnt embedding of days of the week[/caption]\\r\\n\\r\\n[caption id=\"attachment_5463\" align=\"aligncenter\" width=\"490\"] Fig. 4 Learnt embedding of months[/caption]\\r\\n\\r\\n[caption id=\"attachment_5462\" align=\"aligncenter\" width=\"600\"] Fig. 5 Learnt embedded of Rossmann stores[/caption]\\r\\n\\r\\nEntity embedding may be applied to many other problems to find the hidden relations between entities based on their interaction with the external environment. For example, based on huge databases about the relations of genes, mutations, proteins, medicines and diseases, one may map those entities into multi-dimensional spaces which can guide the understanding of biological process or drug discovery etc. This an exciting direction for further exploration.\\r\\nWhich tools did you use?\\r\\nI used a new python neural network frame work Keras. It is simple, flexible and powerful. It can use Theano or TensorFlow as the backend. I also used many common python packages like sklearn, numpy and pandas. I used Nvidia GTX 980 GPU to run the neural network as it is more than one order of magnitude faster than a CPU.\\r\\nWhat was the run time for both training and prediction of your winning solution\\r\\nIt takes 20 minutes to train one network on GPU. For our finial submission we averaged the result of 10 networks, so altogether it takes about 3.5 hours. The time spend on the prediction is little.', 'With fewer than 500 North Atlantic right whales left in the world\\'s oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. In the NOAA Right Whale Recognition challenge, 470 players on 364 teams competed to build a model that could\\xa0identify any\\xa0individual,\\xa0living North Atlantic right whale from its\\xa0aerial photographs. The deepsense.io team entered the competition spurred by a recent improvements in their image recognition skills and ended up taking 1st place. In this blog, they share their pipeline, their solution\\'s \"most valuable player\", and what they\\'ve taken away from the competition experience.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nRobert began assembling the\\xa0deepsense.io\\xa0team about one and a half years ago with the goal of creating a machine learning powerhouse in Warsaw, taking perhaps an unusual approach of seeking out people who did not specialize in data science, but rather algorithmics. Guided by two reasons: first and foremost, he wanted to take a fresh approach to machine learning; second that his alma mater, the Faculty of Mathematics, Informatics and Mechanics at Warsaw University was full of the latter.\\r\\n\\r\\n[caption id=\"attachment_5487\" align=\"aligncenter\" width=\"885\"] Top row: IWAN GROZNY, MACIEJK, MARCIN MUCHA Bottom row: MAREK CYGAN, MILLCHECK, ROBIBOK[/caption]\\r\\n\\r\\nAs a result, almost everyone on the deepsense.io team has a strong background in computer science, having competed and won various computer science competitions (including an ACM ICPC win, Google Code Jam victory, and multiple IOI gold medals). He sought out both students and employees, even leading to a situation where one of the members was leading a course another attended. It seems we get along pretty well though.\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nThis was actually our second image processing contest, the first being Diabetic Retinopathy Detection. Even though we didn’t manage to finish in the “money pool” on that one, we certainly learned a lot (you can read more here).\\r\\n\\r\\nWe felt unsatisfied, as we believed our image recognition skills have vastly improved since then and wanted to showcase our progress. Right Whale Recognition was the next such competition, so we jumped straight in. We had no domain knowledge, so we could only go on the information provided by the organizers (well honestly that and Wikipedia). It turned out to be enough though. Robert says it cannot happen again, so we’re currently in the process of hiring a marine biologist ;) Right or wrong, we won’t let any whale catch us off-guard next time.\\r\\nHow did you get started competing on Kaggle?\\r\\nTo be frank, it was pretty incidental. Robert and Jan Kanty were coding a “homebrewed” Linear Regression loaded with our tricks (of varying worth) and we wanted to test it on an actual dataset. One of our friends suggested Kaggle, praising it for its well-prepared datasets and before we knew it, we achieved master status and were hungry for more.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nOur preprocessing pipeline is nicely described in the following picture:\\r\\n\\r\\n[caption id=\"attachment_5472\" align=\"aligncenter\" width=\"975\"] deepsense.io North Atlantic right whale preprocessing pipeline[/caption]\\r\\n\\r\\nThere are two initial phases. In the first, we train a neural net to output coordinates of the bounding box of a whale’s head (as seen in the above figure #2).\\r\\n\\r\\nDuring the second, another network is trained to output two special points on whale’s head (bonnet-tip and blowhead in #3). We use these two points to properly rotate and scale the photo (#4), and as a result get a normalized view of the whale’s head.\\r\\n\\r\\nWhen looking for the bounding box and the points on the head, we used softmax classification (classes being approximate values of the coordinates) instead of regression - it simply performed better. Seems like determining if a part of an image contains something is easier than determining where it is, who knew?\\r\\n\\r\\nAs you can see in the pictures below, processing the photos this way makes the whales’ heads clearly visible. Good for us that the north atlantic right whales form a counter-culture and decided to get those white tattoos on their faces.\\r\\n\\r\\n[caption id=\"attachment_5473\" align=\"aligncenter\" width=\"700\"] Processed images[/caption]\\r\\n\\r\\nFinally, after preprocessing, the conv-nets are trained on these standardized photos.\\r\\n\\r\\nWhat proved to be the MVP of the solution, was when we added additional target attributes to the data - whether the callosity pattern was continuous and whether it was symmetrical. Such “easy” targets made sure that our networks knew where to put their focus even on the earliest epochs - not only vastly reducing the training time, but also preventing overfitting.\\r\\n\\r\\nAnother thing we did was manually “kicking” the learning rate when we found\\xa0the improvements too stale. Seems like it can, after some commotion, help the network make progress again.\\r\\n\\r\\n[caption id=\"attachment_5474\" align=\"aligncenter\" width=\"703\"] Increasing the learning rate mid-training[/caption]\\r\\n\\r\\nAnd here is the final network’s architecture:\\r\\n\\r\\n[caption id=\"attachment_5488\" align=\"aligncenter\" width=\"162\"] The final network[/caption]\\r\\n\\r\\nThis is a pretty brief description, if one is interested in the details, we recommend our blog post.\\r\\nHow did you spend your time on this competition?\\r\\nWe’ve been determined to try out as many different approaches and ideas as possible. As a result, at least half of our time (especially at the start of the competition) was spent on things that have not worked well enough. A non-exhaustive list would be:\\r\\n\\nUnsupervised cropping\\nOther methods of supervised cropping: regression, training a net to distinguish samples of heads and non-heads\\nSpatial Transformer Networks\\nDeep Residual Networks\\nTriplet training\\n\\r\\nWe have also spent some time doing manual annotations for the training data. Though, not as much as one would suspect. With the right tools (and perseverance) it takes around 13 hours to recreate them - a mundane, but feasible task. Interestingly, it’s quite hard to pinpoint a single idea that (overall) contributed to such a significant improvement to the score while taking as little time.\\r\\nWhich tools did you use?\\r\\nAside from the ConvNet trio Python+NumPy+Theano, we were using Sloth for location annotations and a small image-labeling program written in Julia.\\r\\n\\r\\nWe have also implemented an ad-hoc (but useful) tool that helped us in managing the plethora of different experiments that we had been running, and sharing their results with the rest of the team. We hope to release it someday, so stay tuned!\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nFirst and foremost, the satisfaction that our efforts can actually help preserve an endangered species. When we started fiddling with machine learning, helping to make a difference in the real-world was a distant dream. Now we feel that we can cross “making the world a better place” off our bucket lists.\\r\\n\\r\\nWe have also learned a lot, data-scientifically that is. At multiple points during this competition we had to make judgment calls between repairing the methodological purity and moving forward. There is a lot to learn from every\\xa0such choice.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nIt’s very important to strike the right balance between theory and practice. Try out things while immersing in theory. Play around with different techniques and ideas, implement them, make them work, and find their limitations.\\r\\n\\r\\nAlso, don’t be afraid to participate in competitions - they’re one of the best training grounds that you’ll find. Especially, if you choose the worthwhale ones ;)\\r\\nTeamwork\\nHow did your team work together?\\r\\nWe believe we kept the right proportion of order and chaos. We would assess our progress and assign tasks once every 2-3 weeks, which I believe is often enough to get everyone on track and combine ideas and findings and scarcely enough to allow each teammate’s individuality to thrive. Ad hoc communication was also present throughout the competition, especially when not needed - we thank Marcin for his immense contribution in this area.\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nA lot of those, actually. Like how long am I supposed to microwave food? It’s always either ice cold or steaming hot, no middle ground whatsoever. And don’t even get me started on milk. Maybe if we all join forces we can find the underlying Dirac measure.\\r\\nA question for fellow Kagglers?\\r\\nOver the course of the competition we’ve looked at an extensive number of whale pictures, and we suspect others have too. Which one was your favorite? Don’t tell us why, just show the picture. :)', 'Kaggle\\'s annual Santa\\xa0optimization competition wrapped up in early January with a nail-biting finish. When the dust settled, team Woshialex and Weezy had managed to take\\xa02nd place in the competition\\xa0and also\\xa0take home the\\xa0Rudolph prize. (This prize is awarded to the team that held 1st place on the leaderboard for the longest period of time.) In this blog the data scientists on the team, Mirsad and Qi, share the details of their\\xa0simulated annealing algorithm, what worked / what didn\\'t work, and why they benefited from teaming up.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\nMirsad: I just got my Ph.D. (two weeks before competition start) in Combinatorial Optimization (Operations Research) at Ecole des mines d\\'Ales - University of Montpellier. The work conducted during the thesis mainly consisted of developing efficient local search algorithms for several difficult optimization problems.\\r\\n\\r\\n[caption id=\"attachment_5483\" align=\"aligncenter\" width=\"381\"] Mirsad (aka weezy) on Kaggle[/caption]\\r\\n\\r\\nQi: I got my Ph.D in theoretical physics (about Lattice Quantum chromodynamics & Charge-Parity violation of the Weak interaction) from Columbia University 4 years ago and after that I worked as a quantitative trader in a hedge fund.\\r\\n\\r\\n[caption id=\"attachment_5484\" align=\"aligncenter\" width=\"381\"] Qi (aka woshialex) on Kaggle[/caption]\\r\\nHow did you get started competing on Kaggle?\\nMirsad: I got started with last year\\'s Christmas Challenge - Helping Santa\\'s Helpers. I am generally quite interested in competing in various optimization competitions and after accidentally discovering that one (a bit late though) I decided to give it a try.\\r\\n\\r\\nQi: A couple of years ago I found that machine learning stuff were very interesting and started to learn by myself, in order to practice these learned skills I found Kaggle which is a perfect place for me to practice.\\r\\nWhat made you decide to enter this competition?\\nMirsad: Christmas optimization competitions are basically the only ones at Kaggle that I can enter, and after a nice experience with the last year\\'s competition, I was quite happy to enter this one as well. Also, it was just about right time for me since I could devote more time to it than I usually could. I would have probably entered this competition whatever the posted problem was, but the fact that this year\\'s problem was really interesting and simply-defined (while in the same time being quite difficult) was just a plus.\\r\\n\\r\\nQi: It is a well defined clean problem so I liked it. Also it requires some hard skills (c++, optimization, etc.) and I think I had a better chance to win this one than other competitions in which people can just play/tune all kinds of existing machine learning algorithms.\\r\\nLet\\'s get technical\\nGeneral procedure\\r\\nOur two-phase approach to solving the problem consists of:\\r\\n\\nGreedy procedure for the initial solution construction\\nSimulated Annealing (SA) algorithm for improving the solution\\n\\r\\nEven though the quality (and structure) of initial solution highly influences the final results, the second solution step i.e. Simulated Annealing algorithm is a core element of our solution and careful algorithm design choices had to be made in order to have competitive results.\\r\\n\\r\\nInitial Solution:\\xa0The basic idea is to cluster the gifts based on longitudes and build the trips by sorting the gifts by latitudes in each cluster. Each trip will include the gifts that are close to each other (i.e. have similar longitudes) and will go from north to south. Total weight in each cluster is limited by 980 (slightly smaller than sleigh capacity of 1000) in order to enable easier search space exploration in the SA algorithm. Initial solution constructed this way had an objective of 12.505B\\r\\nSimulated Annealing\\r\\nIn order to improve our initial solution we developed a Simulated Annealing (SA) algorithm with careful choices of parameters (i.e. starting temperature and cooling rate) and exploring several neighborhoods. Classic moves acceptance probability function, e^(-delta/T), has been used where delta represents the difference in solution objective function (score) and T is the current temperature.\\r\\n\\r\\nMoves inside a single trip and moves between the trips are implemented. In order to have a reasonable computation time, a limited subset of moves is considered; more precisely, only the moves between near trips (routes) are performed.\\r\\n\\r\\nSetting initial temperature (T0) and cooling speed/rate (s) values has shown to be the most important in obtaining high quality results.\\r\\nAfter performing extensive experiments, these values have been set to 0.25 and 1.00002 respectively. In each iteration of SA algorithm (a single iteration consists of evaluating a fix number of moves) current temperature T (starting from T0) is decreased by s.\\r\\n\\r\\n[caption id=\"attachment_5481\" align=\"aligncenter\" width=\"750\"] Illustration of objective function behaviour during the search for different parameter values.[/caption]\\r\\n\\r\\nFinally, let\\'s define the moves in SA. For the sake of simplicity, those moves that do not significantly influence the final result quality are omitted here (those include all the moves inside a trip).\\r\\n\\nShift: Shift the gift from one route to another - gift to shift and route to shift to are chosen randomly (respecting constraints and range) and the best position to shift to is chosen (in terms of objective function).\\nSwap: Swap two gifts between the routes. Routes and first gift are chosen randomly, while the second gift is chosen to minimize the objective.\\nShiftBlock (or ShiftN): Shift a block of gifts (gifts between positions i and i + N (N in {2,3}) from one route to another - routes and beginning position i are chosen randomly as before.\\nSwapBest: Choose two gifts g1 and g2 (from routes r1 and r2 respectively), drop them from the current routes and then insert g1 into the best position in r2 and g2 into the best position in r1 (can be seen as generalization of a Swap move).\\n\\r\\nMoves evaluation is done in parallel, using up to 8 threads. Final submission is obtained on 4-core machine (Intel i5) in cca 10 days of computation with s = 1.00002 and additional 2-3 days with s = 1.00005 (1.00002 run would not terminate before the deadline).\\r\\n\\r\\nOur source code can be found at:\\xa0https://github.com/woshialex/SantaStolenSleigh\\nWhat worked\\r\\nIndeed, Simulated Annealing algorithm seems to fit the problem perfectly given that no running time limit has been imposed and we believe that some sort of SA algorithm has been developed by most of the top ranked teams. Also, quite expected, choosing higher values for initial temperature and smaller cooling rate produces better results. However, running time increases quickly and one has to find a good balance.\\r\\n\\r\\nHaving said that, it was crucial to develop a \"good\" algorithm quickly in order to perform a run with desired parameters that would terminate before the end of the competition.\\r\\n\\r\\nIt was very important to make a multi-threaded version of the algorithm which enabled us to produce high quality solutions much more quickly than before. The first version of multi-threaded program has produced 12.389B solution in around 30h, which indeed enabled us to win the Rudolph prize (prize for the longest period of time at the top of the leaderboard), even though that seemed to be out of reach just the day before.\\r\\nWhat didn\\'t work\\r\\nRunning the algorithm iteratively, starting from the best solution and using different values of parameters showed to be\\xa0less good than running the algorithm starting from the initial solution with a high enough initial temperature and smaller cooling rate.\\r\\n\\r\\nHowever, we mainly made this type of runs for the first two or three weeks in the competition, which is mostly\\xa0due to the fact that we wanted to see quick improvements and did not have enough CPU power to run many things in parallel. Several procedures/techniques that were improving intermediate solutions, but could not make any improvements whatsoever when running on the final solution generated by a \"slow\" run.\\r\\n\\r\\nWe also tried to use some other well-known optimization techniques other than SA, such as Tabu Search, Mixed Integer Programming, Hill Climbing, etc, but none of them showed to be promising or capable of improving results.\\r\\nWhat was your most important insight into the data?\\r\\nObviously, given that gift positions correspond to the points on the earth (and excluding the oceans) and a huge number of gifts is to be delivered (total of 100,000), choosing a given initial solution construction has shown to be important. It was quite easy to figure out quickly that most of the trips should be close to full since we have a sleigh base weight.\\r\\n\\r\\nAdditionally, due to the fact that not many points between Antarctica and other continents exist, the set of gifts with latitude < -60 and the set of gifts with latitude > -60 have been considered separately in initial solution procedure which improves the score from 12.70B to 12.505B. The choice of initial solution then naturally influenced some choices made in SA algorithm.\\r\\nWere you surprised by any of your findings?\\r\\nNot really. Even if we had to deal with a very large-size problem and complicated objective function \\xa0the (non-linear) solution still remained\\r\\n\"simple\" and similar to some common algorithms developed for classical Capacitated Vehicle Routing Problem (CVRP). What surprised us a bit is that items weight shown to be almost irrelevant when studying the data and developing an algorithm and we believe that changing the distribution of weights in the data would not change much, even though in the beginning of the competition we were thinking that something will have to be done about the weights.\\r\\nWhich tools did you use?\\r\\nSA algorithm has been implemented in C++ and without using any external libraries, while, for simplicity reasons, initial solution has been generated in python.\\r\\nHow did you spend your time on this competition?\\r\\nRoughly, we spent 50% of the time on developing an algorithm and tuning, while the other half was just waiting for the runs to finish.\\r\\nIndeed, no changes were made in our algorithm in the last two weeks of the competition which is a bit odd.\\r\\n\\r\\nHaving more powerful machine(s) to run the algorithm would have enabled us to spend more time on developing the algorithm or\\r\\nto use even higher initial temperature (T0) or smaller cooling speed (1.00001 for example), possibly producing a solution good enough\\r\\nto win the final prize.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\nMirsad: First of all, collaborating with Qi was a pleasure and a great experience. Winning the Rudolph prize, especially after we thought it was gone in the middle of the competition, was a great thing. Nevertheless, we feel that we were\\xa0a bit too \"relaxed\" in the last week or two and that we could have done something to improve the final solution (especially trying to run on more powerful machines) without putting a lot of effort in, but just by running the code differently.\\r\\n\\r\\nQi: 1) Collaboration is really extremely helpful when I have\\xa0a great team mate. 2) Never be over confident, always try all of your best if you want to win the competition.\\r\\nTeamwork\\nHow did your team form?\\r\\nWe formed a team quite early in the competition (10 days from the start); both of us had\\xa0placed in the top five or six during that time\\xa0and we thought that joining forces could help and was maybe the only option if we wanted to have a chance of winning one of the prizes.\\r\\nHow did competing on a team help you succeed?\\r\\nOne thing that helps for sure is having more CPU power than when you compete alone and that was one of the reasons we formed a team.\\r\\n\\r\\nOther thing that certainly helped is that we had some amount of time spent on the top of the leaderboard before making a team (not much though) and at the end it showed to be important given that the competition was very tight.\\r\\n\\r\\nWe had slightly different methods when made a merge and we could quickly find some improvements by just combining the source.\\r\\n\\r\\nAfterwards, being able to distribute the work to be done and exchange ideas has certainly helped, at least to gain some time.\\r\\nJust for Fun\\nWhat kind of Kaggle competition would you like to see?\\nMirsad: This kind of competition is perfect for me. Kaggle\\'s team is really doing a great job in defining the problems - both times I competed, I was amazed by the simplicity and elegance of the problem definition and the fact that it was still extremely\\xa0difficult to solve.\\r\\n\\r\\nQi: There are many machine learning competitions on Kaggle, but very few of the competitions require hard coding skills. I\\'d love to see more like this one. (Like some of the competitions on topcoder).\\r\\nBios\\nMirsad Buljubasic holds a Ph.D. in Combinatorial Optimization from Montpellier University, France, and a M.S. in Computer Science, from University of Sarajevo, Bosnia and Herzegovina. He is currently a post-doc researcher in LGI2P laboratory - Ecole des mines d\\'Ales.\\r\\n\\r\\nQi Liu earned his Ph.D. in theoretical physics and is currently working at a hedge fund.', 'Rossmann operates over 3,000 drug stores in 7 European countries. In their first Kaggle competition, Rossmann Store Sales, this drug store giant challenged Kagglers to forecast\\xa06 weeks of daily sales for 1,115 stores located across Germany. The competition attracted 3,738 data scientists, making it our second most popular competition by participants ever.\\r\\n\\r\\nNima Shahbazi took second place in the competition, using his background in data mining to gain an edge. By fully exploring and understanding the dataset, Nima was able to engineer features that many participants overlooked. In fact, one valuable feature was developed from selected data that many Kagglers had removed from their training set entirely.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI am a PhD candidate at the Lassonde School of Engineering at York University under the supervision of Prof.\\xa0Jarek Gryz and co-supervision of Prof. Aijun An. My main research area is data mining and machine learning. I love problem solving and challenging myself to find the best model for regression and classification/clustering problems. Before entering Kaggle competitions, I used to work on data analysis problems; the most important one was the time series prediction in FOREX market for which I have designed many algorithmic trading strategies.\\r\\n\\r\\n[caption id=\"attachment_5498\" align=\"aligncenter\" width=\"300\"] Nima\\'s Kaggle profile[/caption]\\r\\nHow did you get started competing on Kaggle?\\r\\nAbout 9 months ago, I was informed by one of my friends that the IEEE International Conference on Data Mining series (ICDM) has established an interesting contest in data mining. In that competition participants are tasked with identifying a set of user connections across different devices without using common user handle information such as name, email, phone number, etc. Moreover, participants were going to be asked to figure out the likelihood that a set of different IDs from different domains belong to the same user and at what performance level. I got very excited and start working on that problem. Finally I ranked 7th in that competition and have learned lots of new approaches for machine learning and data mining problems. I realized that in order to be successful in this field you should challenge yourself with real world problems. Although the theoretical knowledge is a must, without having experience in real world problems you will not able to succeed.\\r\\nWhat made you decide to enter this competition?\\r\\nAfter the ICDM contest I realized that some Kaggle competitions are referenced in NIPS papers. That made me more motivated to join other competitions. I found the Rossmann challenge very interesting since a sales forecast is useful for any company. I really wanted to learn the latest data mining approaches for solving these problems as in the Kaggle community some of the participants share their approaches at the end of the competition. I entered to be involved in the competition and to give myself a chance to win. At the end, I was able to design a model which was consistent both on the public and private leaderboards.\\r\\nLet\\'s Get Technical\\nWhat was your most important insight into the data? What preprocessing and supervised learning methods did you use?\\r\\nI spent a great amount of time digging through the data. It was mentioned in the competition evaluation page that any “day” and “store” with 0 sales is ignored in scoring the\\xa0test or train set. But in the test and train data we do have many stores with sales equal to zero (why bother to include them!). At first it might seem that we can totally remove the rows with sales equal to zero (which are the stores that are closed on that specific day). Also all the scripts in the Kaggle community remove the zero sales before starting to create a model. I thought that there must be something related to these zero sale days. So I did not remove them and started extracting knowledge from them. I was surprised when I found out that there was\\xa0a relation between consequence close (zero sales) and unexpected sales before or after opening the store. For example see Figure 1 for store number 1039. The zero sales (store closed) are highlighted with red and corresponding sales before and after those days are shown with green bars.\\r\\n\\r\\n[caption id=\"attachment_5499\" align=\"aligncenter\" width=\"600\"] Figure 1. Unusual sales before/after consecutive close[/caption]\\r\\n\\r\\nIf I removed the zero sale rows I would not be able to tell my model to learn that specific pattern of unusual sales. So I go went\\xa0the data and find those zero sales and create a dummy variable called MyRowHoliday to capture the information for these unusual sales. This dummy variable assigned positive integers before and after the consecutive close and “-1” on other days as shown in Figure 2.\\r\\n\\r\\n[caption id=\"attachment_5500\" align=\"aligncenter\" width=\"600\"] Figure 2. MyRowHoliday codes for more than two consecutive close[/caption]\\r\\n\\r\\nI have created 5 more dummy features (refurbishment, MystateHoliday, MySchoolHoliday, MyPromo and Promo2Active) that change these categorical variables to large range of numbers. For example, in the original data the days that have promotions are marked with 1; otherwise 0. I found out that this information is not sufficient for the learners because the beginning and end of the promotion have some effect on sales. Like other teams, I used extreme gradient boosting (xgboost) as a learning method.\\xa0\\nWere you surprised by any of your findings?\\r\\nYes, I always got surprised when even one of my finding showed better performance on the leaderboard. The most important one was when I added four more features for time series analysis. Those were simple moving averages (MA_Fast, MA_Slow, MA_Customer_Fast and MA_Customer_Slow) for sales and customers over different time windows. The moving averages were split by important feature like store number, day of week and promotion. The model I built based on moving averages blended well with my previous model and made my rank 3rd in the last week of the competition.\\r\\nWhich tools did you use?\\r\\nFor preprocessing and exploratory data analysis I used R and I usually write the code both in R and Python.\\r\\nHow did you spend your time on this competition?\\r\\nI spent more than 70% on feature engineering, and 30% on feature selection, model ensembling, and model tuning.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nI did\\xa0not use a\\xa0super-fast machine. For this competition I used a machine with 8-core CPU and 16GB of RAM. I ran the models in parallel on my laptop and my own desktop computer. When I woke up I started coding and digging into the data, and while I was sleeping I let the computers run my algorithm for model tuning. The winning solution has 15 models which took more than 25 hours to build and predict.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nFirst of all, the money. To be honest I was thinking about the prize since I joined the competition. And now I am confident in my abilities that I can win a prize in a world-wide competition with more than 3,300 teams around the world. Plus, I learn a lot anytime I go through the Kaggle forum or scripts.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nFirst of all, make sure you understand the math behind regression and classification and the way that a model learns. And the most important thing you should learn is how do learning methods dealing with regularization to avoid overfitting. Second, fully understand the principles of cross validation, and which type of cross validation fit your problem. Finally, do not spend lots of time tuning the model. Instead, spend your time extracting features and understanding the data. The more you play with the data the more you will find interesting insights.\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nI recently saw a competition on a specific cancer with the goal of\\xa0helping to prevent it by identifying at-risk populations. I would really like to see more cancer related data mining competition and specifically any research related to Cholangiocarcinoma cancer. The rates for this kind of cancer have been rising worldwide over the past several decades [1] and beside that one of my relative is suffering from it.\\r\\nWhat is your dream job?\\r\\nA dream job for me is a job in which I can work for myself.\\r\\nBio\\nNima Shahbazi is a second-year PhD Student in the Data Mining and Database Group at York University. He previously worked in big data analytics, specifically on Forex Market. His current research interests include Mining Data Streams, Big Data Analytics and Deep Learning.\\r\\n\\r\\n\\xa0\\r\\n\\r\\n[1] Patel T.\\xa0\"Worldwide trends in mortality from biliary tract malignancies\". BMC Cancer\\xa02: 10.', 'With fewer than 500 North Atlantic right whales left in the world\\'s oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. In the NOAA Right Whale Recognition challenge, 470 players on 364 teams competed to build a model that could\\xa0identify any\\xa0individual,\\xa0living North Atlantic right whale from its\\xa0aerial photographs.\\r\\n\\r\\nFelix Lau entered the competition with the goal of practicing new techniques\\xa0in deep learning, and ended up taking second place. This blog shares his background, some of the limitations he ran into, and a high level overview of his approach. For a more technical description of his winning solution, don\\'t miss the post in his personal blog.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\n[caption id=\"attachment_5510\" align=\"aligncenter\" width=\"300\"] Felix (aka felixlaumon) on Kaggle[/caption]\\r\\n\\r\\nI am currently working at a fashion consulting company as a computer vision research engineer. I have trained a number of deep neural networks at work. I participated some image Kaggle competitions before (Diabetic Retinopathy Detection and the 1st Data Science Bowl). Although I didn’t do well in those competitions, the experiences helped me to get started in this competition quickly.\\r\\nWhat made you decide to enter this competition?\\r\\nI was inspired by the new deep learning ideas and techniques proposed in the last few months, and I wanted to apply them to a real problem. At the end, even though some of the ideas didn’t turn out to work well for this problem, it was still a good experience because I now understand the limitations of some of the approaches.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nAll my approaches were based on deep convolutional neural network as it is used by most, if not all, winners of the previous image Kaggle competitions. However it turned out for this dataset, the neural network didn\\'t manage to extract the right features from the raw images. In particular, the classifier had troubles focusing on the “whale face” on its own, as I suspect the “whale name” alone was not a strong enough training signal.\\r\\n\\r\\n[caption id=\"attachment_5506\" align=\"aligncenter\" width=\"306\"] Saliency map of the neural network trained on raw image[/caption]\\r\\n\\r\\nThe approach that I found worked well was to first predict the blowhead and bonnet of the whale and crop the raw images accordingly. The goal is to guide the network to focus on the callosity pattern on the back of the whale. Then I trained a classifier using the cropped images to predict the final “whale name”.\\r\\n\\r\\n\\xa0\\r\\n\\r\\n\\r\\n\\r\\nYou should check out my blog post\\xa0for the details. Below is a summary of my best approach.\\r\\n\\r\\nBlowhead and Bonnet Localizer\\r\\n\\r\\nThe blowhead and bonnet localizer was based on the VGG-net. The goal is to output the x-y coordinates of the 2 key points (blowhead and bonnet) to create aligned whale face images. I trained the network with mean squared error as its error function. Real-time augmentation (e.g. rotation, translation, contrast augmentation) was applied during training. I found that test-time augmentation improved the accuracy of the coordinates quite significantly.\\r\\n\\r\\nClassifier\\r\\n\\r\\nThe classifier took the aligned face images to output the probability of the final “whale name”. Because the images fed to this network are always aligned, neither train-time nor test-time augmentation was applied. I tried out different network architectures and found that residual network (ResNet) works quite well.\\r\\n\\r\\nThe final classifier was an ensemble of the following networks:\\r\\n\\n3 x 19-layer VGGNet\\n1 x 31-layer ResNet\\n1 x 37-layer ResNet\\n1 x 67-layer ResNet\\n\\nWhich tools did you use?\\nSoftware – I used Python as a programming language. For frameworks, I used Theano and Lasagne to build up the neural networks, nolearn for the training loop, my nolearn_utils for real-time augmentation, scikit-learn for ensembling, and scikit-image for image processing.\\r\\n\\r\\nHardware – I used GTX 980ti and GTX 670 for training the model locally, and AWS EC2 g2.xlarge for optimizing their hyperparameters (with Docker and Docker Machine).\\r\\nHow did you spend your time on this competition?\\r\\nMost of my work was\\xa0done in the last 3 weeks. I spent about 20% of my time building up a baseline submission, 30% building up the infrastructure and code refactoring, 50% experimenting with alternative approaches.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nIt is\\xa0still a surprise to me that the model trained from raw images (an end-to-end approach) did not perform well. Providing additional training signals (e.g. bonnet and blowhead coordinates, type of the callosity pattern as in the 1st place approach) proved to be a very important trick for this competition.\\r\\n\\r\\nWhen the dataset size is limited, it seems augmenting the training labels is just as important as augmenting the training data (i.e. image perturbation)\\r\\nDo you have any advice for those just getting started in data science?\\r\\nBuilding models is only a small part of what a data scientist do. Understanding the problem, collecting the dataset, designing an evaluation metric, and communicating the findings effectively are just as important.\\r\\nBio\\r\\nYou can find Felix on Twitter and on his blog.\\r\\n\\r\\n\\r\\n\\r\\nFor more on the NOAA Right Whale Recognition competition, click the tag below!\\r\\n\\r\\n\\xa0', 'The Stock Market Challenge, Winton\\'s second recruiting competition on Kaggle,\\xa0asked participants to predict intra and end of day stock returns.\\xa0The competition was crafted by research scientist at Winton to mimic the type of problem that they work on everyday. Mendrika Ramarlina finished third in the competition\\xa0with a combination of simple models and intelligently engineered features.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI come from a software engineering background. I have 4 years of experience building data-centric web applications. I have been taking online machine learning classes, reading research papers, and implementing models since 2012.\\r\\n\\r\\n[caption id=\"attachment_5523\" align=\"aligncenter\" width=\"300\"] Mendrika on Kaggle[/caption]\\r\\nHow did you get started competing on Kaggle?\\r\\nI found out about Kaggle from an article on Bloomberg from back in 2012. At the time, I had just completed Andrew Ng\\'s class. I wanted to start working on projects involving machine learning, but wasn\\'t sure what to work on. Kaggle competitions were a great way to get to started.\\r\\nWhat made you decide to enter this competition?\\r\\nForecasting stock returns is a very interesting problem. I have been applying machine learning to the financial market for a few years now. I joined to see how some of my approaches would fare in comparison to others\\'.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nFeature engineering was a key element in my approach. My final solution was an ensemble made up of 2 SVMs and 1 regularized linear regression all trained on handcrafted features such as peak-to-valley drawdown magnitude and duration or cumulated intraday returns.\\r\\n\\r\\n\\n\\nWhich tools did you use?\\r\\nFor preprocessing and model training, I used a standard pythonic machine learning stack: Pandas, Numpy, Scikit-Learn. I used matplotlib for visualization, and I did use some Excel for data exploration.\\r\\nHow did you spend your time on this competition?\\r\\nI spent the majority of my time analyzing the data, engineering features. Instead of training and fine- tuning complex models, my approach was to use simple models, then create features that would help improve the performance of these models.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nThat it is very difficult to predict minute-wise intraday data using historical data alone.\\r\\nDo you have any advice for those just getting started in data science?\\n\\nDon\\'t get hung up on one technology stack, tool, or algorithm\\nUnderstand how different statistical models “learn” from the data\\nKeep up with research\\n\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nI would start a competition to predict what crime will occur when and where, given historical crime\\xa0data.\\r\\nBio\\nMendrika Ramarlina is a founder and machine learning engineer at Madagascar Innovation Lab, a startup that uses machine learning to solve problems with social impacts in Madagascar. His interests include deep learning and reinforcement learning.', 'This blog was originally published on May 7, 2015 when Marios Michailidis was ranked #2.\\xa0Marios is now the number 1 data scientist out of 465,000 data scientists! We wanted to re-share the original post, with a few\\xa0additions and updates from Marios. We\\'ve also just published a post from Marios on his experience chasing the #1 spot on Kaggle, and what he\\'s taken away from the experience.\\xa0\\n\\r\\n\\r\\n\\xa0\\r\\n\\r\\nThere are Kagglers, there are Master Kagglers, and then there are top 10 Kagglers. Who are these people who consistently win Kaggle competitions? In this series we try to find out how they got to the top of the leaderboards.\\r\\n\\r\\nFirst up is KazAnova -- Marios Michailidis -- the current number 2 (now #1) out of nearly 300,000 (now\\xa0465,000+)\\xa0data scientists. Marios is a PhD student in machine learning at UCL and a Manager of Data Science at dunnhumby (organizer of the Kaggle competitions \\'Shopper Challenge\\' and \\'Product Launch Challenge\\').\\r\\nMarios Michailidis Q&A\\nHow did you start with Kaggle competitions?\\r\\nI wanted a new challenge in the field and learn from the Grand Masters.\\r\\n\\r\\nI was doing software development about machine learning algorithms, which also led to creating a GUI for credit scoring/analytics by the name KazAnova- a nick name I frequently use in Kaggle to keep reminding myself the passion I have for the field and how it started, but I could only go so far by myself.\\r\\n\\r\\nKaggle seemed the right place to learn from the experts.\\r\\nWhat is your first plan of action when working on a new competition?\\n\\nFirst of all to understand the problem and the metric we are tested on- this is key.\\nTo as-soon-as possible create a reliable cross-validation process that best would resemble the leaderboard or the test set in general as this will allow me to explore many different algorithms and approaches, knowing the impact they could yield.\\nUnderstand the importance of different algorithmic families, to see when and where to maximize the intensity (is it a linear or non-linear type of problem?)\\nTry many different approaches/techniques on a the given problem and seize it from all possible angles in terms of algorithms \\'selection, hyper parameter optimization, feature engineering, missing values\\' treatment- I treat all these elements as hyper parameters of the final solution.\\n\\nWhat does your iteration cycle look like?\\n\\nSacrifice a couple of submissions in the beginning of the contest to understand the importance of the different algorithms -- save energy for last 100 meters.\\nDo the following process for multiple models\\r\\n\\nSelect a model and do a recursive loop with the following steps:\\r\\n\\nTransform data (scaling, log(x+1) values, treat missing values, PCA or none)\\nOptimize hyper parameters of the model\\nDo feature engineering for that model (as in generate new features)\\nDo features\\' selection for that model (as in reducing them)\\nRedo previous steps as optimum parameters are likely to have changed slightly\\n\\n\\nSave hold-out predictions to be used later (meta-modelling)\\nCheck consistency of CV scores with leaderboard. If problematic, re-assess cross-validation process and re-do steps\\n\\n\\nCreate partnerships. Ideally you look for people that are likely to have taken different approaches than you have. Historically (in contrast) I was looking for friends; people I can learn from and people I can have fun with - not so much winning.\\nFind a good way to ensemble\\n\\r\\n[caption id=\"attachment_4846\" align=\"aligncenter\" width=\"500\"] Screen from the KazAnova Analytics GUI.[/caption]\\r\\n\\r\\n\\xa0\\r\\nWhat are your favorite machine learning algorithms?\\r\\nI like Gradient Boosting and Tree methods in general:\\r\\n\\nScalable\\nNon-linear and can capture deep interactions\\nLess prone to outliers\\n\\nWhat are your favorite machine learning libraries?\\n\\nScikit for forests.\\nXGBoost for GBM.\\nLibLinear for linear models.\\nWeka for all.\\nEncog for neural nets.\\nLasagne for nets, although I learnt it very recently.\\nRankLib for functions like NDCG.\\n\\r\\n[caption id=\"attachment_4847\" align=\"aligncenter\" width=\"500\"] Image from slides \\'Introduction to Boosted Trees\\' by Tianqi Chen (XGBoost)[/caption]\\r\\n\\r\\n\\xa0\\r\\nWhat is your approach to hyper-tuning parameters?\\r\\nI do this very manually.\\r\\n\\r\\nI have only tried once to use something like Gridsearch. I feel I learn more about the algorithms and why they work the way they do by doing this manually. At the same time I feel that \"I do something, it\\'s not only the machine!\".\\r\\n\\r\\nAfter 60+ competitions I\\'ve found that I can get to the top 90% of the best hyper parameters with the first try, so the manual approach has paid off!\\r\\nWhat is your approach to solid CV/final submission selection and LB fit?\\r\\nIn regards to CV, I try to best resemble what I am being tested on.\\r\\n\\r\\nIn many situations a random split would not work. For example: In the Acquire valued shoppers\\' challenge we were mainly tested on different products (offers) than these available on the train set. I made my CV to always try to predict 1 offer using the rest of the offers as this could resemble the test leaderboard better than a random split.\\r\\n\\r\\nAbout final selection, I normally go for best Leaderboard submission and best CV submission. In the case of a happy collision, I select something as different as possible with respectable CV result just in case I am lucky!\\r\\n\\r\\n(A prayer to the god of overfitting is my secret 3rd submission)\\nIn a few words: What wins competitions?\\n\\nUnderstand the problem well\\nDiscipline ; To have a well-thorough and documented approach that you follow religiously and defines all the modelling process/framework from how you cross-validate, select models, avoids over fitting (which requires a lot of ...discipline).\\nAllow room to try problem-specific things or new approaches within that framework\\nThe hours you put in\\nHave access to the right tools\\nMake key partnerships\\nEnsembling\\n\\nWhat is your favourite Kaggle competition and why?\\r\\nThe Acquire valued shoppers\\' challenge, not only because I won and it is relevant to what my team does, but I also had the honour to collaborate with Gert Jacobusse.\\r\\n\\r\\n[caption id=\"attachment_4848\" align=\"aligncenter\" width=\"500\"] The final private leaderboard for the Valued Shopper\\'s Challenge[/caption]\\r\\nWhat was your least favourite Kaggle competition experience?\\nDecMeg, BCI and such channel-wave type of competitions. They have big data and are very domain specific.\\r\\n\\r\\nI found it hard to even make my CV working properly, plus I did quite bad. Hopefully I will improve.\\r\\nWhat field in machine learning are you most excited about?\\r\\nI like recommender systems if it can be considered a separate field.\\r\\n\\r\\nThere is a broad spectrum of techniques you can use (which are field specific) and to be able to understand what the customer likes is very challenging and rewarding.\\r\\nWhich machine learning researchers do you study?\\r\\nI study: Steffen Rendle, Leo Breiman, Alexander Karatzoglou, Michael Jahrer & Andreas Töscher, the Machine Learning and Data Mining Group at National Taiwan University, and Jun Wang & Philip Treleaven.\\r\\nCan you tell us something about the last algorithm you hand-coded?\\r\\nIt was LibFM for Avazu competition as I believed it could work well in that particular problem. I could not make it work as well as LibFFM apparently.\\r\\nHow important is domain expertise for you when solving data science problems?\\r\\nFor some competitions it is really important.\\r\\n\\r\\nThe fact that I am employed in the recommendation science field and the kind of work that we do within my team, has helped me win the Acquire valued Shoppers challenge.\\r\\n\\r\\nHowever I think you can go a long way by following standard approaches even if you don\\'t know the field well, which is also the beauty of machine learning and the fact that some algorithms do a significant job for you.\\r\\n\\r\\n\\nWhat do you consider your most creative trick/find/approach?\\r\\nI do multiple ensemble meta-stacking if I can use the term.\\r\\n\\r\\nDuring the course of my univariate model tuning I save all the models\\' outputs. Then I make meta-models with the univariate models selections and most of the times I end up with different ensembles of Meta models.\\r\\n\\r\\nSometimes I go to third Meta model with Meta models as inputs (Meta-Meta model).\\r\\nHow are you currently using data science at work and does competing on Kaggle help with this?\\nClassified!\\r\\n\\r\\nKaggle does help in optimizing my methods, learning new skills, meet nice people with same passion, be up to date with new tools and generally stay in touch with what\\'s going on in the field.\\r\\nWhat is your opinion on the trade-off between high model complexity and training/test runtime?\\r\\nThat is a big discussion in principle.\\r\\n\\r\\nI guess there is a trade-off, but there needs to be an understanding that better models are not necessarily the most interpretable ones and that a more complex model (that is likely to score better) is not necessarily less stable/more dangerous.\\r\\n\\r\\nI guess the optimum solution should be somewhere in the middle (e.g. not an ensemble of 100 models nor Naive Bayes)\\r\\n\\r\\n[caption id=\"attachment_5532\" align=\"aligncenter\" width=\"426\"] THE BEST 8 FINISHES FOR KAZANOVA.[/caption]\\r\\nHow did you get better at Kaggle competitions?\\r\\nDid I ?! :D\\r\\n\\r\\nI guess what has helped a lot is:\\r\\n\\nSeeing previous solutions and end-of-competition threads\\nParticipate in Kaggle forums\\nLearn the tools\\nRead papers, websites, machine learning tutorials\\nOptimize processes (use sparse matrices, cut unnecessary steps, write more efficient code)\\nSave everything I\\'ve done and reuse (and improve). E.g. I keep a separate folder for each competition I\\'ve completed.\\nDedicate time (had to reduce video games)\\nCollaborate with others\\n\\r\\nI have found the following resources useful:\\r\\n\\nThis benchmark by Paul Duan in the Amazon competition (my first ever attempt with Python) for a general modelling framework.\\nFrom the same competition : Python code to achieve 0.90 AUC with logistic regression from Miroslaw Horbal to create pairwise interactions with cross-validation.\\nFor text analysis, this benchmark from Abhishek: Beating the benchmark in StumbleUpon Evergreen Challenge\\nXGBoost benchmark in Higgs Boson competition by Bing Xu\\nTinrtgu\\'s FTRL Logistic model in Avazu: Beat the benchmark with less than 1MB of memory\\nData science Bowl tutorial for image classification: IPython Notebook Tutorial.\\nH2O (R) deep learning benchmark from Arno Candel in Africa Soil competition\\nLasagne and nolearn tutorial for Otto competition (by the admin) :\\nAndrew Ng\\'s Coursera course in Machine learning\\nUniversity of Utah Machine learning slides.\\nWikipedia and Google\\n\\nAre partnerships important in achieving good results?\\r\\nVery.\\r\\n\\r\\nSometimes you cannot measure the impact from one competition only as what you learn from the others may be applicable in the future too. I\\'ve been very lucky to have made good and fun collaborations so far and I have learnt from all, especially:\\r\\n\\n\\nFrom Gert I\\'ve learnt model ensembling, feature engineering and Fourier transforms.\\nTriskelion and Phil Culliton: Vowpal Wabbit\\nTVS to avoid over-fitting\\nBluefool (or Domcastro) 1st derivatives and BART\\nMike, Python!\\nGiulio, competition management and Lasagne.\\n\\n\\r\\nI am SO PROUD that I had the honour/chance/opportunity to play, learn and exchange bits and bytes with Kagglers, like:\\r\\n\\r\\nGert, Triskelion, Yan Xu, SRK, Rafael, Faron, Clobber,\\xa0Bluefool,\\xa0Mark Landry,\\xa0Giulio, and Abhishek\\r\\n\\r\\nI would keep monitoring their progress :) .\\r\\nBio\\nMarios Michailidis\\xa0is Manager of Data science in dunnhumby and part-time PhD in machine learning at University College London (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK Market and has led many analytics projects with various themes including: Acquisition, Retention, Uplift, fraud detection, portfolio optimization and more. In his spare time he has created KazAnova, a GUI for credit scoring 100% made in Java.\\r\\n\\r\\nMarios loves competing on Kaggle and learning new machine learning tricks. He told us he will create something good for the ML community soon...', 'Next up in our series Profiling Top Kagglers is\\xa0Lucas Eustaquio Gomes da Silva (better known as Leustagos on Kaggle). Leustagos is one of only 13 data scientists to ever hold the #1 spot on Kaggle, and he has been a consistent face at the top of our user rankings since joining the community four years ago. In this blog, Leustagos shares what he\\'s learned in his years competing, his typical approach to a new competition, and also how Kaggle has\\xa0helped him develop professionally as a data scientist. Leustagos is currently busy kicking some serious cancer ass\\xa0so\\xa0we want to send him extra thanks for taking the time to share this interview with us.\\r\\nHow did you start with Kaggle competitions?\\r\\nI started back in the second half of 2011 just after doing the Andrew Ng Coursera class.\\xa0I found people talking about ML competitions on the course forum and got curious about it. I tried taking part in one competition and placed pretty badly. After this I really wanted to learn about the magic used by the winners.\\r\\nWhat is your first plan of action when working on a new competition?\\r\\nCheck the dataset to understand how to build a validation set.\\r\\n\\r\\n[caption id=\"attachment_5552\" align=\"aligncenter\" width=\"420\"] Leustagos\\' top 8 finishes[/caption]\\r\\nWhat does your iteration cycle look like?\\n\\nUnderstand the dataset. At least enough to build a consistent validation set.\\nBuild a consistent validation set and test its relationship with the leaderboard score.\\nBuild a very simple model.\\nLook for approaches used in similar competitions in the past.\\nStart feature engineering, step by step to create a strong model.\\nThink about ensembling, be it by creating alternate versions of the feature set or using different modeling techniques (xgb, rf, linear regression, neural nets, factorization machines, etc).\\n\\nWhat are your favorite machine learning algorithms?\\r\\nGradient boosted trees by far! I like GBT because it gives pretty good results right off the bat. Look at how many competitions are won using them!\\r\\nWhat are your favorite machine learning libraries?\\nxgboost, scikit-learn and pandas\\nWhat is your approach to hyper-tuning parameters?\\r\\nI look for params used in previous competitions and tweak them one by one until the outcome stops improving.\\r\\nWhat is your approach to solid CV/final submission selection and LB fit?\\r\\nWe are usually asked to choose 2 submissions. My first submission is always the one that performed more consistently both on cv and the leaderboard. Not necessarily the best leaderboard score if I think its overfit. The second submission is usually one that performs good enough but its somewhat diverse from\\xa0the first one.\\r\\nIn a few words: What wins competitions?\\r\\nGood feature engineering, strong ensembling skills and, many times, team work.\\r\\nWhat is your favourite Kaggle competition and why?\\nGlobal Energy Forecasting Competition 2012 - Wind Forecasting. It was the first competition that I was a prize winner and \\xa01st place at that. It felt so good!\\r\\nHave you ever hit a plateau during a competition? What, if anything, helped you move beyond this?\\r\\nMany times! I usually try to take another look at the dataset to check for other\\xa0angles. Seek information in the forums, scripts, and more importantly, when this happens I try and brainstorm with my fellow teammates if I\\'m on a team. If I\\'m not, and none of the previous options worked, I look for a team.\\r\\nWhat field in machine learning are you most excited about?\\r\\nI like working with time series and classification tasks. Not exactly a field, but I\\'m not tied to any industry in particular. I just like a good challenge.\\r\\nWhich machine learning researchers do you study?\\r\\nI don\\'t study anyone specifically, but I\\'m willing to learn anything that works in practice. For that\\xa0I look into papers. Of course, when dealing with a new problem I look for the state of the art too. Nowadays its very hard to find good and useful papers because researchers are forced to write many of them so the quality goes down.\\r\\nCan you tell us something about the last algorithm you hand-coded?\\r\\nI don\\'t remember hand coding anything more complex than a linear regression. I\\'m more empirical than theoretical and given my background in software developing I try to avoid coding from scratch as much as possible. Its very error prone. I prefer to understand algorithms at a high level and to know the task they are best at solving. After that is just putting together pieces of a puzzle.\\r\\nHow important is domain expertise for you when solving data science problems?\\r\\nIt helps, but its not mandatory.\\r\\nWhat do you consider your most creative trick/find/approach?\\r\\nI like calculating smoothed likelihoods to replace categorical variables with high cardinality. I know some ways of doing so. Applying mathematical transformations to output to more closely match the error function can be very tricky too.\\r\\nHow are you currently using data science at work and does competing on Kaggle help with this?\\r\\nI\\'m working with threat detection in enterprise networks. We analyse network traffic and use machine learning to identify malicious behaviour, like a command and control attack. Competing on Kaggle, besides being addictively fun, helps to broaden my skills and approaches. There is always something to learn.\\r\\nWhat is your opinion on the trade-off between high model complexity and training/test runtime?\\r\\nI really depends on the application. In competitions, better performance outweighs everything. For real applications, the solution I seek for is usually the simplest solution that meets the client\\'s performance criteria. It must be at least good enough in performance, and it must be easy to use and maintain. Many clients cannot afford much trickery for feature engineering or\\xa0complex algorithms in their workflow.\\r\\nWhat is your view on automating Machine Learning?\\r\\nIt can work. I don\\'t know how it will shape the data scientist job, but I think there will always be a need for one. A automated system is useless if there is no one that understands how to operate it and interpret its outcome.\\r\\nHow did you get better at Kaggle competitions?\\r\\nTrial and error, reading the forums and copying successful approaches, learning from team mates and always, always, trying to be very creative on top of all that. I like to to add my own personal touch.\\r\\nWhat have you learned from doing Kaggle competitions?\\r\\nI\\'ve learned how to do competitive machine learning, which is a bit different from practical machine learning. But I\\'ve learned how to do the practical too, as those simple concepts of dealing with data and algorithms are needed at every corner in competitive machine learning.\\r\\nBio\\nLucas Eustaquio Gomes da Silva, also known as Leustagos, is a mostly self taught data scientist who graduated in control and automation engineering. He currently works in the data mining field using machine learning techniques to identify and alert clients\\xa0about malicious traffic in corporate networks.', 'Genentech Cervical Cancer Screening was a competition only open to Kaggle Masters\\xa0that ran from December 2015 through January\\xa02016. \\xa0The competition asked top Kagglers to use a dataset of de-identified health records to predict which women would\\xa0not be screened for cervical cancer\\xa0on the recommended schedule.\\xa0Cervical cancer results in approximately 275,000 deaths every year, but it is potentially preventable and curable with regular screenings. Giulio & Michael took first place in this highly competitive challenge, proving their feature engineering skills are some of the best in the world.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\nGiulio: I’m a Data Scientist with a HealthCare Insurance plan in Washington State. My academic background is in Statistics and Biostatistics. I have worked a lot with clinical data but I’ve recently transitioned to consumer analytics. I use Machine Learning and Advanced Analytics to mine speech, text, weblogs and improve member experience.\\r\\n\\r\\n[caption id=\"attachment_5564\" align=\"aligncenter\" width=\"300\"] Giulio on Kaggle[/caption]\\r\\n\\r\\nMichael: I work on applied machine learning for a product company. We integrate and apply algorithms with our platform.\\r\\n\\r\\n[caption id=\"attachment_5563\" align=\"aligncenter\" width=\"300\"] Michael on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\nGiulio: I work with claims data a lot and I had thought that my experience was going to help me in this competition. Instead the competition was framed in such a way that I ended up generating most of my insights in a purely data driven way as opposed to using industry knowledge.\\r\\nMichael: Since I am working usually on low level data processing it was fun to handle 150GB of raw data with custom C++ code.\\r\\nHow did you get started competing on Kaggle?\\nGiulio: I started almost 3 years ago. At that time my main goal was to get exposure to how Machine Learning was utilized in other industries.\\r\\nMichael: I’m a Kaggle Early Adopter and have participated since day one. I spent much time before that in the Netflix Prize. It’s still fun to process data from new challenges and apply models, ensembles and some secret sauce to.\\r\\nWhat made you decide to enter this competition?\\nGiulio: Many Kaggle competitions are about modelling and ensembling. Most of what I do in a business setting is about feature engineering, and modelling takes only 10% of my time. I was eager to see where my feature engineering skills stood among the best data scientists in the world. This was the perfect competition due the the fact that data was provided in raw transactional form.\\r\\nMichael: I like competitions with huge datasets, the more data the better.\\r\\n\\r\\n[caption id=\"attachment_5560\" align=\"aligncenter\" width=\"781\"] By using Giulio\\'s dataset (0.96294 single model private score) train and test features are concatenated (2859630 rows) to train an autoencoder neural net. It is an unsupervised learner, that reconstructs itself. Architecture is 98259-4000-4000-4000-2-4000-4000-4000-98259. Middle and output layer is linear, others are ReLU. 20 epochs minibatch SGD on GPU, which took over a day to run. The scatterplot shows the middle layer activations of the train part with targets overlaid (blue=0, red=1). This means that every patient is a dot in this plot.[/caption]\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\nGiulio & Michael: There wasn’t really much processing, at least in the typical Kaggle fashion. But this was a feature engineering heavy competition and we used a lot of SQL to transform transactional data into features. As for models, we used a lot of XGBoost and Michael’s custom coded neural networks.\\r\\nWere you surprised by any of your findings?\\nGiulio: I was. Some findings were counterintuitive to my industry experience. For example, prescriptions should be a very powerful predictor of cervical cancer screening, as a large portion of the population will get a screening as a prerequisite for oral contraceptives prescriptions. In this case though, prescription data added very little lift on top of other features based on diagnosis, procedures and providers.\\r\\nWhich tools did you use?\\nGiulio: mostly SQL for feature engineering. Then a simple IPython notebook to test model performance.\\r\\nMichael: only C++\\r\\nHow did you spend your time on this competition?\\nGiulio: I spent close to 90% of my time on feature engineering. I knew Michael was one of the best modellers on Kaggle and he was willing to undertake most of the modelling. That worked very well for the team. I didn’t have to worry about tuning models and Michael didn’t have to worry about generating features.\\r\\nMichael: I spend 30% to build the one-hot features, 10% to merge our features and 60% to build and tune the ensemble.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\nGiulio: I estimate that only the feature engineering process can take up to a couple of days. A simple model that, alone can place 4th, takes about 2 hours to train, but some of our best models can take much longer.\\r\\nMichael: Total computational time was a few weeks for a single node. It can be speedup if the machine has many cores since xgboost performance speed is nearly linear with number of cores.\\r\\n\\r\\n[caption id=\"attachment_5561\" align=\"aligncenter\" width=\"781\"] A xgboost model results in AUC:0.827738 by using this 2D data as input. To visualize the decision surface a X/Y grid in [-10:0.01:+10] interval is created and scored. The plot shows the model output (blue=0, red=1).[/caption]\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\nGiulio: Kaggle Master competitions are exponentially more difficult and competitive than regular competitions. You get to face the best of the best and these folks will leave no stone unturned. I was shocked at the amount of progress made, and I was even more shocked at how good all the teams were even without any prior industry knowledge.\\r\\nMichael: Large datasets and clean data like here has beautiful behavior. Everything is stable and act as good base to optimize into. An improvement at local validation turned into improvement on the leaderboard, this is what I test first when I enter a competition. There are many counterexamples of competitions where this is not the case (How Much Did it Rain?, Rossmann..).\\r\\nDo you have any advice for those just getting started in data science?\\nGiulio: data science to me is more about enjoying the journey than about reaching a status. Everybody doing analytics can claim to be a data scientist, but it is far easier to succeed long term if you truly enjoy the process of learning something new every day and feeling challenged by new problems.\\r\\nMichael: drawing conclusions by working on one dataset is often not complete. In kaggle I found a lot of different real world datasets and by competing I know what is state-of-the-art and where I am standing. And finally, finding the truth.\\r\\n\\r\\n[caption id=\"attachment_5562\" align=\"aligncenter\" width=\"781\"] Overlay both plots show which patient would be scored as a screener (red=1) or non-screener (blue=0).[/caption]\\r\\nTeamwork\\nHow did your team form?\\nGiulio: Michael and I were both doing well on our own early on but the competition was fierce. Since we had different backgrounds and had taken different approaches, we thought we could benefit from a joint effort.\\r\\nHow did your team work together?\\nGiulio & Michael: We used Google Drive and Dropbox to share code and data. We used Skype for quick chats and emails for everything else.\\r\\nHow did competing on a team help you succeed?\\nGiulio:\\xa0For me the key part was that Michael was willing to focus on modelling and ensembling. That allowed me to focus on feature engineering. Without that type of freedom I would have never had enough time to find one of the decisive insights on the data that came 3 days from the end of the competition.\\r\\nMichael:\\xa0The key is features + models, like in every competition. Features though had more importance in this competition than in others. Ensembling a few different models (variations of xgb and neural nets) gave us the last boost to end up in first position.\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\nGiulio: I really enjoy competitions based on feature engineering more than modelling. I’d like to see competitions where submissions consist of feature datasets, and the scoring mechanism is limited to run those features through a simple pre-determined model. Essentially every team would use the same model and the only difference would be based on features.\\r\\nWhat is your dream job?\\nGiulio:\\xa0A fellow Kaggler and friend of mine, Mark Landry, actually holds that job. He is a Competitive Data Scientist, and gets to solve Kaggle problems for fun and work :-)\\r\\nBio\\nGiulio is a Data Scientist with Premera Blue Cross. His background is in Statistics and Biostatistics. Most of his work is focused on Machine Learning and Advanced Analytics.\\xa0He enjoys long distance running and has ran several marathons.\\r\\n\\r\\nMichael works at Opera Solutions. He transfers the knowledge gained from various data mining competitions to the platform solution. His background is Statistics, Software Design and Electronic Engineering.', 'Homesite Quote Conversion\\xa0challenged Kagglers to\\xa0predict which\\xa0customers would purchase an insurance plan after being given a quote. Team New Model Army | CAD & QuY finished in the money in 3rd place out of 1,924 players on 1,764 teams. In this post, the long-time teammates who formed the New Model Army half of the team\\xa0share their approach, why feature engineering is important, and why it pays to be paranoid in data science.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI (Konrad Banachewicz) have a PhD in statistics and a borderline obsession with all things data. I started competing on Kaggle at the dawn of time (or dawn of Kaggle anyway) and it has been a very interesting journey so far.\\r\\n\\r\\n[caption id=\"attachment_5570\" align=\"aligncenter\" width=\"300\"] Konrad on Kaggle[/caption]\\r\\n\\r\\nI (Mike Pearmain) have a degree in Mathematics, and similar to Konrad have an obsession with data, and specifically data science pipelines for dynamic self optimizing systems.\\r\\n\\r\\n[caption id=\"attachment_5571\" align=\"aligncenter\" width=\"300\"] Mike\\xa0on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nDomain knowledge: not really – between the two of us, we have quite a bit of experience in financial risk modeling, but since the data was anonymized, it was not very useful here. On the other hand, a number of things (dataset preparation, building pipelines, parameter optimization) that we learned to build along the way – in other Kaggle contests – really worked well together this time around.\\r\\nHow did you get started competing on Kaggle?\\r\\nI heard somewhere about this thing called “data mining contests” and after checking out a few – now mostly defunct – competitors, I ended up on Kaggle. From the start, I really liked the idea of competing against other people in a setting like this: well defined problems, smart competition, clear rules, and a possibility to win money – what’s not to like?\\r\\n\\r\\nMike and I met while working as external contractors for the same client in the financial industry – this encounter is one of precious few long term benefits to come out of that particular assignment ☺ We talked a bit about this and that over coffee or lunch, discovered we had a shared interest in data science and decided to give Kaggle teamwork a try. The rest, as they say, is history.\\r\\nWhat made you decide to enter this competition?\\r\\nCursory examination suggested that a lot of the stuff we developed on previous competitions could be put together into a nice pipeline here. In particular, we wanted to try a multi-stage stacking (ensemble of ensembles, really) – and the moderate data size made this one possible.\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nIn general, diversity was the name of the game:\\r\\n\\nWe created multiple transformations of the dataset: treating all categorical variables as integers (for tree-based models), replacing factors by response rates, adding differences of highly correlated pairs of numerical variables; a single dataset yielding best results on individual model level was the one where we combined all those approaches.\\nWe used qualitatively different models: xgboost and random forest for tree-based stuff, keras and h2o for deep neural networks, svm with radial kernel and some logistic regression (which worked surprisingly well on the non-linear transformations of the dataset).\\n\\n[table width=\"500px\" class=\"table table-bordered\"]\\r\\nmodel,headcount\\r\\nR::h2o.deeplearning,14\\r\\nPython::extraTrees,171\\r\\nPython::keras,3\\r\\nPython::LogisticRegression,29\\r\\nR::earth,6\\r\\nR::ranger,22\\r\\nPython::svc-rbf,9\\r\\nPython::xgboost,53\\r\\n[/table]\\r\\nTable 1: summary of types of models used as level 1 metafeatures. The reason there behind the overrepresentation of extraTrees is that – while obviously less powerful than xgboost – they are very cheap computationally, so we could add more “tree stuff” into the ensemble quickly.\\r\\nSome of those had pretty horrible performance (scoring AUC below 0.7), but we kept them anyway – our assumption was that methods like xgboost (especially with optimized parameters) are pretty good at discarding useless features.\\r\\n\\nWhat was your most important insight into the data?\\r\\nWe were quite surprised that almost all variables were relevant, which made us abandon the idea of doing feature selection.\\r\\nWere you surprised by any of your findings?\\r\\nA kind of “plateau” in the achievable results: it was reasonably easy to reach a score around 0.965, but breaching the 0.97 threshold took quite a bit of effort. This was certainly one of the most saturated leaderboards we have seen. This observation was confirmed by examination of cross-validated results for different models we used to “mix” the level 1 metafeatures: the variation across folds was bigger than the distance between the extreme scores within top 10. This uncertainty kept us on our toes until the very end.\\r\\n\\r\\n[table width=\"500px\" class=\"table table-bordered\"]\\r\\nmodel,mean,std\\r\\nglmnet,0.968701,0.000937\\r\\nxgboost,0.968951,0.000962\\r\\nnnet,0.968056,0.001001\\r\\nhillclimb,0.968048,0.000940\\r\\nranger,0.967308,0.000951\\r\\n[/table]\\r\\nTable 2: summary statistics for different models used for mixing level 1 metafeatures. “Hillclimb” is our implementation of the “libraries of models” approach of Caruana et al, other names correspond to the R packages used.\\nWhich tools did you use?\\r\\nWe used R for preparing variants of the dataset and ensemble construction (mostly because we had a working implementation an ensembler from prior contests) and Python for pretty much anything else, like metafeatures generation. Compiling a multithreaded version of xgboost proved especially useful.\\r\\nHow did you spend your time on this competition?\\r\\nFeature engineering took about 40 pct of the time, as did metafeatures generation. The rest was parameter tuning at different stages in the ensemble.\\r\\nRather than manual hyper parameter tuning, we used Bayesian Optimization for a more automated approach. (The same approach we took is now a script on the BNP Paribas competition)\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nPutting a number on the whole process is tricky: from the very beginning, we decided to go for the stacked ensemble approach, which means that for any model we tried, we generated stacked predictions across the entire training set and threw them into a shared folder. We sort of accumulated the training set over time.\\r\\n\\r\\nBuilding an ensemble predictor using multithreaded xgboost took under an hour, if we also used bagging then the training time scaled almost linearly (so for instance a 10-bag would run overnight on a 16GB/i7 Macbook Pro).\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nIn no particular order:\\r\\n\\nBragging rights\\nWe are both Masters now, which means New Model Army can attack private contests\\nThe prize\\nOur approach to competitions is one step closer to being a properly streamlined process\\n\\nDo you have any advice for those just getting started in data science?\\r\\nApart from the obvious (learn Python, read winners solution descriptions on Kaggle :-), feature engineering is an extremely important skill to acquire: you can’t really follow a course to learn it, so you need to practice and grab/borrow/steal ideas whenever you can.\\r\\n\\r\\nSmart features combined with a linear model often beat more sophisticated approaches to a variety of problems.\\r\\nTeamwork\\nHow did your team form?\\r\\nWe have worked together – on Kaggle and outside of it – before, so joining forces on this one was a natural thing to do.\\r\\nHow did your team work together?\\r\\nBased on past experience, at the start of Homesite we had a pretty good idea of what worked and what did not in our earlier attempts.\\r\\nHow did competing on a team help you succeed?\\r\\nWe have enough in common background-wise to communicate efficiently, while the differences allow us to view problems from different angles. Paranoid people live longer, so occasionally double-checking each other’s code helped us ferret out a few nasty bugs which could have led to an epic overfit.', 'Santa\\'s Stolen Sleigh, Kaggle\\'s annual optimization competition, wrapped up in early January with many familiar faces from previous years on the leaderboard. For the third year in a row, Marcin & Marek competed together and finished in-the-money, continuing to prove their optimization prowess. This year they took third place as team \"master.exploder@deepsense.io\" by carefully\\xa0designing their local search algorithm. In the end, they discovered its moves were not quite greedy enough to take the top spot. This blog shares the approach of these long-standing teammates and experts in the field.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nWe are both active academic researchers in the field of algorithmics and faculty members of the Institute of Informatics, at Warsaw University. (Faculty pages for Marcin and\\xa0Marek). We are particularly interested in ways of dealing with computational hardness: mainly approximation algorithms, and in Marek’s case parameterized complexity.\\r\\n\\r\\n[caption id=\"attachment_5580\" align=\"aligncenter\" width=\"601\"] Marcin (left) & Marek (right) on Kaggle[/caption]\\r\\n\\r\\nWe also both have a long history of competing in all kinds of programming contests. Quick Topcoder style contests, ACM ICPC, marathons, 24-hour challenges - we have done all of these many times.\\r\\nWhat made you decide to enter this competition?\\r\\nThe Kaggle Santa contest seemed perfectly suited for us. Not only are the problems computationally hard, which is right up our alley, but also the long duration gives these contests a bit of a research flavor. What‘s more, we do not really have that much free time on our hands, less than ever in fact, with both of us becoming fathers quite recently. In particular, the TopCoder style 2-week marathons are really not something we are able to compete in these days. Kaggle’s holiday contest allowed for a much more relaxed attitude (the Rudolf prize havoc in the first half of the competition is a bit stressful though).\\r\\n\\r\\nBecause of all this, we were very happy to take a shot at the sleigh packing contest two years ago, had a blast and actually ended up taking the top spot. We then participated last year, and managed to claim all three prizes. After this, taking up the challenge again was a no-brainer.\\r\\nLet\\'s Get Technical\\nWhat was your most important insight into the data?\\r\\nNot really an insight, but what definitely shaped our whole participation was the early realization that this was a local search problem, and most likely no other approach would be able to compete with it. We later attempted to complement it with MIP modelling. Initially, the MIP model helped us evaluate the quality of the trips produced by the local search solver, and identify its weaknesses. However, we failed at adapting it to larger pieces of the data.\\r\\nWere you surprised by any of your findings?\\r\\nWe did run into a couple of rather surprising outcomes. One was our complete failure to get anything out of parallel tempering (which is a parallel simulated annealing minus the annealing, sort of). We knew it was hard to set up and took a long time to arrive at good solutions, but in our case, it just seemed not to work at all. The other was our initial failure when attempting to make a multithreaded solver work. As it turns out, multithreaded programming in C++ is harder than we thought, and we learned a lot this way.\\r\\nWhich tools did you use?\\r\\nWe used the same set of tools we usually use for these kind of problems: C++ (with GCC and clang compilers) for the main solution, and python/R/awk/bash for visualizations and log analysis. We also spent quite a lot of time playing with the FICO optimizer, trying to design MIP formulations that solve in reasonable time for non-trivial sets of gifts.\\r\\nHow did you spend your time on this competition?\\r\\nBecause of the Rudolf Prize, the initial couple weeks are always a bit crazy in these contests. Our first goal was to obtain a fully functional optimizer as fast as possible, in order to not lose points in the Rudolf ranking. Then, we spent a lot of time modifying the optimizer, playing with parameters, and generally trying to be ahead of the pack. This happened to be quite stressful at times. On one of the mornings, after a couple days of relative stability on the leaderboard, during a single hour three different teams claimed the top spot for the very first time. We spent a good amount of time frantically going through contest rules looking for some hidden meaning of this, apparently there was none.\\r\\n\\r\\nAfter the initial craze, we cooled off when it became clear that with our current solution, we would not be able to compete with the top team (at that time, i.e. woshialex & weezy). We spent a lot of time trying to figure out the weaknesses in our solution and find ways to go around them. We arrived at several new ideas this way and spent a lot of time implementing and trying them out.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nIt seemed rather clear from the get-go, that this contest would be about implementing the best possible local search algorithm, most likely simulated annealing. This means spending most of your “thinking” time designing the state representation, the moves, the temperature schedule, etc.\\r\\n\\r\\nIn the end, it seems our moves were not greedy enough. In particular, almost all of our moves were attempting to perform a completely random local modification on the current solution. In contrast, the moves used by the top two teams tried to locally perform a modification that was in some sense optimal (or at least the second place team, we do not really understand the first place solution at the moment). This always has pros and cons. The convergence is clearly much faster, but the search space becomes much more disconnected (or rather very loosely connected), and the very best solutions might not be achievable anymore.\\r\\n\\r\\nEven now, almost a month after the contest has ended, we are having a hard time truly understanding the reasons for greedy moves’ superior performance, so this is definitely something to take away from this competition.\\r\\nTeamwork\\nHow did your team form?\\r\\nOur offices at Warsaw University are opposite each other, and we bump into each other and chat on a regular basis. We also both have a very strong drive to compete in programming contests of all kinds, so it only seemed natural to form a team. And then, after doing very well in the sleigh packing contest, there was really no reason to abandon the winning formula. This year we got some extra support from our deepsense.io R&D team, and it was not just cheering. We got to use one of the multiprocessor Xeon machines for the duration of the contest, which does make a huge difference!\\r\\nHow did your team work together?\\r\\nIt does not make much sense to jointly develop a hacky/messy C++ optimizer. Instead, we decided that one of us would focus on developing this program (this would be Marek, who is a faster and less error-prone coder), and the other would perform explore alternative solution paths (i.e. MIP modelling) and perform other tasks not related to the main optimizer. This worked really well in the previous contest, where the problem had much more structure, but not so well in this one.\\r\\nHow did competing on a team help you succeed?\\r\\nThis is most likely obvious to anyone who competed with a team at least once, but we will say it anyway: a team is never just a sum of its members. The interactions between its member, inspirations, feeding of each others ideas, etc., brings the joint performance to a whole new level. It also makes it a much more enjoyable experience as well as an opportunity to learn.\\r\\nBios\\nMarek Cygan, PhD\\r\\nMarek is the world champion in ACM ICPC 2007 and the world champion in Google Code Jam 2005. The main themes of Marek’s research interests are different aspects of algorithmics. He focuses on approximation algorithms and fixed parameter tractability. After being a post-doc at the University of Maryland and IDSIA, University of Lugano, Marek returned to the Institute of Informatics at University of Warsaw, where he is currently an Assistant Professor. Moreover, Marek is an active contributor to deepsense.io R&D team, which focuses on Deep Learning.\\r\\n\\r\\nMarcin Mucha, PhD\\r\\n\\r\\nPhD, DSc. Master’s degree in Computer Science and Mathematics from University of Warsaw. He specializes in algorithmics. Once Marcin completed his PhD he spent a year at Max Planck Institute in Saarbruecken, Germany as a post-doc. Then he returned to University of Warsaw, where he is currently employed as an Assistant Professor. His research focuses on approximation algorithms for hard problems. However, he is also interested in other areas, e.g. online algorithms and graph algorithms. He has got a significant experience in practical applications of combinatorial optimization and Machine Learning methods. Finalist of the ACM ICPC World Finals 1996. Additionally, Marcin is an active contributor to deepsense.io R&D team, which focuses on Deep Learning.', 'AirBnB New User Bookings was a popular recruiting competition that challenged Kagglers to predict the first country\\xa0where a new user would book travel. This was the first recruiting competition on Kaggle with scripts enabled. AirBnB encouraged participants to prove their chops through their collaboration and code sharing in addition to\\xa0their final models. Sandro Vega Pons took 3rd place, ahead of 1,462 other competitors, using an ensemble of GradientBoosting, MLP, a RandomForest, and an ExtraTreesClassifier. In this blog, Sandro explains his approach and shares key scripts that illustrate important aspects of his analysis.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI currently work as a postdoctoral researcher at the NeuroInformatics Laboratory, FBK in Trento, Italy. My current research is mainly focused on the development of machine learning and network theory methods for neuroimaging data analysis. I hold a M.Sc. in Computer Science and a Ph.D. in Applied Mathematics.\\r\\n\\r\\n[caption id=\"attachment_5589\" align=\"aligncenter\" width=\"300\"] Sandro\\'s profile on Kaggle and LinkedIn[/caption]\\r\\nHow did you get started competing on Kaggle?\\r\\nI first heard about Kaggle around three years ago, when a colleague showed me the website. At that time I was starting to work with scientific and analytic Python packages (numpy, scipy, scikit-learn, etc.) and I found Kaggle competitions a perfect way of getting familiar with them. The first competition in which I seriously participated was Connectomics, which was somewhat related to my research activities in that time. From that moment on, I have tried to be always active in at least one competition, even though many times I cannot dedicate as much time as I would like to the competitions.\\r\\n\\r\\n[caption id=\"attachment_5590\" align=\"aligncenter\" width=\"412\"] Sandro\\'s top 8 finishes[/caption]\\r\\nWhat made you decide to enter this competition?\\r\\nAfter ending up in the top 10% of some competitions, I was still looking for one in which I could finish in the top10, in order to get the master tier. I found this competition very interesting for many reasons, e.g.: there was room for creativity in feature engineering, the problem seemed to be complex enough (12-class classification, highly unbalanced, classes very overlapped, I was not very familiar with the evaluation measure, etc.) and the data was not too big (I could work on my laptop while on Christmas holidays). Then, after trying some initial ideas and seeing that the results were promising, I got completely hooked on\\xa0this competition.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nMy solution was basically based on feature engineering in both users and sessions data and a 3-layers architecture for ensembling classifiers (see Figure 1).\\r\\n\\r\\n[caption id=\"attachment_5586\" align=\"aligncenter\" width=\"592\"] Fig. 1[/caption]\\r\\n\\r\\nFeature Engineering:\\r\\n\\r\\nI extracted features from both users and sessions files. In the case of users, I applied a one-hot-encoding representation for categorical features and computed several features from age and dates, using different techniques for dealing with missing values. The\\xa0sessions data was aggregated by user\\'s id and different features based on counts, frequency, unique values, etc. were computed. The full feature engineering code can be found in this script.\\r\\n\\r\\nLearning architecture:\\r\\n\\n1st Layer: I tried many classifiers at this level. Since the beginning, I got the best individual results with XGBoost GradientBoosting. Later on, I was able to get comparable results with deep MLP implemented on Keras. I also used other classifiers (with a bit worse performance) implemented in scikit-learn, like RandomForest, ExtraTreesClassifier and LogisticRegression. My best submission was computed by using 5 versions of GradientBoosting (trained with different parameters, class weights and subset of features), 5 versions of deep MLP (trained with different parameters, class weights and subset of features), a RandomForest and an ExtraTreesClassifier.\\r\\n\\n2nd Layer: I implemented my own blending strategies based on the scipy.optimization package. A detailed description, source code of these ensembling methods, and how they can be used inside a 3-layer learning architecture can be found in this python notebook. I implemented two methods, called EN_optA and EN_optB in the script. I used the two methods and a calibrated version of them (using the CalibratedClassifierCV from scikit-learn) to end up with four ensemble predictions.\\r\\n\\n3rd Layer: This last layer consists of a weighted average of the 4 predictions obtained in the previous layer.\\n\\nWhat was your most important insight into the data?\\n\\nAlthough the two problems are different, after some feature engineering on the data of this competition, the resulting classification problem, was somehow similar (number of classes, number of features, number of samples) to that of the Otto Group Product Classification Challenge. The algorithms I used for that competition (ending up 34th from 3514 teams) were a good starting point.\\nThe two proposed ensembling techniques (EN_optA and EN_optB) minimize an objective function and the best results were obtained when minimizing the multiclass logloss. This means that in practice, the two methods basically became implementations of logistic regression. However, I always got better results with the proposed techniques rather than with the sklearn implementation of LogisticRegression. They also outperformed non linear classifiers like GradientBoosting (using multiclass logloss as objective function). I made a quick experiment in the last part of this\\xa0notebook in order to explore the behavior of the applied techniques in more detail. The idea was to compare the performance of the different ensembling methods on synthetic data for different number of classes. The results of this experiment (for the limited set of parameters values I explored) show that LogisticRegression (as implemented in sklearn) and GradientBoosting produce better results for problems with a few number of classes. Nevertheless, as the number of classes increases, the proposed ensembling techniques become the best option.\\n\\r\\n[caption id=\"attachment_5588\" align=\"aligncenter\" width=\"600\"] Fig. 2[/caption]\\r\\n\\nThe further combination of the two proposed ensembling techniques and their calibrated versions, gave me a significant final boost. This means that they produce comparable results, but different enough to blend well. This was also verified in the experiment with synthetic data (Figure 2 above), where EN_3rd_layer always outperformed EN_optA and EN_optB.\\n\\nWere you surprised by any of your findings?\\n\\nI was happily surprised of being able to create deep MLP models producing results that were comparable to the best XGBoost ones. Moreover, I am always a bit surprised about how well deep learning models (like deep MLP) and tree-based models (like GradientBoosting) complement each other, and therefore how good they perform when ensembled. The combination of these two types of learning algorithms seems to be a recurrent choice in Kaggle competitions. This perhaps requires a more systematic study, beyond intuitive explanations based on the general understanding of the two types of models.\\nI always got the best results when using multi-class log-loss as objective function. Specifically, for the the definition of EN_optA and EN_optB (second layer), I tried with different objective functions like mlogloss, mlogloss@5 (multi-class log-loss after keeping only the 5 classes with highest probabilities), ndcg@5 (the competition evaluation metric) and mlogloss + 2 - ndcg@5 (a combination of mlogloss and ndcg after converting ndcg to a dissimilarity measure). I was expecting the ndcg not to work well, since it is a non-smooth function and therefore not suitable for gradient based optimization. However, I was not sure whether some combination of mlogloss and ndcg could give any improvement.\\n\\nWhich tools did you use?\\r\\nPython (numpy, pandas, scipy, xgboost, keras, scikit-learn).\\r\\nHow did you spend your time on this competition?\\r\\nAt the beginning of the competition I focused on creating a very simple working pipeline that, by only using users data, was able to produce a meaningful submission. I made public this pipeline (it is based on simple feature engineering and XGBoost) on this script. After that, I focused on feature engineering on both users and sessions data and updated my model with the features computed from session data. I was then inactive for more than one month, but when I got back to the competition, I started working on other classifiers. The idea was to tune some classifiers that would potentially blend well with my best XGBoost model. During the final days I focused on ensembling. I first started with a more traditional stacked generalization approach based on LogisticRegression and GradientBoosting, but as I didn’t get very good results, I moved on to building my own ensemble strategies.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\n\\nThe importance of keeping an eye on the competition forum and contributing to it (I didn’t do much of that before this competition) when possible. Something similar can be said about competition scripts, they can be useful to others, but you can also get valuable feedback.\\nEven though my NN models were very simple, I spent quite a bit of time with tools like Theano and Keras. This gave me the opportunity to understand many concepts of deep learning that were not familiar to me.\\nThe importance of spending time in understanding the evaluation metric and how to optimize it. This competition is an example of why directly using an evaluation metric as objective function is not always the optimal solution.\\n\\nDo you have any advice for those just getting started in data science competitions?\\n\\nPlan the time that will be spent on the competition. Do not focus on being in the top of the leaderboard from\\xa0the beginning, make a plan and work accordingly. For example, do not start with complicated ensemble techniques before squeezing out single classifiers.\\nTry many ideas, but be careful with the reproducibility of the code (e.g. take care of seeds for random number generators).\\nDo not move forward to complex ideas before having a deep understanding of the evaluation measure and a reliable validation strategy. Depending on the competition, this can be trivial or very tricky.\\n', 'Prudential Life Insurance Assessment ran on Kaggle from November 2015 to February 2016. It was our most popular recruiting challenge\\xa0to date, with a total of 2,619 data scientists competing for a career opportunity and the $30,000 prize\\xa0pool.\\xa0Bogdan Zhurakovskyi took second place, and learned an important lesson:\\xa0there is no innate hierarchy to the accuracy of different machine learning algorithms.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI am Ph.d. candidate in statistics. Since I started competing on Kaggle I have gained a lot of practice which has improved my skills significantly.\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Bogdan on Kaggle[/caption]\\r\\nWhat made you decide to enter this competition?\\r\\nI like competitions where are a lot of teams and forum discussions. You can get a lot of knowledge from them.\\r\\nLet\\'s Get Technical\\nWhat was your most important insight into the data?\\r\\nTill this competition I mistakenly believed that there is some hierarchy among algorithms in terms of accuracy. What I mean is that, for example, gradient boosting gives you the best accuracy, followed by svm and random forest, and in the end linear models. If you want to improve your accuracy further then you make an ensemble of different models. But that is not always true. There are datasets where a linear model can beat a gradient boosting model. This was a new discovery for me.\\r\\n\\r\\nBelow is a boxplot and distribution of kappa score obtained by using train test split function from sklearn 200 times (split koef = 0.3). As one can see, they are almost the same. But I spent a lot of time to find the parameters of xgboost to be so close to linear regression which gave me the best results. So there is no reason to use complex models if one can get the same results with a simple one.\\r\\n\\r\\n\\n\\nWhich tools did you use?\\r\\nPython scikit-learn and XGBoost libraries.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nDo not neglect any of your ideas. The gold can hide in the most unexpected places.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nTry to understand the math behind ML algorithms. Do not use an algorithm\\xa0without understanding it. Otherwise, at some point, your progress is going to stop.\\r\\nJust for Fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nI would love to see some weather prediction competitions. Because we all know how “accurate” modern models are :). Maybe some smart guy could finally improve that thing (joking).\\r\\nWhat is your dream job?\\r\\nArtificial Intelligence Developer.\\r\\nBio\\nBogdan Zhurakovskyi\\xa0is a PhD Candidate in Probability Theory and Mathematical Statistics at the Kyiv Polytechnic Institute, supervised by Alexander Ivanov. His research interests include nonlinear regression models, detection of hidden periodicities, and most of statistical machine learning.', 'AirBnB New User Bookings\\xa0challenged Kagglers to predict the first country\\xa0where a new user would book travel. Participants were\\xa0given a list of users along with their demographics, web session records, and some summary statistics.\\xa0Keiichi Kuroyanagi (aka Keiku) took\\xa02nd place, ahead of 1,462 other competitors using 1,312 engineered features and a stacked generalization architecture. In this blog, Keiku provides an in-depth view of his approach, final architecture, and why he didn\\'t get punished by\\xa0a leaderboard shakeup.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI\\'m currently working as a consultant for companies of various industries using data science skills. I support mainly the marketing of the companies by means of machine learning techniques, e.g. regression, classification and clustering. Prior to this, I earned\\xa0an MSc in condensed matter physics at the Department of Physics. I have researched Anderson localization of Bose-Einstein condensation of cold atom in a quasiperiodic optical lattice at university.\\r\\n\\r\\n[caption id=\"attachment_5601\" align=\"aligncenter\" width=\"300\"] Keiku on Kaggle[/caption]\\r\\nWhat made you decide to enter this competition?\\r\\nAt first sight, I thought\\xa0that I would\\xa0not be good at this competition, because the \"date_account_created\" of the test dataset was the last 3 months of the whole dataset. Cross-validation for this kind of dataset becomes very\\xa0difficult. I\\'m not good at this kind of cross-validation. That is why I decided to join\\xa0this competition -- to overcome my\\xa0weakness in this area. I submitted first submission on Jan. 25, 2016. (This competition was started Nov. 25, 2015 and ended Feb. 11, 2016 (78 total days)). I just started as a way to study\\xa0in the last 3 weeks of this competition.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nVarious datasets were given to us in this competition. It allowed for some creative feature engineering. I created some features as follows:\\r\\n\\nAs for the numerical features, I used the raw of variables of the dataset except \"age\".\\nI created age features cleaning up some abnormal values.\\nAlso, I divided the above\\xa0age features in a bucket and joined \"age_gender_bkts\" dataset to \"train_users\" and \"test_users\" dataset.\\nI encoded the categorical features using one-hot encoding.\\nI joined \"countries\" dataset to \"train_users\" and \"test_users\" dataset.\\nI calculated the lag of \"date_first_booking\" and \"date_account_created\" and divided this lag feature into four categories (0, [1, 365], [-349,0), NA).\\nSimilarly, I calculated the lag of \"date_first_booking\" and \"timestamp_first_active\" and divided this lag feature into three categories (0, [1,1369], NA).\\nI summarized \"secs_elapsed\" and counted the numbers of rows of \"sessions\" dataset by user_id, action (similarly, action_type, action_detail and device_type).\\n\\r\\nI created a total of 1,312 features from the given dataset.\\r\\n\\r\\nThen, I calculated out-of-fold CV predictions of 18 models (stacked generalization). The approach of using stacked generalization has won some competitions (See also the Kaggle Ensembling Guide). In many cases, a target variable is predicted in stacked generalization, but I calculated not only \"country_destination\" (target variable) but also \"age\" and the above categorized lag features (explanatory variable).\\r\\n\\r\\nI built the XGBoost model using above base features and out-of-fold CV predictions. My XGBoost model set the custom function of NDCG@5 as \"eval_metric\" parameter. When I made several attempts to build it, I found that some features decreased the NDCG@5 score, so I selected randomly features at the ratio of 90% and built repeatedly a single XGBoost many times. Finally, I selected the best XGBoost model (5 fold-CV: 0.833714) from the built models and I got Public: 0.88209/Private: 0.88682 using the best XGBoost model. (It has published code here: 2nd Place Solution).\\r\\n\\r\\n[caption id=\"attachment_5604\" align=\"aligncenter\" width=\"720\"] Learning architecture[/caption]\\r\\nWere you surprised by any of your findings?\\r\\nThis competition was expected to have a Leaderboard Shakeup (See also Expected Leaderboard Shakeup), but I got comparatively stable results (Public LB: 2nd/Private LB: 2nd). My local 5 fold-CV score was relevant to Public Leaderboard score. As for the final two submissions, one was the best model where\\xa0both scores were\\xa0good, and the other was the best model where\\xa0the score of validation in the last 6 weeks was good. The former (Public: 0.88209/Private: 0.88682) was slightly higher private score than the latter (Public: 0.88195/Private: 0.88678).\\r\\n\\r\\n\\r\\n\\r\\nI think that the below features slightly prevented a Leaderboard Shakeup for my models. I checked the feature importance of the best XGBoost model. I found that the out-of-fold CV predictions of categorized lag features were very important. As far as I saw in the forum, many of the participants may have not created these features.\\r\\n\\r\\n\\nWhich tools did you use?\\r\\nI used R in this competition. I used DescTools package for the preprocessing. Desc() function was helpful to check the statistical information about the dataset. As for the modeling, I used XGBoost and glmnet.\\r\\nWords of Wisdom\\nDo you have any advice for those just getting started in data science?\\r\\nTry Kaggle competitions at all times. The tasks of Kaggle competitions show\\xa0one of the issues of the company or the challenges in research. The experiences of Kaggle competitions are able to be utilized for similar\\xa0issues that we have at work or in research. We can learn not only machine learning techniques, but a lot of other things from Kaggle competitions.\\r\\nBio\\nKeiichi Kuroyanagi has worked as a consultant at Financial Engineering Group, Inc. He holds an MSc in condensed matter physics at Department of Physics from Keio University. He is about to start to develop the market of non-financial field besides financial field.', 'Telstra Network Disruptions challenged Kagglers to predict the severity of service disruptions on their network. Using a dataset of features from their service logs, participants were tasked with predicting if a disruption was a momentary glitch or a total interruption of connectivity. 974 data scientists vied for a position at the top of the leaderboard, and an opportunity to join Telstra\\'s Big Data team. Mario Filho, a self-taught data scientist, took first place in his first \"solo win\". In this blog, he shares\\xa0a high-level view of his approach.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nMy background in machine learning is completely “self-taught”. It all began in 2012 when I decided to learn Calculus on my own through the videos from a MIT class. Since then I found a wealth of education materials available online through MOOCs, academic papers and lectures in general.\\r\\n\\r\\nSince February 2014 I have worked as a machine learning consultant, having worked in projects from small startups and Fortune 500 companies during this period.\\r\\n\\r\\n[caption id=\"attachment_5609\" align=\"aligncenter\" width=\"300\"] Mario on Kaggle[/caption]\\r\\nWhat made you decide to enter this competition?\\r\\nI was looking for competitions that were ending soon, and this one had only 19 days before the end. And I like competitions with multiple files because it gives you many possibilities to create features.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nThe most important preprocessing that I did was manipulating the tables to extract features that looked relevant. It was a very manual process of repeatedly looking at the data, extracting features and testing in my CV.\\r\\n\\r\\nFor neural networks the extra step was standardizing the data.\\r\\n\\r\\nMy best model was a Gradient Boosted Trees ensemble, using XGBoost (surprise!), trained with all my features.\\r\\n\\r\\nThe winning solution was a three-layer stacked ensemble with 15 models, mostly composed by GBTs, Neural Networks and Random Forests.\\r\\nWhat was your most important insight into the data?\\r\\nDefinitely finding that the ordering of records in the additional files had high predictive power. I noticed there was some powerful pattern when I saw an unusual gap between the participants in the leaderboard, and knew that I had to find it if I wanted to end in a good position.\\r\\nWhich tools did you use?\\nPandas for data manipulation. Scikit-learn, XGBoost and Keras for modeling.\\r\\nHow did you spend your time on this competition?\\r\\nI would say 70% feature engineering and 30% with ensembling and tuning.\\r\\nWords of Wisdom\\nWhat have you taken away from this competition?\\r\\nI learned a lot about different ways that you can explore the data and extract features. It’s my first “solo” win, which is quite nice, and I liked the jump to the 12th place in the global ranking.\\r\\nJust for fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nI would like to see more machine learning applied to mental health. For example, today people with depression have to try many different medications to find which works for them, so trying to predict which treatment is most likely to work well for a depressed patient would be nice.\\r\\nBio\\nMario Filho is a data science consultant focused in helping companies around the world use machine learning to maximize the value they get from data to achieve their business goals. Besides that, he mentors individuals who want to learn how to apply machine learning algorithms to real world data sets.', 'The Homesite Quote Conversion competition asked the Kaggle community\\xa0to predict\\xa0which customers would\\xa0purchase a quoted insurance plan in order to help\\xa0Homesite to better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments. The 1764 competing teams faced an anonymized dataset with around 250k training samples in almost 300 dimensions. They were challenged to predict the probability a customer would purchase an insurance plan given a quote. Team KazAnova | Faron | clobber managed to win a head-to-head race at the end and finished in 1st place.\\r\\nOur Team\\nMarios Michailidis | KazAnova\\n\\xa0is Manager of Data Science at dunnhumby and part-time PhD student in machine learning at University College London (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK and has led many analytics projects with various themes including acquisition, retention, uplift, fraud detection and portfolio optimization. In his spare time he has created KazAnova GUI for credit scoring. Marios has 70 Kaggle competitions on his turtle shell and started competing on Kaggle to find new challenges and to learn from the best.\\r\\n\\r\\nMathias Müller | Faron\\n\\xa0holds an AI & ML focused Diplom (eq. MSc.) in Computer Science from Humboldt University of Berlin. He tinkered with computer vision in the context of bio-inspired visual navigation of autonomous flying quadrocopters during his studies. Currently, he is working as a ML-Engineer for FSD in the automotive sector. Mathias stumbled upon Kaggle while looking for a more ML-focused platform compared to TopCoder, where he entered his first predictive modeling competition. Besides, he likes to contribute to the amazing XGBoost.\\r\\n\\r\\nNing Situ | clobber\\n\\xa0is a Software-Engineer at Microsoft. He is currently working on a real-time data processing system hosted on Cassandra. Ning obtained his PhD in Computer Science from University of Houston for doing research in melanoma recognition and considers Kaggle a wonderful place to gain knowledge in Data Science through real projects.\\r\\n\\r\\n\\xa0\\r\\n\\n\\n\\nKazAnova | Marios\\n\\n\\n\\nFaron | Mathias\\n\\n\\n\\nclobber | Ning\\n\\n\\n\\nOur Solution\\r\\nFrom the start we were pursuing an ensembling approach and therefore we tried to get as many diverse models as possible. We were using the following software & tools:\\r\\n\\nScientific Python Stack (including\\xa0NumPy,\\xa0SciPy,\\xa0Pandas,\\xa0StatsModels,\\xa0Matplotlib)\\nXGBoost\\nScikit-Learn\\nKeras\\nLasagne & NoLearn\\nLibFM\\nKazAnova GUI\\nXgbfi\\n\\r\\nto build a pool of around 500 base models from which we selected about 125 for ensembling. Our final ensemble\\r\\nconsists of 3 meta layers. It achieves a public leaderboard AUC of 0.97062 and a private leaderboard AUC of 0.97024:\\r\\n\\n\\n\\nFig. 1: Final Ensemble\\n\\n\\nData Preprocessing & Feature Engineering\\r\\nThis dataset required hardly any cleaning and we encoded the provided data in many different ways in order to illuminate the input space as much as possible:\\r\\n\\nCategorical ⇒ ID per category\\nCategorical ⇒ value count for each category\\nCategorical ⇒ out-of-fold likelihood for each category with respect to the target attribute\\nCategorical ⇒ one-hot encoding\\nNumerical ⇒ as is\\nNumerical ⇒ percentile transformation\\nOne-hot encodings and value counts of all features with distinct values below a threshold\\n\\r\\nRegarding feature engineering, we extracted year, month, day & weekday out of Original_Quote_Date, explored summary statistics and 2-, 3- & 4-way feature interactions (sums, differences, products and quotients). The search for the latter\\xa0was initiated due to the positive effect of the \"golden features\" (differences of highly correlated features) used in the \"Keras around 0.9633*\" public script. We searched interaction candidates either by simple target correlation checks or through logistic regression and XGBoost as wrappers. The space of possible 2-way interactions was searched\\xa0exhaustively and random searches with adjusted sampling probabilities based on feature rankings were used to find higher-order interactions. Afterwards, we applied the feature selection methods described below to the most promising candidates. Feature interactions helped to get diverse as well as better performing models, which added significant value to our ensemble.\\r\\n\\n\\n\\nFig. 2: Search for feature interactions with XGBoost as wrapper\\n\\n\\nFeature Ranking & Subset Selection\\r\\nFeature subset selection turned out to be useful to increase both single model performance and diversity. It was also playing an important role during our meta modeling.\\xa0We applied and combined a variety of wrapper and embedded methods to rank the features of the base and meta levels:\\r\\n\\nForward Selection & Backwards Elimination: either via brute-force if feasible or greedy in conjunction with feature rankings.\\nSingle Feature AUC: by training a model like XGBoost on each feature seperately or calculating gini coefficients on binned versions of the features.\\r\\n\\n\\n\\nFig. 3: Example of single feature scoring\\n\\n\\nXGBoost: we used Xgbfi to rank the features by different metrics like FScore, wFScore, Gain and ExpectedGain (see Fig. 4).\\r\\n\\n\\n\\nFig. 4: Feature ranking metrics regarding XGBoost\\n\\n\\nNoise Injection: we replaced features by noise or added noise to the features after model training and monitored the impact on the validation errors.\\r\\n\\n\\n\\nFig. 5: Noise injection in order to detect unimportant features\\n\\n\\n\\nModeling\\r\\nWe started with building some marginally tuned models and constructed our meta modeling early on to get an idea about what adds diversity and to optimize the ensemble performance alongside. Neural networks have been the best performing stackers, followed by XGBoost and logistic regression. Besides, XGBoost and logistic regression showed to be considerably more sensitive to feature selection at the meta levels than neural networks.\\xa0In general, it turned out to be more useful to train on many different combinations of feature subsets and feature representations than to tune hyperparameters, which is why several (base) models share\\xa0the same parameter settings.\\r\\n\\r\\nOur best base model (XGBoost) emerged from feature subset selection and utilization of feature interactions and scores Top15 at the private leaderboard. Regarding public leadboard scores,\\r\\nwe got the following ranking of algorithms at the base level:\\r\\n\\nXGBoost: ~0.969\\nKeras, Lasagne, GBM: ~0.967\\nRandom Forest,\\r\\nExtra Trees: ~0.966\\nLogistic Regression: ~0.965\\nFactorization Machine: ~0.955\\n\\r\\nAround 20% of our selected base models were \"sub-models\" - trained on different partitions of the data specified by feature values (e.g. one model per weekday). We used KazAnova GUI\\'s optimized binning based on Weights of Evidence (WOE) and Information Value (IV) to ensure sufficiently large data partitions for features with higher cardinality of distinct values like GeographicField6B:\\r\\n\\n\\n\\nFig. 6: Binning of GeographicField6B\\n\\n\\nOther Things ... which added diversity or increased single model performance:\\n\\nGradient boosting from random forest predictions (adopted from Initialized Gradient Boosted Regression Trees and applied to XGBoost)\\nFeature-specific dropout probabilities within XGBoost (non-uniform colsample, special case: adding features at later boosting stages)\\nXGBoost feature (interactions) embeddings as inputs for neural networks or linear models\\nBoosting of random forests (by setting XGB\\'s num_parallel_tree > 1)\\nModel training with different objectives\\n\\nCross Validation\\r\\nWe were using 5-fold stratified cross validation during the whole competition with a small set of seeds for the base models. At first, our meta models were tuned on fixed-seeded folds for the sake of having a common reference,\\xa0but we were forced to avoid this seed later in the competition once we realized that we had overused it. After that we continued to train our models on different seeds and we sampled different seeded 5-folds out of our oof-predictions in order to compare the scores of the folds the models had been trained on and other folds. Our best public LB submission (0.97065) showed some irregularities with respect to these analyses and therefore we did not select it for final evaluation.\\r\\n\\r\\nWe also took the public leaderboard scores as additional source of information into account and checked for a pair of submissions, whether the observed public LB score difference matched the AUC variations between the local folds of these two submissions. This helped us to identify inconsistencies: In general, an improvement of the CV-mean did not imply AUC improvements on all 5 folds. We saw absolute AUC differences up to 0.000085 for a single fold between two submissions with the same CV-mean. Besides, each fold had a slightly different correlation to the public LB score. We hit a suspicious looking plateau around the LB score of 0.97025, which did not match the normal pattern between AUC changes of our local folds and the public LB score. In fact, it turned out to be caused by a bug in our sources, leading to faulty calculations of 3-way interactions for the test data.\\r\\n\\n\\n\\nFig. 7: Suspicious plateau around the LB score of 0.97025\\n\\n\\nMarios\\' Corner: Some Tips Regarding Cross Validation\\r\\nReliable cross validation, detection of tiny inconsistencies and dealing with serious overfitting issues have probably been the most important ingredients in our solution. So I thought this post would be a good place to share some aspects that I have been considering useful over my past 70+ Kaggle competitions to set up a reliable CV:\\r\\n\\nStratified k-fold. Unless the dataset you are being tested on (e.g the test set ) is in future period, then most of the times use random stratified k-fold - it seems to work\\xa0well. However, if the test data is in the future - and time seems to be an important factor like in stock marker or store sales - then you need to formulate your CV process to always train on past data and test on future data (e.g. do a time split).\\n\\n\\nTreat your CV like your test data. That means if there is something that you cannot know or do for your test data, then you have to treat the validation data as if you don\\'t know it too.\\r\\nExample: Let\\'s say you run a neural net (that is easy to overfit / underfit) and you use the validation data to determine at which exact epoch to stop the training and you check the performance for the same validation data. Can you use the test data to know when to stop the training? No, you can\\'t, because you don\\'t know the labels for the test data. Hence it is invalid to use this for validation. Instead, you could use a fixed amount of epochs that works well on average for all your folds. From my perspective, the best way to do that would be to split your training data into 3 parts when you do CV and use them\\r\\n\\nto train the model,\\nto validate when to stop the training,\\nto see the performance on a real holdout set.\\n\\r\\nGenerally always keep in mind that you need to treat your validation data as much as you can as your test data (e.g. like you don\\'t know the labels), otherwise you may overfit.\\n\\n\\nAlways test against the metric you are being tested on. If it is AUC, then AUC. If it is RMSE, then RMSE and so on.\\n\\n\\nModel type and size of dataset. It is more difficult to overfit with 500,000 rows versus with 10,000 rows. If you have less than 400-300, you are kind of doomed! From my experience, it is difficult to form a reliable CV process with so little data. All neural nets are more prone to overfitting than linear models.\\n\\n\\nTest your CV. Sacrifice a couple of submissions to see whether you got it right. Once you formulated your CV, test it! Bear in mind for anything less than 5,000 rows (empirically), big variations have to be expected (e.g. you might improve in CV, but not on public leaderboard).\\n\\n\\nDon\\'t overtune your models. For example, when you run logistic regression and you try to find the best value for regularization and your C becomes 1.02412353563, you are definitely overfitting! Do sizable increases/decreases of your hyperparameters. For example, try C=1.2, then C=1.4, then 1.6 and so on.\\n\\n\\nGet intuition from your hyper parameters. Try to see if changes of the hyperparameters yield meaningful results. Most of the times, the hyperparameters have a peak performance for a specific value. Let’s say the more you increase C in logistic regression the better the performance for your metric becomes. However that is only true until C reaches 2.0 (for example). After that, further increases in C decreases performance. If you don\\'t encounter something like that, or for some reason the best value of your hyperparameter has many peaks, this most of the times indicates a problem regarding your CV.\\n\\n\\nYou can always put more size in your validation. If there is too much volatility in your results, increase you validation size. Bagging helps a lot too to stabilize your results (at the cost of more time though).\\n\\nTrivia Corner\\n[simpleresponsiveslider]\\r\\n\\r\\nRead about Marios\\' experience on Kaggle and spot chasing retirement.', 'The Second National Data Science Bowl, a data science competition where the goal was to automatically determine cardiac volumes from MRI scans, has just ended. We participated with a team of 4 members from the Data Science lab at Ghent University in Belgium and finished 2nd!\\r\\n\\r\\nThe team kunsthart (artificial heart in English) consisted of Ira Korshunova, Jeroen Burms, Jonas Degrave (@317070), 3 PhD students, and professor Joni Dambre. It\\'s also a follow-up of last year\\'s team ≋ Deep Sea ≋, which finished in first place for the First National Data Science Bowl.\\r\\nOverview\\r\\nThis blog post is going to be long, here is a clickable overview of different sections.\\r\\n\\nIntroduction\\nPre-processing and data augmentation\\nNetwork architectures\\nTraining and ensembling\\nSoftware and hardware\\nConclusion\\n\\nIntroduction\\nThe problem\\r\\nThe goal of this year\\'s Data Science Bowl was to estimate minimum (end-systolic) and maximum (end-diastolic) volumes of the left ventricle from a set of MRI-images taken over one heartbeat. These volumes are used by practitioners to compute an ejection fraction: fraction of outbound blood pumped from the heart with each heartbeat. This measurement can predict a wide range of cardiac problems. For a skilled cardiologist analysis of MRI scans can take up to 20 minutes, therefore, making this process automatic is obviously useful.\\r\\n\\r\\n\\r\\n\\r\\nUnlike the previous Data Science Bowl, which had very clean and voluminous data set, this year\\'s competition required a lot more focus on dealing with inconsistencies in the way the very limited number of data points were gathered. As a result, most of our efforts went to trying out different ways to preprocess and combine the different data sources.\\r\\nThe data\\r\\nThe dataset consisted of over a thousand patients. For each patient, we were given a number of 30-frame MRI videos in the DICOM format, showing the heart during a single cardiac cycle (i.e. a single heartbeat). These videos were taken in different planes including the multiple short-axis views (SAX), a 2-chamber view (2Ch), and a 4-chamber view (4Ch). The SAX views, whose planes are perpendicular to the long axis of the left ventricle, form a series of slices that (ideally) cover the entire heart. The number of SAX slices ranged from 1 to 23. Typically, the region of interest (ROI) is only a small part of the entire image. Below you can find a few of SAX slices and Ch2, Ch4 views from one of the patients. Red circles on the SAX images indicate the ROI\\'s center (later we will explain how to find it), for Ch2 and Ch4 they specify the location of SAX slices projected on the corresponding view.\\r\\n\\n\\n\\nsax_5\\nsax_9\\nsax_10\\nsax_11\\nsax_12\\nsax_15\\n\\n\\n\\n\\n\\n\\n2Ch\\n4Ch\\n\\n\\n\\r\\nThe DICOM files also contained a bunch of metadata. Some of the metadata fields, like PixelSpacing and ImageOrientation were absolutely invaluable to us. The metadata also specified patient’s age and sex.\\r\\n\\r\\nFor each patient in the train set, two labels were provided: the systolic volume and the diastolic volume. From what we gathered (link), these were obtained by cardiologists by manually performing a segmentation on the SAX slices, and feeding these segmentations to a program that computes the minimal and maximal heart chamber volumes. The cardiologists didn’t use the 2Ch or 4Ch images to estimate the volumes, but for us they proved to be very useful.\\r\\n\\r\\nCombining these multiple data sources can be difficult, however for us dealing with inconsistencies in the data was more challenging. Some examples: the 4Ch slice not being provided for some patients, one patient with less than 30 frames per MRI video, couple of patients with only a handful of SAX slices, patients with SAX slices taken in weird locations and orientations.\\r\\nThe evaluation\\r\\nGiven a patient’s data, we were asked to output a cumulative distribution function over the volume, ranging from 0 to 599 mL, for both systole and diastole. The models were scored by a Continuous Ranked Probability Score (CRPS) error metric, which computes the average squared distance between the predicted CDF and a Heaviside step function representing the real volume.\\r\\n\\r\\nAn additional interesting novelty of this competition was the two stage process. In the first stage, we were given a training set of 500 patients with a public test set of 200 patients. In the final week we were required to submit our model and afterwards the organizers released the test data of 440 patients and labels for 200 patients from the public test set. We think the goal was to compensate for the small dataset and prevent people from optimizing against the test set through visual inspection of every part of their algorithm. Hand-labeling in the first stage was allowed on the training dataset only, for the second stage it was also allowed for 200 validation patients.\\r\\nThe solution: traditional image processing, convnets, and dealing with outliers\\r\\nIn our solution, we combined traditional image processing approaches, which find the region of interest (ROI) in each slice, with convolutional neural networks, which perform the mapping from the extracted image patches to the predicted volumes. Given the very limited number of training samples, we tried combat overfitting by restricting our models to combine the different data sources in predefined ways, as opposed to having them learn how to do the aggregation. Unlike many other contestants, we performed  no hand-labelling .\\r\\nPre-processing and data augmentation\\r\\nThe provided images have varying sizes and resolutions, and do not only show the heart, but the entire torso of the patient. Our preprocessing pipeline made the images ready to be fed to a convolutional network by going through the following steps:\\r\\n\\napplying a zoom factor such that all images have the same resolution in millimeters\\nfinding the region of interest and extracting a patch centered around it\\ndata augmentation\\ncontrast normalization\\n\\r\\nTo find the correct zooming factor, we made use of the PixelSpacing metadata field, which specifies the image resolution. Further we will explain our approach to ROI detection and data augmentation.\\r\\nDetecting the Region Of Interest through image segmentation techniques\\r\\nWe used classical computer vision techniques to find the left ventricle in the SAX slices. For each patient, the center and width of the ROI were determined by combining the information of all the SAX slices provided. The figure below shows an example of the result.\\r\\n\\r\\n[caption id=\"\" align=\"alignnone\" width=\"1665\"] ROI extraction steps[/caption]\\r\\n\\r\\nFirst, as was suggested in the Fourier based tutorial, we exploit the fact that each slice sequence captures one heartbeat and use Fourier analyses to extract an image that captures the maximal activity at the corresponding heartbeat frequency (same figure, second image).\\r\\n\\r\\nFrom these Fourier images, we then extracted the center of the left ventricle by combining the Hough circle transform with a custom kernel-based majority voting approach across all SAX slices. First, for each fourier image (resulting from a single sax slice), the $$N$$ highest scoring Hough circles for a range of radii were found, and from all of those, the $$M$$ highest scoring ones were retained. $$N$$, $$M$$ and the range of radii are metaparameters that severely affect the robustness of the ROI detected and were optimised manually. The third image in the figure shows an example of the $$M$$ best circles for one slice.\\r\\n\\r\\nFinally, a ‘likelihood surface’ (rightmost image in figure above) was obtained by combining the centers and scores of the selected circles for all slices. Each circle center was used as the center for a Gaussian kernel, which was scaled with the circle score, and all these kernels were added. The maximum across this surface was selected as the center of the ROI. The width and height of the bounding box of all circles with centers within a maximal distance (another hyperparameter) of the ROI center were used as bounds for the ROI or to create an ellipsoidal mask as shown in the figure.\\r\\n\\r\\nGiven these ROIs in the SAX slices, we were able to find the ROIs in the 2Ch and 4Ch slices by projecting the SAX ROI centers onto the 2Ch and 4Ch planes.\\r\\nData augmentation\\r\\nAs always when using convnets on a problem with few training examples, we used tons of data augmentation. Some special precautions were needed, since we had to preserve the surface area. In terms of affine transformations, this means that only skewing, rotation and translation was allowed. We also added zooming, but we had to correct our volume labels when doing so! This helped to make the distirbution of labels more diverse.\\r\\n\\r\\nAnother augmentation here came in the form of shifting the images over the time axis. While systole was often found in the beginning of a sequence, this was not always the case. Augmenting this, by rolling the image tensor over the time axis, made the resulting model more robust against this noise in the dataset, while providing even more augmentation of our data.\\r\\n\\r\\nData augmentation was applied during the training phase to increase the number of training examples. We also applied the augmentations during the testing phase, and averaged predictions across the augmented versions of the same data sample.\\r\\nNetwork architectures\\r\\nWe used convolutional neural networks to learn a mapping from the extracted image patches to systolic and diastolic volumes. During the competition, we played around a lot with both minor and major architectural changes. Our base architecture for most of our models was based on VGG-16.\\r\\n\\r\\nAs we already mentioned, we trained different models which can deal with different kinds of patients. There are roughly four different kinds of models we trained: single slice models, patient models, 2Ch models and 4Ch models.\\r\\nSingle slice models\\r\\nSingle slice models are models that take a single SAX slice as an input, and try to predict the systolic and diastolic volumes directly from it. The 30 frames were fed to the network as 30 different input channels. The systolic and diastolic networks shared the convolutional layers, but the dense layers were separated. The output of the network could be either a 600-way softmax (followed by a cumulative sum), or the mean and standard deviation of a Gaussian (followed by a layer computing the cdf of the Gaussian).\\r\\n\\r\\nAlthough these models obviously have too little information to make a decent volume estimation, they benefitted hugely from test-time augmentation (TTA). During TTA, the model gets slices with different augmentations, and the outputs are averaged across augmenations and slices for each patient. Although this way of aggregating over SAX slices is suboptimal, it proved to be very robust to the relative positioning of the SAX slices, and is as such applicable to all patients.\\r\\n\\r\\nOur single best single slice model achieved a local validation score of 0.0157 (after TTA), which was a reliable estimate for the public leaderboard score for these models. The approximate architecture of the slice models is shown on the following figure.\\r\\n\\n\\n2Ch and 4Ch models\\r\\nThese models have a much more global view on the left ventricle of the heart than single SAX slice models. The 2Ch models also have the advantage of being applicable to every patient. Not every patient had a 4Ch slice.\\r\\nWe used the same VGG-inspired architecture for these models. Individually, they achieved a similar validation score (0.0156) as was achieved by averaging over multiple sax slices. By ensembling only single slice, 2Ch and 4Ch models, we were able to achieve a score of 0.0131 on the public leaderboard.\\r\\nPatient models\\r\\nAs opposed to single slice models, patient models try to make predictions based on the entire stack of (up to 25) SAX slices. In our first approaches to these models, we tried to process each slice separately using a VGG-like single slice network, followed by feeding the results to an overarching RNN in an ordered fashion. However, these models tended to overfit badly. Our solution to this problem consists of a clever way to merge predictions from multiple slices. Instead of having the network learn how to compute the volume based on the results of the individual slices, we designed a layer which combines the areas of consecutive cross-sections of the heart using a truncated cone approximation.\\r\\n\\r\\nBasically, the slice models have to estimate the area $$A_i$$ (and standard deviation thereof) of the cross-section of the heart in a given slice $$i$$. For each pair of consecutive slices $$i$$ and $$i+1$$, we estimate the volume of the heart between them as $$V_i = \\\\frac{1}{3} * h_{i, i+1} * (A_i + \\\\sqrt{A_i A_{i+1}} + A_{i+1})$$, where $$h_{i, i+1}$$ is the distance between the slices. The total volume is then given by $$V = \\\\sum_i V_i$$.\\r\\n\\r\\nOrdering the SAX slices and finding the distance between them was achieved through looking at the SliceLocation metadata fields, but this field was not very reliable in finding the distance between slices, neither was the SliceThickness. We looked for the two slices that were furthest apart, drew a line between them, and projected every other slice onto this line. This way, we estimated the distance between two slices ourselves.\\r\\n\\r\\nOur best single model achieved a local validation score of 0.0105 using this approach. This was no longer a good leaderboard estimation, since our local validation set contained relatively few outliers compared to the public leaderboard in the first round. The model had the following architecture:\\r\\n\\r\\n\\n\\n\\n\\n\\nLayer Type\\nSize\\nOutput shape\\n\\n\\n\\n\\nInput layer\\n\\n(8, 25, 30, 64, 64)*\\n\\n\\nConvolution\\n128 filters of 3x3\\n(8, 25, 128, 64, 64)\\n\\n\\nConvolution\\n128 filters of 3x3\\n(8, 25, 128, 64, 64)\\n\\n\\nMax pooling\\n\\n(8, 25, 128, 32, 32)\\n\\n\\nConvolution\\n128 filters of 3x3\\n(8, 25, 128, 32, 32)\\n\\n\\nConvolution\\n128 filters of 3x3\\n(8, 25, 128, 32, 32)\\n\\n\\nMax pooling\\n\\n(8, 25, 128, 16, 16)\\n\\n\\nConvolution\\n256 filters of 3x3\\n(8, 25, 256, 16, 16)\\n\\n\\nConvolution\\n256 filters of 3x3\\n(8, 25, 256, 16, 16)\\n\\n\\nConvolution\\n256 filters of 3x3\\n(8, 25, 256, 16, 16)\\n\\n\\nMax pooling\\n\\n(8, 25, 256, 8, 8)\\n\\n\\nConvolution\\n512 filters of 3x3\\n(8, 25, 512, 8, 8)\\n\\n\\nConvolution\\n512 filters of 3x3\\n(8, 25, 512, 8, 8)\\n\\n\\nConvolution\\n512 filters of 3x3\\n(8, 25, 512, 8, 8)\\n\\n\\nMax pooling\\n\\n(8, 25, 512, 4, 4)\\n\\n\\nConvolution\\n512 filters of 3x3\\n(8, 25, 512, 4, 4)\\n\\n\\nConvolution\\n512 filters of 3x3\\n(8, 25, 512, 4, 4)\\n\\n\\nConvolution\\n512 filters of 3x3\\n(8, 25, 512, 4, 4)\\n\\n\\nMax pooling\\n\\n(8, 25, 512, 2, 2)\\n\\n\\nFully connected (S/D)\\n1024 units\\n(8, 25, 1024)\\n\\n\\nFully connected (S/D)\\n1024 units\\n(8, 25, 1024)\\n\\n\\nFully connected (S/D)\\n2 units (mu and sigma)\\n(8, 25, 2)\\n\\n\\nVolume estimation (S/D)\\n\\n(8, 2)\\n\\n\\nGaussian CDF (S/D)\\n\\n(8, 600)\\n\\n\\n\\r\\n* The first dimension is the batch size, i.e. the number of patients, the second dimension is the number of slices. If a patient had fewer slices, we padded the input and omitted the extra slices in the volume estimation.\\r\\n\\r\\nOftentimes, we did not train patient models from scratch. We found that initializing patient models with single slice models helps against overfitting, and severely reduces training time of the patient model.\\r\\n\\r\\nThe architecture we described above was one of the best for us. To diversify our models, some of the good things we tried include:\\r\\n\\nprocessing each frame separately, and taking the minimum and maximum at some point in the network to compute systole and diastole\\nsharing some of the dense layers between the systole and diastole networks as well\\nusing discs to approximate the volume, instead of truncated cones\\ncyclic rolling layers\\nleaky RELUs\\nmaxout units\\n\\r\\nOne downside of the patient model approach was that these models assume that SAX slices nicely range from one end of the heart to the other. This was trivially not true for patients with very few (< 5) slices, but it was harder to detect automatically for some other outlier cases as in figure below, where something is wrong with the images or the ROI algorithm fails.\\r\\n\\n\\n\\nsax_12\\nsax_15\\nsax_17\\nsax_36\\nsax_37\\nsax_41\\n\\n\\n\\n\\n\\n\\n2Ch\\n4Ch\\n\\n\\n\\nTraining and ensembling\\nError function. At the start of the competition, we experimented with various error functions, but we found optimising CRPS directly to work best.\\r\\n\\r\\nTraining algorithm. To train the parameters of our models, we used the Adam update rule (Kingma and Ba).\\r\\n\\r\\nInitialization. We initialised all filters and dense layers orthogonally (Saxe et al.). Biases were initialized to small positive values to have more gradients at the lower layer in the beginning of the optimization. At the Gaussian output layers, we initialized the biases for mu and sigma such that initial predictions of the untrained network would fall in a sensible range.\\r\\n\\r\\nRegularization. Since we had a low number of patients, we needed considerable regularization to prevent our models from overfitting. Our main approach was to augment the data and to add a considerable amount of dropout.\\r\\nValidation\\r\\nSince the trainset was already quite small, we kept the validation set small as well (83 patients). Despite this, our validation score remained pretty close to the leaderboard score. Also, in cases where it didn’t, it helped us identify issues in our models, namely problematic cases in the test set which were not represented in our validation set. We noticed for instance that quite some of our patient models had problems with patients with too few SAX slices (< 5).\\r\\nSelectively train and predict\\r\\nBy looking more closely at the validation scores, we observed that most of the accumulated error was obtained by wrongly predicting only a couple of such outlier cases. At some point, being able to handle only a handful of these meant the difference between a leaderboard score of 0.0148 and 0.0132!\\r\\n\\r\\nTo mitigate such issues, we set up our framework such that each individual model could choose not to train on or predict a certain patient. For instance, models on patients’ SAX slices could choose not to predict patients with too few SAX slices, models which use the 4Ch slice would not predict for patients who don’t have this slice. We extended this idea further by developing expert models, which only trained and predicted for patients with either a small or a big heart (as determined by the ROI detection step). Further down the pipeline, our ensembling scripts would then take these non-predictions into account.\\r\\nEnsembling and dealing with outliers\\r\\nWe ended up creating about 250 models throughout the competition. However, we knew that some of these models were not very robust to certain outliers or patients whose ROI we could not accurately detect. We came up with two different ensembling strategies that would deal with these kind of issues.\\r\\n\\r\\nOur first ensembling technique followed the following steps:\\r\\n\\nFor each patient, we select the best way to average over the test time augmentations. Slice models often preferred a geometric averaging of distributions, whereas in general arithmetic averaging worked better for patient models.\\nWe average over the models by calculating each prediction’s KL-divergence from the average distribution, and the cross entropy of each single sample of the distribution. This means that models which are further away from the average distribution get more weight (since they are more certain). It also means samples of the distribution closer to the median-value of 0.5 get more weight. Each model also receives a model-specific weight, which is determined by optimizing these weights over the validation set.\\nSince not all models predict all patients, it is possible for a model in the ensemble to not predict a certain patient. In this case, a new ensemble without these models is optimized, especially for this single patient. The method to do this is described in step 2.\\nThis ensemble is then used on every patient on the test-set. However, when a certain model’s average prediction disagrees too much with the average prediction of all models, the model is thrown out of the ensemble, and a new ensemble is optimized for this patient, as described in step 2. This meant that about ~75% of all patients received a new, ‘personalized’ ensemble.\\n\\r\\nOur second way of ensembling involves comparing an ensemble that is suboptimal, but robust to outliers, to an ensemble that is not robust to them. This approach is especially interesting, since it does not need a validation set to predict the test patients. It follows the following steps:\\r\\n\\nAgain, for each patient, we select the best way to average over the test time augmentations again.\\nWe combine the models by using a weighted average on the predictions, with the weights summing to one. These weights are determined by optimising them on the validation set. In case not all models provide a prediction for a certain patient, it is dropped for that patient and the weights of the other models are rescaled such that they again sum to one. This ensemble is not robust to outliers, since it contains patient models.\\nWe combine all 2Ch, 4Ch and slice models in a similar fashion. This ensemble is robust to outliers, but only contains less accurate models.\\nWe detect outliers by finding the patients where the two ensembles disagree the most. We measure disagreement using CRPS. If the CRPS exceeds a certain threshold for a patient, we assume it to be an outlier. We chose this threshold to be 0.02.\\nWe retrain the weights for the first ensemble, but omit the outliers from the validation set. We choose this ensemble to generate predictions for most of the patients, but choose the robust ensemble for the outliers.\\n\\r\\nFollowing this approach, we detected three outliers in the test set during phase one of the competition. Closer inspection revealed that for all of them either our ROI detection failed, or the SAX slices were not nicely distributed across the heart. Both ways of ensembling achieved similar scores on the public leaderboard. (0.0110)\\r\\nSecond round submissions\\r\\nFor the second round of the competition, we were allowed to retrain our models on the new labels (+ 200 patients). We were also allowed to plan two submissions. Of course, it was impossible to retrain all of our models during this single week. For this reason, we chose to only train our 44 best models, according to our ensembling scripts.\\r\\n\\r\\nFor our first submission, we splitted of a new validation set. The resulting models were combined using our first ensembling strategy.\\r\\n\\r\\nFor our second submission, we trained our models on the entire training set (i.e. there was no validation split). We assembled them using the second ensembling method. Since we had no validation set to optimise the weights of the ensemble, we computed the weights by training an ensemble on the models we trained with a validation split, and transferred them over.\\r\\nSoftware and hardware\\r\\nWe used Lasagne, Python, Numpy and Theano to implement our solution, in combination with the cuDNN library. We also used PyCUDA for a few custom kernels. We made use of scikit-image for pre-processing and augmentation.\\r\\n\\r\\nWe trained our models on the NVIDIA GPUs that we have in the lab, which include GTX TITAN X, GTX 980, GTX 680 and Tesla K40 cards. We would like to thank Frederick Godin and Elias Vansteenkiste for lending us a few extra GPUs in the last week of the competition.\\r\\nConclusion\\r\\nIn this competition, we tried out different ways to preprocess data and combine information from different data sources, and thus, we learned a lot in this aspect. However, we feel that there is still a room for improvement. For example, we observed that most of our error still hails from a select group of patients. These include the ones for which our ROI extraction fails. In hindsight, hand-labeling the training data and training a network to do the ROI extraction would be a better approach, but we wanted to sidestep doing a lot of this kind of manual effort as much as possible. In the end, labeling the data would probably have been less time intensive.\\r\\n\\r\\nThe code is now available on GitHub: https://github.com/317070/kaggle-heart\\n\\r\\n\\r\\nThis article was originally published on Ira Korshunova\\'s blog.\\r\\n\\r\\n\\xa0', 'The Allen Institute for Artificial Intelligence (AI2) competition ran on Kaggle from October 2015 to February 2016. 170 teams with 302 players competed to pass 8th grade science exams with flying colors. Alejandro Mosquera took third place in the competition using\\xa0a Logistic Regression \\xa03-class classification model over Information Retrieval, neural network embeddings, and heuristic/statistical corpus features. In this blog, Alejandro describes his approach and the surprising conclusion that sometimes simpler models outperform ensemble methods.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nMy PhD topic is related to social media text analysis but for the last 4 years I have been working in cyber-security analytics.\\r\\n\\r\\n[caption id=\"attachment_5700\" align=\"aligncenter\" width=\"300\"] Alejandro on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nEven though I have an NLP background I haven\\'t had much exposure to QA tasks before. I think that being familiar with the right tools is always an advantage, especially when you deal with difficult problems like this one.\\r\\nHow did you get started competing on Kaggle?\\r\\nEverything started with the Microsoft Malware Classification Challenge. I had quite a lot of domain knowledge so I\\'ve managed to stay in the top-20 for most of the competition with a simple Random Forest and around 25 features. However, at that time I wasn’t very familiar with Kaggle mechanics nor some of the usual methods that make you improve your score when you run out of ideas for new features, (e.g. stacking, blending…etc) and I ended up dropping quite a lot of positions in the private LB. Good learning experience though.\\r\\nWhat made you decide to enter this competition?\\r\\nI\\'ve missed most of the previous NLP-related competitions in Kaggle so I thought that was a good idea to participate in this one.\\r\\nLet\\'s Get Technical\\nOverview\\r\\nThe solution is based on a Logistic Regression (LR) 3-class classification model over Information Retrieval (IR), neural network embeddings (W2V) and heuristic/statistical corpus features. The rationale behind using 3 classes (Answer A, Answer B, Other) instead of 4 (one per answer) was to augment the training dataset and deal better with low confidence predictions.\\r\\nFeature Selection / Extraction\\nWhat were the most important features?\\r\\nI used 22 features in total (11 for each Question/Answer pair and then pairwise combinations between these for each question):\\r\\n\\nES_raw_lemma: IR scores by using ES and raw/lemmatized KB.\\nES_lemma_regex: Regex scoring (number of characters matched) after IR results.\\nW2V_COSQA: Cosine similarity between question and answer embeddings.\\nCAT_A1: Is multiphrase\\xa0question + short response?\\nCAT_A2: Is fill the _______ + no direct question + long response?\\nCAT_A3: Is multiphrase question + long response?\\nANS_all_above: Is \"all of the above\" answer?\\nANS_none_above: Is \"none of the above\" answer?\\nANS_both: Is \"both X and Y\" answer?\\nES_raw_lemmaLMJM: IR scores by using ES and raw/lemmatized KB with LMJM scoring.\\nES_lemma_regexLMJM: Regex scoring (number of characters matched) after IR results using LMJM.\\n\\r\\nThe scaled LR coefficients show that both corpus heuristics and embedding-based features have the highest weight, followed by combined IR scores.\\r\\n\\r\\n\\nHow did you select features?\\r\\nThe best features were selected by using cross-validation and also manual analysis of the training data.\\r\\nDid you make any important feature transformations?\\r\\nMy data pipeline was the following:\\r\\n\\nNoise reduction (question boilerplate removal)\\nStopword removal (standard English NLTK stopwords)\\nLemmatization (NLTK Wordnet Lemmatizer)\\n\\n\\nDid you find any interesting interactions between features?\\r\\nI\\'ve found a huge performance increase when using a 3-class classification approach, rather than using a 2-class or 4-class one. This third class was used to distribute the probabilities over the remaining two questions.\\r\\nDid you use external data?\\r\\nThe KB was indexed in ES and was automatically generated by querying the Quizlet API for a fixed set of science topic keywords. I\\'ve also added the CK-12 workbooks published in the forum. This was a one-off process so no information from the training or testing set was used in order to build the KB. The machine learning model is fully standalone and does not require any additional external data in order to generate predictions.\\r\\n\\r\\nI\\'ve also used the following multi-state question/answer datasets in order to augment the original training data:\\r\\n\\nRegents 4 grade QA dataset (from Aristo website)\\nMulti-State QA dataset (from Aristo website)\\n\\nTraining Methods\\nWhat training methods did you use?\\nUnsupervised: Gensim\\'s Word2Vec in order to extract embeddings from the KB.\\r\\n\\r\\nSupervised: 3-class\\xa0\\xa0Logistic Regression over IR, W2V and heuristic/statistical corpus features. Local CV scores (5-fold) were around 0.59-0.61 depending on the feature set.\\r\\nDid you ensemble the models?\\r\\nI\\'ve tried ensembling XGBoost and RNN models but nothing could beat the simpler linear model.\\r\\nInteresting findings\\nWhat was the most important trick you used?\\r\\nI jumped to the first position after treating the QA challenge as a question-answer pair ranking problem. I\\'ve also expanded the Word2Vec model vocabulary by using fuzzy string matching. It helped me to improve the score during the first stage of the competition.\\r\\nWhat do you think set you apart from others in the competition?\\r\\nI think my knowledge base was quite good in terms of coverage. I\\'ve found Wikipedia IR models to be broader but also noisier.\\r\\nWas there anything you tried that didn\\'t improve your score?\\r\\nI couldn\\'t get anything useful out of state-of-the-art RNNs or CNNs. I\\'ve also tried to use resources such as ConceptNet or WordNet in order to augment my KB or improve the quality of the embeddings. However, none of these experiments gave me any substantial improvement.\\r\\nWhat was your most important insight into the data?\\r\\nI\\'ve found that questions with short answers (less than 4 words) were positively sensitive to n-gram matching (more than 0.7 accuracy by using IR only) while the longer ones required combined knowledge (inference) to be solved correctly.\\r\\n\\r\\n\\n\\r\\n\\r\\n[caption id=\"attachment_5704\" align=\"aligncenter\" width=\"650\"] Distribution of Q&A categories in train and validation sets[/caption]\\r\\nWere you surprised by any of your findings?\\r\\nI was expecting RNNs to perform better, especially taking into account the state-of-the-art in QA. During my experiments I couldn\\'t make them outperform the simpler linear model.\\r\\nWhich tools did you use?\\r\\nMy tools of choice were Elasticsearch 2.0, Gensim and the Sklearn/Numpy stack.\\r\\nHow did you spend your time on this competition?\\r\\nI spent the first weeks of the competition trying to build a relevant knowledge base and understanding how the features from the questions and the answers could be combined in the most optimal way. The last two weeks were mostly dedicated to model tuning and feature selection.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nThe slowest part is the knowledge base generation and indexing in ElasticSearch. That takes around a couple of hours in a normal computer. Feature generation with the initial data (including the Word2Vec embeddings) needs another two hours for each dataset.\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\r\\nThere was the possibility to submit only one model in this competition, I had two and I chose the one with higher CV score rather than the one with higher LB score (which turned out not being the best performing with the second-stage dataset). Cross-validation is helpful but not always conclusive.\\r\\nJust for fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nI think that anything related to reinforcement learning would be interesting, like for example building a model able to play a video game or solve a computer-related task.\\r\\nBio\\nAlejandro Mosquera is a Sr. Principal Research Engineer at Symantec. When he is not taking down phishing and malware campaigns he works towards finishing his PhD in Human Language Technologies at the University of Alicante (Spain).', 'The Yelp Restaurant Photo Classification\\xa0recruitment competition ran on Kaggle from December 2015 to April 2016. 355 Kagglers accepted Yelp\\'s challenge to predict multiple attribute labels for restaurants based on user-submitted photos. Dmitrii Tsybulevskii took the cake by finishing in 1st place with his winning solution. In this blog, Dmitrii dishes the details of his approach including how he tackled the multi-label and multi-instance aspects of this problem which made this competition a unique challenge.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI hold a degree in Applied Mathematics, and I\\'m currently working as a software engineer on computer vision, information retrieval and machine learning projects.\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Dmitrii on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nYes, since I work as a computer vision engineer, I have image classification experience, deep learning knowledge, and so on.\\r\\nHow did you get started competing on Kaggle?\\r\\nAt first I came to Kaggle through the MNIST competition, because I\\'ve had interest in image classification and then I was attracted to other kinds of ML problems and data science just blew up my mind.\\r\\nWhat made you decide to enter this competition?\\r\\nThere are several reasons behind it:\\r\\n\\nI like competitions with raw data, without any anonymized features, and where you can apply a lot of feature engineering.\\nQuite large dataset with a rare type of problem (multi-label, multi-instance). It was a good reason to get new knowledge.\\n\\nLet\\'s get technical:\\nWhat preprocessing and supervised learning methods did you use?\\r\\nOutline of my approach depicted below:\\r\\n\\r\\n\\nPhoto-level feature extraction\\r\\nOne of the most important things you need for training deep neural networks is a clean dataset. So, after viewing the data, I decided not to train a neural network from scratch and not to do fine-tuning. I\\'ve tried several state-of-the-art neural networks and several layers from which features were obtained. Best performing (in decreasing order) nets were:\\r\\n\\nFull ImageNet trained Inception-BN\\nInception-V3\\nResNet\\n\\r\\nThe best features were obtained from the antepenultimate layer, because the last layer of pretrained nets are too \"overfitted\" to the ImageNet classes, and more low-level features can give you a better result. But in this case, dimensions of the features are much higher (50176 for the antepenultimate layer of \"Full ImageNet trained Inception-BN\"), so I used PCA compression with ARPACK solver, in order to find only few principal components.\\xa0In most cases feature normalization was used.\\r\\n\\r\\n\\nHow did you deal with the multi-instance aspect of this problem?\\r\\nIn this problem we only needed in the bag-level predictions, which makes it much simpler compared to the instance-level multi-instance learning. I used a paradigm which is called \"Embedded Space\", according to the paper: Multiple Instance Classification: review, taxonomy and comparative study. In the Embedded Space paradigm, each bag X is mapped to a single feature vector which summarizes the relevant information about the whole bag X. After this transform you can use ordinary supervised classification methods.\\r\\n\\r\\nFor the business-level (bag-level) feature extraction I used:\\r\\n\\nAveraging of photo-level features\\r\\nSimple, but very efficient in the case of outputs of neural networks.\\nFisher Vectors\\r\\nFisher Vector was the best performing image classification method before \"Advent\" of deep learning in 2012. Usually FV was used as a global image descriptor obtained from a set of local image features (e.g. SIFT), but in this competition I used them as an aggregation of the set of photo-level features into the business-level feature. With Fisher Vectors you can take into account multi-instance nature of the problem.\\nVLAD descriptor\\r\\nVery similar to Fisher Vectors.\\n\\r\\nAfter some experimentation, I ended up with a set of the following business-level features:\\r\\n\\nAveraging of L2 normalized features obtained from the penultimate layer of [Full ImageNet Inception-BN]\\nAveraging of L2 normalized features obtained from the penultimate layer of [Inception-V3]\\nAveraging of PCA projected features (from 50716 to 2048) obtained from the antepenultimate layer of [Full ImageNet Inception-BN]\\nL2 normalized concatenation of 2., 3.\\nPCA projected 4. to 128 components.\\nFisher Vectors over PCA projected 3. to 64 components.\\nVLAD over PCA projected 3. to 64 components.\\n\\nHow did you deal with the multi-label aspect of this problem?\\r\\nI used Binary Relevance (BR) and Ensemble of Classifier Chains (ECC) with binary classification methods in order to handle the multi-label aspect of the problem. But my best performing single model was the multi-output neural network with the following simple structure:\\r\\n\\r\\n\\r\\n\\r\\nThis network shares weights for the different label learning tasks, and performs better than several BR or ECC neural networks with binary outputs, because it takes into account the multi-label aspect of the problem.\\r\\nClassification\\n\\r\\n\\r\\nNeural network has much higher weight(6) compared to the LR(1) and XGB(1) at the weighing stage. After all, 0, 1 labels were obtained with a simple thresholding, and for all labels a threshold value was the same.\\r\\nWhat didn\\'t work for you?\\n\\nLabel powerset for multi-label classification\\nRAkEL for multi-label classification\\nVariants of ML-KNN for multi-label classification\\nRemoving duplicate photos\\nXGBoost. I added some XGBoost models to the ensemble just out of respect to this great tool, although local CV score was lower.\\nMISVM for multi-instance classification\\nMore image crops in the feature extractor\\nStacking. It\\'s pretty easy to overfit with a such small dataset, which has only 2000 samples.\\n\\nWere you surprised by any of your findings?\\n\\nFeatures extracted from the Inception-V3 had a better performance compared to the ResNet features. Not always better error rates on ImageNet led to the better performance in other tasks.\\nSimple Logistic Regression outperforms almost all of the widely used models such as Random Forest, GBDT, SVM.\\nBinary Relevance is a very good basiline for the multi-label classification.\\n\\nWhich tools did you use?\\nMXNet, scikit-learn, Torch, VLFeat, OpenCV, XGBoost, Caffe\\nHow did you spend your time on this competition?\\r\\n50% feature engineering, 50% machine learning\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nFeature extraction: 10 hours\\r\\nModel training: 2-3 hours\\r\\nWords of wisdom:\\nWhat have you taken away from this competition?\\r\\nA \"Prize Winner\" badge and a lot of Kaggle points.\\r\\nDo you have any advice for those just getting started in data science?\\r\\nKaggle is a great platform for getting new knowledge.\\r\\nJust for fun:\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\nI\\'d like to see reinforcement learning or some kind of unsupervised learning problems on Kaggle.\\r\\nBio\\nDmitrii Tsybulevskii is a Software Engineer at a photo stock agency. He holds a degree in Applied Mathematics, and mainly focuses on machine learning, information retrieval and computer vision.', 'The Homesite Quote Conversion competition challenged Kagglers to predict the customers most likely to purchase a quote for home insurance based on an anonymized database of information on customer and sales activity. 1925 players on 1764 teams competed for a spot at the top and team Frenchies found themselves in the money with their special blend of 600 base models. Nicolas, Florian, and Pierre describe how the already highly separable classes challenged them to work collaboratively to eke out improvements in performance through feature engineering, effective cross validation, and ensembling.\\r\\nFrenchies\\r\\nThe team started off with Pierre and Florian as they are longtime friends. Nicolas asked to join later in the competition and it was one of the best decisions of this challenge! All of us were finalists in the “Cdiscount.com” competition hosted on datascience.net, the “French Kaggle”. It was a real pleasure for all of us to work as French guys and to demonstrate our skill on an international contest.\\r\\nNicolas Gaude\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Nicolas on Kaggle[/caption]\\r\\n\\r\\nWorking for Bouygues Telecom, a French telecom operator with 15M subscribers, I’m heading its data-science team with a focus on production efficiency and scalability. With a 10 years background in embedded software development, I moved to the big data domain 3 years ago and fell in love with machine learning. Kaggle is for me a unique opportunity to sharpen my skills and to compete with other data scientists around the world. And honestly Kaggle is the only place where a 0.0001% improvement matters so much that you can go for 100’s models ensemble to get to the top, and that’s a lot of fun.\\r\\nFlorian Laroumagne\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Florian on Kaggle[/caption]\\r\\n\\r\\nCurrently working as a BI Analyst at EDF (the major French and worldwide electricity provider) I graduated from the ENSIIE, a top French maths & IT engineering school. I had some statistical and machine learning courses however I had no opportunity in my professional life to apply it. To improve my skills, I followed some MOOC (on “france-universite-numerique” and on “Coursera”) about statistics with R, big data and machine learning. After having acquired theoretical lessons, I wanted to put them into practice. This is how I ended up on Kaggle.\\r\\nPierre Nowak\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Pierre on Kaggle[/caption]\\r\\n\\r\\nI graduated from ENSIIE & Université d’Evry Val d’Essonne with a double degree in Financial Mathematics. My interest in machine learning came with my participation in a text mining challenge hosted by datascience.net. I have been working for 7 months at EDF R&D first on text mining problems and recently changed to forecasting daily electricity load curves. Despite the fact many people say Kaggle is brute force only, I find it to be the place to learn brand new algorithms and techniques. I especially had the opportunity to learn Deep Learning with Keras and next level blending thanks to Nicolas and some public posts from Gilberto and the Mad Professors.\\r\\nBackground\\r\\nNone of us had prior background on the business of Homesite since we do not work in the same field. However, we weren’t hurt by this. We think the fact the data was anonymized brought most of the competitors approximately to the same level.\\r\\n\\r\\nAbout the technologies used, there are two schools inside our team. Nicolas was pro-efficient in python while Florian was more R focused. Pierre was quite polyvalent and was the glue between the 2 worlds.\\r\\nThe solution\\nFeature engineering\\r\\nWe have to admit that feature engineering wasn’t very easy for us. Sure, we tried some differences between features which can then be selected (or not) via a feature selection process but at the end, we had only the basic dataset with a few engineered features. The kept ones were:\\r\\n\\nCount of 0, 1 and N/A row-wise\\nPCA top component features\\nTSNE 2D\\nCluster ID generated with k-means\\nSome differences among features (especially the “golden” features found in a public script)\\n\\r\\nThis challenge was really fun because even at the beginning of the competition, the AUC was really high (around 97% already). As we can see, the two classes are in fact quite easily separable:\\r\\n\\r\\n[caption id=\"attachment_5744\" align=\"aligncenter\" width=\"1024\"] Dataset plotted against the top 2 components of PCA (negative in green, positive in red)[/caption]\\r\\n\\r\\n[caption id=\"attachment_5747\" align=\"aligncenter\" width=\"1024\"] Dataset plotted against the top 2 dimensions of TSNE (negative in green, positive in red)[/caption]\\r\\nDataset modeling\\r\\nAs other teams, we encoded categorical features. Most of the time, it was done using a very common “label encoder”: all features where replaced with an ID. Despite the simplicity of this method, it works quite well for tree-based classifiers. However for linear ones it’s not recommended, that’s why we also generated “one hot encoded” features. Finally we also tried target encoding in order to find a ratio of the categorical features related to the target. It didn’t improve our score a lot but was worth having in our blend.\\r\\n\\r\\nNow that we have different versions of the dataset, we also split it. We used a full version (all features, all rows) for the majority of our classifiers but we also trained weaker models based on a subset of columns. For example we trained a model on the “personal” columns only, another one on the “geographical” columns only and so on.\\r\\nTraining & Ensembling\\r\\nWith all the different versions of the dataset, we were able to train them using well known and well performing machine learning models, such as:\\r\\n\\nLogistic Regression\\nRegularized Greedy Forest\\nNeural Networks\\nExtra Trees\\nXGBoost\\nH2O Random Forest (just 1 or 2 models into our first stage: not really important)\\n\\r\\nOur base level consists of around 600 models. 100 were built by “hand” with different features and hyperparameters of all of the above technologies. Then, to add some diversity we built a robot creating the 500 remaining models. This robot automatically trained models with XGBoost, Logistic Regression and Neural Networks, all based on randomly chosen features.\\r\\n\\r\\nAll our models were built on a 5 fold stratified CV. It allowed us to have a local way to check our improvement and to avoid overfitting the leaderboard. Furthermore, with the CV we were able to use an ensemble method.\\r\\n\\r\\nExample of the diversity between two models, despite the fact they are highly correlated:\\r\\n\\r\\n[caption id=\"attachment_5746\" align=\"aligncenter\" width=\"484\"] Predictions of an RGF model plotted against predictions of a XGB model[/caption]\\r\\n\\r\\nTo blend our 600 models, we tried different ways. After some failures, we retained 3 well performing blenders: a classical Logistic Regression, an XGBoost and (a bag) of Neural Networks. Those three blends naturally outperformed our best single model and were able to capture the information in different manners. Then, we transformed our predictions into ranks and we simply averaged the ranks of the 3 blends to have our final submission.\\r\\n\\r\\nHere is a sketch that sums up this multi-level stacking:\\r\\n\\r\\n\\nWords of wisdom\\r\\nHere is a short list of what we learnt and / or what worked for us:\\r\\n\\nRead forums, there are lots of useful insights\\nUse the best script as a benchmark\\nDon’t be afraid to generate lot of models and keep all the data created this way. You could still select them later in order to blend them… Or let the blender take the decision for you :)\\nValidate the behavior of your CV. If you have a huge jump in local score that doesn’t reflect on the leaderboard there is something wrong\\nGrid search in order to find hyperparameters works fine\\nBlending is a powerful tool. Please read the following post if you haven\\'t have already: http://mlwave.com/kaggle-ensembling-guide/\\nBe aware of\\xa0the standard deviation when blending. Increasing the metric is good but not that much if the SD increases\\nNeural nets are capricious :( bag them if necessary\\nMerging is totally fine and helps each teammates to learn from others\\nIf you are a team, use collaborative tools: Skype, Slack, svn, git...\\n\\nRecommendations to those just starting out\\r\\nAs written just above, read the forums. There are nice tips, starter codes or even links to great readings. Don’t hesitate to download a dataset and test lot of things on it. Even if most of the tested methods fail or give poor results, you will acquire some knowledge about what is working and what is not.\\r\\n\\r\\nMerge! Seriously, learning from others is what makes you stronger. We all have different insights, backgrounds or techniques which can be beneficial for your teammates. Last but not least, do not hesitate to discuss your ideas. This way you can find some golden thoughts that can push you to the top!', 'The Yelp Restaurant Photo Classification competition challenged Kagglers to assign attribute labels to restaurants based on a collection of user-submitted photos. In this recruitment competition, 355 players tackled the unique multi-instance and multi-label problem and in this blog the 2nd place winner describes his strategy. His advice to aspiring data scientists is clear: just do it and you will improve. Read on to find\\xa0out how Thuyen Ngo dodged overfitting with his solution and why it doesn\\'t take an expert in computer vision to work with image data.\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI am a PhD student in Electrical and Computer Engineering at UC Santa Barbara. I am doing research in human vision and computer vision. In a nutshell I try to understand how humans explore the scene and apply that knowledge to computer vision systems.\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Thuyen (AKA Plankton) on Kaggle[/caption]\\r\\nHow did you get started competing on Kaggle?\\r\\nMy labmate introduced Kaggle to me about a year ago and I participated in several competitions since then.\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nAll of my projects involved images. So It\\'s fair to say that I have some \"domain knowledge\". However, for this competition I think knowledge from image processing is not as important as machine learning. Since everyone uses similar image features (from pre-trained convolutional neural networks, which have been shown to contain good global descriptions of images and therefore are very suitable to our problem), the difficult part is to choose a learning framework that can combine information from different instances in an effective way.\\r\\nWhat made you decide to enter this competition?\\r\\nI am interested in image data in general (even though in the end there was not much image analysis involved). The problem is very interesting itself since there\\'s no black-box solution that can be applied directly.\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nLike most participants, I used pre-trained convolutional networks to extract image features, I didn\\'t do any other preprocessing or network fine-tuning. I started with the inception-v3 network from google but ended up using the pre-trained resnet-152 provided by Facebook.\\r\\n\\r\\nMy approach is super simple. It\\'s just a neural network.\\r\\nHow did you deal with the multi-instance and multi-label aspect of this problem?\\r\\nI used multilayer perceptron since it gave me the flexibility to handle both the multiple label and multiple instance at the same time. For multiple label, I simply used 9 sigmoid units and for multiple instance, I employed something like the attention mechanism in the neural network literature. The idea is to let the network learn by itself how to combine information from many instances (which instance to look at).\\r\\n\\r\\nEach business is represented by a matrix of size N x 2048, where N is the number of images for that business. The network is composed of four fully connected (FC) layers. Attention model is a normal FC layer but the activation is a softmax over images, weighting the importance of each image for a particular feature. I experimented with many different architectures, in the end, the typical architecture for the final submission is as follows:\\r\\n\\r\\n\\r\\n\\r\\nThe model is trained using the business-level labels (each business is a training example) as opposed to image-level labels (like many others). I used the standard cross entropy as the loss function. The training is done with Nesterov\\'s accelerated SGD.\\r\\nWhat was your most important insight into the data?\\r\\nFinding the reliable local validation is quite challenging to me. It\\'s a multiple label problem, and thus there\\'s no standard stratified split. I tried some greedy methods to do stratified 5-fold split but it didn\\'t perform very well. At the end I resorted to a random 5-fold split. My submission is normally the average of 5 models from 5-fold validation.\\r\\n\\r\\nAnother problem is that we only have 2000 businesses for training and another 2000 test cases. Even though it sounds a lot of data, training signals (the labels) are not that many. In combination with the instability of F measure, it makes the validation even more difficult.\\r\\n\\r\\nSince the evaluation metric is F1 score, it is reasonable to use F-measure as the loss function, but somehow I couldn\\'t make it work as well as the cross entropy loss.\\r\\n\\r\\nWith limited labeled data, my approach would have badly overfitted the data (it has more than 2M parameters). I used dropout for almost all layers, applied L2 regularization and early stopping to mitigate overfitting.\\r\\nHow did you spend your time on this competition?\\r\\nMost of the time for machine learning (training), 1% for preprocessing I guess.\\r\\nWhich tools did you use?\\r\\nI used Tensorflow/torch for feature extraction (with the provided code from Google/Facebook) and Lasagne (Theano) for my training.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nIt takes about two and a half hours to train one model and about a minute to make the predictions for all test images.\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\r\\nNeural networks can do any (weird) thing :)\\r\\nDo you have any advice for those just getting started in data science?\\r\\nJust do it, join Kaggle, participate and you will improve.\\r\\nBio\\nThuyen Ngo is a PhD student in Electrical and Computer Engineering at the University of California, Santa Barbara.', 'The annual March Machine Learning Mania competition sponsored by SAP challenged Kagglers to predict the outcomes of every possible match-up in the 2016 men\\'s NCAA basketball tournament. Nearly 600 teams competed, but only the first place forecasts were robust enough against upsets to top this year\\'s bracket. In this blog post, Miguel Alomar describes how calculating the offensive and defensive efficiency played into his winning strategy.\\r\\n\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\r\\nI earned a Master’s Degree in Computer Science from UIB in Mallorca, Spain. For nearly 20 years, I have been involved in software development, business intelligence and data warehousing. Recently, I have developed an interest in analytics and forecasting.\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Miguel (AKA Mallorqui) on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\nIn Spain, I played amateur basketball for 10 years. I like to think that is the reason I won.\\r\\n\\r\\nThe truth is I missed most of the basketball games this season and did not have a good feel for the any of the team’s quality. That most likely helped me because if I had seen more games, my judgment may have changed some of the forecasts. Normally, I am pretty bad at picking winners.\\r\\nHow did you get started competing on Kaggle?\\r\\nI found Kaggle through some data science lessons I was taking on Coursera.\\r\\nWhat made you decide to enter this competition?\\r\\nI really like analytics and sports so I thought it was a perfect competition for me.\\r\\n\\r\\nBut the key factor is that moderators and other members make it easy to enter, they provide lots of help, data, advice and feedback. Data is already formatted and prepared so the data gathering and manipulating task is made very easy. Some members of the community seemed more interested in sharing and discovering new methods and insights than in winning the competition.\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nI used logarithmic regression and random forests. I did try ADA Boost but didn’t get very good results so I didn’t use it in my final model.\\r\\nWhat was your most important insight into the data?\\r\\nThe data behind this competition is very simple, the box stats from basketball games are very simple to understand. The key factor for me was the offensive and defensive efficiency, how to calculate those? What weight to give to strength of schedule? Can you \"penalize\" a team because they haven’t played against the best teams in the nation? Can you lower their rating for something that didn’t happen?\\r\\n\\r\\nThose are the kind of questions I was trying to answer, I developed several models with different degrees of adjusted efficiency ratings and checked their scores against past seasons.\\r\\n\\r\\nSince my scores in Stage1 of the competition were not very good, I kept changing my model after Stage1 was closed.\\r\\n\\r\\nMy goal for next year is to formally test those different models to find out if there is any validity to my ideas.\\r\\n\\r\\n\\nWere you surprised by any of your findings?\\r\\nAfter building the submission files, I put them into brackets using a script provided by one of the Kaggle members. My first model had a more conservative look to it and my second model (the final winner) just didn’t look right to me. Teams like SF Austin, Indiana and Gonzaga were predicted to go very far in the bracket. I almost scrapped it but since it was my 2nd model I decided to go with it. This model got most of the first round upsets right, that surprised me.\\r\\n\\r\\n[caption id=\"attachment_5765\" align=\"aligncenter\" width=\"1024\"] Click to expand.[/caption]\\r\\n\\r\\nWhich tools did you use?\\r\\nI used R, R studio and SQL.\\r\\nHow did you spend your time on this competition?\\r\\nI would say my time allocation was 35% reading forums and blogs, 15% manipulating data, 25% building models and 25% evaluating results.\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nFive minutes. I trained my model using only 2016 data, so the amount of data to process is very small.\\r\\nBio\\nMiguel Alomar has a Master’s Degree in Computer Science from UIB in Mallorca, Spain. For nearly 20 years, he has been involved in software development, business intelligence and data warehousing.', 'The BNP Paribas Claims Management competition ran on Kaggle from February to April 2016. Just under 3000 teams made up of over 3000 Kagglers competed to predict insurance claims categories based on data collected during the claim filing process. The anonymized dataset challenged competitors to dig deeply into data understanding and feature engineering and the keen approach taken by Team Dexter\\'s Lab claimed first place.\\r\\n\\r\\nThe basics\\nWhat was your background prior to entering this challenge?\\nDarius: BSc and MSc in Econometrics at Vilnius University (Lithuania). Currently work as an analyst at a local credit bureau, Creditinfo. My work mainly involves analyzing and making predictive models with business and consumer credit data for financial sector companies.\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Darius (AKA raddar) on Kaggle[/caption]\\r\\n\\r\\nDavut: BSc and MSc in Computer Engineering and Electronics Engineering (Double Major), and currently PhD student in Computer Engineering at Istanbul Technical University (Turkey). I work as a back-end service software developer at company which provides stock exchange data to users. My work is not related to any data science subjects but I work on Kaggle when I get any spare time. I live in Istanbul - traffic is a headache here - I spend almost 4-5 hours in traffic and during my commute I code for Kaggle :)\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Davut on Kaggle[/caption]\\r\\n\\r\\nSong: Two masters (Geological Engineering and Applied Statistics). Currently I am working in an insurance company. My work is mainly building models - pricing models for insurance products, fraud detection, etc.\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Song (AKA onthehilluu) on Kaggle[/caption]\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\nDavut: Song has lots of experience in the insurance field and Darius in the finance field. At first, we were stuck with mediocre results for 2-3 weeks until Darius came up with a great idea which we then had nice discussions about. Both perspectives helped us improve our score and lead us to the victory.\\r\\nHow did you get started competing on Kaggle?\\nDarius: I\\'ve heard of Kaggle few years ago, but just recently started Kaggling. I was looking for challenges and working with different types of data. Surprisingly, my data insights and feature engineering was good enough to claim prize money in my very first serious competition. Kaggle has become my favorite hobby since.\\r\\n\\r\\nDavut: Three years ago, I took a course during my Master\\'s degree when a professor gave us a term project from Kaggle (Adzuna Job Salary Prediction). I did not participate then but 6 months passed and I started to Kaggle. The Higgs Boson Machine Learning Challenge was my first serious competition and since then I\\'ve participated in more competitions and met great data scientists and friends.\\r\\n\\r\\nSong: I have been studying machine learning by myself. Kaggle is an excellent site to learn by doing and learn from each other.\\r\\nWhat made you decide to enter this competition?\\nDarius: I like working with anonymous data and I thought that I had an edge over the competition as I had discovered interesting insights in previous competitions as well. And Davut wanted to team up in a previous competition, so we joined forces early in the competition.\\r\\n\\r\\nDavut: Kaggle became kind of an addiction to me like many others :) After Prudential, I wanted to participate in one more.\\r\\n\\r\\nSong: Nothing special.\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\nDarius: The most important part was setting a stratified 10-fold CV scheme early on. For most of the competition, a single XGBoost was my benchmark model (in the end, the single model would have scored 4th place). In the last 2 weeks, I made a few diverse models such as rgf, lasso, elastic net, and SVM.\\r\\n\\r\\nDavut: We got different feature sets, and trained various diverse models on those such as knn, extra tree classifiers, random forest, and neural networks. We also tried different objectives in XGBoost.\\r\\n\\r\\nSong: In our final model, we had XGBoost as an ensemble model, which included 20 XGBoost models, 5 random forests, 6 randomized decision tree models, 3 regularized greedy forests, 3 logistic regression models, 5 ANN models, 3 elastic net models and 1 SVM model.\\r\\nWhat was your most important insight into the data?\\nDarius: The most important insight was understanding what kind of data we were given. It is hard to make assumptions about anonymous data, but I dedicated 3 weeks of competition time for data exploration, which paid its dividends.\\r\\n\\r\\nFirst, as every feature in the given dataset was scaled and had some random noise introduced, I figured that identifying how to deal with noise and un-scaling the data could be important. I thought of a simple but fast method to detect the scaling factor for integer type features. It took some time, but in the end it was a crucial part of our winning solution.\\r\\n\\r\\nSecond, given our assumptions about variable meanings, we built efficient feature interactions. We devised, among other ideas, a lag and lead feature based on our impression that we were dealing with panel data. In the end, our assumptions about panel data and variable meaning were not realistic (indeed it would mean that the same client could face hundreds or thousands of claims). However, our lag and lead features did bring significant value to our solution, which is certainly because it was an efficient way to encode interactions. This is consistent with the other top two teams\\' solutions, which also benefited from encoding some interactions between v22 and other variables with different methods aside from lag and lead. In our opinion, there is certainly very interesting business insight for the host in these features.\\r\\n\\r\\nWere you surprised by any of your findings?\\nDarius: To my surprise, our approach was not overfitting at all. Other than that, I believed in our assumptions (be they correct or not) and we figured that other teams were just doing approximations of our findings - which other top teams admitted.\\r\\n\\r\\nHow did you spend your time on this competition?\\r\\nSome Kagglers start to train models in first place and keep doing that till the end and only focus on ensembling. But we focused on how to improve a single model with new features. We spent 45 days on feature engineering, then rest of the time for model training and stacking.\\r\\n\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\nOur single best model only takes less than an hour to train on an 8-12 core machine. However, the ensemble itself takes several days to finish.\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\nDarius: I tried new XGBoost parameters, which I have not tried before which also proved to be helpful in this competition. Also created my own R wrapper for rgf. Also got noticed by top Kagglers, which I did not think would happen so soon.\\r\\n\\r\\nDavut: The team play was amazing, and we had so much fun during the competition, tried so many crazy ideas which failed mostly but still it was really fun :)\\r\\n\\r\\nSong: Keep learning endlessly. Taking a competition in a team is really a happy journey.\\r\\nDo you have any advice for those just getting started in data science?\\nDarius: Be prepared to work hard as good results don\\'t come easy. Make a list of what you want to get good at first and prioritize. Don\\'t let XGBoost be the only tool in your toolbox.\\r\\n\\r\\nDavut: Spend sufficient time on feature engineering, study the previous competitions\\' solutions, no matter how much time has passed. For example, our winning approach is so similar to Josef Feigl\\'s winner solution in Loan Default Prediction.\\r\\n\\r\\nSong: Keep learning.\\r\\nJust for fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\nDarius: As an econometrician, I love competitions which involve predicting future trends. I\\'d love to put ARIMA and other time series methods into action more often.\\r\\n\\r\\nDavut: In the Higgs Boson Challenge, high-energy physicists and data scientists competed together. I liked the spirit then and remember Lubos Motl and his posts brought new aspects to the approaches. I would like to pose a multidisciplinary problem.\\r\\n\\r\\nSong: Any problem balancing exploration and exploitation.\\r\\nWhat is your dream job?\\nDarius: Making global impact to people\\'s lives with data science projects.\\r\\n\\r\\nDavut: Using data science for early diagnosis for severe diseases like cancer, heart attack, etc.\\r\\n\\r\\nSong: Data Scientist.\\r\\nAcknowledgments\\r\\nWe want to thank this Kaggle blog post, which helped us greatly with shaking some of our prior beliefs about the data and helping with brainstorming new ideas.', 'A total of 2,552 players on over 2,000 teams participated in the Home Depot Product Search Relevance competition which ran on Kaggle from January to April 2016. Kagglers were challenged to predict the relevance between pairs of real customer queries and products. In this interview, the first place team describes their winning approach and how computing query centroids helped their solution overcome misspelled and ambiguous search terms.\\r\\n\\r\\nThe Basics\\nWhat was your background prior to entering this challenge?\\nAndreas: I have a PhD in Wireless Network Optimization using statistical and machine learning techniques. I worked for 3.5 years as Senior Data Scientist at AGT International applying machine learning in different types of problems (remote sensing, data fusion, anomaly detection) and I hold an IEEE Certificate of Appreciation for winning first place in a prestigious IEEE contest. I am currently Senior Data Scientist at Zalando SE.\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Andreas on Kaggle[/caption]\\r\\n\\r\\nAlex: I have a PhD in computer science and work as data science consultant for companies in various industries. I have built models for e-commerce, smart home, smart city and manufacturing applications, but never worked on a search relevance problem.\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Alex on Kaggle[/caption]\\r\\n\\r\\nNurlan: I recently completed my PhD in biological sciences where I worked mainly with image data for drug screening and performed statistical analysis for gene function characterization. I have also experience in application of recommender system approaches for novel gene function predictions.\\r\\n\\r\\n[caption id=\"attachment_5597\" align=\"aligncenter\" width=\"300\"] Nurlan on Kaggle[/caption]\\r\\n\\r\\nHow did you get started competing on Kaggle?\\nNurlan: The wide variety of competitions hosted on Kaggle motivated me to learn more about applications of machine learning across various industries.\\r\\n\\r\\nAndreas: The opportunity to work with real-world datasets from various domains and also interact with a community of passionate and very smart people was a key driving factor. In terms of learning while having fun, it is hard to beat the Kaggle experience. Also, exactly because the problems are coming from the real world, there are always opportunities to apply what you learned in a different context, be it another dataset or a completely different application domain.\\r\\n\\r\\nAlex: I was attracted by the variety of real world datasets hosted on Kaggle and the opportunity to learn new skills and meet other practitioners. I was a bit hesitant to join competitions in the beginning as I was not sure if I would be able to dedicate the time for it, but then never regretted to get started. The leaderboard, the knowledge exchange in forums and working in teams creates a very exciting and enjoyable experience, and I was often able to transfer knowledge gained on Kaggle to customer problems in my day job.\\r\\n\\r\\nWhat made you decide to enter this competition?\\nAlex: Before Home Depot, I participated in several competitions with anonymized datasets where feature engineering was very difficult or didn’t work at all. I like the creative aspect of feature engineering and I expected a lot of potential for feature engineering in this competition. Also I saw a chance to improve my text mining skills on a very tangible dataset.\\r\\n\\r\\nNurlan: I had two goals in this competition: mastering state of the art methods in natural language processing and model ensembling techniques. Teaming up with experienced kagglers and kaggle community through forums provided opportunities to achieve my goals.\\r\\n\\r\\nAndreas: Learning more about both feature engineering and ML models that are doing well in NLP was a first driver. The decent but not overwhelming amount of data gave also good opportunities for ensembling and trying to squeeze the most out of the models, something that I enjoy doing when there are no inherent time or other business constraints (as is often the case in commercial data science applications).\\r\\n\\r\\nLet’s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\n[caption id=\"attachment_5785\" align=\"aligncenter\" width=\"1024\"] Figure 1: Overview of our prediction pipeline - most important features and models highlighted in orange.[/caption]\\r\\n\\r\\nPreprocessing and Feature Engineering\\r\\n\\r\\nOur preprocessing and feature engineering approach can be grouped into five categories: keyword match, semantic match, entity recognition, vocabulary expansion and aggregate features.\\r\\n\\r\\nKeyword Match\\r\\n\\r\\nIn keyword match we counted the number of matching terms between search term and different sections of product information and also stored the matching term position. To overcome the misspellings we used fuzzy match where we counted the character n-grams matches instead of complete term. We also computed tf-idf normalized scores of the matching terms to normalize for the non-specific term matches.\\r\\n\\r\\nSemantic Match\\r\\n\\r\\n[caption id=\"attachment_5787\" align=\"aligncenter\" width=\"1024\"] Figure 2: Visualization of word embedding vectors trained on product descriptions and titles - related words cluster in word embedding space (2D projection using multi-dimensional scaling on cosine distance matrix, k-means clustering).[/caption]\\r\\n\\r\\nTo capture the semantic similarity (e.g. shower vs bathroom) we performed matrix decomposition using latent semantic analysis (LSA) and non-negative matrix factorization (NMF). To further catch the similarities that were not captured with LSA or NMF, which were trained on Home Depot corpus, we used pre-trained word2vec and GloVe word embeddings that are trained on various external corpora. Among LSA, NMF, GloVe and word2vec, GloVe word embeddings gave the best performance. See in figure 2 how it captures similar entities.\\r\\n\\r\\nMain Entity Extraction\\r\\n\\r\\nThe main motivation was to extract main entities being searched and being described in the queries and product titles respectively. Our primary approach was to include positional information of the matched terms but oob error analysis revealed that it was not enough. We also experimented with POS tagging but we noticed that many of the terms that represent entity attributes and specifications were also captured as nouns and there was no obvious pattern to distinguish them from the the main entity terms. Instead, we decided to extract last N terms as potential main entities after reversing the order of the terms whenever we see prepositions such as \"for\", \"with\", \"in\", etc., which were usually followed by entity attributes/specifications.\\r\\n\\r\\nVocabulary Expansion\\r\\n\\r\\nTo catch \\'pet\\' vs \\'dog\\' type of relationships we performed vocabulary expansion for main entities extracted from the search terms and product titles. Vocabulary expansion included synonym, hyponym and hypernym extraction from WordNet.\\r\\n\\r\\nAggregate Features\\r\\n\\r\\nSee “What was your most important insight into the data?” section for details.\\r\\n\\r\\nFeature Interactions\\r\\n\\r\\nWe also performed basis expansions by including polynomial interaction terms between important features. These features also contributed further to the performance of our final model.\\r\\n\\r\\nSupervised Learning Methods\\r\\n\\r\\nApart from the usual suspects like xgboost, random forest, extra trees and neural nets, we worked quite a lot with combinations of unsupervised feature transformations and generalized linear models, especially sparse random and Gaussian projections as well as Random Tree Embeddings (which did really good). On the supervised part, we tried a large number of Generalized Linear Models using the different feature transformations and different loss functions. Bayesian Ridge and Lasso with some of the transformed features did really well, the first also getting almost no hyperparameter tuning (and thus saving time). Another thing that worked really good was the regression through classification approach based on Extra Tree Classifiers. Selecting the optimal number of classes and tweaking the model to get reliable posterior probability estimates was important and took computational effort but it contributed some of the best models (just next to the very best xgboost models).\\r\\n\\r\\nThe idea was always to get models that are individually good on their own but have as little correlation as possible so that they can contribute meaningfully in the ensemble. The feature transformations, different loss functions, regression through classification, etc. all played well in this general goal.\\r\\n\\r\\n[caption id=\"attachment_5788\" align=\"aligncenter\" width=\"600\"] Figure 3. Comparison of unsupervised random tree embedding and supervised classification in separating the relevant and non-relevant points (2D projections).[/caption]\\r\\n\\r\\nThe two figures above are showing the effectiveness of the unsupervised Random Tree Embedding transform (upper of the two pictures). The separation visualized here is between two classes only (highly relevant points tend to be high on the left and not relevant low and towards the right) and it is mingled. But we need to consider that this is a 2D projection done in a completely unsupervised way (the classes are actually visualized on top of the data and the labels were not used for anything other than visualization). For comparison, the other image (bottom picture) visualizes the posterior estimates for the two classes derived from a supervised Extra Tree classification algorithm (again the highly relevant area is up and to the left, while the non-relevant bottom right).\\r\\n\\r\\nHow did you settle on a strong cross-validation strategy?\\nAlex: I think everyone joining the competition realized very early that a simple cross-validation does not properly reflect the generalization error on the test set. The amount of search terms and products only present in the test set biased the cross-validation error and lead to overfitted models. To avoid that, I tried first to generate cross-validation folds that account for both unseen search terms and products simultaneously, but I was not able to come up with a sampling strategy that meets these requirements. I finally got the idea to “ensemble” multiple sampling schemes and it turned out to work very well. We created two runs of 3-fold cross-validation with disjoint search terms among the folds, and one 3-fold cross-validation with disjoint product id sets. Taking the average error of the three runs turned out to be a very good predictor for the public and private leaderboard score.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\n[caption id=\"attachment_5789\" align=\"aligncenter\" width=\"697\"] Figure 4. Information extraction about relevance of products to the query and quantification of query ambiguity by aggregating the products retrieved for each query.[/caption]\\r\\n\\r\\nIn the beginning we were measuring search term to product similarity by different means, but search terms were quite noisy (i.e. misspellings). Since most of the products retrieved are relevant, we clustered products for each query, then computed cluster centroid and used this centroid as a reference. Calculating similarity of the products to the query centroid provided powerful information (See figure above, left panel).\\r\\n\\r\\nOn top of this, some queries are ambiguous (e.g. ‘manual’ as opposed to ‘window lock’) and these ambiguous terms would be unclear for the human raters too and might lead to less relevant score. We decided to include this information as well by computing the mean similarity of the products to the query centroid for each query. Figure above (right panel) shows this relationship.\\r\\n\\r\\nWere you surprised by any of your findings?\\nAndreas: One surprising finding was that the residual errors of our predictions were exhibiting a strange pattern (different behavior in the last few tens of thousands of records), that hinted towards a bias somewhere in the process. After discussing it, we thought that a plausible explanation was a change of annotators or change in the annotations policy. We decided to model this by adding a binary variable (instead of including the id directly) and it proved a good bet.\\r\\n\\r\\nAlex: I was surprised by the excellent performance of word embedding features compared to classical TF-IDF approach, even though the word embeddings were trained on a rather small corpus.\\r\\n\\r\\nWhich tools did you use?\\nAndreas: We used a Python tool chain, with all of the standard tools of the trade (scikit-learn, nltk, pandas, numpy, scipy, xgboost, keras, hyperopt, matplotlib). Sometimes R was also used for visualization (ggplot).\\r\\n\\r\\nHow did you spend your time on this competition?\\nAlex: We spent most of the time on preprocessing and feature engineering. To tune the models and the ensemble, we reused code from previous competitions to automate hyperparameter optimization, cross-validation and stacking, so we could run them overnight and while we were at work.\\r\\n\\r\\nWhat was the run time for both training and prediction of your winning solution?\\nAlex: To be honest, recalculating the full feature extraction and model training pipeline takes several days, although our best features and models would finish after a few hours. We often tried to remove models and features to reduce the complexity of our solution, but it almost always increased the prediction error. So we kept adding new models and features incrementally over several weeks, leading to more than 20 independent feature sets and about 300 models in the first ensemble layer.\\r\\n\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\nAlex: Never give up and keep working out new ideas, even if you are falling behind on the public leaderboard. Never throw away weak features or models, they could still contribute to your final ensemble.\\r\\n\\r\\nNurlan: Building a cross-validation scheme that\\'s consistent with leaderboard score and power of ensembling.\\r\\n\\r\\nAndreas: Persistency and application of best practices on all aspects (cross-validation, feature engineering, model ensembling, etc.) is what makes it work. You cannot afford to skip any part if you want to compete seriously in Kaggle these days.\\r\\n\\r\\nDo you have any advice for those just getting started in data science?\\nAlex: Data science is a huge field - focus on a small area first and approach it through hands-on experimentation and curiosity. For machine learning, pick a simple toy dataset and an algorithm, automate the cross validation, visualize decision boundaries and try get a feeling for the hyperparameters. Have fun! Once you feel comfortable, study the underlying mechanisms and theories and expand your experiments to more techniques.\\r\\n\\r\\nNurlan: This was my first competition and in the beginning I was competing alone. The problem of unstable local CV score demotivated me a bit as I couldn\\'t tell how much my new approach helped until I made a submission. Once I joined the team, I learnt great deal from Alexander and Andreas. So get into a team with experienced Kagglers.\\r\\n\\r\\nAndreas: I really recommend participating in Kaggle contests even for experienced data scientists. There is a ton of things to learn and doing it while playing is fun! Even if in the real world you will not get to use an ensemble of hundreds of models (well most of the time at least), learning a neat trick on feature transformations, getting to play with different models in various datasets and interacting with the community is always worth it. Then you can pick a paper or ML book and understand better why that algorithm worked or did not work so well for a given dataset and perhaps how to tweak it in a situation you are facing.\\r\\n\\r\\nTeamwork\\nHow did your team form?\\nAlex: Andreas and me are former colleagues and after we left the company we always planned to team up once for a competition. I met Nurlan at a Predictive Analytics meet-up in Frankfurt and invited him to join the team.\\r\\n\\r\\nHow did your team work together?\\nAlex: We settled on a common framework for the machine learning part at the very beginning and synchronized changes in the machine learning code and in hyper parameter configuration using a git repository. Nurlan and me had independent feature extraction pipelines, both producing serialized pandas dataframes. We shared those and the oob predictions using cloud storage services. Nurlan produced several new feature sets per week and kept Andreas and me very busy tuning and training models for them. We communicated mostly via group chat in Skype, only had two voice calls during the whole competition.\\r\\n\\r\\nHow did competing on a team help you succeed?\\nAndreas: We combined our different backgrounds and thus were able to cover a lot of alternatives fast. Additionally, in this contest having a lot of alternate ways of doing things like pre-processing, feature engineering, feature transformations, etc. was quite important in increasing the richness of the models that we could add in our stacking ensemble.\\r\\n\\r\\nJust for fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\nAlex: If I had access to a suitable dataset, I would run a competition on predictive maintenance to predict remaining useful lifetime of physical components. Also I would love to work on a competition where reinforcement learning can be applied.\\r\\n\\r\\nThe Team\\nDr. Andreas Merentitis received B.Sc., M.Sc., and Ph.D. degrees from the Department of Informatics and Telecommunications, National Kapodistrian University of Athens (NKUA) in 2003, 2005, and 2010 respectively. Between 2011-2015 he was Senior Data Scientist at AGT International. Since 2015 he works as Senior Data Scientist at Zalando SE. He has more than 30 publications in machine learning, distributed systems, and remote sensing, including publications in flagship conferences and journals. He was awarded an IEEE Certificate of Appreciation as a core member of the team that won the first place in the “Best Classification Challenge” of the 2013 IEEE GRSS Data Fusion Contest. He has a master ranking in Kaggle.\\r\\n\\r\\nAlexander Bauer is a data science consultant with 10 years of experience in statistical analysis and machine learning. He holds a degree in electrical engineering and a PhD in computer science.\\r\\n\\r\\nNurlanbek Duishoev received his BSc and PhD degrees in biological sciences from Middle East Technical and from Heidelberg University respectively. His research focused on drug screenings and biological image data analysis. He later moved on to apply recommender system approaches for gene function prediction. The wealth of data being generated in the biomedical field, like in many other industries, motivated him to master state-of-the-art data science techniques via various MOOCs and participate in Kaggle contests.', 'The Home Depot Product Search Relevance competition which ran on Kaggle from January to April 2016 challenged Kagglers to use real customer search queries to predict the relevance of product results. Over 2,000 teams made up of 2,553 players grappled with misspelled search terms and relied on natural language processing techniques to creatively engineer new features. With their simple yet effective features, Team Turing Test found that a carefully crafted minimal model is powerful enough to achieve a high ranking solution. In this interview, Team Turing Test team walks us through their full approach which landed them in third place.\\r\\n\\r\\nThe basics\\nWhat was your background prior to entering this challenge?\\nChenglong Chen: I have received my Ph.D. degree in Communication Engineering from Sun Yat-sen University, Guangzhou, China, in 2015. As a Ph.D. student, I mainly focused on passive digital image forensics, and applied various machine learning methods, e.g., SVM and deep learning, to detect whether a digital image has been edited/doctored. I have participated in a few Kaggle competitions before this one.\\r\\n\\r\\n[caption id=\"attachment_5809\" align=\"aligncenter\" width=\"300\"] Chenglong on Kaggle[/caption]\\r\\n\\r\\nIgor Buinyi: I was a Ph.D. student at the Institute of Physics, Academy of Sciences of Ukraine. Later I received my MA degree in Economics from Kyiv School of Economics, then applied my skills in the financial sector and computer games industry. I have solid skills in statistics, data analysis and data processing, but I started seriously working on machine learning only recently after having graduated with the Udacity Data Analyst Nanodegree. Now I analyze customer behavior at Elyland.\\r\\n\\r\\n[caption id=\"attachment_5810\" align=\"aligncenter\" width=\"300\"] Igor on Kaggle[/caption]\\r\\n\\r\\nKostiantyn Omelianchuk: I have an MS degree in System Analysis and Management from Kyiv Polytechnic Institute and 3 years of data analysis and data processing experience in the financial sector and game development industry. Now I analyze customer behavior at Elyland.\\r\\n\\r\\n[caption id=\"attachment_5811\" align=\"aligncenter\" width=\"300\"] Kostia on Kaggle[/caption]\\r\\n\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\nChenglong: I have limited knowledge of NLP tasks. In addition to the CrowdFlower Search Results Relevance competition at Kaggle (where I ended up in 1st place), reading forum posts, previous Kaggle winning solutions, related papers, and lots of Google searches have given me many inspirations.\\r\\n\\r\\nIgor: In a student project, I used NLP and machine learning to analyze the Enron email dataset.\\r\\nHow did you get started competing on Kaggle?\\nKostia: Almost a year ago I read an article about a solution to a Kaggle competition. I was very impressed by that text, so I registered at Kaggle and invited Igor to do the same. We then participated in Springleaf Marketing Response, however, we only finished in the top 15%.\\r\\n\\r\\nChenglong: It dates back 2012. At that time, I was taking Prof. Hsuan-Tien Lin\\'s Machine Learning Foundations course on Coursera. He encouraged us to compete on Kaggle to apply what we have learnt to real world problems. From then on, I have occasionally participated in competitions I find interesting.\\r\\nWhat made you decide to enter this competition?\\nIgor: I just graduated from the Udacity Nanodegree program and was eager to apply my new skills in a competition, no matter what. At that time this competition had started, so I joined and invited Kostia to the team.\\r\\n\\r\\nChenglong: I have prior successful experience in the CrowdFlower Search Relevance competition on Kaggle which is quite similar to this one. I also had some spare time and wanted to strengthen my skills.\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\nThe documentation and code for our approach are available here. Below is a high level overview of the method.\\r\\n\\r\\n[caption id=\"attachment_5799\" align=\"aligncenter\" width=\"1024\"] Fig. 1 Overall flowchart of our method.[/caption]\\r\\n\\r\\nThe text preprocessing step included text cleaning (removing special characters, unifying the punctuation, spell correction and thesaurus replacement, removing stopwords, stemming) and finding different meaningful parts in text (such as concatenating digits with measure units, extracting brands and materials, finding part-of-speech tags for each word, using patterns within the text to identify product names and other important words).\\r\\n\\r\\nOur features can be grouped as follows:\\r\\n\\n\\nBasic features like word count, character count, percentage of digits, etc.\\nIntersection and distance features (various intersections between search term and other text information, Jaccard and Dice coefficients calculated using different algorithms).\\nBrand/material match features. Brands and materials were among the key determinants of the search results relevance.\\nFirst and last ngram features. They are designed to put more weight on the first and last ngram. The general idea would be to incorporate position weighting into these features.\\nLSA features. Most of our models are not efficient in dealing with sparse TFIDF vector space features. So we used the dimension reduced version via SVD.\\nTFIDF features in different combinations. They allow accounting for word frequency within the whole text corpus or particular query-product pair.\\nWord2vec features from pretrained models or models trained on the HomeDepot data.\\nWordNet similarity features. It allowed us to assess the closeness of the words as defined by NLTK WordNet. We calculated synset similarity for pairs of important words (ideally, we wanted important words to be product names and their main characteristics) or even whole strings.\\nQuery expansion. The idea was to group similar queries together and then estimate how common a particular product description was. The relevance distribution was significantly skewed to 3, so in each small subset the majority of relevances would be closer to 3 than to 1. Thus, a higher intersection of a particular product description with the list of the most common words would indicate higher relevance.\\nDummies: brands, materials, important words.\\n\\n\\nHow did you settle on a strong cross-validation strategy?\\nChenglong: After seeing quite a few LB shakeups in Kaggle competitions, the first thing I do after entering a Kaggle competition is performing some data analysis and setting up an appropriate cross-validation strategy. This is quite important as the CV results will act as our guide for optimizing in the remaining procedure (and we don’t want to be misled). It also will help to accelerate the circle of change-validate-check. After some analysis, we have found that there are search terms only in the training set, only in the testing set and those in both training and testing set. This also applies for product uid. Taking these into consideration, we have designed our splitter for the data. Using this splitter, the gap between local CV and public LB is consistent around 0.001~0.002 which is within the 2-std range of CV for both single models and ensembled models. In the following figure, we compare different splits of product uid and search term via Venn-Diagram.\\r\\n\\r\\n[gallery size=\"full\" ids=\"5800,5801,5802\"]\\r\\n[gallery size=\"full\" ids=\"5803,5804,5805\"]\\r\\nFig. 2 Comparison of different splits on search term (top row) and product uid (bottom row). From left to right, shown are actual train/test split, naive 0.69 : 0.31 random shuffle split, and the proposed split.\\r\\n\\r\\nWhat was your most important insight into the data?\\nChenglong: Including this competition, I have participated in two relevance prediction competitions. From my point of view, the most important features are those measuring the distance between searching query and the corresponding results. Such distance can be measured via various distance measurements (Jaccard, Dice, cosine similarity, KL, etc.), and from different level (char ngram, word ngram, sentence, document), and with different weighting strategy (position weighting, IDF weighting, BM25 weighting, entropy weighting).\\r\\n\\r\\nKostia: We found the data too noisy to be captured by a single unified text processing approach. Therefore, we needed to apply as many preprocessing/feature extraction algorithms as we could in order to get a better performance.\\r\\n\\r\\nIgor: We also found it extremely important to account for the number of characters in a word while generating various features. For example, the two strings ‘first second’ and ‘first third’ have Jaccard coefficient of 1/(2+2-1)=1/3 in terms of words and 5/(11+10-5)=5/16 in terms of characters. Such incorporation of the number of characters information into the intersection features, distance features and TFIDF features was very beneficial.\\r\\n\\r\\nWere you surprised by any of your findings?\\nKostia: I discovered that routine work gives 90% of the result. Only the minor part comes from luck and knowledge of ‘magic’ data science, at least in this competition. One could also easily compensate the lack of particular knowledge with some invested time and effort. Perseverance and desire to learn are the key factors for good performance.\\r\\n\\r\\nChenglong: Some data in the testing set are poisoned data that are not used in both public LB and private LB.\\r\\n\\r\\nIgor: During the competition we had to add more and more features to move forward. In total we ended up with about 500 features used in our single models. After the competition had ended, we tried to produce a simple model as required by the Kaggle solution template. We were surprised that our 10-feature model would yield private RMSE of 0.44949 without any ensembling, enough to be on the 31st leaderboard place. It shows that a solution to such a problem could be simple and effective at the same time. (To be clear, we did not use the released information about the relevance from the test set to produce our simple model).\\r\\n\\r\\nWhich tools did you use?\\nChenglong: We mostly used Python with packages such as numpy, pandas, regex, mathplotlib, gensim, hyperopt, keras, NLTK, sklearn, xgboost. I also used R with Rtsne package for computing the TNSE features. Igor and Kostia have used Excel to perform some descriptive analysis and generate some charts.\\r\\n\\r\\nHow did you spend your time on this competition?\\nIgor: Kostia and I spent almost equal amounts of time on feature engineering and machine learning. For some tasks we employed specialization: I focused on text preprocessing and using NLTK WordNet, Kostia got the most from word2vec. At some point we realized that we had chances to win, so we had to properly document our work and adhere to the requirements of the winning solutions. So, in four final weeks of the competition we spent much time on rewriting and clearing our code and recalculating features according to a unified text processing algorithm (which did not lead to an improvement of the results; we even lose some performance due to a reduced variance of our features).\\r\\n\\r\\nChenglong: In the very beginning of the competition, I focused on figuring out the appropriate CV strategy. After that I have started to reproduce the solution I used in the CrowdFlower competition. Meanwhile, I spent quite some time refactoring the whole framework for the purpose of adding data processor/feature generator/model learner easily. With a scalable codebase, I spent about 70% of the time figuring out and generating various and effective features.\\r\\n\\r\\nKostia: During the final week the whole team spent a few days on discussing patterns in the dataset (Fig. 3) and trying to figure out how to deal with them. Since our model predictions (one coming from Chenglong with some our features and another from me and Igor) for the test set were quite different, we expected that one of our approaches would be more robust to the leaderboard shakeup. In fact, we did not observe a major difference between the models’ performance in the public and private sets.\\r\\n\\r\\n[caption id=\"attachment_5814\" align=\"aligncenter\" width=\"764\"] Fig. 3 Some charts that demonstrate the patterns in the dataset.[/caption]\\r\\n\\r\\nWhat was the run time for both training and prediction of your winning solution?\\nIgor: The text preprocessing and feature generation part takes a few days. Though a single xgboost model can be trained in about 5 minutes, training and predicting all models for the winning solution takes about 1 full day of computation.\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\n\\n\\nSpelling correction/synonyms replacement/text cleaning are very useful for searching query relevance prediction as also revealed in CrowdFlower.\\nIn this competition, it is quite hard to improve the score via ensemble and stacking. To get the most out of ensemble and stacking, one should really focus on introducing diversity. To that goal, try various text processing, various feature subsets, and various learners. Team merging also contributes a lot of diversity!\\nThat said, know how to separate the effect of an improved algorithm from the effect of increased diversity. Careful experiments are necessary. If some clear and effective solution does not work as well as your old models, do not discard it until you compare both approaches in the same conditions.\\nKeep a clean and scalable codebase. Keep track/log of the change you have made, especially how you create those massive crazy ensemble submissions.\\nSet up an appropriate and reliable CV framework. This allows trying various local optimizations on the features and models and getting feedback without the need to submit them. This is important for accelerating the change-validate-check cycle.\\n\\n\\nTeamwork\\nHow did your team form?\\nIgor: Kostia and I worked together since the start of the competition. Before the merger deadline, the competitors started to merge very intensely. We also realized that we had to merge in order to remain on the top. If we remember correctly, at the moment of our final merger Chenglong was the only sole competitor in the top 10 and there were few other teams of two in the top 10. So, our merger was natural.\\r\\nHow did your team work together?\\nChenglong: For most of our discussion, we used Skype. For data and file sharing, we used Google Drive and Gmail.\\r\\nHow did competing on a team help you succeed?\\nChenglong: About one or two weeks before the team merging deadline, I was the only one in the top 10 that competed solely. If I hadn\\'t merged with Igor and Kostia, I might not have been able to enter the top 10 in the private LB. Competing on a team helps to introduce variants for obtaining better results. Also, we have learned a lot from each other.\\r\\n\\r\\nIgor: All top teams in this competition merged during the later stages, and this fact signifies the importance of merging for achieving top performance. In our team we were able to quickly test a few ideas due to information sharing and cooperation.\\r\\n\\r\\nJust for fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\nKostia: Developing an AI or recommendation tool for poker.\\r\\nWhat is your dream job?\\nChenglong: Chef.', 'The Home Depot Product Search Relevance competition challenged Kagglers to predict the relevance of product search results.  Over 2000 teams with 2553 players flexed their natural language processing skills in attempts to feature engineer a path to the top of the leaderboard. In this interview, the second place winners, Thomas (Justfor), Sean (sjv), Qingchen, and Nima, describe their approach and how diversity in features brought incremental improvements to their solution.\\r\\n\\r\\nThe basics\\nWhat was your background prior to entering this challenge?\\nThomas is a pharmacist, with his PhD in Informatics and Pharmaceutical Analytics and works in Quality in the pharmaceutical industry. At Kaggle he joined earlier competitions and got the Script of the Week award.\\r\\n\\r\\nSean is an undergraduate student in computer science and mathematics at the Massachusetts Institute of Technology (MIT).\\r\\n\\r\\n[gallery columns=\"2\" size=\"large\" ids=\"5841,5844\"]\\r\\n\\r\\nQingchen is a data scientist at ORTEC Consulting and a PhD researcher at the Amsterdam Business School. He has experience competing on Kaggle but this was the first time with a competition related to natural language processing.\\r\\n\\r\\nNima is a PhD candidate at the Lassonde School of Engineering at York University focusing on research in data mining and machine learning. He has also experience competing on Kaggle but up to now focused on other types of competitions.\\r\\n\\r\\n[gallery columns=\"2\" size=\"large\" ids=\"5843,5842\"]\\r\\n\\r\\nBetween the four of us, we have quite a bit of experience with Kaggle competitions and machine learning, but minor experience in natural language processing.\\r\\n\\r\\nWhat made you decide to enter this competition?\\r\\n\\r\\nFor all of us, the primary reason was that we wanted to learn more about natural language processing (NLP) and information retrieval (IR). This competition turned out to be great for that, especially in providing practical experience.\\r\\n\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\n\\r\\nAll of us have strong theoretical experience with machine learning in general, and it naturally helps with the understanding and implementation of NLP and IR methods. However, none of us have had any real experience in this domain.\\r\\n\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nThe key to this competition was mostly preprocessing and feature engineering as the primary data is text. Our processed text features can broadly be grouped into a few categories: categorical features, counting features, co-occurrence features, semantic features, and statistical features.\\r\\n\\r\\n\\nCategorical features:  Put words in categories such as colors, units, brands, core. Count the number of those words in the query/title and count number of intersection between query and title for each category.\\nCounting features: Length of query, number of common grams between query and title, Jacquard similarity, etc.\\nCo-occurrence features:  Measures of how frequently words appear together. e.g., Latent Semantic Analysis (LSA).\\nSemantic features:  Measure how similar the meaning of two words is.\\nStatistical features: Compare queries with unknown score to queries with known relevance score.\\n\\r\\n\\r\\nIt seems that a lot of the top teams had similar types of features, but the implementation details are probably different. For our ensemble we used different variations of xgboost along with a ridge regression model.\\r\\n\\r\\n[caption id=\"attachment_5850\" align=\"aligncenter\" width=\"600\"] Word cloud of Home Depot product search terms.[/caption]\\r\\n\\r\\nFor models and ensemble we started with random forest, extra trees and gbm-models. Furthermore xgboost and ridge were in our focus. Shortly prior to the end of the competition we found out, that first random forest and then extra trees did not help our ensembles anymore. So we focused on xgboost, gbm and Ridge. \\r\\nOur best single model was a xgboost-model and scored 0.43347 on the public LB. The final ensemble consists of 19 models based on xgboost, gbm and Ridge. The xgboost-models were made with different parameters including binarizing the target, objective reg:linear, and objective count:poisson. We found, that the Ridge Regression helped in nearly every case, so we included it in the final ensemble.\\r\\n\\r\\n[caption id=\"attachment_5880\" align=\"aligncenter\" width=\"800\"] Our data processing pipeline.[/caption]\\r\\n\\r\\nWere you surprised by any of your findings?\\r\\n\\r\\nA surprising finding was the large number of features which had predictive ability. In particular, when we teamed up, it was better to combine our features than to ensemble our results. This is quite unique as most of the time new features are more likely to cause overfit but not in this case. As a result, adding more members to the team was highly likely to improve score which is why the top-10 were all teams of at least 3 people.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nWe used mainly Python 3 and Python 2. The decision for Python 2 is interesting as some of the used libraries are still not available for Python 3. In our processing chain we used the Python standard tools for machine learning (scikit-learn, nltk, pandas, numpy, scipy, xgboost, gensim). Nima used R for feature generation.\\r\\n\\r\\nHow did you spend your time on this competition?\\r\\n\\r\\nAfter teaming up, Sean and Nima spent most of their time on feature engineering and Thomas and Qingchen spent most of their time on model tuning.\\r\\n\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\n\\r\\nIn general, training/prediction time is very fast (minutes), but we used some xgboost parameters that took much longer to train (hours) for small performance gains. Text processing and feature engineering took a very long time (easily over 8 hours for a single feature set).\\r\\n\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\r\\n\\r\\nFirst of all quite a lot of Kaggle ranking points and Thomas got his Master badge! Overall this was a very difficult competition and we learned a lot about natural language processing  and information retrieval in practice. It now makes sense why Google is able to use such a large number of features in their search algorithm as many seemingly insignificant features in this competition were still able to provide a tangible performance boost.\\r\\n\\r\\nTeamwork\\nHow did your team form?\\r\\n\\r\\nInitially Thomas and Sean teamed up as Sean had strong features and Thomas experience in models and Kaggle. The models were complementing well and ensembling brought the team into the top-10. A further boost was made when Qingchen joined with his features and models. At this point we (and other teams) realized that it\\'s a necessity to form larger teams in order to be competitive as combining features really helps improve performance. We decided to ask Nima to join us as he had an excellent track record and was also doing quite well on his own.\\r\\n\\r\\nWorking together was quite interesting as we are from Germany, US, Netherlands and Canada. The different time zones made direct communication difficult; we opted therefore for mail communication. For getting results and continue working on ideas the different time zones were helpful. \\r\\n\\r\\nTeam bios\\nDr. Thomas Heiling is a pharmacist, with his PhD in Informatics and Pharmaceutical Analytics and works in Quality in the pharmaceutical industry. \\r\\n\\r\\nSean J. Vasquez is a second year undergraduate student at the Massachusetts Institute of Technology (MIT), studying computer science and mathematics. \\r\\n\\r\\nQingchen Wang is a Data Scientist at ORTEC Consulting and a PhD researcher in Data Science and Marketing Analytics at the Amsterdam Business School.\\r\\n\\r\\nNima Shahbazi is a second-year PhD student in the Data Mining and Database Group at York University. He previously worked in big data analytics, specifically on Forex Market. His current research interests include Mining Data Streams, Big Data Analytics and Deep Learning.', 'The Avito Duplicate Ads Detection competition ran on Kaggle from May to July 2016 and attracted 548 teams with 626 players. In this challenge, Kagglers sifted through classified ads to identify which pairs of ads were duplicates intended to vex hopeful buyers. This competition, which saw over 8,000 submissions, invited unique strategies given its mix of Russian language textual data paired with 10 million images. In this interview, team ADAD describes their winning approach which relied on feature engineering including an assortment of similarity metrics applied to both images and text.\\r\\nThe basics\\nWhat was your background prior to entering this challenge?\\nMario Filho: My background in machine learning is completely “self-taught”. I found a wealth of education materials available online through MOOCs, academic papers and lectures in general. Since February 2014 I have worked as a machine learning consultant in projects from small startups and Fortune 500 companies.\\r\\n\\r\\n[caption id=\"attachment_6009\" align=\"aligncenter\" width=\"948\"] Mario on Kaggle[/caption]\\r\\n\\r\\nGerard Toonstra: I worked as a scientific developer at Thales for 3 years, which introduced me into more scientific development methods and algorithms. Most of the specific ML knowledge was acquired through courses on Coursera and just getting your hands dirty in Kaggle competitions and the forum interactions.\\r\\n\\r\\n[caption id=\"attachment_6010\" align=\"aligncenter\" width=\"950\"] Gerard on Kaggle[/caption]\\r\\n\\r\\nKele Xu: I am a PhD student with the topic on \"silent speech interface\".\\r\\n\\r\\n[caption id=\"attachment_6011\" align=\"aligncenter\" width=\"950\"] Kele on Kaggle[/caption]\\r\\n\\r\\nPraveen Adepu: Academically, I have Bachelor of Technology and working as full stack BI Technical Architect/Consultant.\\r\\n\\r\\n[caption id=\"attachment_6013\" align=\"aligncenter\" width=\"949\"] Praveen on Kaggle[/caption]\\r\\n\\r\\nGilberto Titericz: I\\'m graduated in electronic engineering and M.S. in wireless communication area. In 2011 I started to learn data science by myself and after joining Kaggle I started to learn even more.\\r\\n\\r\\n[caption id=\"attachment_6012\" align=\"aligncenter\" width=\"949\"] Gilberto on Kaggle[/caption]\\r\\nHow did you get started competing on Kaggle?\\nMario Filho: I heard about Kaggle when I was taking my first courses about data science, and after I learned more about it I decided to try some competitions.\\r\\n\\r\\nGerard Toonstra: I was active in the Netflix grand prize quite some while ago and at the end, it pointed to the Kaggle site as another potential source of getting your hands dirty. I ignored that up to July last year when I decided to start on the Avito click challenge. It\\'s pretty cool that exactly one year later, after some avid kaggling, I\\'m part of the 3rd place submission.\\r\\n\\r\\nKele Xu: I participated in KDD Cup 2015, and ended as 40th there. That was my first competition. After KDD Cup 2015, I became a Kaggler, which helped me to learn a lot during the last year.\\r\\n\\r\\nPraveen Adepu: I am new to machine learning, R and Python. I like learning by doing and I realised Kaggle is the best fit for my learning by participating in competitions. Initially I experimented with few competitions to find learning patterns and started working seriously from last 6 months and I learnt a lot in the past 6 months and looking forward to learn more from Kaggle.\\r\\n\\r\\nGilberto Titericz: After the Google AI challenge 2011 I was searching on the internet for other online competition platform and found Kaggle.\\r\\nWhat made you decide to enter this competition?\\nMario Filho: At the time it was the only competition that had a reasonable dataset size and was not too focused on images. So I thought it would be possible to get a good result with feature engineering and the usual tools.\\r\\n\\r\\nGerard Toonstra: I feel that my software engineering background can give me an edge in certain competitions. The huge amount of data requires a bit of planning and modularizing the code. I started doing that in the Dato competition, continued doing it a bit better for Home Depot and in Avito I started some more serious pipelining and feature engineering. It\\'s not that the pipelines are sophisticated, the only purpose is to reduce feature building time. Instead of waiting for one script to finish in 8 hours, I just build features in parts and glue/combine them together.\\r\\n\\r\\nKele Xu: When I decide to go to a new competition, I would like to select some topic which I have no experience before. On that case, I will take more from the competition. In fact, before this competition, I have few experiments on NLP topics. That’s the main reason I entered this competition.\\r\\n\\r\\nPraveen Adepu:\\r\\n\\nI like feature engineering, this completion requires lot of hand craft feature engineering\\nVery high LB bench mark score attracted me a lot to test my learning skills from previous competitions.\\r\\nI left with lot of feature engineering ideas even after passing the bench mark in couple of weeks so planned to work bit more time on this competition.\\n\\nGilberto Titericz: Lately I have interest in competitions involving image processing and deep learning.\\r\\nLet\\'s Get Technical\\nWhat preprocessing and supervised learning methods did you use?\\nMario Filho: I stemmed and cleaned the text fields, then used tf-idf to compute similarities between them. Used the hash similarity script available in the forums to compute the similarity between images. After creating and testing lots of features, I used XGBoost to train a model.\\r\\n\\r\\nGerard Toonstra: I had one feature array for textual features, which is essentially pretty much what others did in the forum. Then I built another feature set with additional text features, a minhash similarity feature, tf-idf+svd over description and then I did work on image hashes: ahash,phash,dhash with min, max, avg features and the count of zero divided by max number of images. I kept focusing on making these normalized distance metrics, so divide by \\'length\\' where appropriate or the number of images. As things advanced, I also did image histograms on RGB, histograms on HSV and Earth Mover Distance between images.\\r\\n\\r\\nI ended up only using four model types: logistic regression over tf-idf features over cleaned text, XGB, Random Forests and FTRL.\\r\\n\\r\\nKele Xu: Here, I am only using XGBoost. XGBoost is really competitive here. My best single XGBoost model can get to top 14 in both of public and private Leaderboard.\\r\\n\\r\\nPraveen Adepu: Not any special pre-processing methods but just followed all best practices of feature engineering and taken care of processing times when creating new features.\\r\\nUsed - XGBoost, h2o Random Forest, h2o deep learning, Extra Trees and Regularised Greedy Forest.\\r\\n\\r\\nGilberto Titericz: Most preprocessing was done in text features, like stop words removal and stemming. Also I build a very interesting feature using deep learning. For this I used the MXNet Inception 21k pre-trained model to predict one class for each one of the 10M images in dataset. Measuring some statistics between those classes helped improve our models.\\r\\n\\r\\nSupervised learning methods used are based in gradient boosting (XGBoost), RandomForest, ExtraTrees, FTRL, Linear Regression, Keras and MXNet. We also used libffm and RGF algorithms, but we dropped it in the end.\\r\\nWhat have you taken away from this competition?\\nMario Filho: I learned new techniques for calculating image similarity based on hashes, and 2 or 3 Russian words.\\r\\n\\r\\nGerard Toonstra: The benefit of working in teams and learning from that experience. I really wanted to learn about stacking. There is a good description on mlwave.com, but when you get around to actually doing that in practice, there are still questions that pop up. The other observation is that as a competition draws to a close, it\\'s all hands on deck and you have to put the last effort in, especially the last week. I did not have enough hardware resources and used in the most extreme case three 16-cpu machines spread across three geographical zones on Google Cloud. Despite the hardware challenges and exploration we had to do, I noticed some solo competitors who were able to gain ground quickly and rocket up. I really respect that. It tells me that those guys have acquired so much experience that they consistently make right decisions on what to spend time on and I still have a long way to go.\\r\\n\\r\\nKele Xu: I have learned a lot from our teammates, especially on the hyper-parameter optimization for XGBooost model and the ensemble techniques. Also, I got to know how to do some NLP tasks.\\r\\n\\r\\nPraveen Adepu: I think joining the team at right time and with right team members is best decision I have made in this competition.\\r\\n\\r\\nI started with two goals while joining the team:\\r\\n\\nLearn from the team while contributing to the best of my knowledge\\nLearn ensembling/stacking\\n\\r\\nI would like to take this opportunity to say thank you my entire team Gilberto, Mario, Gerard and Kele.\\r\\n\\r\\nGilberto - thank you for teaching many many concepts including stacking\\r\\nMario - thank you for guiding me while experimenting many models\\r\\nGerard - never forget our initial discussion on feature engineering\\r\\nKele - thank you for the invite to join the team and making this happen\\r\\n\\r\\nI learnt a lot from this team in one month than alone in the past 6 months and looking forward to work with them in near future.\\r\\n\\r\\nGilberto Titericz: I learned a lot about text and image distance metrics.\\r\\nDo you have any advice for those just getting started in data science?\\nMario Filho: if you plan to apply machine learning, try to understand the fundamentals of the models, concepts like validation, bias and variance. Really understand these concepts and try to apply them to data.\\r\\n\\r\\nGerard Toonstra: The first thing is to understand cross validation score and its specific relation to the leaderboard in that competition and how to establish a sound folding strategy in general. After that, I recommend establishing a very simple model based on features that do not overfit and evaluate the behavior. Then add a couple of features one by one that are unlikely to overfit and keep evaluating. Record the difference between CV and LB when you make submissions. As you establish confidence in your local CV, start working out more features and do a couple of in-between checks to ensure you\\'re not overfitting locally; especially depending on how some aggregation features are built, they can have overfitting effects.\\r\\n\\r\\nWhen you get started, don\\'t allow yourself to get bogged down by endless hours of parametrizing and hyperparameterizing the models. You may get the extra +0.00050 to jump up 3 positions, but it\\'s not what most on the LB are doing. Figure out how to create new informative features, which features should be avoided and when you grow tired of that, only then spend some time optimizing what you have.\\r\\n\\r\\nKele Xu: In fact, I think Kaggle is quite a good platform for both the data scientists and those who are just getting started, as Kaggle can provide a platform for the new guys to test how each algorithm performs.\\r\\n\\r\\nAs to the technique suggestions, I would say a solid local CV is the key point to win a competition. Usually, I will spend 2-4 weeks to test my local CV.\\r\\n\\r\\nFeature engineering is more important than hyper-parameter optimization.\\r\\n\\r\\nAlthough XGBoost is enough to get a relative good rank in some competitions, we should also test mode models to add the diversity of the models, not just select different seed or set different max depth for tree-based methods.\\r\\n\\r\\nThe last thing is: just test your ideas, and you can get some feedback from the LB. When you get to higher rank in one competition, you will have the motivation to make it better.\\r\\n\\r\\nPraveen Adepu: I think, I do call these my experience rather than advice:\\r\\n\\nStart with clear objectives, learn slowly and progressively with basics and learn quickly with advanced concepts and end with mastering\\nNever be afraid to start and fail\\nClear understanding of local CV, underfit and overfit\\nStart learning from feature engineering and end with stacking\\n\\nGilberto Titericz: Read a lot of related material. Search for problems to solve. Read about problems\\' solutions. And make your fingers work, program and learn from your mistakes.\\r\\nBios\\nMario Filho is a machine learning consultant focused in helping companies around the world use machine learning to maximize the value they get from data to achieve their business goals. Besides that, he mentors individuals who want to learn how to apply machine learning algorithms to real world data sets.\\r\\n\\r\\nGerard Toonstra graduated as a nautical officer+engineer, but mostly worked as software engineer and started his own company in Brazil working with drones for surveying. He now works as scrum master for the BI department at Coolblue in The Netherlands.\\r\\n\\r\\nKele Xu is a PhD student and writing his PhD thesis at Langevin Institute (University of Pierre and Marie Curie). His main interests are include: silent speech recognition, machine learning and computer vision.\\r\\n\\r\\nPraveen Adepu is currently working as BI Technical Architect/Consultant at Fred IT Melbourne based IT product company and main interests in machine learning and data architecture.\\r\\n\\r\\nGilberto Titericz is an electronics engineer with a M.S. in telecommunications. For the past 16 years he\\'s been working as an engineer for big multinationals like Siemens and Nokia and later as an automation engineer for Petrobras Brazil. His main interests are in machine learning and electronics areas.\\r\\n\\r\\n\\nKernel Corner\\r\\nGetting hashes from images was an important strategy in detecting similarities across the 10 million images\\xa0in this competition as Gerard Toonstra\\xa0explains:\\r\\n\\nphash calculates a hash in a special way such that it reduces the dimensionality of the source image into a 64-bit number. It captures the structure of the image. When you then subtract two hashes, you get a number that resembles the \\'structural distance\\' between the two images. The higher the number, the more the distance between the two.\\n\\r\\nKaggler Run2 shared code on Kaggle Kernels which allowed Kagglers to incorporate distance metrics from images without performing heavy duty image processing or deep learning.\\r\\n\\r\\n[caption id=\"attachment_6014\" align=\"aligncenter\" width=\"776\"] Part of Kaggler Run2\\'s script Get Hash From Images.[/caption]', 'Facebook ran its fifth recruitment competition on Kaggle, Predicting Check Ins, from May to July 2016. This uniquely designed competition invited Kagglers to enter an artificial world made up of over 100,000 places located in a 10km by 10km square. For the coordinates of each fabricated mobile check-in, competitors were required to predict a ranked list of most probable locations. In this interview, the second place winner Markus Kliegl discusses his approach to the problem and how he relied on semi-supervised methods to learn check-in locations\\' variable popularity over time.\\r\\n\\r\\n[latexpage]\\r\\n\\r\\nThe basics\\nWhat was your background prior to entering this challenge?\\r\\n\\r\\nI recently completed a PhD in mathematical fluid dynamics. Through various courses, internships, and contract work, I had some background in scientific computing, inverse problems, and machine learning.\\r\\n\\r\\n[caption id=\"attachment_6020\" align=\"aligncenter\" width=\"950\"] Markus on Kaggle[/caption]\\r\\n\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nThe overall approach was to use Bayes\\' theorem: Given a particular data point (x, y, accuracy, time), I would try to compute for a suitably narrowed set of candidate places the probability\\r\\n\\r\\n$$ P(place | x, y, accuracy, time) \\\\propto P(x, y, accuracy, time | place) P(place) \\\\,, $$\\r\\n\\r\\nand rank the places accordingly. A la Naive Bayes, I further approximated\\r\\n\\r\\nP(x, y, accuracy, time | place) as\\r\\n\\r\\n$$  P(x, y, accuracy, time | place) \\\\approx P(x, y | place) \\\\cdot $$\\r\\n$$  P(accuracy | place) \\\\cdot P(time\\\\, of\\\\, day | place) \\\\cdot $$\\r\\n$$ P(day\\\\, of\\\\, week | place) \\\\,. $$\\r\\n\\r\\nI decided on this decomposition after a mixture of exploratory analysis and simply trying out different assumptions on the independence of variables on a validation set.\\r\\n\\r\\nOne challenge given the data size was to efficiently learn the various conditional distributions on the right-hand side. Inspired by the effectiveness of ZFTurbo\\'s \"Mad Scripts Battle\" kernel early in the competition, I decided to start by just learning these distributions using histograms.\\r\\n\\r\\nTo make the histograms more accurate, I made them periodic for time of day and day of week and added smoothing using various filters (triangular, Gaussian, exponential). I also switched to C++ to further speed things up. (Early in the competition this got me to the top of the leaderboard with a total runtime of around 40 minutes single-threaded, while others were already at 15-50 hours. Unfortunately, I could not keep things this fast for very long.)\\r\\n\\r\\nFor later submissions, I averaged the P(x, y | place) histograms with Gaussian Mixture Models.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nThe relative popularity of places, P(place), varied substantially over time (really it should be written as P(place, time)), and it seemed hard tome to forecast it from the training data (though others like Jack (Japan) in third place had some success doing this). Since the quality of the predictions even with a rough guess for P(place) was already fairly high, however, I realized a semi-supervised approach might stand a good chance of being able to learn P(place, time). My final solution performed 20 semi-supervised iterations on the test data.\\r\\n\\r\\n[caption id=\"attachment_6025\" align=\"aligncenter\" width=\"735\"] The number of checkins over time varied quite irregularly for many places. A semi-supervised approach helped me overcome this irregularity.[/caption]\\r\\n\\r\\nGetting this to actually work well took some effort. There is more discussion in this thread.\\r\\n\\r\\nWere you surprised by any of your findings?\\r\\n\\r\\nAccuracy was quite mysterious at first. I initially focused on analyzing the relationship between accuracy and the uncertainty in the x coordinate and tried to incorporate that into my model. However, this helped only a tiny bit. I eventually came to the conclusion that accuracy is most gainfully employed directly by adding a factor P(accuracy | place): different places attract different mixes of accuracies. As suggested in the forums, this makes sense if one thinks of accuracy as a proxy for device type.\\r\\n\\r\\nAnother surprise was this: On the last day, I tried ensembling different initial guesses for P(place), but this improved the score only by 0.00001 over the best initial guess, which in turn was only 0.00015 better than the worst initial guess. Though I was disappointed to not be able to improve my score in this way (rushed experiments on a small validation set had looked a little more promising), this insensitivity to the initial guess is actually a good property of the solution. It speaks to the stability of convergence of the algorithm.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\n[caption id=\"attachment_6026\" align=\"aligncenter\" width=\"725\"] The (x, y) distributions of checkins looked multimodal. KDE or Gaussian Mixture Models were thus natural to try for learning P(x, y | place).[/caption]\\r\\n\\r\\nI used Python with the usual stack (pandas, matplotlib, seaborn, numpy, scipy, scikit-learn) for data exploration and for learning Gaussian Mixture Models for the P(x, y | place) distributions. The main model is written in C++. Finally, I used some bash scripts and the GNU parallel utility to automate parallel runs on slices of the data.\\r\\n\\r\\nHow did you spend your time on this competition?\\r\\n\\r\\nI spent a little time early on exploring the data, in particular doing case studies of individual places. After that, I spent almost all my time on implementing, optimizing, and tuning my custom algorithm.\\r\\n\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\n\\r\\nAside from one-time learning of Gaussian Mixture Models (which probably took around 40 hours), the run time was around 60 CPU hours. Since the problem parallelizes well, the non-GMM run time was about 15 hours on my laptop. For the last few days of the competition, I borrowed compute time on an 8-core workstation, where the run time ended up at around around 4-5 hours.\\r\\n\\r\\nIn this Github repository, I also posted a simplified single-pass version that would have gotten me to 6th place and that runs in around 90 minutes single-threaded on my laptop (excluding the one-time GMM training time). Compared to my full solution, this semi-supervised online learning version also has the nicer property of never using any information from the future.\\r\\n\\r\\n\\r\\nBio\\n\\r\\nMarkus Kliegl recently completed a PhD in Applied and Computational Mathematics at Princeton University. His current interests lie in machine learning research and applications.', 'From May to July 2016, over one thousand Kagglers competed in Facebook\\'s fifth recruitment competition: Predicting Check-Ins. In this challenge, Kagglers were required to predict the most probable check-in locations occurring in artificial time and space. As the first place winner, Tom Van de Wiele, notes in this winner\\'s interview, the uniquely designed test dataset contained about one trillion place-observation combinations, posing a huge difficulty to competitors. Tom describes how he quickly rocketed from his first getting started competition on Kaggle to first place in Facebook V through his remarkable insight into data consisting only of x,y coordinates, time, and accuracy using k-nearest neighbors and XGBoost. \\r\\n\\r\\nThe basics\\nWhat was your background prior to entering this challenge?\\r\\n\\r\\nI have completed two Master programs at two different Belgian universities (Leuven and Ghent), one in Computer Science (2010) and one in Statistics (2016). I graduated from the Statistics program during the Kaggle competition and was combining it with a full time job at a manufacturing plant at Eastman in Ghent during the past couple of years. Initially I started as an automation engineer and in a next phase I was mostly working on process improvements using the six sigma methodology. In the beginning of 2015 I got to the really good stuff when I started working with the data science group at Eastman where I am currently employed as an analytics consultant. We solve various complex analysis problems with an amazing team and mostly rely on R which we often combine with Shiny to develop interactive web applications.\\r\\n\\r\\n[caption id=\"attachment_6073\" align=\"aligncenter\" width=\"950\"] Tom Van de Wiele on Kaggle.[/caption]\\r\\n\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\n\\r\\nI have always had a passion for modeling complex problems and think that this mindset helped me to do well more than anything else. The problem setting is very tangible and all four predictors can be interpreted by anyone so that made it a very accessible contest where mobile data domain knowledge doesn’t really help. The problem can be translated to a classification setting with the only major complication that there are a large number of classes (>100K). I did however read a lot about other winning solutions prior to the contest. The learning from the best post on this blog has been especially useful.\\r\\n\\r\\nHow did you get started competing on Kaggle?\\r\\n\\r\\nThrough a Kaggle ‘Getting Started’ competition :) I was a passive user for a long time before entering my first competition. Like many others I wanted to compete one day but never really took the step to my first submission. Things changed when a colleague with a chemical engineering background wanted to get into machine learning and participated in the Kobe Bryant shot selection competition. He asked some great questions and I tried to point him into the right direction but his questions got me excited enough to download the data and implement my suggestions. Two evenings later I got close to the top 10 on the leaderboard at the time with about 500 participants and I started to dream about future competitions. That second evening was the launch date of the Facebook V competition and I wouldn’t have to dream for long!\\r\\n\\r\\nWhat made you decide to enter this competition?\\r\\n\\r\\nThe promise of a possible interview at Facebook was a strong motivation to participate although I considered it to be highly unlikely given that it was my first featured Kaggle competition and I already had a fully booked agenda. My second main motivation was the promise of learning new techniques and insights from other contestants. \\r\\n\\r\\nIn hindsight I am very happy to be interviewing at one of the best companies in the world for machine learning professionals but I am even more grateful for everything I learned from my own struggles and the other participants. A competition setting makes you think outside of the box and continuously challenge your approach. The tremendous code sharing on the forums was a great catalyst in this process.\\r\\n\\r\\nLet\\'s get technical\\r\\nExtended details of the technical approach can be found on my blog. The R code is available on my GitHub account along with high level instructions to construct the final submission.\\r\\n\\r\\nWhat was your general strategy?\\r\\n\\r\\nThe main difficulty of this problem is the extended number of classes (places). With 8.6 million test records there are about a trillion (10^12) place-observation combinations. Luckily, most of the classes have a very low conditional probability given the data (x, y, time and accuracy). The major strategy on the forum to reduce the complexity consisted of calculating a separate classifier for many x-y rectangular grids. It makes much sense to make use of the spatial information since this shows the most obvious and strong pattern for the different places. This approach makes the complexity manageable but is likely to lose a significant amount of information since the data is so variable. I decided to model the problem with a single stacked two level binary classification model in order to avoid to end up with many high variance models. The lack of any major spatial patterns in the exploratory analysis supports this approach.\\r\\n\\r\\nGenerating a single classifier for all place-observation combinations would be impractical even with a powerful cluster. My approach consists of a stepwise strategy in which the place probability (the target class) conditional on the data is only modeled for a set of place candidates. A simplification of the overall strategy is shown below:\\r\\n\\r\\n[caption id=\"attachment_6056\" align=\"aligncenter\" width=\"1024\"] The overall strategy used[/caption]\\r\\n\\r\\nThe given raw train data is split in two chronological parts, with a similar ratio as the ratio between the train and test data. The summary period contains all given train observations of the first 408 days (minutes 0-587158). The second part of the given train data contains the next 138 days and will be referred to as the train/validation data from now on. The test data spans 153 days as mentioned before.\\r\\n\\r\\nThe summary period is used to generate train and validation features and the given train data is used to generate the same features for the test data.\\r\\n\\r\\nThe three raw data groups (train, validation and test) are first sampled down into batches that are as large as possible but can still be modeled with the available memory. I ended up using batches of approximately 30,000 observations on a 48GB workstation. The sampling process is fully random and results in train/validation batches that span the entire 138 days’ train range.\\r\\n\\r\\nNext, a set of models using 430 numeric features is built to reduce the number of candidates to 20 using 15 XGBoost models in the second candidate selection step. The conditional probability P(place_match|features) is modeled for all ~30,000*100 place-observation combinations and the mean predicted probability of the 15 models is used to select the top 20 candidates for each observation. These models use features that combine place and observation measures of the summary period.\\r\\n\\r\\nThe same features are used to generate the first level learners. Each of the 100 first level learners are again XGBoost models that are built using ~30,000*20 feature-place_match pairs. The predicted probabilities P(place_match|features) are used as features of the second level learners along with 21 manually selected features. The candidates are ordered using the mean predicted probabilities of the 30 second level XGBoost learners.\\r\\n\\r\\nAll models are built using different train batches. Local validation is used to tune the model hyperparameters.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nI think I had a good insight into several of the accuracy related patterns. The accuracy distribution seems to be a mixed distribution with three peaks which changes over time. It is likely to be related to three different mobile connection types (GPS, Wi-Fi or cellular). The places show different accuracy patterns and features were added to indicate the relative accuracy group densities. The middle accuracy group was set to the 45-84 range. I added relative place densities for 3 and 32 approximately equally sized accuracy bins.\\r\\n\\r\\n[caption id=\"attachment_6053\" align=\"aligncenter\" width=\"799\"] Mean variation from the median in x versus 6 time and 32 accuracy groups.[/caption]\\r\\n\\r\\nIt was also discovered that the location is related to the three accuracy groups for many places. This pattern was captured by the addition of additional features for the different accuracy groups. \\r\\n\\r\\n[caption id=\"attachment_6055\" align=\"aligncenter\" width=\"1024\"]The x-coordinates seem to be related to the accuracy group for places like 8170103882.[/caption]\\r\\n\\r\\nStudying the places with the highest daily counts also pointed me towards obvious yearly patterns which were translated to valuable features. The green line in the image below goes back 52 weeks since the highest daily count.\\r\\n\\r\\n[caption id=\"attachment_6054\" align=\"aligncenter\" width=\"824\"]Clear yearly pattern for place 5872322184. The green line goes back 52 weeks since the highest daily count.[/caption]\\r\\n\\r\\nWere you surprised by any of your findings?\\r\\n\\r\\nThe strength of K nearest neighbors was remarkable in this problem. Nearest neighbor features make up a large share of my solution and the leading public script relied on the K nearest neighbor classifier. I was also surprised that I couldn’t find clear spatial patterns in the data (e.g. a party district). \\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nAll code was implemented in R and I created an Rcpp package to address the major bottleneck using C++. The most important package I used was by far the data.table package. I was not familiar with the syntax heading into the competition but going through the trouble of learning it enabled me to handle the dimensions of the problem. Other critical tools are the xgboost package and the doParallel package. The exploratory data analysis lead to a Shiny application which was shared with the other participants.\\r\\n\\r\\nHow did you spend your time on this competition?\\r\\n\\r\\nI was forced to spend the first 10 days of the competition thinking about possible high level approaches due to other priorities and ended up with an approach that strongly resembled my final framework. The next 10 days were used to generate about 50 features and build the framework except for the second level learners. This intermediate result got me to the first spot on the public leaderboard and encouraged me to expand the feature set. I spent most of the remaining time on detailed feature engineering and started building the second tier of the binary classifier three weeks before the end of the contest. The last two weeks were mostly dedicated to hyperparameter optimization.\\r\\n\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\n\\r\\nRunning all steps to train the model and generate the final submission would take about a month on my 48GB workstation. That seems like a ridiculously long time but it is explained by the extended computation time of the nearest neighbor features. While calculating the NN features I was continuously working on other parts of the workflow so speeding the NN logic up would not have resulted in a better final score. \\r\\n\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\r\\n\\r\\nI learned a lot from the technical issues I ran into but have learned most from the discussions on the forum. It is great to learn from brilliant people like Markus. The way he used semi-parametric learning to learn from the future was an eye-opener. Many others made significant contributions but it was especially useful to learn from Larry Freeman and Ben Hamner that we are better when we work together. An ensemble of top solutions can do much better than my winning submission!\\r\\n\\r\\n[caption id=\"attachment_6060\" align=\"aligncenter\" width=\"871\"] Private leaderboard score (MAP@3) - two teams stand out from the pack.[/caption]\\r\\n\\r\\nDo you have any advice for those just getting started in data science?\\r\\n\\r\\nI would suggest to start with a study of various data science topics. Andrew Ng’s course is an excellent place to start. Getting your hands dirty with appropriate feedback is the next step if you want to get better. Kaggle is of course an excellent platform to do so. I am very impressed with the quality and general atmosphere on the forum and would suggest everyone to start competing!\\r\\n\\r\\nBio\\n\\nTom Van de Wiele recently completed his master of statistical data analysis at the University of Ghent. Tom has a background in computer science engineering and works in the data science group of Eastman as an Analytics Consultant where he works on various complex data challenges. His current interests lie in applied machine learning and statistics.', 'The Facebook recruitment challenge, Predicting Check Ins, ran from May to July 2016 attracting over 1,000 competitors who made more than 15,000 submissions. Kagglers competed to predict a ranked list of most likely check-in places given a set of coordinates. Using just four variables, the real challenge was making sense of the enormous number of possible categories in this artificial 10km by 10km world. The third place winner, Ryuji Sakata, AKA Jack (Japan), describes in this interview how he tackled the problem using just a laptop with 8GB of RAM and two hours of run time.\\r\\n\\r\\nThe basics\\nWhat was your background prior to entering this challenge?\\r\\n\\r\\nI\\'m working as a data scientist at an electronics company in Japan. My major at university (Aeronautics and Astronautics) is actually far from my current job, and I didn\\'t have knowledge about data science at all. I got to know this field after starting work, and keep learning until now.\\r\\n\\r\\n[caption id=\"attachment_6068\" align=\"aligncenter\" width=\"950\"] Ryuji Sakata, AKA Jack (Japan) on Kaggle.[/caption]\\r\\n\\r\\nHow did you get started competing on Kaggle?\\r\\n\\r\\nI joined Kaggle about two and a half years ago in order to learn data mining in practice. The first competition I attended was Allstate Purchase Prediction Challenge and then I realized the fun of data mining. Since then, I spent much of my spare time on Kaggle. Competing with other Kagglers is always very exciting and it has been a good learning experience!\\r\\n\\r\\nWhat made you decide to enter this competition?\\r\\n\\r\\nThis competition seemed to be something special compared with other challenges. The objective variable we had to predict has so many categories, and the number of variables we can use to predict is only 4! I was attracted to the concept that this competition was testing Kaggler\\'s ingenious ideas.\\r\\n\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nMy basic idea is very similar to that of Markus Kliegl as written in his Winner\\'s Interview, which is Naive Bayes approach.\\r\\n\\r\\n\\r\\n\\r\\nWhat I had to do is to find place which maximizes above probability. Brief steps of my approach are as follows:\\r\\n\\r\\n\\nTo narrow down candidate places for each record, I counted check-ins of each place using 100 x 200 grid, and places which have at least 2 check-ins are treated as candidates of each cell.\\nEstimate p(place_id) from past trend of check-in frequency by xgboost regression (refer to Figure A).\\nEstimate each distribution by using histogram (refer to Figure B).\\nCalculate the product of probabilities of all candidates for each record.\\nSelect top 3 places which have high probability and create submission.\\n\\r\\n\\r\\nRegarding the second step, it seemed that time variation of check-in frequency is quite different between places, but I believe that some patterns of trend exists. So I decided to predict the number of future check-ins of each place from history of all places. The concept is as shown in the figure below.\\r\\n\\r\\n[caption id=\"attachment_6064\" align=\"aligncenter\" width=\"1024\"] Figure A[/caption]\\r\\n\\r\\nRegarding the third step, I also illustrated the concept of estimation in the figure below.\\r\\n\\r\\n[caption id=\"attachment_6065\" align=\"aligncenter\" width=\"1024\"] Figure B[/caption]\\r\\n\\r\\nThere are some additional notes regarding the above figure:\\r\\n\\r\\n\\nAfter counting check-ins for each bin, counted values are smoothed by using neighbor bins.\\nAbout \"x\", \"y\", and \"accuracy\", the concept of time decay was introduced by multiplying weight when counting check-ins in the following manner.\\n\\nAbout \"y\", it seemed that distribution follows a normal distribution for many places, so I also estimated the center and standard deviation of places and calculated the probability from that.\\nAbout \"time of day\", \"day of week\", and \"accuracy\", small constant (pseudocount) is added after counting check-ins in order to avoid probability becoming zero.\\n\\r\\n\\r\\nSimple version of my solution is uploaded on my kernel here.\\r\\n\\r\\nHow did you spend your time on this competition?\\r\\n\\r\\nThe approach was almost fixed in the early stage of competition, and I conducted trials and errors many and many times to maximize the local validation score. I spent much time to optimize hyperparameters to estimate distributions, such as:\\r\\n\\r\\n\\nThe number of bins for each variable\\nHow to smooth the distribution (number of neighbors and weight)\\nHow to decay the counting weight according to time elapse\\nWhat pseudocount should be added\\n\\r\\n\\r\\nAt the end of competition, however, I couldn’t improve the score by changing the above parameters, so I shifted to improving the precision of p(place_id).\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nI always use R for Kaggle competitions just because I am familiar with it. However, I would like to master python too in future.\\r\\n\\r\\nWhat was the run time for both training and prediction of your winning solution?\\r\\n\\r\\nMy solution takes only about 2 hours by 8GB RAM laptop, and maybe this is the most prominent feature of my approach. Some other competitors seemed to apply xgboost for many small cells, and took very long time, but it was quite unbearable to me! I decided to focus on achieving both accuracy and speed. Thanks to that, I can conduct the trial and error method so many times.\\r\\n\\r\\nWords of wisdom\\nWhat have you taken away from this competition?\\r\\n\\r\\nThrough this great competition, I gained a little more confidence in this field, but at the same time, I was surprised by other Kagglers\\' ideas which I would have never came up with. From this experience, I realized there are still many things I have to learn. So I would like to keep learning.\\r\\n\\r\\nDo you have any advice for those just getting started in data science?\\r\\n\\r\\nI am still a beginner in data science, but if there is one thing to say, “What one likes, one will do well.” For me, it is Kaggle. I always enjoy competing and communicating with other Kagglers!\\r\\n\\r\\nBio\\nRyuji Sakata works at Panasonic group as a data scientist. He has been involved in data science for about 3 years. He holds a master\\'s degree in Aeronautics and Astronautics at Kyoto University.\\r\\n', 'The Draper Satellite Image Chronology Competition (Chronos) ran on Kaggle from April to June 2016. This competition, which was novel in a number of ways, challenged Kagglers to put order to time and space. That is, given a dataset of satellite images taken over the span of five days, the 424 brave competitors were required to determine their correct order. The challenge, which Draper hosted in order to contribute to a deeper understanding of how to process and analyze images, was a first for Kaggle--it allowed hand annotation as long as processes used were replicable.\\r\\n\\r\\nWhile the winners of the competition used a mixture of machine learning, human intuition, and brute force, Damien Soukhavong (Laurae), a Competitions and Discussion Expert on Kaggle, explains in this interview how factors like the limited number of training samples which deterred others from using pure machine learning methods appealed to him. Read how he ingeniously minimized overfit by testing how his XGBoost solution generalized on new image sets he created himself. Ultimately his efforts led to an impressive leap in 242 positions from the public to private leaderboard.\\r\\n\\r\\n\\nThe basics\\nWhat was your background prior to entering this challenge?\\r\\n\\r\\nI hold an MSc in Auditing, Management Accounting, and Information Systems. I am self-taught in Data Science, Statistics, and Machine Learning since 2010. This autodidactism also helped me earn my MSc with distinctions with a thesis on data visualization. I worked with data for about 6 years from now. Although data science can be technical, I feel more with a creative and designing mind than a technical mind!\\r\\n\\r\\n[caption id=\"attachment_6147\" align=\"aligncenter\" width=\"948\"] Damien Soukhavong (Laurae) on Kaggle.[/caption]\\r\\n\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\n\\r\\nI love manipulating images and generating code to process them, along with helping researchers making reproducible research in the image manipulation field. It helped me to look at convenient features that may at least generalize on unknown images unrelated to this competition.\\r\\n\\r\\nHow did you get started competing on Kaggle?\\r\\n\\r\\nI found Kaggle randomly when a laboratory researcher asked me to look up for online competitions involving data. Kaggle delighted me quickly, as they provide different competition datasets, along with a convenient interface, straightforward to use even for newcomers. There are even tutorial competitions, which are like fast introductions to data science and machine learning techniques to get you productive immediately.\\r\\n\\r\\nWhat made you decide to enter this competition?\\r\\n\\r\\nThree reasons that may look as disadvantages to enter this competition:\\r\\n\\r\\n\\nIt is an image-based competition, requiring preprocessing of images, along the selection of features which are not apparent at first sight.\\nIt is about ordering images in time with a tiny dataset. Hence, one would not throw a Deep Learning model out of the box and expect it to work.\\nOverfitting is an issue \"thanks to\" the 70 training sets provided: I personally like leakage and overfitting issues, as fighting them is like avoiding the mine in a minefield (think: Minesweeper).\\n\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nA visual overview of my methods is on the following picture:\\r\\n\\r\\n[caption id=\"attachment_6135\" align=\"aligncenter\" width=\"1754\"] A visual overview of methods used. Click to open up a larger view.[/caption]\\r\\n\\r\\nFor the preprocessing method, I started by registering and masking each set of images, so it aligns them and so they contain the information that are mutual on all the image of a same set. Then, I used a feature extraction technique to harvest general describing features from each image. I generated all the permutations of each set of five images along with their theoretical performance metric, raising the training sample size to 8400 (120 permutations per set, 72 sets), and reducing the overfitting issue to a very residual issue. This also turned the (5-class) ranking problem into a regression problem.\\r\\n\\r\\n                       var divElement = document.getElementById(\\'viz1472412374932\\');                    var vizElement = divElement.getElementsByTagName(\\'object\\')[0];                    vizElement.style.width=\\'100%\\';vizElement.style.height=(divElement.offsetWidth*0.75)+\\'px\\';                    var scriptElement = document.createElement(\\'script\\');                    scriptElement.src = \\'https://public.tableau.com/javascripts/api/viz_v1.js\\';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                \\r\\nFor the supervised learning method, I used Extreme Gradient Boosting (XGBoost) with a custom objective and custom evaluation function.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\n\\r\\nHand labeling images was easy once you trained yourself to recognize the related features (think: the neural network in your brain), and I could go for a (near) perfect score if I wanted to. However, my interest was to use pure machine learning techniques, which are generalizing on unknown samples. Thus, I did not explore the manual way for too long.\\r\\n\\r\\nA simple example for hand-labeling using objects: \\r\\n\\r\\n[caption id=\"attachment_6144\" align=\"aligncenter\" width=\"899\"] Simple example for hand-labeling.[/caption]\\r\\n\\r\\nI made a tutorial for recognizing the quantifying the apparition and removal of objects: \\r\\n\\r\\n[caption id=\"attachment_6141\" align=\"aligncenter\" width=\"1024\"] A tutorial shared on the competition forums.[/caption]\\r\\n\\r\\nWere you surprised by any of your findings?\\r\\n\\r\\nI have found several interesting findings that surprised me:\\r\\n\\r\\n\\nLeave-one-out cross-validation (LOOCV) might be a good method to handle the tiny training set to validate a supervised machine learning model, however some image sets are leaking into others which make the cross-validation invalid right at the beginning!\\nUsing the file size of pictures was leaking information out of the box… I believe one can get 0.30 (or more) using only the image file sizes with a simple predictive model.\\nWorking with JPEG (1GB) pictures instead of TIFF (32GB) worked better than expected (I was expecting around 0.20 only).\\nUsing a scoring model from the three highest scoring predictions gave a light boost on the performance metric. However, using predicted scores from all the 120 permuted sets per set to create the scoring model was giving out worse results than random predictions (overfitting).\\n\\r\\n\\r\\nThis shows the model used can generalize to Draper\\'s test sets. However, one must test the model in a real situation. Does it generalize? To test this hypothesis, I took two different image sets:\\r\\n\\r\\n\\n20 random locations in Google Earth where the satellite images were crystal clear, and day-to-day (100 pictures, 20 sets in total)\\n250 pictures from different areas I took myself when commuting during workdays (250 pictures, 50 sets)\\n\\r\\n\\r\\nOn Google Earth, computing the performance metric from my predictions gave a score of 0.112, which is better than pure randomness. There is a slight bias towards a good prediction, though. Having access to the predictions and to the right order of the pictures, I assumed a uniform distribution of the predictions and ran a Monte-Carlo simulation over it. After 100,000 trials, here are the results:\\r\\n\\r\\n[caption id=\"attachment_6137\" align=\"aligncenter\" width=\"486\"] Results of Monte-Carlo simulation.[/caption]\\r\\n\\r\\nI will not say it is bad, but it is not that good either. The mean is 0.09, while the standard deviation is 0.07. Assuming a normal distribution, we have about 90% chance to predict better than random numbers. Reaching 0.442 (the maximum after 100,000 random tries) is clearly not straightforward.\\r\\n\\r\\nNow coming for my personal pictures, I got... 0.048 only, which is disappointing. To ensure this is not a mistake, I ran the same Monte-Carlo simulation I used previously but on the new predictions:\\r\\n\\r\\n[caption id=\"attachment_6136\" align=\"aligncenter\" width=\"486\"] Results of second Monte-Carlo simulation[/caption]\\r\\n\\r\\nClearly, having 64% chance to predict better than random is on the low-end. The mean is 0.02, and the standard deviation is 0.07. With more samples to train the model on, overfitting should be less an issue than it is on non-aerial imagery.\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nI used macros in ImageJ for the feature extraction (more specifically: Fiji version, a supercharged ImageJ version), and R + XGBoost for the final preprocessing and supervised learning. When using XGBoost, I used my custom objective and evaluation functions to not only get the global performance metric, but also account for the ranking of predictions in the set (for voting which picture order is the most probable).\\r\\n\\r\\nXGBoost was in version 0.47. The current XGBoost version 0.60 refuses to run my objective and evaluation functions properly.\\r\\n\\r\\nHow did you spend your time on this competition?\\r\\n\\r\\nI spent about 20% of the time reading the threads on the forum, as they are a wealth of information you may not find yourself. 60% of the time was spent on preprocessing and feature engineering, and the 20% left on predictive modeling, validating, and submitting.\\r\\n\\r\\nI also tested different models and features after my first idea (ironically, it consumed over 1 day):\\r\\n\\r\\n\\nDeep Learning gives no result, as it predicts random numbers and never seems converge even after 500 epochs (I tried data augmentation, image manipulation, and many architectures such as CaffeNet, GoogLeNet, VGG-16, ResNet-19, ResNet-50…).\\nNeural networks on the file sizes… are incredible as they abuse leakage. They will not generalize on a real scenario.\\nRandom Forests are… overfitting severely and hard to control. They do overfit with enough noise, and this dataset is a good one for such issue (in pictures there can be… clouds!).\\nData Visualization using Tableau, to lookup for the best interactions between features. It is clearly hard to notice the interactions, although XGBoost managed a great score for such hard task.\\n\\nWhat was the run time for both training and prediction of your solution?\\r\\n\\r\\nEarly on entering the competition, I set a macro in ImageJ very quickly to extract features and to save them in a CSV file. It took about 1 hour, which allowed me to setup the final preprocessing and XGBoost code properly in R. Afterwards, it took only 30 minutes to make the code work properly, preprocess the data, train the model, and make my first submission. I spent 10 more minutes to cross-validate using five targeted folds by categorizing pictures by theme, and make a new submission (that was slightly better than the former).\\r\\n\\r\\nIn total, this took me about 1 hour and a half. Knowing my cross-validation method was correct, I did not care about seeing such low performing score on the public leaderboard with only 17% of testing samples (thanks to the forum threads). This ensured a push by over 242 places from the public to the private leaderboard (ending 32nd), the former being a sample feedback on unknown samples, and the latter being the one where we must maximize our performance (but we cannot see this private leaderboard until ending of the competition).\\r\\n\\r\\n\\nWords of wisdom\\nThe future of satellite imagery and technology will allow us to revisit locations with unprecedented frequency. Now that you\\'ve participated in this competition, what broader applications do you think there could be for images such as these?\\r\\n\\r\\nThere are many applications for satellite imagery and its associated technology. As a spatial reconstruction on a time dimension, one may:\\r\\n\\r\\n\\nAnalyze the land usage over time\\nPlan the development of urban and rural areas\\nManage resources and disasters in a better way (like analyzing the damage of a fire)\\nWork on a very large database usable for virtual reality (so you can feel how San Francisco looks from your own town for instance)\\nInitiate statistical studies\\nRegulate the environment (in a legal way)\\nPinpoint tactical and safe areas for humans (moving hospitals, etc.)\\n\\r\\n\\r\\nCurrently, there are satellites capable of recording things we cannot see as humans. One of the most known is Landsat, whose pictures are usable for finding minerals or many other diverse elements (gas, oil...). Using data science and machine learning should give a hefty boom in predictive modeling from satellite imagery for businesses.\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\n\\r\\nThe benefits I have taken from this competition were: \\r\\n\\r\\n\\nWorking with a tiny training set, as there were only 70 training sets.\\nFighting overfitting, as learning observations is easier than learning to generalize with such tiny training set!\\nRationally transforming the ranking problem into a regression problem, a skill that requires regular practice and smart ideas.\\n\\r\\n\\r\\nA minor benefit was using ImageJ not for pure research, but for a data science competition. I was not expecting to use it at all. I provided an example starter script for using ImageJ in this competition:\\r\\n\\r\\n[caption id=\"attachment_6140\" align=\"aligncenter\" width=\"866\"] Code from starter script for using ImageJ shared as a competition kernel.[/caption]\\r\\n\\r\\nDo you have any advice for those just getting started in data science?\\r\\n\\r\\nSeveral key pieces of advice for newcomers in data science:\\r\\n\\r\\n\\nI cannot say it enough times: efficiency is key.\\nAnother important piece of advice: learn to tell stories (storytelling). Non-technical managers will not care about \"but my XGBoost model had 92% accuracy!\" when they will ask you the value to get from such model in their business environment. A good story is worth thousands of words in less than ten seconds!\\n\\r\\n\\r\\nMore specific advice about predictive modeling for newcomers:\\r\\n\\r\\n\\nKnowing your features and understanding how to validate your models, allows you to fight overfitting and underfitting appropriately.\\nUnderstanding the \"business\" behind what you are doing in data science when working with a dataset has high value: domain knowledge is a starting key to success.\\nCreating (only) (good) predictive models in a business environment does not mean you are a good data scientist: statistical and business knowledge must be learnt a way or another.\\nThinking you can go the XGBoost way in any industry is naïve. Try to explain all the interactions caught between [insert 100+ feature names] in a 100+ tree XGBoost model.\\nYour Manager will not like seeing you lurk for improving your model accuracy from 55.01% to 55.02% during one week of work. Where is the value?\\n\\r\\n\\r\\nN.B: Many ideas you may have can fail miserably. It is valuable to be brave and scrape what you did to start from scratch. Otherwise, you might stick forever the wrong way in a specific project, trying to push something you cannot push any further. Learning from mistakes is a key factor for self-improvement.\\r\\n\\r\\nFor pure supervised machine learning advice, three elements to always keep an eye on:\\r\\n\\r\\n\\nChoosing the appropriate way to validate a model depending on the dataset, and this comes with experience.\\nLooking up for potential leakage, as it may be what invalidates your model at the end when dealing with unknown samples.\\nEngineering features pushes your score higher than tuning hyperparameters in 99.99% of cases, unless you are using a horrible combination of hyperparameters right at the beginning.\\n\\nJust for fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\n\\r\\nI would run a data compression problem. It would open the way for finding novel methods to optimize the loss of information to a minimum, while decreasing the feature count to a bare minimum, using supervised methods.\\r\\n\\r\\nWhat is your dream job?\\r\\n\\r\\nBeing a data science evangelist and managing talents!\\r\\n\\r\\nBio\\n\\nDamien Soukhavong is a data science and an artificial intelligence trainer. Graduated from an MSc in Auditing, Accounting Management, and Information Systems, he seeks the maximization of value of data in companies using data science and machine learning, while looking for efficiency in performance. He mentors individually creative professionals and designers who are investing time to push data science for their daily creative/design job.', 'The Avito Duplicate Ads Detection competition ran from May to July 2016. This competition, a feature engineer\\'s dream, challenged Kagglers to accurately detect duplicitous duplicate ads which included 10 million images and Russian language text. In this winners\\' interview, Stanislav Semenov and Dmitrii Tsybulevskii describe how their single XGBoost model scores among the top three and their ensemble snagged them first place. Stanislav\\'s third Avito competition was a special one, too; his first place win as part of Devil Team boosted him to #1 Kaggler status!\\r\\n\\r\\nThe basics:\\nWhat was your background prior to entering this challenge?\\nDmitrii Tsybulevskii: I hold a degree in Applied Mathematics, and I’ve worked as a software engineer on computer vision and machine learning projects.\\r\\n\\r\\n[caption id=\"attachment_6109\" align=\"alignright\" width=\"950\"] Dmitrii Tsybulevskii on Kaggle.[/caption]\\r\\n\\r\\nStanislav Semenov: I hold a Master\\'s degree in Computer Science. I\\'ve worked as a data science consultant, teacher of machine learning classes, and quantitative researcher.\\r\\n\\r\\n[caption id=\"attachment_6108\" align=\"alignright\" width=\"949\"] Stanislav Semenov on Kaggle.[/caption]\\r\\n\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\nDmitrii Tsybulevskii: Yes, I’ve worked on image duplicate detection and text classification problems before, and I know the Russian language.\\r\\n\\r\\nStanislav Semenov: This is my 3rd Avito competition on Kaggle! And yes, I know the Russian language, too.\\nWhat made you decide to enter this competition?\\nDmitrii Tsybulevskii: A lot of raw data, both text and images - large field for feature engineering, and I like feature engineering.\\r\\n\\r\\nStanislav Semenov: A large area for feature engineering.\\nLet’s get technical:\\nWhat preprocessing and supervised learning methods did you use?\\nIt was all about feature engineering. So we tried to generate as many strong features as we could. XGBoost was the only learning method used. Our single XGBoost model can get to the top three! Our final model just averaged XGBoost models with different random seeds.\\r\\n\\r\\nWe used following text preprocessing:\\r\\n\\nstemming\\nlemmatization\\ntransliteration\\n\\r\\n\\r\\nOur features:\\r\\n\\r\\n\\ndifferent similarity features between title-title, title-description, title-json like Cosine distance, Levenshtein, Jaccard, NCD, etc\\ndifferent features of exact match of words in title, description\\ngeneral features such as prices, places, number of images, exact match of title, description, etc\\ndifferent similarity features of trained w2v models\\nLSI features of ads union, ads XOR\\none-hot-encoding of categoryID\\nratios of the title, description, json lengths\\ndistances between BRIEF image descriptors\\ndistances between color histogram in LAB space, HOG histograms\\ndistances between features, extracted with pretrained neural network MXNet BN-Inception-21k, first averaged PCA components of this features\\nnumber of matches computed with AKAZE local visual feature detector & descriptor\\n\\nThe most important trick was to submit our best result before 2 hours of competition ending. That was EXTREMELY fun! =)\\nDid knowing Russian help you in this competition? If so, how?\\nStanislav Semenov: Not so much. Of course, you can see where your model is wrong and a close look at the ads. But it did not give any new information.\\r\\n\\r\\nDmitrii Tsybulevskii: On the one hand it was comfortable to work with Russian texts, because you know what the ads were about. On the other hand we had no killer features based on it.\\nWhich tools did you use?\\nJupyter Notebook, XGBoost, Pandas, scikit-learn, VLFeat, OpenCV, MXNet\\nWhat was the run time for your winning solution?\\nFeature extraction: 3-4 days\\r\\nModel training: 1-2 weeks\\nWords of wisdom:\\nWhat have you taken away from this competition?\\nDmitrii Tsybulevskii: I have learned about NCD distance and some convenient things about team cooperation.\\r\\n\\r\\nStanislav Semenov: A lot of fun and much needed ranking points. ;)\\nDo you have any advice for those just getting started in data science?\\nStanislav Semenov: Solving practical problems is your best friend.\\r\\n\\r\\nDmitrii Tsybulevskii: Kaggle is a great platform for getting new knowledge.\\nBio\\nStanislav Semenov is a Data Scientist and Quantitative Researcher.\\r\\n\\r\\nDmitrii Tsybulevskii is a software engineer. He holds a degree in Applied Mathematics. His main interests are computer vision and machine learning.', 'The Avito Duplicate Ads competition ran on Kaggle from May to July 2016. Over 600 competitors worked to feature engineer their way to the top of the leaderboard by identifying duplicate ads based on their contents: Russian language text and images. TheQuants, made up of Kagglers Mikel, Peter, Marios, & Sonny, came in second place by generating features independently and combining their work into a powerful solution. \\r\\n\\r\\nIn this interview, they describe the many features they used (including text and images, location, price, JSON attributes, and clustered rows) as well as those that ended up in the \"feature graveyard.\" In the end, a total of 587 features were inputs to 14 models which were ensembled through the weighted rank average of random forest and XGBoost models. Read on to learn how they cleverly explored and defined their feature space to carefully avoid overfitting in this challenge.\\r\\n\\r\\nThe basics\\nWhat was your background prior to entering this challenge?\\nMikel Bober-Irizar: Past Predictive modelling competitions, financial predictions and medical diagnosis.\\nPeter Borrmann: Ph.D. in theoretical physics, research assistant professor as well as previous Kaggle experiences.\\nMarios Michailidis: I am a Part-Time PhD student at UCL , data science manager at dunnhumby and fervent Kaggler.  \\nSonny Laskar: I am an Analytics Consulting Manager with Microland working on implementing Big Data Solutions; mostly dealing with IT Operations data.\\nHow did you get started with Kaggle?\\nMikel Bober-Irizar: I wanted to learn about machine learning and use that knowledge to compete in competitions.\\nPeter Borrmann: I wanted to improve my skillset in the field.\\nMarios Michailidis: I wanted a new challenge and learn from the best.\\nSonny Laskar: I got to know about Kaggle few years back when I was pursuing my MBA.\\r\\n\\r\\n[caption id=\"attachment_6119\" align=\"aligncenter\" width=\"889\"] The Quants Team[/caption]\\r\\n\\r\\nSummary\\nOur approach to this competition was divided into several parts :\\n\\n\\n Merging early based on standing of the leaderboard.\\n Generate features independently (on cleaned or raw data) that would potentially capture the similarity between the contents of 2 ads and could be further divided to more categories (like text similarities or image similarities).\\n Build a number of different classifiers and regressors independently with a hold out sample. \\n Combine all members\\' work\\n Ensemble the results through weighted rank average of a 2-layer meta-model network ( StackNet).\\n\\r\\n\\r\\n\\r\\n[caption id=\"attachment_6118\" align=\"aligncenter\" width=\"587\"]Approach summary.[/caption]\\r\\n\\r\\nData Cleaning and Feature Engineering\\nData Cleaning\\nIn order to clean the text, we applied stemming using the NLTK Snowball Stemmer, and removed stopwords/punctuation as well as transforming to lowercase. In some situations we also removed non-alphanumeric characters too. \\nFeature Engineering vol 1: Features we actually used  \\nIn order to pre-emptively find over-fitting features, we built a script that looks at the changes in the properties (histograms and split purity) of a feature over time, which allowed us to quickly (200ms/feature) identify overfitting features without having to run overnight XGBoost jobs.\\nAfter removing overfitting features, our final feature space had 587 features derived from different themes:\\nGeneral: \\n\\nCategoryID, parentCategoryID raw CategoryID, parentCategoryID one-hot (except overfitting ones).\\nPrice difference / mean.\\nGeneration3probability (output from model trained to detect generationmethod=3).\\n\\nLocation: \\n\\nLocationID & RegionID raw.\\nTotal latitude/longtitude.\\nSameMetro, samelocation, same region etc.\\nDistance from city centres (Kalingrad, Moscow, Petersburg, Krasnodar, Makhachkala, Murmansk, Perm, Omsk, Khabarovsk, Kluichi, Norilsk)\\r\\n\\t\\tGaussian noise was added to the location features to prevent overfitting to specific locations, whilst allowing XGBoost to create its own regions.\\n\\nAll Text: \\n\\nLength / difference in length.\\nnGrams Features (n = 1,2,3) for title and description (Both Words and Characters).\\n\\nCount of Ngrams (#, Sum, Diff, Max, Min).\\nLength / difference in length.\\nCount of Unique Ngrams.\\nRatio of Intersect Ngrams.\\nRatio of Unique Intersect Ngrams.\\n\\nDistance Features between the titles and descriptions:\\n\\n Jaccard \\n Cosine \\n Levenshtein  \\n Hamming \\n\\nSpecial Character Counting & Ratio Features:\\n\\nCounting & Ratio features of Capital Letters in title and description.\\nCounting & Ratio features of Special Letters (digits, punctuations, etc.) in title and description.\\n\\nSimilarity between sets of words/characters.\\nFuzzywuzzy distances.\\n jellyfish  distances.\\nNumber of overlapping sets of n words (n=1,2,3).\\nMatching moving windows of strings.\\nCross-matching columns (eg. title1 with description2).\\n\\nBag of words: \\nFor each of the text columns, we created a bag of words for both the intersection of words and the difference in words and encoded these in a sparse format resulting in ~80,000 columns each. We then used this to build Naive Bayes, SGD and similar models to be used as features.\\nPrice Features: \\n\\nPrice Ratio.\\nIs both/one price NaN.\\nTotal Price.\\n\\nJSON Features: \\n\\nAttribute Counting Features.\\n\\nSum, diff, max, min.\\n\\nCount of Common Attributes Names.\\nCount of Common Attributes Values.\\n Weights of Evidence  on keys/values, XGBoost model on sparse encoded attributes.\\n\\nImage Features: \\n\\n# of Images in each Set.\\nDifference Hashing of images.\\nHamming distance between each pair of images.\\nPairwise comparison of file size of each image.\\nPairwise comparison of dimension of each image.\\nBRISK keypoint/descriptor matching.\\nImage histogram comparisons.\\nDominant colour analysis.\\nUniqueness of images (how many other items have the same images).\\nDifference in number of images.\\n\\nClusters: \\n\\r\\nWe found clusters of rows by grouping rows which contain the same items (eg. if row1 has items 123, 456 and row2 has items 456, 789 they are in the same cluster). We discovered that the size of these clusters was a very good feature (larger clusters were more likely to be non-duplicates), as well as the fact that clusters always the same generationMethod. Adding cluster-size features gave us a 0.003 to 0.004 improvement.\\r\\n\\nFeature Engineering vol 2 : The ones that did not make it\\n Overfitting was probably the biggest problem throughout the competition, and lots of features which (over)performed in validation didn\\'t do so well on the leaderboard. This is likely because the very powerful features learn to recognise specific products or sellers that do not appear in the test set. Hence, a feature graveyard was a necessary evil.\\nTF-IDF: \\nThis was something we tried very early into the competition, adapting our code from the Home Depot competition. Unfortunately, it overfitted very strongly, netting us 0.98 val-auc and only 0.89 on LB. We tried adding noise, reducing complexity, but in the end we gave up. \\nWord2vec: \\nWe tried both training a model on our cleaned data and using the pretrained model posted in the forums. We tried using word-mover distance from our model as features, but they were rather weak (0.70AUC) so in the end we decided to drop these for simplicity. Using the pre-trained model did not help, as the authors used MyStem for stemming (which is not open-source) so we could not replicate their data cleaning. After doing some transformations on the pre-trained model to try and make it work with our stemming (we got it down to about 20% missing words), it scored the same as our custom word2vec model.\\nAdvanced cluster features: \\nWe tried to expand the gain from our cluster features in several ways. We found that taking the mean prediction for the cluster as well as cluster_size * (1-cluster_mean) provided excellent features in validation (50% of gain in xgb importance), however these overfitted. We also tried taking features such as the standard deviation of locations of items in a cluster, but these overfitted too.\\nGrammar features: \\nWe tried building features to fingerprint different types of sellers, such as usage of capital letters, special characters, newlines, punctuation etc. However while these helped a lot in CV, they overfitted on the leaderboard.\\nBrand violations: \\nWe built some features based around words that could never appear together in duplicate listings. (For example, if one item wrote \\'iPhone 4s\\' but the other one wrote \\'iPhone 5s\\', they could not be duplicates). While they worked well at finding non-duplicates, there were just too few cases where these violations occurred to make a difference to the score.\\nValidation Scheme\\nInitially, we were using a random validation set before switching to a set of non-overlapping items, where none of the items in the valset appeared in the train set. This performed somewhat better, however we had failed to notice that the training set was ordered based on time! We later noticed this (inspired by this post)  and switched to using last 33% as a valset.\\r\\n\\r\\n[caption id=\"attachment_6121\" align=\"aligncenter\" width=\"308\"] Validation scheme.[/caption]\\r\\n\\r\\n\\r\\n This set correlated relatively well with the leaderboard until the last week, when we were doing meta-modelling and it fell apart - at a point where it would be too much work to switch to a better set. This hurt us a lot towards the end of the competition. \\nModelling\\nModelling vol 1 : The ones that made it\\n In this section we built various models (classifiers and regressors) on different input data each time (since the modelling process was overlapping with the feature engineering process. All models were training with the first 67% of the training data and validated on the remaining 33%. All predictions were saved (so that they can be used later for meta modelling. The most dominant models were: \\nXGBoost: \\n Trained with all 587 of our final features with 1000 estimators, maximum depth equal to 20 and minimum child of 10, and particularly high Eta (0.1) - bagged 5 times. We also replaced nan values with -1 and Infinity values with 99999.99. It scored 0.95143 on private leaderboard. Bagging added 0.00030 approximately.\\nKeras: Neural Network\\n Trained with all our final features, transformed with standard scaler as well as with logarithm plus 1, where all negative features have been replaced with zero. The main architecture involved 3 hidden layers with 800 hidden units plus 60% dropout. The main activation function was Softmax and all intermediate ones were standard rectifiers (Relu). We bagged it 10 times. It scored 0.94912 on private leaderboard. It gave +0.00080-90 when rank-averaged with the XGBoost model\\nModelling vol 2: The ones that didn\\'t \\n We build a couple of deeper Xgboost models with higher Eta (0.2) that although performed well in cv, they overfitted the leaderboard.\\n We used a couple of models to predict generation method in order to use that as feature for meta-modelling but it did not add anything so we removed it.\\nMeta-Modelling\\nThe previous modelling process generated 14 different models including linear models as well as XGBoosts and NNs, that were later used for meta-modelling \\n For validation purposes we splitted the remaining (33%) data again into 67-33 in order to tune the hyper parameters of our meta-models that used as input the aforementioned 14 models.  Sklearn\\'s Random Forest which performed slightly better than XGBoost (0.95290 vs 0.95286).  Their rank average yielded our best Leaderboard score of  0.95294\\nThe Modelling and Meta-Modelling process is also illustrated below : \\r\\n\\r\\n[caption id=\"attachment_6120\" align=\"aligncenter\" width=\"699\"]The Quants ensemble.[/caption]\\r\\n\\r\\n\\r\\nThanks\\nThanks to the competitors for the challenge, Kaggle for hosting, Avito  for organizing. Thanks to the open source community and the research that makes it all possible.\\nTeamwork\\nHow did your team form?\\nEarly on into the competition Peter & Sonny & Mikel formed a team as they held the top 3 spots at the time, and  decided to join forces to see how far they could go. Later on, Marios was spotted lurking at the bottom of the leaderboard, and was asked to join because of his extensive Kaggle experience.\\r\\n\\r\\nHow did your team work together?\\nWe were all quite independent, branching out and each working on our own features as there was lots of ground to cover, while also brainstorming and discussing ideas together. At the end we came together to consolidate everything into one featurespace and to build models for it. \\nBios\\n\\n\\nMikel Bober-Irizar  (anokas) is a young and ambitious Data Scientist and Machine Learning Enthusiast. He has been participating in various predictive modelling competitions, and has also developed algorithms for various problems, including financial prediction and medical diagnosis. Mikel is currently finishing his studies at Royal Grammar School, Guildford, UK, and plans to go on to study Math or Computer Science.\\r\\n\\r\\n\\n\\n\\n\\nPriv.-Doz. Dr. Peter Borrmann (NoName) is head of The Quants Consulting focusing on quantitative modelling and strategy. Peter studied in Göttingen, Oldenburg and Bremen and has a Ph.D. in theoretical physics. He habilitated at the University of Oldenburg  where he worked six years as a research assistant professor. Before starting his own company Peter worked at IBM Business Consulting Services in different roles.\\r\\n\\n\\n\\nMarios Michailidis (KazAnova) is Manager of Data Science at Dunnhumby and part-time PhD in machine learning at University College London (UCL) with a focus on improving recommender systems. He has worked in both marketing and credit sectors in the UK Market and has led many analytics projects with various themes including: Acquisition, Retention, Uplift, fraud detection, portfolio optimization and more. In his spare time he has created KazAnova, a GUI for credit scoring 100% made in Java. He is former  Kaggle #1.\\r\\n\\n\\n\\nSonny Laskar (Sonny Laskar) is an Analytics Consulting Manager at Microland (India) where he is building IT Operations Analytics platform. He has over eight years of experience spread across IT Infrastructure, Cloud and Machine learning. He holds an MBA from India\\'s premiere B School IIM, Indore. He is an avid break dancer and loves solving logic puzzles.\\r\\n', 'Can you put order to space and time? This was the challenge posed to competitors of the Draper Satellite Image Chronology Competition (Chronos) which ran on Kaggle from April to June 2016. Over four-hundred Kagglers chose a path somewhere between man and machine to accurately determine the chronological order of satellite images taken over five day spans.\\r\\n\\r\\nIn collaboration with Kaggle, Draper designed the competition to stimulate the development of novel approaches to analyzing satellite imagery and other image-based datasets. In this spirit, competitors’ solutions were permitted to rely on hand annotations as long as their methodologies were replicable. And indeed the top-three winners used non-ML approaches.\\r\\n\\r\\nIn this interview, Vicens Gaitan, a Competitions Master, describes how re-assembling the arrow of time was an irresistible challenge given his background in high energy physics. Having chosen the machine learning only path, Vicens spent about half of his time on image processing, specifically registration, and his remaining efforts went into building his XGBoost model which landed him well within the top 10%.\\r\\n\\r\\nThe basics\\nWhat was your background prior to entering this challenge?\\r\\n\\r\\nI’d like to define myself as a high-energy physicist, but this was in the 90’s ... ooh, I’m talking about the past century! At that time we used neural networks to classify events coming from the detectors, and NN were cool. Currently I’m managing a team of data scientists in a software engineering firm, and working actively on machine learning projects. Now NN are again cool ☺\\r\\n\\r\\nDo you have any prior experience or domain knowledge that helped you succeed in this competition?\\r\\n\\r\\nIn fact, no. It was a very good opportunity to learn about image processing.\\r\\n\\r\\nHow did you get started competing on Kaggle?\\r\\n\\r\\nI discovered Kaggle through the Higgs Boson challenge. Immediately I realized that competing on Kaggle is the most efficient way to follow the state-of-the art in machine learning, and compare our own methodologies against the best practitioners in the world.\\r\\n\\r\\nWhat made you decide to enter this competition?\\r\\n\\r\\nThe “time arrow” is still an open problem in science.  It was interesting to see if a machine learning approach could say something interesting about it.  Moreover, I have always been fascinated by satellite imagery. I could not resist the temptation to enter the challenge!\\r\\n\\r\\nLet\\'s get technical\\nWhat preprocessing and supervised learning methods did you use?\\r\\n\\r\\nFrom the start, I realize that the first thing to be done was to “register” the images: scale and rotate the images in order to match them, in the same way we do when building photo panoramas.\\r\\n\\r\\nBecause there was no available image registration package for R, I decide to build the process from scratch. And this was a lucky decision, because during this process I generated some features that were very useful for the learning task.\\r\\nThe registration process is described in detail in this notebook:\\r\\n\\r\\n\\r\\nThe main idea in the “registration” process is to find “interesting points” that can be identified across images. Those will be the so called keypoints.  A simple way to define keypoints is to look for “corners”, because a corner will continue to be a corner (approximately) if you rotate and scale the images.  \\r\\n\\r\\n[caption id=\"attachment_6162\" align=\"aligncenter\" width=\"857\"] Left: Image gradient square blurred. Right: Keypoints (local maxima from left image).[/caption]\\r\\n\\r\\nOnce you have the key-points, it is necessary to describe them in an economical way using few numbers, to allow finding the matches efficiently.  30x30 rotated patches around the key-points, subsampled to 9x9 seem to work well for the granularity of these images.\\r\\n\\r\\n[caption id=\"attachment_6161\" align=\"aligncenter\" width=\"671\"] Example of keypoint descriptors.[/caption]\\r\\n\\r\\nTo do the match, k-nearest neighbors of descriptors comes into play and the magic of RANSAC to select the true matches over the noise.\\r\\n\\r\\n[caption id=\"attachment_6164\" align=\"aligncenter\" width=\"929\"] In red, all the key-points. In green, the ones matching between images.[/caption]\\r\\n\\r\\nOnce we have the matching points, it is easy to fit a homomorphic transformation between the images, and then compare them and look for temporal differences.\\r\\n\\r\\n\\r\\n\\r\\nAre you able to do that by hand? I’m not, so I take the machine learning path:  \\r\\n\\r\\nThe number of available classified images is low, so it seems more promising to build a model at keypoint level (we have one hundred per image) using the descriptor information, and then average them for each image.  Also it seems to be a good idea to predict temporal differences between keypoints, instead to predict directly de-position in the ordered sequence.\\r\\n\\r\\nLet’s describe the procedure:\\r\\n\\r\\nFeature engineering:  (this was the easiest one, because it was already done after the registration process)\\r\\n\\r\\na) Key point detection For each image: Detect keypoints. Generate descriptors: 30x30 (downsampled to 9x9) oriented patches for each keypoint\\r\\n\\r\\nb) Inter-set registering: For all sets, for every couple of image: Identify common points using RANSAC and fit a homomorphic transformation. Keep values of the transformation and patches for the identified inliers.  The parameters of the homomorphic transformation will result in informative feature when comparing couples of images.\\r\\n\\r\\nc) Extra set registering: Interestingly, the same idea can be used between images coming from different sets.  Sampling from the full database of descriptors, using knn, find candidates to neighbor sets, and then register every possible image from one set to every image to the second set. Look for “almost perfect” matching taking into account a combination of number of identified common keypoints and the rmse in the overlapping region as a matching value. This procedure automatically detects “neighbor sets”\\r\\n\\r\\nd) Set Clustering: using the previous matching information build a graph with sets as nodes and edges between neighbor sets with weight proportional to the matching value. It is relatively easy to select couple of images in neighbor sets with “perfect match” (High number of inliers, and very low rmse in the intersecting area) The connected component of this graph gives the set clusters without need to explicitly georeference the images by hand. This saves a lot of manual work.\\r\\n\\r\\n\\nPATCH LEVEL MODEL (Siamese gradient boosting)\\r\\n\\r\\nThe model is trained over pairs of common keypoints in images of the same set, trying to predict the number of days between one image and the other (with sign). The features used are:\\r\\n\\r\\nPatch im1 (9x9), Patch im2 (9x9), coefficients from the homomorphic transformation (h1..h9) , number of inliers between both images and rmse in the overlapping region\\r\\n\\r\\n\\r\\nThe model is trained with XGBoost to a reg:linear objective using 4-fold cross validation (assuring that images in the same cluster belongs to the same fold). Surprisingly, this model is able to discriminate the time arrow quite well.\\r\\n\\r\\n\\r\\n\\nIMAGE LEVEL MODEL\\r\\n\\r\\nAverage the contribution of all patches in an image. Patches with absolute value of  DeltaT below a certain threshold level (alpha) are discarded. If the Patch Level Model were “perfect” this would result in the average of difference in days from one image to the rest of images in the set, so the expected values for an ordered set will be -2.5,-1.5,0,1.5,2.5 respectively. In practice, the ordering is calculated by sorting this average contribution.\\r\\n\\r\\nAdditionally, the average is refined by adding the average of images from overlapping sets (using the graph previously defined) taking into account the number of inliers between both images and the rmse in the intersection of them, weighted with a parameter beta, and iterating until convergence.\\r\\n\\r\\nThe optimal alpha and beta are adjusted using public leader board feedback (the training set is not representative enough of the test set) and as expected, there was some overfitting and a dropping of five positions in the private leaderboard.\\r\\n\\r\\nThe local CV obtained is 0.83 +/-0.05.\\r\\n\\r\\nThe model scored 0.76786 in the public leaderboard and 0.69209 in the private. I’m convinced that with more training data, this methodology can get a score of.8x or even .9x.\\r\\n\\r\\nWhat was your most important insight into the data?\\r\\nProbably the fact that by comparing local keypoint descriptors it is possible to obtain some information about the deltaT between them. Not only the number of days, but also the sign. Of course, this is a very weak model, but ensembling for all keypoints in an image, and taking into account the deltaT obtained in the neighbor sets (selecting the image with “perfect match”).\\r\\n\\r\\nWere you surprised by any of your findings?\\r\\n\\r\\nInitially, I only used keypoints matched by RANSAC between couples of images. This implies that the descriptors are very similar. When I try to add all keypoints, the result doesn’t improve. This is telling us that the temporal information is coded in subtle variations of pixel intensities, and not big changes of macroscopic objects, like other presence or not of a car, for example.\\r\\n\\r\\nIn fact, the bigger surprise is that these descriptors are able to code some information about the time arrow\\r\\n\\r\\nWhich tools did you use?\\r\\n\\r\\nI tried to develop the full pipeline in R. I used mainly the “imager” package for image processing and “xgboost” for the model. The “FNN” for fast k-nearest neighbors library was also very helpful because of its speed.\\r\\n\\r\\nHow did you spend your time on this competition? (For example: What proportion on feature engineering vs. machine learning?)\\r\\n\\r\\nHalf of the competition was devoted to developing the registration process.  After that, building the model was relatively fast.\\r\\n\\r\\nWhat was the run time for both training and prediction of your winning solution? \\r\\n\\r\\nThe keypoint extraction and descriptor calculation takes about 2 hours on a 12 core machine for the full dataset. Registration (especially the extra-set one) is more CPU consuming and lasts for more than 8 hours for the full dataset.\\r\\n\\r\\nThen, model training can be done in few hours (I’m doing 4-fold cross validation for selecting the hyperparameters).\\r\\n\\r\\nWords of wisdom\\nThe future of satellite imagery and technology will allow us to revisit locations with unprecedented frequency. Now that you\\'ve participated in this competition, what broader applications do you think there could be for images such as these?\\r\\n\\r\\nAfter the surprising fact that it is possible to correlate time arrow with image information, why not to try to predict changes in other indicators (with changes in a time interval of several days) that can be weakly correlated with the images: \\r\\n\\r\\nPredict risk of failure of infrastructures, demographics, social behavior, level of wealth, happiness ... I’m just dreaming.\\r\\n\\r\\nWhat have you taken away from this competition?\\r\\nKnowledge in a previously unknown area. The fact that XGBoost can be useful for image challenges (you doubt it).  And the most important, a lot of fun.\\r\\n\\r\\nDo you have any advice for those just getting started in data science?\\r\\nThe only way to learn is to try by yourself.  Don’t be afraid of problems in which you don’t have background.  There is a lot of very interesting resources out there (forums, code... ), but the only way is hands in.\\r\\n\\r\\nJust for fun\\nIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers?\\r\\n\\r\\nI think that the most interesting open problem is, given a dataset and a learner, try to predict the optimal set of hyperparameters for that combination of dataset-model.  I know, the problem is how to build a labeled set for this supervised problem...  Maybe reinforcement learning can be a way to do that, but wait a moment... what should be the optimal hyperparameters for the policy learner?...  It seems I’m entering in a kind of Goedelian loop...\\r\\nBetter forget it ;)  Why not to try to predict TV audiences based on time-stamp and EPG information?\\r\\n\\r\\nWhat is your dream job?\\r\\n\\r\\nI already have it ;)\\r\\n\\r\\nBio\\nVicens Gaitan is R&D director in the Grupo AIA innovation area. He studied physics and received a PhD in Machine Learning applied to experimental High Energy Physics in 1993 with the ALEPH collaboration at CERN. Since 1992 he has worked at AIA in complex problem solving and algorithmic development applied to model estimation, simulation, forecasting and optimization, mainly in the energy, banking, telecommunication and retail sectors for big companies. He has experienced knowledge in advanced methodologies including machine learning, numerical calculations, game theory and graph analysis, using lab environments like Python or R, or in production code in C and Java.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nLEMMATIZER\\n#Cool, stop words removed, now onto lemmatization\\nwordnet_lemmatizer = WordNetLemmatizer()\\n\\nlemm_sent= []\\ntemp_sent=[]\\nfor line in filtered_sent:\\n    for word in line:\\n        temp_sent.append(wordnet_lemmatizer.lemmatize(word))\\n    lemm_sent.append(tuple(temp_sent))\\n    temp_sent=[]\\n\\n#print (lemm_sent)\\n\\nflat_list = [item for sublist in lemm_sent for item in sublist]\\n#print (flat_list)\\n\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = (set(stopwords.words('english')))\n",
    "\n",
    "#Union operation on sets\n",
    "stop_words = stop_words | {\",\",\".\",\";\",\"(\",\")\",\"'\",\"?\",\"...\",'-','=',':=','+','/',':'}\n",
    "\n",
    "\n",
    "\n",
    "sentence_array = []\n",
    "preword_array = []\n",
    "\n",
    "#Read the cleantext contents\n",
    "\n",
    "for line in cleantext:\n",
    "    sentence_array.append(line)\n",
    "    for word in line.split():\n",
    "        preword_array.append(word)\n",
    "\n",
    "\n",
    "#print (preword_array)\n",
    "#Clean sentence up so that new sentence contains only strings as a list of cleaned-up sentences.\n",
    "newsentence_array = []\n",
    "for line in sentence_array:\n",
    "    if line == '':\n",
    "        continue\n",
    "    newsentence_array.append(line)\n",
    "#punc = string.punctuation\n",
    "\n",
    "#preword_array = word_tokenize(preword_array)\n",
    "\n",
    "setword_array = []\n",
    "\n",
    "print(newsentence_array)\n",
    "\n",
    "filtered_sent=[]\n",
    "xsent = []\n",
    "for line in newsentence_array:\n",
    "    word_tokens =  word_tokenize(line)\n",
    "    xsent = [w.lower() for w in word_tokens if not w in stop_words ]\n",
    "    filtered_sent.append(xsent)\n",
    "\n",
    "#print (filtered_sent)\n",
    "\n",
    "'''\n",
    "\n",
    "LEMMATIZER\n",
    "#Cool, stop words removed, now onto lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemm_sent= []\n",
    "temp_sent=[]\n",
    "for line in filtered_sent:\n",
    "    for word in line:\n",
    "        temp_sent.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    lemm_sent.append(tuple(temp_sent))\n",
    "    temp_sent=[]\n",
    "\n",
    "#print (lemm_sent)\n",
    "\n",
    "flat_list = [item for sublist in lemm_sent for item in sublist]\n",
    "#print (flat_list)\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS Tagging\n",
    "tagged_docs=[]\n",
    "\n",
    "for doc in newsentence_array:\n",
    "    tagged_docs.append(nltk.pos_tag(word_tokenize(doc)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('approach', 'NN'), ('was', 'VBD'), ('actually', 'RB'), ('quite', 'RB'), ('simple', 'JJ'), ('.', '.'), ('The', 'DT'), ('only', 'JJ'), ('attributes', 'VBZ'), ('I', 'PRP'), ('used', 'VBD'), ('where', 'WRB'), ('the', 'DT'), ('approximate', 'JJ'), ('betting', 'NN'), ('odds', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('information', 'NN'), ('on', 'IN'), ('past', 'JJ'), ('voting', 'NN'), ('.', '.'), ('I', 'PRP'), ('sought', 'VBD'), ('patterns', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('voting', 'VBG'), ('behaviour', 'NN'), ('of', 'IN'), ('all', 'DT'), ('countries', 'NNS'), ('and', 'CC'), ('combined', 'VBD'), ('that', 'IN'), ('knowledge', 'NN'), ('with', 'IN'), ('this', 'DT'), ('year', 'NN'), (\"'s\", 'POS'), ('betting', 'NN'), ('odds', 'NNS'), ('.', '.'), ('I', 'PRP'), ('used', 'VBD'), ('cross-validation', 'NN'), ('to', 'TO'), ('select', 'VB'), ('my', 'PRP$'), ('model', 'NN'), ('and', 'CC'), ('to', 'TO'), ('avoid', 'VB'), ('overfitting', 'VBG'), ('it', 'PRP'), ('.', '.'), ('Predicting', 'VBG'), ('the', 'DT'), ('finalists', 'NNS'), ('I', 'PRP'), ('trusted', 'VBD'), ('the', 'DT'), ('bookmakers', 'NNS'), ('on', 'IN'), ('this', 'DT'), ('one', 'CD'), ('and', 'CC'), ('just', 'RB'), ('took', 'VBD'), ('the', 'DT'), ('top', 'JJ'), ('ten', 'NN'), ('countries', 'NNS'), ('from', 'IN'), ('each', 'DT'), ('semi-final', 'JJ'), ('group', 'NN'), ('.', '.'), ('I', 'PRP'), ('got', 'VBD'), ('the', 'DT'), ('betting', 'VBG'), ('odds', 'NNS'), ('from', 'IN'), ('Betfair', 'NNP'), ('.', '.'), ('Learning', 'VBG'), ('the', 'DT'), ('voting', 'NN'), ('patterns', 'VBZ'), ('A', 'DT'), ('simple', 'JJ'), ('approach', 'NN'), ('worked', 'VBD'), ('well', 'RB'), ('enough', 'RB'), ('here', 'RB'), ('.', '.'), ('The', 'DT'), ('idea', 'NN'), ('was', 'VBD'), ('to', 'TO'), ('calculate', 'VB'), (',', ','), ('for', 'IN'), ('each', 'DT'), ('country', 'NN'), (',', ','), ('the', 'DT'), ('average', 'JJ'), ('points', 'NNS'), ('awarded', 'VBD'), ('to', 'TO'), ('each', 'DT'), ('other', 'JJ'), ('country', 'NN'), ('.', '.'), ('Coming', 'VBG'), ('from', 'IN'), ('Slovenia', 'NNP'), ('which', 'WDT'), ('was', 'VBD'), ('once', 'RB'), ('part', 'NN'), ('of', 'IN'), ('Yugoslavia', 'NNP'), (',', ','), ('together', 'RB'), ('with', 'IN'), ('Croatia', 'NNP'), (',', ','), ('Serbia', 'NNP'), (',', ','), ('Bosnia', 'NNP'), ('and', 'CC'), ('Herzegovina', 'NNP'), (',', ','), ('Macedonia', 'NNP'), ('and', 'CC'), ('Montenegro', 'NNP'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('perhaps', 'RB'), ('not', 'RB'), ('surprising', 'JJ'), ('that', 'IN'), ('our', 'PRP$'), ('voting', 'NN'), ('patterns', 'NNS'), ('are', 'VBP'), ('rather', 'RB'), ('interesting', 'JJ'), (':', ':'), ('AVG', 'NNP'), (\"'\", 'POS'), ('COUNTRY', 'NNP'), ('10.38', 'CD'), ('Serbia', 'NNP'), ('8.53', 'CD'), ('Croatia', 'NNP'), ('8.00', 'CD'), ('Bosnia', 'NNP'), ('and', 'CC'), ('Herzegovina', 'NNP'), ('5.91', 'CD'), ('Macedonia', 'NNP'), ('3.21', 'CD'), ('Norway', 'NNP'), ('3.17', 'CD'), ('Russia', 'NNP'), ('3.07', 'CD'), ('Greece', 'NNP'), ('...', ':'), ('0.18', 'CD'), ('Portugal', 'NNP'), ('0.17', 'CD'), ('Belarus', 'NNP'), ('It', 'PRP'), ('is', 'VBZ'), ('painfully', 'RB'), ('obvious', 'JJ'), ('that', 'IN'), ('Slovenia', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('judging', 'VBG'), ('the', 'DT'), ('quality', 'NN'), ('of', 'IN'), ('the', 'DT'), ('artist', 'NN'), ('alone', 'RB'), ('and', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('well', 'RB'), ('known', 'VBN'), ('that', 'IN'), ('other', 'JJ'), ('countries', 'NNS'), ('follow', 'VBP'), ('similar', 'JJ'), ('patterns', 'NNS'), ('.', '.'), ('It', 'PRP'), ('would', 'MD'), (',', ','), ('therefore', 'RB'), (',', ','), ('seem', 'VBP'), ('like', 'IN'), ('a', 'DT'), ('good', 'JJ'), ('idea', 'NN'), ('to', 'TO'), ('use', 'VB'), ('this', 'DT'), ('knowledge', 'NN'), ('in', 'IN'), ('predicting', 'VBG'), ('this', 'DT'), ('year', 'NN'), (\"'s\", 'POS'), ('voting', 'NN'), ('.', '.'), ('The', 'DT'), ('estimated', 'JJ'), ('average', 'JJ'), ('points', 'NNS'), ('awarded', 'VBN'), ('are', 'VBP'), ('not', 'RB'), ('very', 'RB'), ('stable', 'JJ'), (',', ','), ('especially', 'RB'), ('for', 'IN'), ('newer', 'JJR'), ('countries', 'NNS'), ('.', '.'), ('To', 'TO'), ('remedy', 'VB'), ('this', 'DT'), (',', ','), ('instead', 'RB'), ('of', 'IN'), ('using', 'VBG'), (':', ':'), ('avg', 'NN'), (':', ':'), ('=', 'NN'), ('sum', 'NN'), ('(', '('), ('x', 'NNP'), (')', ')'), ('/', 'VBP'), ('|x|', 'NN'), ('I', 'PRP'), ('used', 'VBD'), ('avg', 'NN'), (\"'\", \"''\"), (':', ':'), ('=', 'NN'), ('(', '('), ('sum', 'NN'), ('(', '('), ('x', 'NNP'), (')', ')'), ('+', 'VBP'), ('1', 'CD'), (')', ')'), ('/', 'NN'), ('(', '('), ('|x|', 'JJ'), ('+', 'NNP'), ('1', 'CD'), (')', ')'), ('The', 'DT'), ('new', 'JJ'), ('estimate', 'NN'), ('got', 'VBD'), ('better', 'JJR'), ('results', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('cross-validation', 'NN'), ('tests', 'NNS'), ('.', '.'), ('Betting', 'VBG'), ('Odds', 'NNP'), ('Using', 'NNP'), ('just', 'RB'), ('the', 'DT'), ('voting', 'NN'), ('patterns', 'NNS'), ('of', 'IN'), ('countries', 'NNS'), ('to', 'TO'), ('predict', 'VB'), ('this', 'DT'), ('year', 'NN'), (\"'s\", 'POS'), ('results', 'NNS'), ('was', 'VBD'), ('not', 'RB'), ('enough', 'RB'), ('.', '.'), ('I', 'PRP'), ('had', 'VBD'), ('to', 'TO'), (',', ','), ('somehow', 'RB'), (',', ','), ('incorporate', 'VB'), ('the', 'DT'), ('approximate', 'JJ'), ('betting', 'NN'), ('odds', 'NNS'), ('as', 'RB'), ('well', 'RB'), ('.', '.'), ('Many', 'JJ'), ('approaches', 'NNS'), ('could', 'MD'), ('have', 'VB'), ('worked', 'VBN'), ('well', 'RB'), ('here', 'RB'), ('.', '.'), ('In', 'IN'), ('the', 'DT'), ('end', 'NN'), ('I', 'PRP'), ('opted', 'VBD'), ('for', 'IN'), ('the', 'DT'), ('one', 'NN'), ('that', 'WDT'), ('gave', 'VBD'), ('the', 'DT'), ('best', 'JJS'), ('cross-validation', 'NN'), ('results', 'NNS'), ('.', '.'), ('I', 'PRP'), ('had', 'VBD'), ('to', 'TO'), ('convert', 'VB'), ('the', 'DT'), ('approximate', 'JJ'), ('betting', 'NN'), ('odds', 'NNS'), ('into', 'IN'), ('something', 'NN'), ('comparable', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('average', 'JJ'), ('points', 'NNS'), ('awarded', 'VBD'), ('.', '.'), ('I', 'PRP'), ('used', 'VBD'), (':', ':'), ('odds', 'NNS'), (\"'\", 'POS'), ('(', '('), ('ctr', 'NN'), (')', ')'), (':', ':'), ('=', '$'), ('1', 'CD'), ('/', 'NNP'), ('log', 'NN'), ('(', '('), ('odds', 'NNS'), ('(', '('), ('ctr', 'NN'), (')', ')'), (')', ')'), ('*', 'VBP'), ('a', 'DT'), ('+', 'NN'), ('b', 'IN'), ('The', 'DT'), ('coefficients', 'NNS'), ('a', 'DT'), ('and', 'CC'), ('b', 'NN'), ('were', 'VBD'), ('chosen', 'VBN'), ('experimentally', 'RB'), (',', ','), ('as', 'IN'), ('the', 'DT'), ('ones', 'NNS'), ('that', 'WDT'), ('gave', 'VBD'), ('the', 'DT'), ('best', 'JJS'), ('cross-validation', 'NN'), ('score', 'NN'), ('.', '.'), ('A', 'DT'), ('small', 'JJ'), ('example', 'NN'), ('will', 'MD'), ('elucidate', 'VB'), ('how', 'WRB'), ('I', 'PRP'), ('calculated', 'VBD'), ('the', 'DT'), ('converted', 'JJ'), ('betting', 'NN'), ('odds', 'NNS'), ('.', '.'), ('odds', 'NNS'), (\"'\", 'POS'), ('(', '('), ('Croatia', 'NNP'), (')', ')'), ('=', 'VBD'), ('1', 'CD'), ('/', 'JJ'), ('log', 'NN'), ('(', '('), ('odds', 'NNS'), ('(', '('), ('Croatia', 'NNP'), (')', ')'), (')', ')'), ('*', '$'), ('4.4', 'CD'), ('+', '$'), ('0.8', 'CD'), ('=', 'NNP'), ('=', 'VBD'), ('1', 'CD'), ('/', 'JJ'), ('log', 'NN'), ('(', '('), ('48', 'CD'), (')', ')'), ('*', 'VBD'), ('4.4', 'CD'), ('+', 'JJ'), ('0.8', 'CD'), ('=', '$'), ('1.94', 'CD'), ('The', 'DT'), ('converted', 'VBN'), ('betting', 'NN'), ('odds', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('top', 'JJ'), ('and', 'CC'), ('bottom', 'JJ'), ('countries', 'NNS'), ('were', 'VBD'), (':', ':'), ('ODDS', 'NNP'), (\"'\", 'POS'), ('COUNTRY', 'NNP'), ('5.23', 'CD'), ('Azerbaijan', 'NNP'), ('3.21', 'CD'), ('Germany', 'NNP'), ('2.54', 'CD'), ('Armenia', 'NNP'), ('...', ':'), ('1.94', 'CD'), ('Croatia', 'NNP'), ('...', ':'), ('1.45', 'CD'), ('Slovenia', 'NNP'), ('1.44', 'CD'), ('Bulgaria', 'NNP'), ('1.44', 'CD'), ('Macedonia', 'NNP'), ('1.44', 'CD'), ('Switzerland', 'NNP'), ('Combining', 'VBG'), ('the', 'DT'), ('voting', 'NN'), ('patterns', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('betting', 'VBG'), ('odds', 'NNS'), ('It', 'PRP'), ('was', 'VBD'), ('now', 'RB'), ('time', 'NN'), ('to', 'TO'), ('bring', 'VB'), ('everything', 'NN'), ('together', 'RB'), ('.', '.'), ('This', 'DT'), ('was', 'VBD'), ('simply', 'RB'), ('a', 'DT'), ('matter', 'NN'), ('of', 'IN'), ('summing', 'VBG'), ('the', 'DT'), ('average', 'JJ'), ('points', 'NNS'), ('awarded', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('converted', 'JJ'), ('betting', 'NN'), ('odds', 'NNS'), ('.', '.'), ('This', 'DT'), ('was', 'VBD'), ('how', 'WRB'), ('I', 'PRP'), ('predicted', 'VBD'), ('Slovenia', 'NNP'), (\"'s\", 'POS'), ('votes', 'NNS'), ('for', 'IN'), ('this', 'DT'), ('year', 'NN'), (':', ':'), ('COUNTRY', 'NNP'), ('AVG', 'NNP'), (\"'\", 'POS'), ('ODDS', 'NNP'), (\"'\", 'POS'), ('SUM', 'NNP'), ('POINTS', 'NNP'), ('Serbia', 'NNP'), ('10.38', 'CD'), ('+', 'VBD'), ('1.84', 'CD'), ('=', 'JJ'), ('12.21', 'CD'), ('12', 'CD'), ('Croatia', 'NNP'), ('8.53', 'CD'), ('+', 'VBD'), ('1.94', 'CD'), ('=', 'JJ'), ('10.47', 'CD'), ('10', 'CD'), ('Bosnia', 'NNP'), ('and', 'CC'), ('Herzegovina', 'NNP'), ('8.00', 'CD'), ('+', 'VBD'), ('1.49', 'CD'), ('=', 'JJ'), ('9.49', 'CD'), ('8', 'CD'), ('Macedonia', 'NNP'), ('5.91', 'CD'), ('+', 'VBD'), ('1.44', 'CD'), ('=', 'JJ'), ('7.35', 'CD'), ('7', 'CD'), ('Azerbaijan', 'NNP'), ('1.80', 'CD'), ('+', 'VBD'), ('5.23', 'CD'), ('=', 'JJ'), ('7.03', 'CD'), ('6', 'CD'), ('Norway', 'RB'), ('3.21', 'CD'), ('+', 'JJ'), ('2.01', 'CD'), ('=', 'JJ'), ('5.22', 'CD'), ('5', 'CD'), ('Greece', 'NNP'), ('3.07', 'CD'), ('+', 'VBD'), ('1.96', 'CD'), ('=', 'JJ'), ('5.03', 'CD'), ('4', 'CD'), ('Sweden', 'NNP'), ('2.85', 'CD'), ('+', 'VBD'), ('2.18', 'CD'), ('=', 'JJ'), ('5.03', 'CD'), ('3', 'CD'), ('Russia', 'NNP'), ('3.17', 'CD'), ('+', 'VBD'), ('1.62', 'CD'), ('=', 'JJ'), ('4.79', 'CD'), ('2', 'CD'), ('Germany', 'NNP'), ('1.50', 'CD'), ('+', 'NNP'), ('3.21', 'CD'), ('=', 'VBD'), ('4.71', 'CD'), ('1', 'CD'), ('Denmark', 'NNP'), ('2.42', 'CD'), ('+', 'VBD'), ('2.25', 'CD'), ('=', 'JJ'), ('4.66', 'CD'), ('0', 'CD'), ('...', ':'), ('We', 'PRP'), ('saw', 'VBD'), ('earlier', 'RBR'), ('that', 'IN'), ('Slovenia', 'NNP'), (\"'s\", 'POS'), ('votes', 'NNS'), ('have', 'VBP'), ('little', 'JJ'), ('to', 'TO'), ('do', 'VB'), ('with', 'IN'), ('song', 'NN'), ('quality', 'NN'), (',', ','), ('as', 'IN'), ('we', 'PRP'), ('usually', 'RB'), ('award', 'VBP'), ('the', 'DT'), ('top', 'JJ'), ('points', 'NNS'), ('to', 'TO'), ('Balkan', 'NNP'), ('countries', 'NNS'), (',', ','), ('no', 'DT'), ('matter', 'NN'), ('how', 'WRB'), ('bad', 'JJ'), ('they', 'PRP'), ('sing', 'VBG'), ('.', '.'), ('The', 'DT'), ('added', 'JJ'), ('betting', 'NN'), ('odds', 'NNS'), ('should', 'MD'), ('not', 'RB'), ('influence', 'VB'), ('the', 'DT'), ('prediction', 'NN'), ('of', 'IN'), ('such', 'JJ'), ('countries', 'NNS'), ('considerably', 'RB'), ('.', '.'), ('On', 'IN'), ('the', 'DT'), ('other', 'JJ'), ('hand', 'NN'), (',', ','), ('if', 'IN'), ('we', 'PRP'), ('take', 'VBP'), ('a', 'DT'), ('country', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('perhaps', 'RB'), ('a', 'DT'), ('bit', 'NN'), ('more', 'RBR'), ('fair', 'JJ'), (',', ','), ('like', 'IN'), ('Israel', 'NNP'), (',', ','), ('we', 'PRP'), ('see', 'VBP'), ('that', 'IN'), ('the', 'DT'), ('final', 'JJ'), ('predictions', 'NNS'), ('are', 'VBP'), ('affected', 'VBN'), ('to', 'TO'), ('a', 'DT'), ('greater', 'JJR'), ('extent', 'NN'), (':', ':'), ('COUNTRY', 'NNP'), ('AVG', 'NNP'), (\"'\", 'POS'), ('ODDS', 'NNP'), (\"'\", 'POS'), ('SUM', 'NNP'), ('POINTS', 'NNP'), ('Armenia', 'NNP'), ('7.50', 'CD'), ('+', 'VBD'), ('2.54', 'CD'), ('=', 'JJ'), ('10.04', 'CD'), ('12', 'CD'), ('Azerbaijan', 'NNP'), ('3.75', 'CD'), ('+', 'VBD'), ('5.23', 'CD'), ('=', 'JJ'), ('8.98', 'CD'), ('10', 'CD'), ('Russia', 'NNP'), ('7.23', 'CD'), ('+', 'VBD'), ('1.62', 'CD'), ('=', 'JJ'), ('8.85', 'CD'), ('8', 'CD'), ('Ukraine', 'NNP'), ('6.30', 'CD'), ('+', 'VBD'), ('1.54', 'CD'), ('=', 'JJ'), ('7.84', 'CD'), ('7', 'CD'), ('Romania', 'NNP'), ('6.07', 'CD'), ('+', 'VBD'), ('1.61', 'CD'), ('=', 'JJ'), ('7.68', 'CD'), ('6', 'CD'), ('Greece', 'NNP'), ('4.08', 'CD'), ('+', 'VBD'), ('1.96', 'CD'), ('=', 'JJ'), ('6.04', 'CD'), ('5', 'CD'), ('Georgia', 'NNP'), ('4.25', 'CD'), ('+', 'VBD'), ('1.77', 'CD'), ('=', 'JJ'), ('6.02', 'CD'), ('4', 'CD'), ('Iceland', 'NNP'), ('3.83', 'CD'), ('+', 'VBD'), ('1.72', 'CD'), ('=', 'JJ'), ('5.55', 'CD'), ('3', 'CD'), ('Serbia', 'NNP'), ('3.71', 'CD'), ('+', 'VBD'), ('1.84', 'CD'), ('=', 'JJ'), ('5.55', 'CD'), ('2', 'CD'), ('Denmark', 'NNP'), ('3.25', 'CD'), ('+', 'VBD'), ('2.25', 'CD'), ('=', 'JJ'), ('5.50', 'CD'), ('1', 'CD'), ('Sweden', 'NNP'), ('3.27', 'CD'), ('+', 'VBD'), ('2.18', 'CD'), ('=', 'JJ'), ('5.45', 'CD'), ('0', 'CD'), ('...', ':'), ('Cross', 'NNP'), ('validation', 'VBP'), ('The', 'DT'), ('most', 'RBS'), ('important', 'JJ'), ('component', 'NN'), ('of', 'IN'), ('my', 'PRP$'), ('solution', 'NN'), ('was', 'VBD'), ('cross-validation', 'NN'), ('and', 'CC'), ('was', 'VBD'), ('probably', 'RB'), ('the', 'DT'), ('reason', 'NN'), ('why', 'WRB'), ('I', 'PRP'), ('won', 'VBD'), ('the', 'DT'), ('competition', 'NN'), ('in', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('place', 'NN'), ('.', '.'), ('It', 'PRP'), ('enabled', 'VBD'), ('me', 'PRP'), ('to', 'TO'), ('try', 'VB'), ('many', 'JJ'), ('different', 'JJ'), ('models', 'NNS'), ('and', 'CC'), (',', ','), ('between', 'IN'), ('them', 'PRP'), (',', ','), ('choose', 'VB'), ('the', 'DT'), ('one', 'NN'), ('that', 'WDT'), ('was', 'VBD'), ('most', 'RBS'), ('likely', 'JJ'), ('to', 'TO'), ('give', 'VB'), ('the', 'DT'), ('best', 'JJS'), ('results', 'NNS'), ('.', '.'), ('The', 'DT'), ('dataset', 'NN'), ('was', 'VBD'), ('split', 'VBN'), ('into', 'IN'), ('partitions', 'NNS'), (',', ','), ('one', 'CD'), ('for', 'IN'), ('each', 'DT'), ('Eurovision', 'NNP'), ('event', 'NN'), ('.', '.'), ('I', 'PRP'), ('then', 'RB'), ('proceeded', 'VBD'), ('to', 'TO'), ('build', 'VB'), ('the', 'DT'), ('model', 'NN'), ('on', 'IN'), ('all', 'DT'), ('but', 'CC'), ('one', 'CD'), ('partition', 'NN'), ('and', 'CC'), ('calculated', 'VBD'), ('the', 'DT'), ('error', 'NN'), ('of', 'IN'), ('that', 'DT'), ('model', 'NN'), ('on', 'IN'), ('the', 'DT'), ('partition', 'NN'), ('that', 'WDT'), ('was', 'VBD'), ('left', 'VBN'), ('out', 'RP'), ('.', '.'), ('The', 'DT'), ('procedure', 'NN'), ('was', 'VBD'), ('repeated', 'VBN'), ('so', 'RB'), ('that', 'IN'), ('each', 'DT'), ('time', 'NN'), ('a', 'DT'), ('different', 'JJ'), ('partition', 'NN'), ('was', 'VBD'), ('left', 'VBN'), ('out', 'RP'), ('.', '.'), ('This', 'DT'), ('gave', 'VBD'), ('me', 'PRP'), ('a', 'DT'), ('fair', 'JJ'), ('estimate', 'NN'), ('of', 'IN'), ('how', 'WRB'), ('the', 'DT'), ('model', 'NN'), ('performs', 'NNS'), ('on', 'IN'), ('unseen', 'JJ'), ('data', 'NNS'), ('.', '.'), ('The', 'DT'), ('cross-validation', 'JJ'), ('procedure', 'NN'), ('in', 'IN'), ('pseudocode', 'NN'), (':', ':'), ('function', 'NN'), ('crossValidation', 'NN'), ('(', '('), ('dataset', 'NN'), (',', ','), ('buildModel', 'NN'), (')', ')'), (':', ':'), ('error', 'NN'), ('=', 'VBZ'), ('0', 'CD'), ('for', 'IN'), ('year', 'NN'), ('in', 'IN'), ('eurovisionEvents', 'NNS'), (':', ':'), ('learnData', 'NN'), ('=', 'NNP'), ('{', '('), ('example', 'NN'), ('|', 'NNP'), ('example', 'NN'), ('in', 'IN'), ('dataset', 'NN'), ('and', 'CC'), ('example.year', 'JJ'), ('!', '.'), ('=', 'JJ'), ('year', 'NN'), ('}', ')'), ('testData', 'NN'), ('=', 'NNP'), ('{', '('), ('example', 'NN'), ('|', 'NNP'), ('example', 'NN'), ('in', 'IN'), ('dataset', 'NN'), ('and', 'CC'), ('example.year', 'VB'), ('==', 'JJ'), ('year', 'NN'), ('}', ')'), ('model', 'NN'), ('=', 'JJ'), ('buildModel', 'NN'), ('(', '('), ('learnData', 'NN'), (')', ')'), ('error', 'NN'), ('+=', 'JJ'), ('testModel', 'NN'), ('(', '('), ('model', 'NN'), (',', ','), ('testData', 'NN'), (')', ')'), ('return', 'NN'), ('error', 'JJ'), ('Conclusion', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('well', 'RB'), ('aware', 'JJ'), ('that', 'IN'), ('certain', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('my', 'PRP$'), ('approach', 'NN'), ('are', 'VBP'), ('not', 'RB'), ('very', 'RB'), ('strong', 'JJ'), ('.', '.'), ('I', 'PRP'), ('had', 'VBD'), ('to', 'TO'), ('do', 'VB'), ('my', 'PRP$'), ('best', 'JJS'), ('with', 'IN'), ('the', 'DT'), ('time', 'NN'), ('that', 'WDT'), ('was', 'VBD'), ('available', 'JJ'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('many', 'JJ'), ('ideas', 'NNS'), ('for', 'IN'), ('next', 'JJ'), ('year', 'NN'), (',', ','), ('which', 'WDT'), ('I', 'PRP'), ('will', 'MD'), (',', ','), ('for', 'IN'), ('the', 'DT'), ('moment', 'NN'), ('at', 'IN'), ('least', 'JJS'), (',', ','), ('keep', 'VB'), ('to', 'TO'), ('myself', 'VB'), (':', ':'), (')', ')'), ('I', 'PRP'), ('really', 'RB'), ('enjoy', 'VB'), ('competing', 'VBG'), ('in', 'IN'), ('events', 'NNS'), ('like', 'IN'), ('this', 'DT'), ('and', 'CC'), ('hope', 'NN'), ('there', 'RB'), ('will', 'MD'), ('be', 'VB'), ('more', 'JJR'), ('to', 'TO'), ('come', 'VB'), ('in', 'IN'), ('the', 'DT'), ('future', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print (tagged_docs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
