{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "path = 'WinnersInterviewBlogPosts.csv'\n",
    "data = pd.read_csv(path, header=None, names=['Title', 'Link', 'Publication_Date', 'Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Publication_Date</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title</td>\n",
       "      <td>link</td>\n",
       "      <td>publication_date</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer scientist Jure Zbontar on winning the...</td>\n",
       "      <td>http://blog.kaggle.com/2010/06/09/computer-sci...</td>\n",
       "      <td>2010-06-09 18:22:29</td>\n",
       "      <td>My approach was actually quite simple. The onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How I won the  Predict HIV Progression data mi...</td>\n",
       "      <td>http://blog.kaggle.com/2010/08/09/how-i-won-th...</td>\n",
       "      <td>2010-08-09 12:35:46</td>\n",
       "      <td>&lt;strong&gt;Initial Strategy&lt;/strong&gt;\\r\\n\\r\\nThe g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How I did it: Lee Baker on winning Tourism For...</td>\n",
       "      <td>http://blog.kaggle.com/2010/09/27/how-i-did-it...</td>\n",
       "      <td>2010-09-27 18:30:25</td>\n",
       "      <td>&lt;strong&gt;About me:&lt;/strong&gt;\\r\\nI’m an embedded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How I did it: The top three from the 2010 INFO...</td>\n",
       "      <td>http://blog.kaggle.com/2010/10/11/how-i-did-it...</td>\n",
       "      <td>2010-10-11 13:31:35</td>\n",
       "      <td>The 2010 INFORMS Data Mining Contest has just ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                                              title   \n",
       "1  Computer scientist Jure Zbontar on winning the...   \n",
       "2  How I won the  Predict HIV Progression data mi...   \n",
       "3  How I did it: Lee Baker on winning Tourism For...   \n",
       "4  How I did it: The top three from the 2010 INFO...   \n",
       "\n",
       "                                                Link     Publication_Date  \\\n",
       "0                                               link     publication_date   \n",
       "1  http://blog.kaggle.com/2010/06/09/computer-sci...  2010-06-09 18:22:29   \n",
       "2  http://blog.kaggle.com/2010/08/09/how-i-won-th...  2010-08-09 12:35:46   \n",
       "3  http://blog.kaggle.com/2010/09/27/how-i-did-it...  2010-09-27 18:30:25   \n",
       "4  http://blog.kaggle.com/2010/10/11/how-i-did-it...  2010-10-11 13:31:35   \n",
       "\n",
       "                                             Content  \n",
       "0                                            content  \n",
       "1  My approach was actually quite simple. The onl...  \n",
       "2  <strong>Initial Strategy</strong>\\r\\n\\r\\nThe g...  \n",
       "3  <strong>About me:</strong>\\r\\nI’m an embedded ...  \n",
       "4  The 2010 INFORMS Data Mining Contest has just ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = data.iloc[:, 3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "content =content.drop(content.index[0])\n",
    "a= content.values[0]\n",
    "list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in a:\n",
    "    list.append(word)\n",
    "    \n",
    "    \n",
    "str = ''.join(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<strong>Initial Strategy</strong>\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n<ol>\\r\\n\\t<li>To make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.</li>\\r\\n\\t<li>To make my training set closely match the test data in order to have a population similar to the test set.<!--more--></li>\\r\\n</ol>\\r\\nI   identified certain areas of the dataset that didn\\'t appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n<a href=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions.png\"><img class=\"alignnone size-large wp-image-484\" style=\"border: 1px solid #CCC;\" title=\"Chris Raimondi Progress in Submissions\" src=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions-1024x741.png\" alt=\"\" width=\"633\" /></a>\\r\\n\\r\\n<strong>Matching Controls</strong>\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n<!--more-->\\r\\n\\r\\n<strong>Recursive Feature Elimination via R \\'caret\\' package</strong>\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R \\'caret\\' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the <a href=\"http://kaggle.com/chess\">Chess contest</a>.<strong>Initial Strategy</strong>\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n<ol>\\r\\n\\t<li>To make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.</li>\\r\\n\\t<li>To make my training set closely match the test data in order to have a population similar to the test set.<!--more--></li>\\r\\n</ol>\\r\\nI   identified certain areas of the dataset that didn\\'t appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n<a href=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions.png\"><img class=\"alignnone size-large wp-image-484\" style=\"border: 1px solid #CCC;\" title=\"Chris Raimondi Progress in Submissions\" src=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions-1024x741.png\" alt=\"\" width=\"633\" /></a>\\r\\n\\r\\n<strong>Matching Controls</strong>\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n<!--more-->\\r\\n\\r\\n<strong>Recursive Feature Elimination via R \\'caret\\' package</strong>\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R \\'caret\\' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the <a href=\"http://kaggle.com/chess\">Chess contest</a>.<strong>Initial Strategy</strong>\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n<ol>\\r\\n\\t<li>To make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.</li>\\r\\n\\t<li>To make my training set closely match the test data in order to have a population similar to the test set.<!--more--></li>\\r\\n</ol>\\r\\nI   identified certain areas of the dataset that didn\\'t appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n<a href=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions.png\"><img class=\"alignnone size-large wp-image-484\" style=\"border: 1px solid #CCC;\" title=\"Chris Raimondi Progress in Submissions\" src=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions-1024x741.png\" alt=\"\" width=\"633\" /></a>\\r\\n\\r\\n<strong>Matching Controls</strong>\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n<!--more-->\\r\\n\\r\\n<strong>Recursive Feature Elimination via R \\'caret\\' package</strong>\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R \\'caret\\' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the <a href=\"http://kaggle.com/chess\">Chess contest</a>.<strong>Initial Strategy</strong>\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n<ol>\\r\\n\\t<li>To make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.</li>\\r\\n\\t<li>To make my training set closely match the test data in order to have a population similar to the test set.<!--more--></li>\\r\\n</ol>\\r\\nI   identified certain areas of the dataset that didn\\'t appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n<a href=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions.png\"><img class=\"alignnone size-large wp-image-484\" style=\"border: 1px solid #CCC;\" title=\"Chris Raimondi Progress in Submissions\" src=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions-1024x741.png\" alt=\"\" width=\"633\" /></a>\\r\\n\\r\\n<strong>Matching Controls</strong>\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n<!--more-->\\r\\n\\r\\n<strong>Recursive Feature Elimination via R \\'caret\\' package</strong>\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R \\'caret\\' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the <a href=\"http://kaggle.com/chess\">Chess contest</a>.<strong>Initial Strategy</strong>\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n<ol>\\r\\n\\t<li>To make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.</li>\\r\\n\\t<li>To make my training set closely match the test data in order to have a population similar to the test set.<!--more--></li>\\r\\n</ol>\\r\\nI   identified certain areas of the dataset that didn\\'t appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n<a href=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions.png\"><img class=\"alignnone size-large wp-image-484\" style=\"border: 1px solid #CCC;\" title=\"Chris Raimondi Progress in Submissions\" src=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions-1024x741.png\" alt=\"\" width=\"633\" /></a>\\r\\n\\r\\n<strong>Matching Controls</strong>\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n<!--more-->\\r\\n\\r\\n<strong>Recursive Feature Elimination via R \\'caret\\' package</strong>\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R \\'caret\\' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the <a href=\"http://kaggle.com/chess\">Chess contest</a>.<strong>Initial Strategy</strong>\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n<ol>\\r\\n\\t<li>To make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.</li>\\r\\n\\t<li>To make my training set closely match the test data in order to have a population similar to the test set.<!--more--></li>\\r\\n</ol>\\r\\nI   identified certain areas of the dataset that didn\\'t appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n<a href=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions.png\"><img class=\"alignnone size-large wp-image-484\" style=\"border: 1px solid #CCC;\" title=\"Chris Raimondi Progress in Submissions\" src=\"http://kaggle.com/blog/wp-content/uploads/2010/08/progress-in-submissions-1024x741.png\" alt=\"\" width=\"633\" /></a>\\r\\n\\r\\n<strong>Matching Controls</strong>\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n<!--more-->\\r\\n\\r\\n<strong>Recursive Feature Elimination via R \\'caret\\' package</strong>\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R \\'caret\\' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the <a href=\"http://kaggle.com/chess\">Chess contest</a>.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'`#\\xa3\\x02\\x00\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantext = BeautifulSoup(str, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Initial Strategy\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n\\nTo make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.\\nTo make my training set closely match the test data in order to have a population similar to the test set.\\n\\r\\nI   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n\\nMatching Controls\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n\\nRecursive Feature Elimination via R 'caret' package\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R 'caret' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the Chess contest.Initial Strategy\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n\\nTo make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.\\nTo make my training set closely match the test data in order to have a population similar to the test set.\\n\\r\\nI   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n\\nMatching Controls\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n\\nRecursive Feature Elimination via R 'caret' package\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R 'caret' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the Chess contest.Initial Strategy\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n\\nTo make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.\\nTo make my training set closely match the test data in order to have a population similar to the test set.\\n\\r\\nI   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n\\nMatching Controls\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n\\nRecursive Feature Elimination via R 'caret' package\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R 'caret' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the Chess contest.Initial Strategy\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n\\nTo make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.\\nTo make my training set closely match the test data in order to have a population similar to the test set.\\n\\r\\nI   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n\\nMatching Controls\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n\\nRecursive Feature Elimination via R 'caret' package\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R 'caret' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the Chess contest.Initial Strategy\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n\\nTo make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.\\nTo make my training set closely match the test data in order to have a population similar to the test set.\\n\\r\\nI   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n\\nMatching Controls\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n\\nRecursive Feature Elimination via R 'caret' package\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R 'caret' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the Chess contest.Initial Strategy\\r\\n\\r\\nThe graph shows both my public and private scores (which were obtained after the contest).  As you can see from the graph, my initial attempts were not very successful.   The training data contained 206 responders and 794 non- responders.  The test data was known to contain 346 of each.  I tried two separate to segmenting my training dataset:\\r\\n\\nTo make my training set closely match the  overall  population (32.6 % Responders) in order to accurately reflect  the  entire dataset.\\nTo make my training set closely match the test data in order to have a population similar to the test set.\\n\\r\\nI   identified certain areas of the dataset that didn't appear to be   randomly partitioned. In order to do machine learning correctly, it is important to have your training data closely match the test dataset. I identified five   separate groups in the data which I began to treat separately.\\r\\n\\r\\nOriginally   I set up a different model for each group, but that became a pain and I   found better results by simply estimating the overall group response   and adjusting the predictions in each group to match the predicted group   mean response.\\r\\n\\r\\n\\nMatching Controls\\r\\n\\r\\nThe group I  had designated “Yellow” [Patients 353:903] did have an average response  of 32.9% (close to the 32.6% overall dataset). I used the matchControls  function from the e1071 package in “R” to pick the best matches in the  “Yellow” group against the “Red” group (the majority of what needed to  be predicted).\\r\\n\\r\\nThis allowed me to best match the features  VL.t0, CD4.t0, and rt184. These were the only three that at that time I  was confident were important, so I wanted to make sure they were  accurately represented.\\r\\n\\r\\nAfter a few more iterations  through match controls I was able to balance the “Yellow” data set to be  as close to the “Red” data set as possible – except for rt184. There  were further imbalances in the test data that were only resolved by  excluding the first 230 rows of the test data in some further  refinements.\\r\\n\\r\\n\\nRecursive Feature Elimination via R 'caret' package\\r\\n\\r\\nI  felt I had now balanced out the training set as best I could in order  to then try to find more features that would predict patient response.\\r\\n\\r\\nI  attended the “’R’ User Conference 2010” in late July and saw a  presentation by Max Kuhn on the ‘caret’ package. I was unaware of this  package and it had many functions that looked interesting – particularly  the rfe function for feature selection.\\r\\n\\r\\nThe rfe function  allowed me to quickly see what features were important. As each amino  acid was represented separately – I had over 600 features and this  obviously needed to be narrowed down.\\r\\n\\r\\nI ran this function countless times, but this is part of the actual output for my last submission:\\r\\n\\r\\nVariables Accuracy Kappa AccuracySD KappaSD Selected\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\n90 0.7233 0.3148 0.04884 0.1121\\r\\n\\r\\n120 0.7383 0.3493 0.05648 0.1393 *\\r\\n\\r\\n150 0.7276 0.3225 0.04698 0.1153\\r\\n\\r\\n[rows omitted]\\r\\n\\r\\nThe top 5 variables (out of 120):\\r\\n\\r\\nVL.t0, QIYQEPFKNLK, rt184, CD4.t0, rt215\\r\\n\\r\\nThe  last line shows you the five judged most important. The rfe function  has selected 120 variables as being optimum, but I went for a smaller  amount for various reasons. What was most impressive to me is that off  the five variables shown here – rt184 and rt215 are both listed. I  didn’t have time to do much research on the topic, but I had read  several papers that had all mentioned rt184 as being important and rt215  was probably the second or third most mentioned RT codon in the few  papers I read.\\r\\n\\r\\nTraining via R 'caret' and ‘randomForest’ packages\\r\\n\\r\\nI  trained my models and made my predictions using the randomForest  function both alone and with some tuning and validation enhancements  from the caret package using variables I had selected in the previous  step. I would highly recommend the caret package to anyone using “R” for  machine learning. I enjoyed this contest immensely and look forward to  some free time to work on the Chess contest.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleantext.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
